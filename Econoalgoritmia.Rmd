---
title: "Econoalgoritmia: Econometría avanzada y ciencia de datos con R"
cover-image: "images/cover.png"
author: "Jeshua Romero Guadarrama"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output: 
  bookdown::pdf_book:
    pandoc_args: ["+RTS", "-K64m", "-RTS", "--csl", "apa-old-doi-prefix.csl"]
    includes:
      in_header: preamble.tex
    citation_package: natbib
    keep_tex: yes
  bookdown::gitbook:
    config:
      toc:
        collapse: subsection
        scroll_highlight: yes
      fontsettings:
        theme: white
        family: serif
        size: 2
    split_by: section+number
    highlight: tango
    includes:
      in_header: [header_include.html]
always_allow_html: yes
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
biblatexoptions:
  - sortcites
link-citations: yes
github-repo: "https://github.com/Jeshua-Romero-Guadarrama/Econoalgoritmia"
description: "Los estudiantes con poca experiencia en estadística y econometría a menudo tienen dificultades para entender los beneficios de desarrollar habilidades de programación al momento de aplicar diversos métodos econométricos. 'Econoalgoritmia: Econometría avanzada y ciencia de datos con R' por Jeshua Romero Guadarrama (2021), ofrece una introducción interactiva a los aspectos esenciales de la programación por medio del lenguaje y software estadístico R, así como una guía para la aplicación de la teoría económica y econométrica en entornos específicos. En otras palabras, el objetivo es que los estudiantes se adentren al mundo de la economía aplicada mediante ejemplos empíricos presentados en la vida diaria y haciendo uso de las habilidades de programación recién adquiridas. Dicho objetivo se encuentra respaldado por ejercicios de programación interactivos generados con DataCamp Light y la incorporación de visualizaciones dinámicas de conceptos fundamentales mediante la flexibilidad de JavaScript, a través de la biblioteca D3.js."
url: 'https://jeshua-romero-guadarrama.github.io/Econoalgoritmia/'
tags: [Academia, Modelos lineales, Econoalgoritmia, Econometría avanzada, Análisis causal, Programación R]
---
# Prefacio {-}

```{r, 1, echo = F}
options(knitr.duplicate.label = "allow")
```

```{r, 2, child="_setup.Rmd"}
```

```{r, 3, eval=my_output == "html", echo=FALSE, results='asis'}
cat('<hr style="background-color:#03193b;height:2px">')
```

<center><img style = 'width:60%;' src='images/Econoalgoritmia.png'></center>

```{r, 4, eval=my_output == "html", echo=FALSE, results='asis'}
cat('<hr style="background-color:#03193b;height:2px">')
```

<center><img style = 'width:30%;' src='images/cover.png'></center>
<br><center><img style='float: center; width:50%' src='images/logo_claim_en_rgb.png'/></center>
<br><center><a href="https://www.jeshuanomics.com/" target="blank">Publicado por Jeshua Romero Guadarrama en colaboración con JeshuaNomics:</a></center>
<br><center><a href="https://github.com/JeshuaNomics" class="fa fa-github"><span class="label">  Git Hub</span></a>
<a href="https://www.facebook.com/JeshuaNomics/" class="fa fa-facebook"><span class="label">  Facebook</span></a>
<a href="https://twitter.com/JeshuaNomics" class="fa fa-twitter"><span class="label">  Twitter</span></a>
<a href="https://www.linkedin.com/in/jeshua-romero-guadarrama/" class="fa fa-linkedin"><span class="label">  Linkedin</span></a>
<a href="https://vk.com/jeshuanomics" class="fa fa-vk"><span class="label">  Vkontakte</span></a>
<a href="https://jeshuanomics.tumblr.com/" class="fa fa-tumblr"><span class="label">  Tumblr</span></a>
<a href="https://www.youtube.com/channel/UCY7f84mJGvMN7TF7XI4-Jgg?view_as=subscriber/" class="fa fa-youtube-play"><span class="label">  YouTube</span></a>
<a href="https://www.instagram.com/JeshuaNomics/" class="fa fa-instagram"><span class="label">  Instagram</span></a></center>

<br> Jeshua Romero Guadarrama es economista por la <a href="http://www.economia.unam.mx/">Universidad Nacional Autónoma de México</a>, quien ha construido el presente proyecto en colaboración con <a href="https://www.jeshuanomics.com">JeshuaNomics</a>, ubicado en la Ciudad de México, se puede contactar mediante el siguiente correo electrónico: jeshuanomics@gmail.com.
<br>
<br> `r sf <- lubridate::stamp_date('Última actualización el martes 21 del 05 de 2021'); sf(Sys.Date())`
<br>

```{r, 5, eval=knitr::opts_knit$get("rmarkdown.pandoc.to") == "html", results='asis', echo=FALSE}
cat('<hr style="background-color:#03193b;height:2px">')
```

Los estudiantes con poca experiencia en estadística y econometría a menudo tienen dificultades para entender los beneficios de desarrollar habilidades de programación al momento de aplicar diversos métodos econométricos. <i>Econoalgoritmia: Econometría avanzada y ciencia de datos con R</i> por Jeshua Romero Guadarrama (2021), ofrece una introducción interactiva a los aspectos esenciales de la programación por medio del lenguaje y software estadístico R, así como una guía para la aplicación de la teoría económica y econométrica en entornos específicos. En otras palabras, el objetivo es que los estudiantes se adentren al mundo de la economía aplicada mediante ejemplos empíricos presentados en la vida diaria y haciendo uso de las habilidades de programación recién adquiridas. Dicho objetivo se encuentra respaldado por ejercicios de programación interactivos generados con DataCamp Light y la incorporación de visualizaciones dinámicas de conceptos fundamentales mediante la flexibilidad de JavaScript, a través de la biblioteca D3.js.

En los últimos años, el lenguaje de programación estadística R se ha convertido en una parte integral del plan de estudios de las clases de econometría que se imparten en las universidades. Regularmente una gran parte de los estudiantes no han estado expuestos a ningún lenguaje de programación antes y, por lo tanto, tienen dificultades para participar en el aprendizaje de R por sí mismos. Con poca experiencia en estadística y econometría, es natural que los novicios tengan dificultades para comprender los beneficios de desarrollar habilidades en R para aprender y aplicar la econometría. Estos incluyen particularmente la capacidad de realizar, documentar y comunicar estudios empíricos y tener las facilidades para programar estudios de simulación, lo cual es útil para, por ejemplo, comprender y validar teoremas que generalmente no se asimilan o entienden fácilmente con el estudio de las fórmulas. Al ser un economistas aplicado y econometrista, me gustaría que mis colegas desarrollen capacidades de gran valor; en consecuencia, deseo compartir con las nuevas generaciones de economistas mis conocimientos.

En lugar de confrontar a los estudiantes con ejercicios de codificación puros y literatura clásica complementaria, he pensado que sería mejor proporcionar material de aprendizaje interactivo que combine el código en R con el contenido del curso de texto *Introducción a la Econometría* de @stock2015 que sirve de base para el presente material. El presente trabajo es un complemento empírico interactivo al estilo de un informe de investigación reproducible que permite a los estudiantes no solo aprender cómo los resultados de los estudios de casos se pueden replicar con R, sino que también fortalece su capacidad para utilizar las habilidades recién adquiridas en otras aplicaciones empíricas.

#### Las convenciones usadas en el presente curso {-}

+ El texto *en cursiva* indica nuevos términos, nombres, botones y similares.

+ El texto **en negrita** se usa generalmente en párrafos para referirse al código **R**. Esto incluye comandos, variables, funciones, tipos de datos, bases de datos y nombres de archivos.

+ <code>Texto de ancho constante sobre fondo gris</code> indica un código **R** que usted puede escribir literalmente. Puede aparecer en párrafos para una mejor distinción entre declaraciones de código ejecutables y no ejecutables, pero se encontrará principalmente en forma de grandes bloques de código **R**. Estos bloques se denominan fragmentos de código.

#### Reconocimiento {-}

A mi alma máter: Universidad Nacional Autónoma de México. Facultad de Economía. Por brindarme valiosas oportunidades que coadyuvaron a mi formación.

```{r, 6, eval=knitr::opts_knit$get("rmarkdown.pandoc.to") == "html", results='asis', echo=FALSE}
cat('<br>
![Creative Commons License](https://mirrors.creativecommons.org/presskit/buttons/88x31/svg/by-nc-sa.eu.svg)

Esta obra está autorizado bajo la [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License](http://creativecommons.org/licenses/by-nc-sa/4.0/).')
```

<!--chapter:end:index.Rmd-->

# Introducción {#Introducción}

```{r, echo = F}
options(knitr.duplicate.label = "allow")
```

```{r, 7, child="_setup.Rmd"}
```

```{r, 8, eval=knitr::opts_knit$get("rmarkdown.pandoc.to") == "html", results='asis', echo=FALSE}
cat('<hr style="background-color:#03193b;height:2px">')
```

El interés en el entorno de software y lenguaje de programación estadística disponible gratuitamente **R** [@R-base] está aumentando, con más de 11000 complementos (muchos de ellos proporcionan métodos de vanguardia) que están disponibles en la Red Integral de Archivos **R** ([CRAN](https: // cran .r-project.org /)), una extensa red de servidores FTP en todo el mundo que almacenan versiones idénticas y actualizadas del código **R** y su documentación. En este contexto, **R** domina como software (comercial) para la computación estadística en la mayoría de los campos de investigación en estadística aplicada. Los beneficios de estar disponible gratuitamente, de código abierto y de tener una comunidad grande y en constante crecimiento de usuarios que contribuyen a CRAN hacen que **R** sea cada vez más atractivo tanto para los economistas empíricos como para los econometristas.

Una ventaja sorprendente de usar **R** en econometría es que permite a los estudiantes documentar explícitamente su análisis paso a paso, de modo que sea fácil de actualizar y expandir. Esto permite reutilizar el código para aplicaciones similares con datos diferentes. Además, los programas **R** son completamente reproducibles, lo que facilita que otros comprendan y validen los resultados.

En los últimos años, **R** se ha convertido en una parte integral del plan de estudios de las clases de econometría que se imparten en la Universidad Nacional Autónoma de México. En cierto sentido, aprender a codificar es comparable a aprender un idioma extranjero y la práctica continua es esencial para el éxito del aprendizaje. 

No hace falta decir que presentar el código **R** desnudo en las diapositivas no anima a los estudiantes a participar en la experiencia práctica por sí mismos; por lo tanto, una introducción amigable a **R** es crucial. En cuanto a la literatura complementaria, hay algunos libros excelentes que tratan sobre **R** y sus aplicaciones a la econometría, por ejemplo, @kleiber2008. Sin embargo, dichas fuentes pueden estar algo más allá del alcance de los estudiantes de pregrado en economía que tienen poca comprensión de los métodos econométricos y poca experiencia en programación. 

En consecuencia, se ha compilado una colección de informes reproducibles para usar en clase. Dichos informes brindan orientación sobre cómo implementar aplicaciones seleccionadas del libro de texto *Introducción a la econometría* [@stock2015] que sirve como base para el curso y los tutoriales que lo acompañan. Dicho proceso fue facilitado considerablemente por **knitr** [@R-knitr] y **R markdown** [@R-rmarkdown]. En conjunto, ambos paquetes de **R** proporcionan potentes funcionalidades para la generación de informes dinámicos que permiten combinar perfectamente texto puro, LaTeX, código **R** y su salida en una variedad de formatos, incluidos PDF y HTML. 

Además, la redacción y distribución de informes reproducibles para su uso en el ámbito académico se ha enriquecido enormemente con el paquete **bookdown** [@R-bookdown], que se ha convertido en la principal herramienta para el presente proyecto. En otras palabras, **bookdown** encuentra fundamento en **R markdown** y permite crear páginas HTML atractivas como esta, entre otras cosas.

Inspirándo en *Introduccion a la Econometria* [@wooldridge2016] y replicando las aplicaciones discutidas en él libro *Using R for Introductory Econometrics* [@heiss2016] se ha usado este poderoso conjunto de herramientas para redactar el presente complemento empírico para cualquier economista en formación. 

De manera similar al libro de @heiss2016, este proyecto no es un libro de texto completo de econometría ni pretende ser una introducción general a **R**. En cualquier caso, ¡mejor que en otro libro de texto introductorio! el presente proyecto es una descripción que sirve como un guión interactivo con el estilo de un informe de investigación reproducible que tiene como objetivo proporcionar a los estudiantes un arreglo de aprendizaje electrónico independiente, entrelazando a la perfección el conocimiento básico teórico y las habilidades empíricas en econometría de pregrado. Por supuesto, la atención se centra en las aplicaciones empíricas con **R**, se dejan de lado las derivaciones y las demostraciones siempre que se pueda. El objetivo es permitir que los estudiantes no solo aprendan cómo los resultados de los estudios de casos se pueden replicar con **R**, sino que también tiene la intención de fortalecer su capacidad para usar las habilidades recién adquiridas en otras aplicaciones empíricas.

Para darse cuenta de esto, cada capítulo contiene ejercicios interactivos de programación en **R**. Estos ejercicios se utilizan como suplementos de fragmentos de código que muestran cómo se pueden implementar las técnicas discutidas anteriormente dentro de **R**. Se generan utilizando el [widget de luz DataCamp](https://github.com/datacamp/datacamp-light), que están respaldados por una sesión **R** que se mantiene en [DataCamp](https://www.datacamp.com/home). Puede jugar el ejercicio de ejemplo que se presenta a continuación.

<iframe src="DCL/intro_1.html" frameborder="0" scrolling="no" style="width:100%;height:360px"></iframe>

Como se puede ver arriba, el widget consta de dos pestañas. **script.R** imita un archivo **.R**, un formato de archivo que se usa comúnmente para almacenar código **R**. Las líneas que comienzan con # se comentan, es decir, no se reconocen como código. Además, **script.R** funciona como una hoja de ejercicios en la que puede escribir la solución que se le ocurra. Si presiona el botón *Ejecutar*, se ejecutará el código, se ejecutarán las pruebas de corrección del envío y se le notificará si su enfoque es correcto. Si no es correcto, recibirá comentarios que sugieren mejoras o sugerencias. La otra pestaña, **R Console**, es una consola **R** completamente funcional que se puede utilizar para probar soluciones a los ejercicios antes de enviarlos. Por supuesto, puede enviar (casi cualquier) código **R** y usar la consola para jugar y explorar. Simplemente escriba un comando y presione la tecla *Enter* en su teclado.

Mirando el widget de arriba, notará que hay un <tt> > </tt> en el panel derecho (en la consola). Este símbolo se llama "aviso" e indica que el usuario puede ingresar el código que se ejecutará. Para evitar confusiones, no se mostrará este símbolo en el curso. La salida producida por el código R se comenta con <tt> #> </tt>.

Por lo general, se muestra el código R junto con la salida generada en fragmentos de código. Como ejemplo, si se considera la siguiente línea de código que se presenta en el fragmento a continuación. Le dice a **R** que calcule el número de paquetes disponibles en **CRAN**. El fragmento de código es seguido por la salida generada.

```{r, 9}
# check the number of R packages available on CRAN
nrow(available.packages(repos = "http://cran.us.r-project.org"))
```

Cada fragmento de código está equipado con un botón en el lado exterior derecho que copia el código en su portapapeles. Esto hace que sea conveniente trabajar con segmentos de código más grandes en su versión de *R*/*RStudio* o en los widgets presentados a lo largo del curso. En el widget de arriba, puede hacer clic en **R Console** y escribir `nrow(available.packages(repos ="http://cran.us.r-project.org"))` (el comando del código fragmento de arriba) y ejecútarlo presionando *Enter* en su teclado.^[La sesión **R** se inicializa haciendo clic en el widget. Esto puede tardar unos segundos. Espere a que el indicador junto al botón *Ejecutar* se ponga verde.]

Se debe tener en cuenta que algunas líneas en el widget están comentadas y le piden que asigne un valor numérico a una variable y luego que imprima el contenido de la variable en la consola. Puede ingresar su enfoque de solución a **script.R** y presionar el botón *Ejecutar* para obtener los comentarios que se describen más arriba. En caso de que no sepa cómo resolver este ejercicio de muestra (no se asuste, probablemente por eso está leyendo esto), un clic en *Pista* le proporcionará algunos consejos. Si aún no puede encontrar una solución, un clic en *Solución* le proporcionará otra pestaña, **Solución.R**, que contiene un código de solución de muestra. A menudo ocurre que los ejercicios se pueden resolver de muchas formas diferentes y **Solution.R** presenta lo que se considera comprensible e idiomático.

## Marco teórico {#MT}

### ¿Qué es la econometría? {-}

Hoy en día la econometría se ha difundido ampliamente entre quienes estudian y buscan aplicar la teoría económica. En general, cualquier licenciatura en economía cuenta con uno o más cursos de econometría, dado que es usual que la econometría se enseñe con la misma relevancia que se le da a los cursos de microeconomía y macroeconomía. No existe posgrado en economía que deje de incorporar el estudio de la econometría como una disciplina fundamental. Incluso, es posible aseverar que en disciplinas distintas a la economía, como en las matemáticas, algunas ingenierías, la sociología y psicología, sus estudiantes reciben algún curso de econometría. 

No sólo en la formación académica la econometría está presente, en la vida laboral se realizan todos los días aplicaciones econométricas: 

- En las oficinas gubernamentales se emplean modelos econométricos para realizar pronósticos de variables económicas. 
- En empresas privadas se utilizan algunas técnicas econométricas para proyectar a futuro variables como ventas, precios y demanda, entre otras variables. En consecuencia, en el mercado existen numerosos servicios de consultoría que han hecho de la econometría un negocio al ofrecer la venta de pronósticos generados a través de modelos econométricos. 
- En el mundo de la investigación científica la econometría es un ingrediente indispensable. Diariamente se publican en todo el orbe una gran cantidad de artículos de economía en revistas especializadas, la evidencia empírica que aportan, generalmente, se sustenta en algún modelo econométrico.

La importancia de esta disciplina es tal que basta escribir en internet la palabra *Econometría* para obtener diez millones de referencias.

Con la econometría se busca comprender fenómenos como el de las crisis, identificar sus causas, valorar sus consecuencias futuras y proponer medidas de política para enfrentarlas. Para ello, la econometría utiliza modelos, con estos se busca representar de forma simplificada a los principales factores causales de un problema de interés. La especificación y estimación de esos modelos requiere del conocimiento de teorías económicas, para poder establecer relaciones sustentadas en datos entre las variables, así como para poder realizar mediciones de dichas relaciones.

No existe una definición única y generalmente aceptable de lo qué es la econometría. Debido a que en ella concurren una gran diversidad de perspectivas teóricas y metodológicas. Por tanto, existen diferentes posturas sobre su significado.

A diferencia de lo que ocurre hoy en día, en los años treinta, época en la que se institucionaliza la econometría, existía cierto consenso metodológico. A ese consenso se le identifica como la "metodología de libro de texto" y su definición de econometría era la siguiente:

"La aplicación de métodos estadísticos y matemáticos al análisis de los datos económicos, con el propósito de dar un contenido empírico a las teorías económicas y verificarlas o refutarlas." (Maddala, 1996, p.1)

Bajo esta última conceptualización la econometría aparece, por un lado, como un mero instrumental técnico al ser la aplicación de métodos matemáticos y estadísticos. Por otro lado, es vista prácticamente como la piedra filosofal, al darle el papel de criterio último de verdad al ser la vía para verificar o refutar teorías. El econometrista aparece en esa definición como un técnico, cuyo único fin es intentar medir lo que la teoría económica ha postulado.

Esta visión de la econometría se ha transformado en los últimos años, en este sentido, vale la pena retomar la definición proporcionada por Aris Spanos:

"La econometría se interesa por el estudio sistemático de fenómenos económicos utilizando datos observables." (Spanos, 1996, p.3)

Este es un enfoque moderno con el cual coincido, lo que hace a la econometría diferente de otros campos de la economía es la utilización de datos observables. Por lo tanto, la econometría tiene una perspectiva empírica, no se reduce a la teoría. La econometría necesariamente hace uso de datos, los cuales no son experimentales sino que son resultado del funcionamiento de la actividad económica y de millones de interacciones sociales. El papel del econometrista no se reduce a medir lo que la teoría económica establece, dado que un economista es un científico social que, a través de un método científico, emprende el estudio de fenómenos sociales relacionados con la economía. Por lo tanto, no es un observador pasivo de la teoría, al contrario, es capaz de contribuir a la teoría.

La econometría que se utiliza hoy en día se ha ido transformando y modernizando, hasta convertirse en una de las herramientas más potentes a
disposición de los economistas y principalmente del análisis empírico de problemas económicos. Esta evolución de la disciplina la sintetiza perfectamente Spanos:

"En el amanecer del siglo veintiuno, la econometría se ha desarrollado desde los modestos orígenes del "ajuste de curvas" por mínimos cuadrados en los inicios del siglo veinte, hasta un poderoso arreglo de herramientas estadísticas para modelar todo tipo de datos, desde las tradicionales series de tiempo a las secciones cruzadas, datos transversales y los datos de panel." (Spanos, 2006, p. 5)

### ¿Qué es la ciencia de datos? {-}

La ciencia de datos es un campo emergente extremadamente transdisciplinario -puente entre diversas áreas teóricas, con fundamentos matemáticos, computacionales, experimentales y sociales-. En este contexto, la ciencia de datos trata con enormes volúmenes de datos complejos, incongruentes y dinámicos de múltiples fuentes, con el objetivo de desarrollar algoritmos, métodos, herramientas y servicios capaces de ingerir dichos conjuntos de datos y generar sistemas semiautomatizados de apoyo a la toma de decisiones. 

Las recientes investigaciones económicas emplean la ciencia de datos para extraer información en busca de patrones o mecanismos que coadyuven a predecir posibles resultados esperados, así como sugerir la agrupación o etiquetado de observaciones retrospectivas y prospectivas, calcular firmas de datos, extraer información valiosa y ofrecer conocimiento procesable basado en evidencia. 

Las técnicas de ciencia de datos comúnmente empleadas por los economistas a menudo implican manipulación (**wrangling**), armonización y agregación de datos, para en etapas posteriores emplear análisis exploratorios o confirmatorios de datos, análisis predictivo, validación y ajuste fino.

### ¿Qué es la econoalgoritmia? {-}

La *econoalgoritmia* es un término acuñado por el economista Jeshua Romero Guadarrama en el presente curso para definir la creación e implementación de algoritmos computacionales en el campo de estudio de la economía aplicada, haciendo uso de un conjunto de herramientas y métodos propios de la econometría y ciencia de datos.

## Colofón {#Colofón}

Este curso fue construido con:

```{r, 10, echo = F}
sessioninfo::session_info("itewrpkg")
```

Puede instalar los paquetes ejecutando las siguientes líneas de código:

```{r, 11, echo = T, eval = F}
install.packages("abind")
install.packages("AER")
install.packages("askpass") 
install.packages("assertthat") 
install.packages("backports") 
install.packages("base64enc") 
install.packages("bdsmatrix") 
install.packages("BH") 
install.packages("bibtex") 
install.packages("bitops") 
install.packages("blob") 
install.packages("bookdown") 
install.packages("boot") 
install.packages("broom") 
install.packages("callr") 
install.packages("car")
install.packages("carData") 
install.packages("cellranger") 
install.packages("cli") 
install.packages("clipr")
install.packages("colorspace") 
install.packages("conquer") 
install.packages("crayon") 
install.packages("crayon")
install.packages("cubature")
install.packages("curl")
install.packages("data.table")
install.packages("DBI")
install.packages("dbplyr")
install.packages("desc")
install.packages("digest")
install.packages("dplyr")
install.packages("dynlm")
install.packages("ellipsis")
install.packages("evaluate")
install.packages("fansi")
install.packages("farver")
install.packages("fastICA")
install.packages("fBasics")
install.packages("fGarch")
install.packages("forcats")
install.packages("foreign")
install.packages("Formula")
install.packages("fs")
install.packages("gbRd")
install.packages("generics")
install.packages("ggplot2")
install.packages("glue")
install.packages("gss")
install.packages("gtable")
install.packages("haven")
install.packages("highr")
install.packages("hms")
install.packages("htmltools")
install.packages("httr")
install.packages("isoband")
install.packages("itewrpkg")
install.packages("jsonlite")
install.packages("KernSmooth")
install.packages("knitr")
install.packages("labeling")
install.packages("lattice")
install.packages("lifecycle")
install.packages("lme4")
install.packages("lmtest") 
install.packages("locpol") 
install.packages("lubridate") 
install.packages("magrittr") 
install.packages("maptools") 
install.packages("markdown") 
install.packages("MASS") 
install.packages("Matrix") 
install.packages("MatrixModels") 
install.packages("matrixStats") 
install.packages("maxLik") 
install.packages("mgcv") 
install.packages("mime") 
install.packages("minqa")
install.packages("miscTools") 
install.packages("modelr") 
install.packages("munsell") 
install.packages("mvtnorm") 
install.packages("nlme") 
install.packages("nloptr") 
install.packages("nnet") 
install.packages("np") 
install.packages("openssl") 
install.packages("openxlsx") 
install.packages("orcutt") 
install.packages("pbkrtest") 
install.packages("pillar") 
install.packages("pkgbuild") 
install.packages("pkgconfig") 
install.packages("pkgload") 
install.packages("plm") 
install.packages("praise") 
install.packages("prettyunits") 
install.packages("processx") 
install.packages("progress") 
install.packages("ps") 
install.packages("purrr") 
install.packages("quadprog") 
install.packages("quantmod") 
install.packages("quantreg") 
install.packages("R6")
install.packages("RColorBrewer") 
install.packages("Rcpp")
install.packages("RcppArmadillo") 
install.packages("RcppEigen")
install.packages("RCurl")
install.packages("rdd")
install.packages("rddtools")
install.packages("Rdpack")
install.packages("rdrobust")
install.packages("readr")
install.packages("readxl")
install.packages("rematch")
install.packages("reprex")
install.packages("rio")
install.packages("rlang")
install.packages("rmarkdown")
install.packages("rprojroot")
install.packages("rstudioapi")
install.packages("rvest")
install.packages("sandwich")
install.packages("scales")
install.packages("selectr")
install.packages("sp")
install.packages("SparseM")
install.packages("spatial")
install.packages("stabledist")
install.packages("stargazer")
install.packages("statmod")
install.packages("stringi")
install.packages("stringr")
install.packages("strucchange")
install.packages("survival")
install.packages("sys")
install.packages("testthat")
install.packages("tibble")
install.packages("tidyr")
install.packages("tidyselect")
install.packages("tidyverse")
install.packages("timeDate")
install.packages("timeSeries")
install.packages("tinytex")
install.packages("TTR")
install.packages("urca")
install.packages("utf8")
install.packages("vars")
install.packages("vctrs")
install.packages("viridisLite")
install.packages("whisker")
install.packages("withr")
install.packages("xfun")
install.packages("xml2")
install.packages("xts")
install.packages("yaml")
install.packages("zip")
install.packages("zoo")
```

```{r, 12, echo = F, eval = F, message = F, warning = F} 
library("abind")
library("AER")
library("askpass") 
library("assertthat") 
library("backports") 
library("base64enc") 
library("bdsmatrix") 
library("BH") 
library("bibtex") 
library("bitops") 
library("blob") 
library("bookdown") 
library("boot") 
library("broom") 
library("callr") 
library("car")
library("carData") 
library("cellranger") 
library("cli") 
library("clipr")
library("colorspace") 
library("conquer") 
library("crayon") 
library("crayon")
library("cubature")
library("curl")
library("data.table")
library("DBI")
library("dbplyr")
library("desc")
library("digest")
library("dplyr")
library("dynlm")
library("ellipsis")
library("evaluate")
library("fansi")
library("farver")
library("fastICA")
library("fBasics")
library("fGarch")
library("forcats")
library("foreign")
library("Formula")
library("fs")
library("gbRd")
library("generics")
library("ggplot2")
library("glue")
library("gss")
library("gtable")
library("haven")
library("highr")
library("hms")
library("htmltools")
library("httr")
library("isoband")
library("itewrpkg")
library("jsonlite")
library("KernSmooth")
library("knitr")
library("labeling")
library("lattice")
library("lifecycle")
library("lme4")
library("lmtest") 
library("locpol") 
library("lubridate") 
library("magrittr") 
library("maptools") 
library("markdown") 
library("MASS") 
library("Matrix") 
library("MatrixModels") 
library("matrixStats") 
library("maxLik") 
library("mgcv") 
library("mime") 
library("minqa")
library("miscTools") 
library("modelr") 
library("munsell") 
library("mvtnorm") 
library("nlme") 
library("nloptr") 
library("nnet") 
library("np") 
library("openssl") 
library("openxlsx") 
library("orcutt") 
library("pbkrtest") 
library("pillar") 
library("pkgbuild") 
library("pkgconfig") 
library("pkgload") 
library("plm") 
library("praise") 
library("prettyunits") 
library("processx") 
library("progress") 
library("ps") 
library("purrr") 
library("quadprog") 
library("quantmod") 
library("quantreg") 
library("R6")
library("RColorBrewer") 
library("Rcpp")
library("RcppArmadillo") 
library("RcppEigen")
library("RCurl")
library("rdd")
library("rddtools")
library("Rdpack")
library("rdrobust")
library("readr")
library("readxl")
library("rematch")
library("reprex")
library("rio")
library("rlang")
library("rmarkdown")
library("rprojroot")
library("rstudioapi")
library("rvest")
library("sandwich")
library("scales")
library("selectr")
library("sp")
library("SparseM")
library("spatial")
library("stabledist")
library("stargazer")
library("statmod")
library("stringi")
library("stringr")
library("strucchange")
library("survival")
library("sys")
library("testthat")
library("tibble")
library("tidyr")
library("tidyselect")
library("tidyverse")
library("timeDate")
library("timeSeries")
library("tinytex")
library("TTR")
library("urca")
library("utf8")
library("vars")
library("vctrs")
library("viridisLite")
library("whisker")
library("withr")
library("xfun")
library("xml2")
library("xts")
library("yaml")
library("zip")
library("zoo")
```

## Una breve introducción a *R* y *RStudio* {#BIRR}

```{r, 13, fig.align='center', echo=FALSE, fig.cap="RStudio: Los cuatro paneles"}
knitr::include_graphics('images/rstudio.jpg')
```

### Primeros pasos en R {-}

El sistema de computación estadístico y gráfico **R** (R Development Core Team 2021b, [http://www.R-project.org/](https://www.r-project.org/)) es un proyecto de software de código abierto, publicado bajo los términos de la GNU General Public License (GPL), Versión 2.^[Los lectores que no estén familiarizados con el software de código abierto pueden visitar [http://www.gnu.org/](https://cran.r-project.org/mirrors.html).] 

El código fuente, así como varias versiones binarias de **R** se pueden obtener, sin costo, mediante el Comprehensive R Archive Network (CRAN;ver [http://CRAN.R-project.org/mirrors.html](http://CRAN.R-project.org/mirrors.html) para encontrar su espejo más cercano). Se proporcionan versiones binarias para versiones de 64 bits de Microsoft Windows, varias plataformas de Linux (incluidos Debian, Red Hat, SUSE y Ubuntu) y Mac OS X.

#### Instalación {-}

La instalación de versiones binarias es bastante sencilla: 

- Simplemente vaya a CRAN. 
- Elija la versión correspondiente a su sistema operativo.
- Siga las instrucciones provistas en el archivo *readme* correspondiente. 

Para Microsoft Windows, esto equivale a descargar y ejecutar el archivo ejecutable de instalación (archivo *.exe*), que lleva al usuario a través de un administrador de configuración estándar. Para Mac OS X, *imagen de disco separada* o los archivos *.dmg* están disponibles para el sistema base, así como para una GUI desarrollada para esta plataforma. Para varios plataformas de Linux, existen binarios preempaquetados (como los archivos *.rpm* o *.deb*) que se pueden instalar con las herramientas de empaquetado habituales en las respectivas plataformas. Además, las versiones de **R** también se distribuyen en muchos de los repositorios estándar de Linux, aunque no necesariamente se actualizan de forma rápida las nuevas versiones de **R** como CRAN.

Para todos los sistemas, en particular aquellos para los que no existe un paquete binario, también existe, por supuesto, la opción de compilar **R** a partir de la fuente. En algunas plataformas, en particular Microsoft Windows, esto puede resultar un poco engorroso porque los compiladores necesarios normalmente no forman parte de una instalación estándar. En otras plataformas, como Unix o Linux, esto a menudo equivale a los pasos habituales de configure/make/install. Finalmente, se recomienda visitar [R Development Core Team (2021d)](https://www.r-project.org/) para obtener información detallada sobre la instalación y administración del software y paquetería de **R**.

#### Paquetes {-}

Como se discutirá con mayor detalle a continuación, la base del software **R** puede ampliarse mediante paquetes, algunos de los cuales forman parte de la instalación predeterminada. Los paquetes se almacenan en una o más bibliotecas (esto es, colecciones de paquetes) en el sistema y pueden cargarse usando el comando **library()**. Mecanografiar **library()** sin argumentos devuelve una lista de todos los paquetes instalados actualmente en todas las bibliotecas. En el mundo de **R**, los paquetes que se encuentran instalados de forma predeterminada se conocen como paquetes "*base*" (contenidos en el código fuente de **R**). De igual forma, los son paquetes incuidos en cada distribución binaria se conocen como paquetes "*recomendados*". Actualmente existen una gran cantidad de paquetes adicionales ([más de 1400](https://cran.r-project.org/web/packages/available_packages_by_date.html)), conocidos como paquetes "*contribuidos*". 

Resulta importante mencionar que todos los paquetes están disponibles en los servidores de CRAN (se recomienda visitar [http://CRAN.R-project.org/web/packages/](https://cran.r-project.org/web/packages/)), y algunos de estos serán necesario a medida que se avance. En particular, el paquete necesario que acompaña a este curso es llamado **AER**. En una computadora conectada a Internet, la instalación del paquete es tan simple como escribir el siguiente código en el área de codificado: 

```{r, 14, echo = T, eval = F, message = F, warning = F} 
install.packages("AER")
```

Este proceso de instalación funciona en todos los sistemas operativos. Además, los usuarios de Windows pueden instalar paquetes usando "Install packages from CRAN". Los usuarios de Mac pueden instalar paquetes usando la opción de menú "Package installer" y luego eligiendo los paquetes a instalar de una lista. Dependiendo de la instalación, en particular de la configuración de las rutas de la biblioteca, se puede usar la función **install.packages()** de forma predeterminada para instalar paquetes. Si se intenta instalar un paquete en un directorio donde el usuario no tiene permisos de escritura. En tal caso, es necesario especificar el argumento **lib** o establecer las rutas de la biblioteca de forma adecuada (se puede consultar **R Development Core Team 2021d** o **?library** para más información). Por cierto, instalar **AER** descargará varios paquetes más de los que depende el buen funcionamiento del **AER**. No es raro que los paquetes dependan de otros paquetes; si este es el caso, el paquete lo “sabe” y asegura que todas las funciones de las que depende estarán disponibles durante el proceso de instalación.

Para usar funciones o conjuntos de datos de un paquete, el paquete debe estar cargado. El comando para cargar paquetes es **library()**, para el paquete **AER**:

```{r, 15, echo = T, eval = T, message = F, warning = F} 
library("AER")
```

De ahora en adelante, se asume que **AER** siempre está cargado. Será necesario instalar y cargar más paquetes en capítulos posteriores, y siempre se indicará cuáles son.

En vista del número cada vez mayor de paquetes contribuidos, se ha demostrado que es útil mantener una serie de "vistas de tareas CRAN" que proporcionan una descripción general de los paquetes para determinadas tareas. Las vistas de tareas actuales incluyen econometría, finanzas, ciencias sociales y estadísticas bayesianas (ver [http://CRAN.R-project.org/web/views/](http://CRAN.R-project.org/web/views/) para mas detalles.

### Trabajar con R {-}

Existe una diferencia importante entre la filosofía de **R** y la mayoría de los otros paquetes de econometría. Con muchos paquetes, un análisis conducirá a una gran cantidad de resultados que contienen información sobre estimaciones, diagnósticos de modelos, pruebas de especificación, entre otros. En **R**, un análisis normalmente se divide en una serie de pasos. Los resultados intermedios se almacenan en objetos, con un resultado mínimo en cada paso (a menudo ninguno). En cambio, los objetos se manipulan aún más para obtener la información requerida (programación orientada a objetos).

De hecho, el principio de diseño fundamental subyacente a **R** (antes **S**) es “todo es un objeto”. Por lo tanto, no solo los vectores y las matrices son objetos que pueden ser pasados y devueltos por funciones, sino también las funciones mismas e incluso las *llamadas a funciones*. Esto permite realizar cálculos sobre el lenguaje y puede facilitar considerablemente las tareas de programación, como se ilustrará más adelante:

#### Manipulación sencilla de objetos {-}

Para ver qué objetos están definidos actualmente, la función **objects()** (o la equivalente **ls()**) puede ser usado. De forma predeterminada, la función enumera todos los objetos en el entorno global (es decir, el espacio de trabajo del usuario):

```{r, 16, echo = T, eval = T, message = F, warning = F} 
objects()
```

que devuelve un vector de caracteres de longitud 9, lo que indica que actualmente existen nueve objetos, como resultado de la sesión introductoria.

Sin embargo, esta no puede ser la lista completa de objetos disponibles, dado que algunos objetos ya deben existir antes de la ejecución de cualquier comando, entre ellos la función **objects()**, que se acaba de llamar. La razón es que la lista de búsqueda, que puede ser consultada mediante la función **search()**, comprende no sólo el medio ambiente global "*.GlobalEnv*" ( siempre en la primera posición) sino también varios paquetes adjuntos, incluido el paquete **base** al final.

```{r, 17, echo = T, eval = T, message = F, warning = F} 
search()
```

Lamando a la función **objects("package:base")** se pueden mostrar los nombres de más de mil objetos definidos en la **base**, incluyendo la función **objects()** en sí misma.

Los objetos se pueden crear fácilmente asignando un valor a un nombre usando el operador de asignación **<-**. Por ejemplo, se crea un vector **x** en que el numero $2$ esté almacenado:

```{r, 18, echo = T, eval = T, message = F, warning = F} 
x <- 2

objects()
```

En este punto, **x** ahora está disponible en el entorno global y se puede eliminar utilizando la función **remove()** ( o equivalente **rm()**):

```{r, 19, echo = T, eval = T, message = F, warning = F} 
remove(x)

objects()
```

#### Llamada de funciones {-}

Si se escribe el nombre de un objeto en el área de **prompt**, éste se imprimirá. En este contexto, el resultado de escribir los nombres de las funciones cambia dependiendo de la sintaxis:

1. Si únicamente se escribe el nombre de la función, suponiendo **foo**, esto significa que se quiere imrpimir el correspondiente código fuente de la función en **R** (intente, por ejemplo, **objects**).
2. Si se escibre el nombre de la función seguido de paréntesis, suponinedo **foo()**, esto implica que se quiere ejecutar una *llamada de función* (intente, por ejemplo, **objects()**).

Los argumentos dentro de una función son válidos:

1. Si se especifican los valores de los argumentos que la función requiere dentro de los paréntesis (**log(base = 2, x = 16)** es una *llamada de función válida*).
2. Si no se especifican los valores de los argumentos porque la función ya posee valores predeterminados (**foo()** por sí solo es una *llamada de función válida*). 

Por lo tanto, a lo largo de este curso se emplean un par de paréntesis después del nombre del objeto para indicar que el objeto discutido es una llamada de función.

Las funciones a menudo tienen más de un argumento (de hecho, no existe un límite para el número de argumentos en las funciones de **R**). Una llamada de función puede usar los argumentos en cualquier orden, siempre que se proporcione el nombre del argumento. Si no se dan los nombres de los argumentos, **R** asume que aparecen en el orden en que se define la función. Si un argumento tiene un valor predeterminado, puede omitirse en una llamada de función. Por ejemplo, la función **log()** tiene dos argumentos, **x** y **base**: 

1. El primero (**x**) puede ser un escalar (aunque en realidad también puede ser un vector), cuyo logaritmo debe tomarse.
2. El segundo (**base**) es la base respecto a la cual se calculan los logaritmos. 

Por lo tanto, las siguientes cuatro llamadas son todas equivalentes:

```{r, 20, echo = T, eval = T, message = F, warning = F} 
log(16, 2)

log(x = 16, 2)

log(16, base = 2)

log(base = 2, x = 16)
```

#### Clases y funciones genéricas {-}

Cada objeto tiene una clase que se puede consultar llamando a la función **class()**. Las clases que se usarán comúnmente en el presente curso incluyen:

- "**data.frame**" (una lista o matriz con una determinada estructura, el formato preferido en el que se deben guardar los datos).
- "**lm**" para objetos de modelo lineal (devuelto cuando se ajusta un modelo de regresión lineal por mínimos cuadrados ordinarios).
- "**matrix**” (que es lo que sugiere el nombre: matrices). 

Para cada clase, se encuentran disponibles métodos específico que se pueden consultar mediante llamadas a las funciones. Sin embargo, existen funciones genéricas (se pueden usar para todos los tipos de clases), los ejemplos típicos de dichas funciones incluyen **summary()** y **plot()**. El resultado de estas funciones depende de la clase del objeto: 

- Cuando se le proporciona un vector numérico, **summary()** devuelve resúmenes básicos de una distribución empírica, como la media, la mediana y la moda.
- Cuando se le proporciona un vector de datos categóricos, **summary()** devuelve una tabla de frecuencias.
- Cuando se le proporciona un objeto de modelo lineal, **summary()** devuelve el resultado de una regresión estándar. 
- Cuando se le proporciona un vector, una matriz o un marco de datos numérico, **plot()** devuelve pares de diagramas de dispersión.
- Cuando se le proporciona un objeto de modelo lineal, **plot()** devuelve pares de diagramas de diagnóstico básicos.

#### Salir de R {-}

Se puede salir de **R** usando la función **q()**:

```{r, 21, echo = T, eval = F, message = F, warning = F} 
q()
```
 
A continuación, **R** pregunta si se desea guardar la imagen del espacio de trabajo. Respondiendo **n** (no) saldrá de **R** sin guardar nada, mientras que respondiendo **y** (yes) guardará todos los objetos actualmente definidos en un archivo **.RData** y el historial de comandos en un archivo **.Rhistory**, ambos en el directorio de trabajo.

#### Gestión de archivos {-}

Si la sesión se **R** inicia en un directorio que tiene archivos **.RData** y/o **.Rhistory**, estos se cargarán automáticamente. En cuanto al directorio de trabajo:

- Para consultar el directorio de trabajo se debe usar **getwd()**. 
- Para cambiar el directorio de trabajo se debe usar **setwd()**. 
- Para consultar los archivos en un directorio se debe usar la función **dir()**.

Los espacios de trabajo guardados de otros directorios se pueden cargar usando la función **load()**. Análogamente, los objetos de **R** se pueden guardar (en formato binario) usando **save()**. 

#### Obtención de ayuda {-}

**R** es un software bien documentado. Se puede acceder a la ayuda sobre cualquier función utilizando el símbolo **?** o la función **help()**. Por lo tanto:

```{r, 22, echo = T, eval = F, message = F, warning = F} 
?options

help("options")
```

Ambos abren la página de ayuda para el comando **options()**. En la parte inferior de una página de ayuda, normalmente se muestran ejemplos prácticos de cómo utilizar dicha función. Estos se pueden ejecutar fácilmente con la función **example()**; por ejemplo, si no se conoce el nombre exacto de un comando, como suele ser el caso de los principiantes, las funciones a utilizar son **help.search()** y **apropos()**:

**help.search()** devuelve archivos de ayuda con alias, conceptos o títulos que coinciden con un "patrón" mediante la coincidencia aproximada. Por lo tanto, si se desea ayuda sobre la configuración de las opciones, pero no se conoce el nombre exacto del comando (es decir, se deconoce la función **options()**), una búsqueda de objetos que contenga el patrón "options" puede ser útil. En otras palabras, **help.search("option")** devolverá una lista (larga) de comandos, marcos de datos, entre otros, que contienen el patrón "options", incluida la entrada que proporciona el resultado deseado:

`options(base)  Options Settings`

El resultado anterior dice que existe una función llamada **options()** el paquete base de **R** que proporciona una capa de configuración de opciones. 

Alternativamente, la función a **apropos()** enumera todas las funciones cuyos nombres incluyen el patrón introducido. Como lo ilustra:

```{r, 23, echo = T, eval = F, message = F, warning = F}
apropos("help")
```

El comando anterior proporciona una lista con solo tres entradas, incluida la función deseada **help()**. Se debe tener en cuenta que **help.search()** busca en todos los paquetes instalados, mientras que a **apropos()** simplemente examina los objetos que se encuentran actualmente en la lista de búsqueda.

#### Viñetas {-}

En un nivel más avanzado, existen las llamadas *viñetas*. Son *archivos PDF* generados a partir de *archivos integrados* que contienen código **R** y documentación (en formato LATEX). Por lo tanto, las viñetas suelen contener comandos que se pueden ejecutar directamente y que reproducen el análisis descrito. El presente curso fue escrito usando las herramientas en las que se basan las viñetas. La función **vignette()** proporciona una lista de viñetas en todos los paquetes adjuntos (el significado de "adjunto" se explicará más adelante). Un ejemplo es el siguiente código **vignette("strucchange-intro", package = "strucchange")**, que abre la viñeta que acompaña al paquete **strucchange**. El paquete trata temas relacionados con las pruebas, el seguimiento y la datación de los cambios estructurales en las regresiones de series de tiempo. Se recomienda consultar el Capítulo \@ref(TP) para obtener más información sobre los detalles de las viñetas e infraestructura relacionada.

#### Demos {-}

También existen "demos" para determinadas tareas. Una *demo* es una interfaz para ejecutar algunas demostraciones de **R** scripts (conocidos como guiones con comandos). Se puede escribir **demo()** para obtener una lista de los temas disponibles. Estos incluyen "**graphics**" y "**lm.glm**", este último proporciona ilustraciones sobre modelos lineales y lineales generalizados. Para los principiantes, correr **demo("graphics")** es muy recomendable.

#### Manuales, preguntas frecuentes y publicaciones {-}

**R** también viene con varios manuales:

- Una introducción a **R**.
- Importación / Exportación de datos en **R**.
- Definición del lenguaje **R**.
- Escritura de extensiones en **R**.
- Instalación y administración en **R**.
- Internos en **R**

Además, existen varias colecciones de preguntas frecuentes ([Frequently Asked Questions](http://CRAN.R-project.org/faqs.html) o [FAQs](http://CRAN.R-project.org/faqs.html)), las cuales consisten en preguntas generales sobre **R** y también sobre problemas específicos de la plataforma en Microsoft Windows y Mac OS X. 

Adicionalmente, existe un boletín en línea llamado *R News*, lanzado en 2001. Actualmente se publica unas tres veces al año y presenta, entre otras cosas, desarrollos recientes en **R** (como cambios en el idioma o nuevos paquetes de complementos), un "nicho de programador" y ejemplos que analizan datos con **R**. Se recomienda visitar [http://CRAN.R-project.org/doc/Rnews/](http://CRAN.R-project.org/doc/Rnews/) para más información.

Para un número creciente de paquetes en **R**, existen publicaciones correspondientes en el *Journal of Statistical Software* (Revista de Software Estadístico que se puede consultar mediante el siguiente vínculo [http://www.jstatsoft.org/](http://www.jstatsoft.org/). Se trata de una revista de acceso abierto que publica artículos y fragmentos de código (así como reseñas de libros y software) sobre temas relacionados con algoritmos y software estadístico. Un volumen especial de Econometría en **R** se puede encontrar actualmente en preparación.

Por último, existe una lista de libros en rápido crecimiento sobre **R**, así como en estadística aplicada usando **R** en todos los niveles, quizás el más completo sea Venables y Ripley (2002). Además, se remite al lector interesado a Dalgaard (2002) para una introducción a la estadística, a Murrell (2005) para gráficos en **R** y Faraway (2005) para comprender a profundidad las implicaciones de la regresión lineal.

### El modelo de desarrollo {-}

Una de las fortalezas de **R** y una característica clave de su éxito es que resulta ser altamente extensible a través de paquetes que proporcionan extensiones a todo lo disponible en el sistema base. Esto incluye no solo el código **R**, sino también código en lenguajes compilados (como *C*, *C++* o *FORTRAN*), conjuntos de datos, archivos de demostración, conjuntos de pruebas, vignettes o documentación adicional. Por lo tanto, cada usuario de **R** puede convertirse fácilmente en un desarrollador de **R** enviando sus paquetes a **CRAN** para compartirlos con la comunidad de **R**. Por lo tanto, los paquetes pueden influir activamente en la dirección en que (partes de) **R** irá en el futuro.

A diferencia de los paquetes **CRAN**, los paquetes base del sistema **R** son mantenidos y desarrollados solo por el equipo central de **R**, que lanza actualizaciones de las principales versiones (es decir, versiones x.y.0) cada dos años (actualmente alrededor del 1 de abril y el 1 de octubre). Sin embargo, como **R** es un sistema de código abierto, todos los usuarios tienen acceso de lectura al repositorio principal o maestro de *SVN* (*SVN* significa Subversion), que funciona como un sistema de control de versiones (se recomienda visitar [http://subversion.tigris.org/](http://subversion.tigris.org/). En consecuencia, cualquiera puede consultar el código fuente completo de la versión de desarrollo de R.

Asimismo, existen varios medios de comunicación dentro de la comunidad de usuarios y desarrolladores en **R**, así como entre los usuarios y el equipo de desarrollo central. Los dos más importantes en **R** son las extensas listas de correo y, como se describió anteriormente, los paquetes CRAN. El proyecto **R** alberga varias listas de correo, incluidas **R-help** y **R-devel**: 

- El primero se utiliza para pedir ayuda sobre el uso de **R**.
- El segundo se utiliza para discutir temas relacionados con el desarrollo de **R** o los paquetes de **R**. 

Además, se pueden informar errores y realizar solicitudes de funciones. La publicación de la guía en [http://www.R-project.org/posting-guide.html](http://www.R-project.org/posting-guide.html) es una buena estrategia para hacer esto de manera efectiva. En adición a estas listas de correo generales, existen listas para *Grupos de Interés Especial* o *Special Interest Groups (SIGs)*, entre ellas almenos una lista podría ser de interés para el lector: La cual se llama *R-SIG-Finance*, que se encuentra dedicada a las finanzas y la econometría (financiera).

#### Conceptos básicos en R {-}

Como se mencionó anteriormente, este curso no pretende ser una introducción a **R**, sino una guía sobre cómo usar sus capacidades para aplicaciones que se encuentran comúnmente en econometría de pregrado. Aquellos que tengan conocimientos básicos en programación en **R** se sentirán cómodos comenzando con el Capítulo \@ref(TP). Sin embargo, esta sección está destinada a quienes no hayan trabajado con **R** o *RStudio* antes. Si al menos sabe cómo crear objetos y llamar a funciones, puede omitirlo. Si desea actualizar sus habilidades o tener una idea de cómo trabajar con *RStudio*, siga leyendo.

En primer lugar, inicie *RStudio* y abra un nuevo script **R** seleccionando *Archivo*, *Nuevo archivo*, *Script R*. En el panel del editor, escriba

```{r, 24, echo = T, eval = T, message = F, warning = F}
1 + 1
```

y hacer clic en el botón etiquetado como *Ejecutar* en la esquina superior derecha del editor. Al hacerlo, su línea de código se envía a la consola y el resultado de esta operación debe mostrarse justo debajo. Como puede ver, **R** funciona como una calculadora. Puede hacer todos los cálculos aritméticos utilizando el operador correspondiente (**+**, **-**, $\textbf{*}$, **/** o **^**). Si no está seguro de lo que hizo el último operador, pruébelo y compruebe los resultados.

#### Vectores {-}

**R** es, por supuesto, más sofisticado que eso. Se puede trabajar con variables o, más generalmente, con objetos. Los objetos se definen mediante el operador de asignación **<-**. Para crear una variable llamada **x** que contenga el valor **10**, se necesita escribir `x <- 10` y hacer clic en el botón *Ejecutar* una vez más. La nueva variable debería haber aparecido en el panel de entorno en la parte superior derecha. Sin embargo, la consola no mostró ningún resultado porque la línea de código no contiene ningúna llamada que creara salida. Cuando ahora se escribe `x` en la consola y se presiona retorno, le pide a **R** que muestre el valor de **x** y el valor correspondiente debe imprimirse en la consola.

**x** es un escalar, un vector de longitud $1$. Se pueden crear fácilmente vectores más largos usando la función **c()** (*c* es para "concatenar" o "combinar"). Para crear un vector **y** que contenga los números $1$ a $5$ e imprimirlo, se debe hacer lo siguiente.

```{r, 25, echo = T, eval = T, message = F, warning = F}
y <- c(1, 2, 3, 4, 5)

y
```

También se puede crear un vector de letras o palabras. Por ahora, solo se necesita recordar que los caracteres deben estar entre comillas; de lo contrario, se analizarán como nombres de objeto.

```{r, 26, echo = T, eval = T, message = F, warning = F}
hello <- c("Hola", "Mundo")

hello
```

Aquí se ha creado un vector de longitud 2 que contiene las palabras **Hola** y **Mundo**.

¡No olvidar guardar el script! Para hacerlo, seleccionar *Archivo* y *Guardar*.

#### Funciones {-}

Se ha visto la función **c()** que se puede usar para combinar objetos. En general, todas las llamadas a funciones tienen el mismo aspecto: el nombre de una función siempre va seguido de paréntesis. A veces, los paréntesis incluyen argumentos.

A continuación, se muestran dos ejemplos sencillos.

```{r, 27, echo = T, eval = T, message = F, warning = F}
# generar el vector `z`
z <- seq(from = 1, to = 5, by = 1)

# calcular la media de las entradas en `z`
mean(z)
```

En la primera línea se usó una función llamada **seq()** para crear exactamente el mismo vector que se hizo en la sección anterior, llamándolo **z**. La función toma los argumentos **from**, **to** y **by** que deberían ser autoexplicativos.

La función **mean()** calcula la media aritmética de su argumento **x**. Como se pasa el vector **z** como argumento **x**, ¡el resultado es **3**!

Si no se está seguro de qué argumentos se espera en una función, se puede consultar la documentación de la función. Digamos que no se está seguro de cómo funcionan los argumentos requeridos para **seq()**. Luego se escribe `?Seq` en la consola. Al presionar regresar, la página de documentación para esa función aparece en el panel inferior derecho de *RStudio*. Allí, la sección *Argumentos* contiene la información que se busca. En la parte inferior de casi todas las páginas de ayuda se encontrará ejemplos sobre cómo utilizar las funciones correspondientes. Esto es muy útil para los principiantes y se recomenda siempre buscar.

Por supuesto, todos los comandos presentados anteriormente también funcionan en el widgets interactivo, que se puede probar a continuación y a lo largo del curso al finalizar los apartados.

```{r, 28, echo = F, results = 'asis'}
write_html(playground = T)
```

## Ejemplos introductorios a sesiones típicas en R {#EISTP}

Esta breve sección, además de proporcionar dos ejemplos introductorios sobre el ajuste de modelos de regresión, describe algunas características básicas de **R**, incluyendo las facilidades y el modelo de desarrollo de software que contribuyen a programar de forma eficiente. Para los más interesados, la parte final de la presente sección describe brevemente la historia de **R**.

Para una primera impresión del software estadístico **R**, resulta necesario: "mirar y sentir". En consecuencia, se ofrecen ejemplos introductorios a sesiones típicas en **R**, en la que se analizarán brevemente dos conjuntos de datos. Lo anterior debería servir como una ilustración de cómo se pueden realizar las tareas básicas y cómo las operaciones empleadas se generalizan, modifican y amplían para tareas más avanzadas. Se sabe que no todos los detalles serán completamente expuestos de manera transparente en esta etapa, pero estos ejemplos deberían coadyuvar a dar una primera impresión de la funcionalidad y sintaxis de **R**. Las explicaciones sobre todos los detalles técnicos se posponen para los capítulos siguientes, donde se proporcionan análisis más completos.

### Ejemplo 1: La demanda de revistas de economía {-}

Se comienza con un pequeño conjunto de datos tomados de @stock2015 y Watson (2007) que proporciona información sobre el número de suscripciones de bibliotecas a revistas económicas en los Estados Unidos de América en el año 2000. El conjunto de datos, originalmente recopilado por Bergstrom (2001), está disponible en el paquete [@R-AER] con el nombre Journals. Se puede cargar via:

```{r, 29, echo = T, eval = T, message = F, warning = F} 
# paquete
data("Journals", package = "AER")
```

Los comandos necesarios para explorar el conjunto de datos son:

```{r, 30, echo = T, eval = T, message = F, warning = F} 
# comando
dim(Journals)

# comando
names(Journals)
```

Los comandos revelan que **Journals** es un conjunto de datos con $180$ observaciones (las revistas) sobre $10$ variables, incluido el número de suscripciones a la bibliotecas (**subs**), el precio (**price**), el número de citas (**citations**) y una variable cualitativa que indica si la revista es publicada por una sociedad o no (**society**).

Aquí, interesa investigar la relación entre la demanda de revistas de economía y su precio. Una medida adecuada del precio de las revistas científicas es el precio por cita. Una gráfica de dispersión (en logaritmos) se puede obtener via:

```{r, 31, eval = T, message = F, warning = F, fig.align='center'} 
# gráfica de dispersión de la suscripción a la biblioteca 
# a partir del precio por cita (ambos en logaritmos) 
plot(log(subs) ~ log(price/citations), data = Journals)
```

La gráfica muestra claramente que el número de suscripciones disminuye con el precio.

El modelo de regresión lineal correspondiente se puede ajustar fácilmente mediante mínimos cuadrados ordinarios (MCO) utilizando la función **lm()** (para un modelo lineal) y la misma sintaxis:

```{r, 32, echo = T, eval = T, message = F, warning = F} 
# modelo de regresión lineal correspondiente a la suscripción a la biblioteca 
# a partir del precio por cita (ambos en logaritmos)
j_lm <- lm(log(subs) ~ log(price/citations), data = Journals)

# agregar línea de mínimos cuadrados
plot(log(subs) ~ log(price/citations), data = Journals)
abline(j_lm)
```

El comando **abline()** agrega la línea de mínimos cuadrados a la gráfica de dispersión construida anteriormente. Un resumen detallado del modelo ajustado **j_lm** se puede obtener via:

```{r, 33, echo = T, eval = T, message = F, warning = F} 
summary(j_lm)
```

Específicamente, el comando anterior proporciona el resumen habitual de los coeficientes (con estimaciones, errores estándar, estadísticos de prueba y valores p) así como la $R^2$ asociada, junto con otra información. Para la regresión de las revistas, la elasticidad estimada de la demanda respecto al precio por cita es de $-0.5331$, que es significativamente diferente de $0$ en todos los niveles convencionales. La $R^2 = 0.557$ del modelo es bastante satisfactorio para una regresión de corte transversal. 

Un análisis más detallado con información adicional, emplenado comando mucho más complejos de **R**, se proporcionan en \@ref(RNFRECD).

### Ejemplo 2: Determinantes de los salarios {-}

En el ejemplo anterior, se mostró cómo ajustar un modelo de regresión lineal simple para obtener una primera impresión de la apariencia y sensación de **R**. Los comandos para llevar a cabo el análisis a menudo se escriben casi por completo en inglés, lo que facilita la forma en que se comunica con el sistema. Para realizar tareas más complejas, los comandos también se vuelven más técnicos; sin embargo, las ideas básicas siguen siendo las mismas. Por lo tanto, todos deberían poder seguir el análisis y reconocer muchas de las estructuras del ejemplo anterior, incluso si en el presente curso no se explican a detalle todos las funciones. Una vez más, el propósito es brindar un ejemplo motivador que ilustre la facilidad con la que se pueden realizar algunas tareas más avanzadas en R. En los capítulos siguientes se proporcionan más detalles, tanto sobre los comandos como sobre los datos.

La aplicación considerada aquí es la estimación de una ecuación salarial en forma semilogarítmica basada en datos tomados de Berndt (1991). Representan una submuestra aleatoria de datos de corte transversal originados en la *Encuesta de Población Actual* de mayo de 1985, que comprende $533$ observaciones. Después de cargar el conjunto de datos **CPS1985** del paquete **AER**. En este caso, primero se necesita cambiar el nombre por alguno de conveniencia:

```{r, 34, echo = T, eval = T, message = F, warning = F} 
data("CPS1985", package = "AER")

cps <- CPS1985
```

Para **cps**, se debe estimar una ecuación salarial con **log(wage)** como la variable dependiente y educación (**education**) y experiencia(**experience**), ambos en número de años, como regresores. Para experiencia (**experience**), también se incluye un término cuadrático. Primero, se estima un modelo de regresión lineal múltiple por MCO (nuevamente a través de **lm()**). Luego, las regresiones de cuantiles se ajustan usando la función **rq()** del paquete **quantreg**. En cierto sentido, la regresión por cuantiles es un perfeccionamiento del modelo de regresión lineal estándar en el sentido de que proporciona una visión más completa de toda la distribución condicional (mediante la elección de cuantiles seleccionados), no solo la media condicional. Sin embargo, la principal razón para seleccionar esta técnica es que ilustra que las funciones de ajuste de **R** para los modelos de regresión suelen poseer una sintaxis prácticamente idéntica. 

De hecho, en el caso de los modelos de regresión por cuantiles, todo lo que se necesita especificar, además de la fórmula y los argumentos de datos ya familiares, es **tau** (el conjunto de cuantiles que se modelarán). En este contexto, el argumento **tau** se establecerá en $0.2$, $0.35$, $0.5$, $0.65$, $0.8$. 

Después de cargar el paquete **quantreg**, ambos modelos se pueden procesar tan fácilmente como

```{r, 35, echo = T, eval = T, message = F, warning = F} 
# paquete
library("survival")
library("quantreg")

# primer modelo
cps_lm <- lm(log(wage) ~ experience + I(experience^2) + education, data = cps)

# segundo modelo
cps_rq <- rq(log(wage) ~ experience + I(experience^2) + education, data = cps, tau = seq(0.2, 0.8, by = 0.15))
```

Estos modelos ajustados ahora podrían evaluarse numéricamente mediante la función **summary()**, como punto de partida. Más adelante se abordarán la diferentes formas en que se puede diagnósticar una regresión a detalle, así como los métodos alternativos. No obstante, en este punto se necesitan evaluar gráficamente ambos modelos; en particular, lo que respecta a la relación entre salarios y años de experiencia. Por lo tanto, se calculan predicciones de ambos modelos para un nuevo conjunto de datos. En el nuevo conjunto **cps2**, la variable educación (**education**) se mantendrá constante en su media, mientras que la variable experiencia (**experience**) varía en el rango de la variable original:

```{r, 36, echo = T, eval = T, message = F, warning = F} 
cps2 <- data.frame(education = mean(cps$education),
                   experience = min(cps$experience):max(cps$experience))

cps2 <- cbind(cps2, 
              predict(cps_lm, newdata = cps2, interval = "prediction"))

cps2 <- cbind(cps2,
              predict(cps_rq, newdata = cps2, type = ""))
```

Para ambos modelos, las predicciones se calculan utilizando el respectivo métodos **predict()** y enlazando los resultados como nuevas columnas a **cps2**. Primero, se visualizan los resultados de las regresiones de cuantiles en un gráfica de dispersión de **log(wage)** contra la experiencia, agregando las líneas de regresión para todos los cuantiles (en el nivel medio de educación):

```{r, 37, eval = T, message = F, warning = F, fig.align='center'} 
# gráfica de dispersión del logaritmo del salario frente a la experiencia
plot(log(wage) ~ experience, data = cps)

# ajuste de regresión de cuantiles para cuantiles variables
for(i in 6:10) lines(cps2[,i] ~ experience, data = cps2, col = "red")
```

Para mantener el código compacto, todas las líneas de regresión se agregan en un ciclo **for()**. El gráfico resultante muestra que los salarios son más altos para las personas con alrededor de 30 años de experiencia. La curvatura de las líneas de regresión es más marcada en los cuartiles inferiores, mientras que la relación es mucho más plana para los cuantiles superiores. La gráfica anterior también se puede ver en gráficas individuales, que se pueden obtener via:

```{r, 38, eval = T, message = F, warning = F, fig.align='center'} 
# coeficientes de regresión cuantílica para cuantiles variables 
# bandas de confianza (gris) y estimación de mínimos cuadrados (rojo)
plot(summary(cps_rq))
```

El gráfico describe los cambios en los coeficientes de regresión sobre cuantiles variables junto con las estimaciones de mínimos cuadrados (ambos junto con intervalos de confianza del $90\%$). Esto muestra que ambos coeficientes de experiencia (**experience**) eventualmente disminuyen en tamaño absoluto (se debe tomar en cuenta que el coeficiente del término cuadrático es negativo) al aumentar el cuantil y que, en consecuencia, la curva es más plana para los cuantiles más altos. El intercepto (la intersección) también aumenta, mientras que la influencia de educación (**education**) no varía tanto con el nivel de cuantiles.

Aunque el tamaño de la muestra en esta ilustración es todavía bastante modesto para los estándares actuales, con $533$ observaciones, muchas observaciones están ocultas debido a la superposición de los diagramas de dispersión. Para evitar este problema y para ilustrar mejor algunas de las comodidades gráficas de **R**. 

De igual forma, las estimaciones de densidad del kernel se puede utilizar: las regiones de alta densidad frente a las de baja densidad en la distribución bivariada pueden identificarse mediante una estimación de densidad de kernel bivariada y mostrarse gráficamente en un mapa de calor. En **R**, se puede obtener la estimación de densidad de kernel bivariada por **bkde2D()** en el paquete **KernSmooth**: 

```{r, 39, eval = T, message = F, warning = F, fig.align='center'} 
# paquete KernSmooth
library("KernSmooth")

# mapa de calor de densidad de kernel bivariado del logaritmo de salario por experiencia
cps_bkde <- bkde2D(cbind(cps$experience, log(cps$wage)), 
                   bandwidth = c(3.5, 0.5), 
                   gridsize = c(200, 200))
```

Como **bkde2D()** no tiene una interfaz de fórmula (a diferencia de **lm()** o **rq()**), por tanto, se deben extraer las columnas relevantes del conjunto de datos **cps** y se seleccionan los anchos de banda y el tamaño de cuadrícula adecuados. Las $200 \times 200$ matrices resultantes de estimaciones de densidad se puede visualizar en un mapa de calor utilizando niveles de gris que codifican los valores de densidad. **R** proporciona la función **image()** ( o **contour()**) para producir tales pantallas, que se pueden aplicar a **cps_bkde** como sigue:

```{r, 40, echo = T, eval = T, message = F, warning = F} 
image(cps_bkde$x1, cps_bkde$x2, cps_bkde$fhat,
      col = rev(gray.colors(10, gamma = 1)),
      xlab = "experience", ylab = "log(wage)")

box()

# ajuste de mínimos cuadrados e intervalo de predicción
lines(fit ~ experience, data = cps2)
lines(lwr ~ experience, data = cps2, lty = 2)
lines(upr ~ experience, data = cps2, lty = 2)
```

Después de dibujar el mapa de calor en sí, se agrega una la línea de regresión para el ajuste del modelo lineal junto con los intervalos de predicción. Comparado con el diagrama de dispersión, esto pone de manifiesto más claramente la relación empírica entre **log(wage)** y **experience**.

Esto concluye los ejemplos introductorios a sesiones típicas en **R**. Más detalles sobre los conjuntos de datos, modelos y funciones de **R** se proporcionan en los siguientes capítulos.

## Una breve historia de R {#UBHR}

Como se señaló anteriormente, el sistema de computación estadística y gráficos **R** (*R Development Core Team 2021b*, [http://www.R-project.org/](http://www.R-project.org/)) es un proyecto de software de código abierto. La historia comienza en Bell Laboratories (de AT&T, ahora Alcatel-Lucent en Nueva Jersey), con el lenguaje *S*, un sistema de análisis de datos desarrollado por John Chambers, Rick Becker y colaboradores desde finales de la década de 1970. En este caso, *Landmarks of the development of S* (Hitos del desarrollo de S) son una serie de libros, a los que se hace referencia por color en la comunidad *S*:

1. Se comienza con el "*libro marrón*" (Becker y Chambers 1984), que presenta "*Old S*”. 
2. La referencia básica para "*New S*", o la versión 2 de *S*, es Becker, Chambers y Wilks (1988). Se conoce como el "*libro azul*".
3. Para la versión 3 de *S* (programación orientada a objetos de primera generación y modelado estadístico), la referencia es Chambers y Hastie (1992). Se conoce como el “*libro blanco*”. 
4. Para la versión 4 de *S* (Chambers 1998). Se conoce como el "libro verde".

El "libro verde" está fundamentado en las distintas versiones de *S*, *Insightful Corporation* (anteriormente *MathSoft* y aún antes *Statistical Sciences*) ha proporcionado una versión comercialmente mejorada y respaldada de *S*, llamado *S-PLUS*, desde 1987. En esencia, esto incluye la original implementación de *S*, que fue licenciada exclusivamente por primera vez y finalmente comprada en 2004. El 23 de marzo de 1999, la Association for Computing Machinery (ACM) nombró a John Chambers como el destinatario de su premio 1998 Software System Award por desarrollar el sistema *S*, señalando que su trabajo: 

"Alterará para siempre la forma en que las personas analizan, visualizan y manipulan los datos".

**R** en sí mismo fue desarrollado inicialmente por Robert Gentleman y Ross Ihaka en la Universidad de Auckland, Nueva Zelanda. Sus inventores describen una versión temprana en un artículo (Ihaka y Gentleman 1996). Diseñaron el idioma para combinar las fortalezas de dos idiomas existentes, *S* y *Scheme* (Steel y Sussman 1975). En palabras de Ihaka y Gentleman (1996):

“[El] lenguaje resultante es muy similar en apariencia a *S*, pero la implementación subyacente y la semántica se derivan de *Scheme*.” 

El resultado fue bautizado como **R**: 

“En parte para reconocer la influencia de *S* y en parte para celebrar [sus] propios esfuerzos.”

El código fuente de **R** fue lanzado por primera vez bajo la Licencia Pública General (LPG) de GNU (General Public License (GPL) de GNU) en 1995. Desde mediados de 1997, ha existido el Equipo Central de Desarrollo R (R Development Core Team), que actualmente consta de 19 miembros y sus nombres están disponibles al escribir la función **contributors()** en una sesión normal de R. En 1998 se estableció la Red Completa de Archivos R ([Comprehensive R Archive Network](http://CRAN.R-project.org/) o [CRAN](http://CRAN.R-project.org/)), que es una familia de sitios espejo en todo el mundo que almacenan versiones idénticas y actualizadas de código y documentación para **R**. 

El primer lanzamiento oficial fue la versión 1.0.0 de **R**, con fecha de 2000-02-29, que implementa el software *S3* según lo documentado por Chambers y Hastie (1992). La versión 2.0.0 de **R** se lanzó en 2004. La versión 2.7.0 del lenguaje **R**, puede verse como una implementación *S4* (Chambers 1998) con numerosos conceptos que van más allá de los diversos estándares de *S*.

La primera publicación de **R** en la literatura de econometría parece haber sido por Cribari-Neto y Zarkos (1999), una revisión de software en el *Journal of Applied Econometrics* titulado "*R: Yet Another Econometric Programming Environment*" ("*R: Otro entorno más de programación econométrica*”). El artículo describe la versión 0.63.1 de **R**, en ese entonces todavía era una versión beta. Tres años más tarde, en una revisión de software adicional en la misma revista, Racine y Hyndman (2002) se centraron en el uso de **R** para enseñar econometría utilizando la versión 1.3.1 de **R**. Hasta donde se sabe, este curso es la primera introducción general a la *Econoalgoritmia* en **R**: Creación e implementación de algoritmos computacionales en economía aplicada haciendo uso de herramientas y métodos propios de la econometría y ciencia de datos.

<!--chapter:end:Capitulo_01.Rmd-->

# Repaso de las nociones fundamentales de R para la econometría y ciencia de datos {#RNFRECD}

```{r, echo = F}
options(knitr.duplicate.label = "allow")
```

```{r, 41, child="_setup.Rmd"}
```

```{r, 42, eval=knitr::opts_knit$get("rmarkdown.pandoc.to") == "html", results='asis', echo=FALSE}
cat('<hr style="background-color:#03193b;height:2px">')
```

El software estadístico **R** se puede utilizar en varios niveles. Por supuesto, la aritmética estándar está disponible y, por lo tanto, se puede usar como una calculadora (bastante sofisticada). También está provisto de un sistema gráfico que escribe en una gran variedad de dispositivos. Adicionalmente, **R** es un lenguaje  de programación con todas las funciones que se pueden emplear para abordar todas las tareas típicas para las que también se utilizan otros lenguajes de programación. Asimismo, se conecta a otros lenguajes, programas y bases de datos, y también al sistema operativo; los usuarios pueden controlar todo esto desde la plataforma de **R**.

En este capítulo, se ilustran algunos de los usos típicos de **R**. A menudo, las soluciones no son únicas; en este sentido, se evitan los atajos sofisticados. Sin embargo, se anima a todos los lectores a explorar soluciones alternativas reutilizando lo aprendido en el curso en otros contextos.

## **R** como calculadora {#RCC}

Los operadores aritméticos estándar **+**, **-**, $\textbf{*}$, **/** y **^** están disponibles, donde **x^y** representa la exponenciación **$x^y$**. Por eso

```{r, 43, echo = T, eval = T, message = F, warning = F} 
1 + 1
2^3
```

En la salida, **[1]** indica la posición del primer elemento del vector devuelto por **R**. Dicho resultado no es sorprendente en este contexto, donde todos los vectores tienen una longitud $1$, pero será útil más adelante.

Las funciones matemáticas comunes, como **log()**, **exp()**, **sin()**, **asin()**, **cos()**, **acos()**, **tan()**, **atan()**, **sign()**, **sqrt()**, **abs()**, **min()** y **max()**, también están disponibles. Específicamente, **log(x, base = a)** devuelve el logaritmo de $x$ a la base $a$, dónde $a$ se encuentra predeterminado por **exp(1)**. Por lo tanto

```{r, 44, echo = T, eval = T, message = F, warning = F} 
log(exp(sin(pi/4)^2) * exp(cos(pi/4)^2))
```

que también muestra que $pi$ es una constante incorporada. Existen otras funciones de conveniencia, como **log10()** y **log2()**, pero aquí se usará principalmente **log()**. Una lista completa de todas las opciones y funciones relacionadas está disponible al escribir **?log**, **?sin**, entre otros. Las funciones adicionales útiles en estadística y econometría son **gamma()**, **beta()**, y sus logaritmos y derivadas (se recomienda consultar **?gama** para mayor información).

### Aritmética vectorial {-}

En **R**, la unidad básica es un vector y, por ende, todas estas funciones operan directamente sobre los vectores. Se genera un vector usando la función **c()**, dónde **c** significa "combinar" o "concatenar". Así pues, el siguiente código genera un objeto $x$, un vector, que contiene las entradas $1.8$, $3.14$, $4$, $88.169$ y $13$:

```{r, 45, echo = T, eval = T, message = F, warning = F} 
x <- c(1.8, 3.14, 4, 88.169, 13)
```

La longitud de un vector está disponible usando **length()**:

```{r, 46, echo = T, eval = T, message = F, warning = F} 
length(x)
```

Se debe tener en cuenta que la asignación de nombres distingue entre mayúsculas y minúsculas; por eso $X$ y $x$ son distintos.

La declaración anterior usa el operador de asignación **<-**, que debe leerse como un solo símbolo (aunque requiere dos pulsaciones de teclas), una flecha que apunta a la variable a la que se asigna el valor. Alternativamente, **=** puede usarse a nivel de usuario, pero como se prefiere **<-** para la programación, se usa en todo este curso. No existe un resultado inmediatamente visible, pero a partir de ahora **x** tiene como valor el vector definido anteriormente y, por esta razón, puede utilizarse en cálculos posteriores:

```{r, 47, echo = T, eval = T, message = F, warning = F} 
2 * x + 3
5:1 * x + 1:5
```

Esto requiere una explicación. En el primer enunciado, los escalares (es decir, vectores de longitud $1$) $2$ y $3$ se reciclan para coincidir con la longitud de **x**, de modo que cada elemento de **x** se multiplica por $2$ antes de agregarle $3$. En la segunda declaración, **x** se multiplicación por los elementos en el vector **5:1** (la secuencia de $1$ a $5$ que se puede ver arriba) y luego el vector **1:5** se agrega elemento por elemento.

También se pueden aplicar funciones matemáticas; así pues, el siguiente comando devuelve un vector que contiene los logaritmos de las entradas originales de **x**:

```{r, 48, echo = T, eval = T, message = F, warning = F} 
log(x)
```

### Vectores de subconjuntos {-}

A menudo es necesario acceder a subconjuntos de vectores. Lo anterior requiere hacer uso del operador **[,**, que se puede utilizar de varias formas para extraer elementos de un vector. Por ejemplo, se puede especificar qué elementos incluir o qué elementos excluir.

Un vector de índices positivos, como el que se codifica a continuación, especifica los elementos a extraer:

```{r, 49, echo = T, eval = T, message = F, warning = F} 
x[c(1, 4)]
```

Alternativamente, un vector de índices negativos, como el que se codifica a continuación, selecciona todos los elementos menos los indicados (obteniendo el mismo resultado de antes):

```{r, 50, echo = T, eval = T, message = F, warning = F} 
x[-c(2, 3, 5)]
```

De hecho, mucho otros métodos se encuentran disponibles para realizar subconjuntos con **[,** que se explican más abajo.

### Modelado de vectores {-}

En estadística y econometría, existen muchos casos en los que se necesitan vectores con patrones especiales. **R** proporciona una serie de funciones para crear dichos vectores, que incluyen:

```{r, 51, echo = T, eval = T, message = F, warning = F} 
ones <- rep(1, 10)
even <- seq(from = 2, to = 20, by = 2)
trend <- 1981:2005
```

Aquí, **ones** es un vector de unos de longitud $10$, **even** es un vector que contiene los números pares del $2$ al $20$ y **trend** es un vector que contiene los números enteros de $1981$ a $2005$.

Dado que el elemento básico es un vector, también es posible concatenar vectores. Por lo tanto:

```{r, 52, echo = T, eval = T, message = F, warning = F} 
c(ones, even)
```

El código anterior crea un vector de longitud $20$ que consta de los vectores previamente definidos **ones** y **even** puesto de cabo a rabo.

## Operaciones con matrices {#OCM}

Una matriz $2 \times 3$ que contiene los elementos $1:6$, por columna, se genera a través de:

```{r, 53, echo = T, eval = T, message = F, warning = F} 
A <- matrix(1:6, nrow = 2)
```

Alternativamente, **ncol** podría haber sido usado, con **matrix(1:6, ncol = 3)** dando el mismo resultado:

```{r, 54, echo = T, eval = T, message = F, warning = F} 
matrix(1:6, ncol = 3)
```

### Álgebra básica matricial {-}

La transposición $A^T$ de $A$ es:

```{r, 55, echo = T, eval = T, message = F, warning = F} 
t(A)
```

Se puede acceder a las dimensiones de una matriz utilizando **dim()**, **nrow()** y **ncol()**; por eso:

```{r, 56, echo = T, eval = T, message = F, warning = F} 
dim(A)
nrow(A)
ncol(A)
```

Los elementos individuales de una matriz, los vectores de fila o columna, e incluso las submatrices completas, pueden extraerse especificando las filas y columnas de la matriz de la que se seleccionan. Lo anterior usa una extensión simple de las reglas para subconjuntos de vectores. En realidad, internamente, las matrices son vectores con un atributo de dimensión que habilita la indexación de tipo fila/columna: 

- El elemento $a_{ij}$ de una matriz $A$ se extrae usando $A[i, j]$. 
- Se pueden extraer filas o columnas enteras mediante $A[i, ]$ y $A[, j]$, respectivamente, que devuelven la fila o columna correspondiente en forma de *vectores*. Esto significa que el atributo de dimensión se descarta (por defecto); por esta razón, el subconjunto devolverá un vector en lugar de una matriz si la matriz resultante tiene solo una columna o fila. 

Ocasionalmente, es necesario extraer filas, columnas o incluso elementos individuales de una matriz, como los es una matriz misma. La eliminación del atributo de dimensión se puede desactivar usando **A[i, j, drop = FALSE]**.

Como ejemplo, el código que se muestra a continuación selecciona una matriz cuadrada que contiene el primer y tercer elemento de cada fila (se debe tener en cuenta que $A$ tiene solo dos filas en el ejemplo):

```{r, 57, echo = T, eval = T, message = F, warning = F} 
A1 <- A[1:2, c(1, 3)]
```

Alternativamente, y de forma más compacta, $A1$ podría haber sido generado usando $A[, -2]$. Resulta importante mencionar que si no se especifica ningún número de fila, se tomarán todas las filas; el $-2$ especifica que se requieren todas las columnas excepto la segunda.

$A1$ es una matriz cuadrada, y si no es singular tiene una inversa. Una forma de verificar la singularidad es calcular el determinante usando la función **det()** disponible en **R**. Aquí, `det(A1)`es igual a $-4$; por eso **A1** no es singular. Alternativamente, sus autovalores (y autovectores) están disponibles usando **eigen()**. Aquí, `eigen(A1)`produce los valores propios $7.531$ y $-0.531$, mostrando nuevamente que $A1$ no es singular.

La inversa de una matriz, si no se puede evitar, se calcula utilizando **solve()**:

```{r, 58, echo = T, eval = T, message = F, warning = F} 
solve(A1)
```

Se puede comprobar que esto es de hecho el inverso de $A1$ multiplicando $A1$ con su inverso. Esto requiere el operador para la multiplicación de matrices, $\textbf{%*%}$:

```{r, 59, echo = T, eval = T, message = F, warning = F} 
A1 %*% solve(A1)
```

De manera similar, las matrices conformables se suman y restan usando los operadores aritméticos **+** y **-**. Vale la pena señalar que en el caso de matrices no conformables, el reciclaje se realiza a lo largo de las columnas. Por cierto, el operador $\textbf{*}$ también funciona para matrices; devuelve la matriz de elementos o el *producto de Hadamard* de matrices conformables. Otros tipos de productos matriciales que a menudo se requieren en econometría son el *producto Kronecker*, disponible a través de la función **kronecker()**, y el producto cruzado $A^{T} B$ de dos matrices, para las cuales se implementa un algoritmo computacionalmente eficiente en la función **crossprod()**.

Además de la descomposición espectral calculada por **eigen()** como se ha mencionado más arriba, **R** proporciona otras descomposiciones matriciales de uso frecuente, incluida la *descomposición de valor singular*, mediante la función **svd()**, la descomposición *QR*, mediante la función **qr()**, y la *descomposición de Cholesky*, mediante la función **chol()**.

### Modelado de matrices {-}

En econometría, existen muchos casos en los que se necesitan construir matrices con patrones especiales. **R** proporciona funciones para generar matrices con tales patrones. Por ejemplo, se puede crear una matriz diagonal con unos en la diagonal usando

```{r, 60, echo = T, eval = T, message = F, warning = F} 
diag(4)
```

que produce la matriz de identidad de $4 \times 4$. De manera equivalente, el mismo resultado se puede obtener mediante **diag(1, 4, 4)**, donde el $1$ se recicla a la longitud requerida $4$. Por supuesto, también se obtienen fácilmente matrices diagonales más generales: **diag(rep(c(1,2), c(10, 10)))** produce una matriz diagonal de tamaño $20 \times 20$ cuyos primeros $10$ elementos diagonales son iguales a $1$, mientras que los restantes son iguales a $2$ (los lectores con conocimientos básicos de regresión lineal notarán que una aplicación podría ser un patrón de heterocedasticidad).

Aparte de configurar matrices diagonales, la función **diag()** también se puede utilizar para extraer la diagonal de una matriz existente; por ejemplo, **diag(A1)**. Adicionalmente, **upper.tri()** y **lower.tri()** se pueden utilizar para consultar las posiciones de los elementos
triangulares superiores o inferiores de una matriz, respectivamente.

### Combinando matrices {-}

También es posible formar nuevas matrices a partir de las existentes para hacer eso se deben usar las funciones **rbind()** y **cbind()**, que son similares a la función **c()** para concatenar vectores; como sugieren sus nombres, combinan matrices por filas o columnas. Por ejemplo, para agregar una columna de unos a la matriz $A1$ se puede usar:

```{r, 61, echo = T, eval = T, message = F, warning = F} 
cbind(1, A1)
```

De igual forma, se puede lograr el mismo resultado combinando $A1$ y $diag(4, 2)$ por filas:

```{r, 62, echo = T, eval = T, message = F, warning = F} 
rbind(A1, diag(4, 2))
```

### R como lenguaje de programación {-}

**R** es un lenguaje de programación orientado a objetos, interpretado y con todas las funciones. En consecuencia, se puede usar para todas las tareas que también se usan en otros lenguajes de programación, no solo para el análisis de datos. Lo que lo hace particularmente útil para la estadística y econometría; es decir, **R** fue diseñado para “programar con datos” (Chambers 1998). Esto tiene varias implicaciones para los tipos de datos empleados y el paradigma utilizado de orientación a objetos.

Un tratamiento en profundidad de la programación en **S**/**R** se da en Venables y Ripley (2000). Si el lector posee conocimientos básicos de alemán, Ligges (2007) es una excelente introducción a la programación con **R**. En un nivel más avanzado, el equipo central de desarrollo de **R** (2021f, g) proporciona orientación sobre la definición del lenguaje y cómo se puede escribir las extensiones del sistema **R**. Los últimos documentos se pueden descargar de *CRAN* y también se envían con cada distribución de **R**.

### La moda de un vector {-}

Probablemente la estructura de datos más simple en **R** es un vector. Todos los elementos de un vector deben ser del mismo tipo; técnicamente, deben ser de la mismo "moda". La moda de un vector $x$ se puede consultar usando **mode(x)**. En este contexto, se pueden crear vectores de moda "numérica", "lógica" y de "caracteres” (aunque existen otras).

Ya se ha visto que crea un vector numérico cumple con la aplicación típica de almacenar los valores de alguna variable numérica en un conjunto de datos.

```{r, 63, echo = T, eval = T, message = F, warning = F} 
x <- c(1.8, 3.14, 4, 88.169, 13)
```

### Vectores lógicos y de caracteres {-}

Los vectores lógicos se componen de las constantes lógicas **TRUE** (cierto) y **FALSE** (falso). En una nueva sesión, los alias **T** y **F** también con compatibles con los operadores lógicos en **R** (esto es, también son utilizados como constantes lógicas). Sin embargo, a diferencia de **TRUE** y **FALSE**, los valores de **T** y **F** puede cambiarse (por ejemplo, utilizando **T** para representar el tamaño de una muestra en un contexto de series de tiempo o utilizando **F** como una variable para denotar un estadístico **F**), por lo que se recomienda no confiar en ellos, sino utilizar siempre **TRUE** y **FALSE**. 

Al igual que los vectores numéricos, los vectores lógicos se pueden crear desde cero. De igual forma, pueden surgir como resultado de una comparación lógica:

```{r, 64, echo = T, eval = T, message = F, warning = F} 
x > 3.5
```

A continuación se explican otras operaciones lógicas.

Se pueden emplear vectores de caracteres para almacenar cadenas. Especialmente en los primeros capítulos de este curso, se usarán principalmente para asignar etiquetas o nombres a ciertos objetos como vectores y matrices. Por ejemplo, se pueden asignar nombres a
los elementos de $x$ mediante:

```{r, 65, echo = T, eval = T, message = F, warning = F} 
names(x) <- c("a", "b", "c", "d", "e")
x
```

Alternativamente, se podría haber usado **names(x) <- letters[1:5]**, donde **letters** y **LETTERS** son vectores integrados que contienen las $26$ letras minúsculas y mayúsculas del alfabeto latino, respectivamente. Aunque no se usan mucho en este curso, se puede notar aquí que las facilidades de manipulación de caracteres en **R** va mucho más allá de estos simples ejemplos, permitiendo, entre otras cosas, cálculos en documentos de texto o cadenas de comandos.

### Más sobre subconjuntos {-}

Habiendo introducido vectores de modas numéricas, de caracteres y lógicos, es útil revisar cómo crear subconjunto de vectores. A estas alturas, se ha visto cómo extraer partes de un vector usando índices numéricos, pero de hecho esto también es posible usando caracteres (si existe un atributo **names**) o lógicos (en cuyo caso los elementos correspondientes a **TRUE** son seleccionados). Por lo tanto, los siguientes comandos producen el mismo resultado:

```{r, 66, echo = T, eval = T, message = F, warning = F} 
x[3:5]
x[c("c", "d", "e")]
x[x > 3.5]
```

El subconjunto de matrices (y también de marcos de datos o matrices multidimensionales) funciona de manera similar.

### Listas {-}

Hasta ahora, solo se han utilizado vectores planos. Ahora se procede a introducir algunas estructuras de datos relacionadas que son similares, pero contienen más información.

En **R**, las *listas* son *vectores genéricos*, donde cada elemento puede ser virtualmente cualquier tipo de objeto (de moda arbitraria); por ejemplo, un vector, una matriz, un marco de datos completo, una función o nuevamente una lista. Se debe tener en cuenta que este último también permite crear estructuras de datos recursivas. Debido a esta flexibilidad, las listas son la base para la mayoría de los objetos complejos en **R**; por ejemplo, para marcos de datos o modelos de regresión ajustados (por nombrar dos ejemplos que se describirán más adelante).

Como ejemplo simple, se crea, usando la función **list()**, una lista que contiene una muestra de una distribución normal estándar (generada con **rnorm()**) más algunas marcas en forma de una cadena de caracteres ("distribución normal") y una lista que contiene los parámetros de población (**list(mean = 0, sd = 1)**).

```{r, 67, echo = T, eval = T, message = F, warning = F} 
mylist <- list(sample = rnorm(5), 
               family = "distribución normal", 
               parameters = list(mean = 0, sd = 1))

mylist$sample
mylist$family
mylist$parameters
mylist$parameters$mean
mylist$parameters$sd
```

Para seleccionar ciertos elementos de una lista, se pueden usar los operadores de extracción **$** o **[[**. Este último es similar a **[,** la principal diferencia es que solo puede seleccionar un único elemento. Por lo tanto, las siguientes declaraciones son equivalentes:

```{r, 68, echo = T, eval = T, message = F, warning = F} 
mylist[[1]]
mylist[["sample"]]
mylist$sample
```

El tercer elemento de **mylist**, al ser una lista, las funciones del extractor también se pueden combinar en ella como se hizo antes:

```{r, 69, echo = T, eval = T, message = F, warning = F} 
mylist[[3]]$sd
```

### Comparaciones lógicas {-}

**R** tiene un conjunto de funciones que implementan comparaciones lógicas estándar, así como algunas funciones adicionales que son convenientes cuando se trabaja con valores lógicos. Los operadores lógicos son **<**, **<=**, **>**, **>=**, **==** (para igualdad exacta) y **!=** ( para la desigualdad). De igual forma, si **expr1** y **expr2** son expresiones lógicas, entonces: 

- **expr1&expr2** representa la intersección entre **expr1** y **expr2** (el operador lógico "**&**" funciona como la intersección "**y**").
- **expr1|expr2** representa la unión entre **expr1** o **expr2** (el operador lógico "**|**" funciona como la unión "**o**").
- **!expr1** representa la negación de **expr1** (el operador lógico "**!**" funciona como la negación "**¬**"). 

Por lo tanto

```{r, 70, echo = T, eval = T, message = F, warning = F} 
x <- c(1.8, 3.14, 4, 88.169, 13)
x > 3 & x <= 4
```

Con el objetivo de evaluar para qué elementos de un vector una determinada expresión es **TRUE**, se puede usar la función **which()**:

```{r, 71, echo = T, eval = T, message = F, warning = F} 
which(x > 3 & x <= 4)
```

Las funciones especializadas which.min() y which.max() están disponibles para calcular la posición del mínimo y el máximo. En adición a **&** y **|**, las funciones **all()** y **any()** comprueban si todas o al menos algunas entradas de un vector son **TRUE**:

```{r, 72,echo = T, eval = T, message = F, warning = F} 
all(x > 3)
any(x > 3)
```

Se necesita cierta precaución al evaluar la igualdad exacta. Cuando se aplica a una entrada numérica, **==** no permite la representación finita de fracciones o el error de redondeo; de ahí situaciones como:

```{r, 73, echo = T, eval = T, message = F, warning = F} 
(1.5 - 0.5) == 1
(1.9 - 0.9) == 1
```

El error anterior ocurre debido a la aritmética de punto flotante (Goldberg 1991). Para estos fines se recomienda hacer uso de la función **all.equal()**:

```{r, 74, echo = T, eval = T, message = F, warning = F} 
all.equal(1.9 - 0.9, 1)
```

Además, la función **identical()** comprueba si dos objetos en **R** son exactamente idénticos.

Debido a la coerción, también es posible calcular directamente vectores lógicos utilizando aritmética ordinaria. Cuando se coacciona a numérico, **FALSE** se convierte en $0$ y **TRUE** se convierte en $1$, como en:

```{r, 75, echo = T, eval = T, message = F, warning = F} 
7 + TRUE
```

### Coerción {-}

Para convertir un objeto de un tipo o clase a otro diferente, **R** proporciona una serie de funciones de coerción, denominadas convencionalmente **as.foo()**, dónde *foo* es el tipo o clase deseada; por ejemplo, numérico (*numeric*), carácter (*character*), matriz (*matrix*), marco de datos (*data.frame*), entre muchos otros. Suelen ir acompañadas de un **is.foo()**, función que comprueba si un objeto es de un tipo o clase *foo*. Por tanto:

```{r, 76, echo = T, eval = T, message = F, warning = F} 
is.numeric(x)
is.character(x)
as.character(x)
```
 
En determinadas situaciones, la coersión también se ve forzada automáticamente por **R**; por ejemplo, cuando el usuario intenta poner elementos de diferentes clases en un solo vector (que solo puede contener elementos de la misma clase). Por ejemplo:

```{r, 77, echo = T, eval = T, message = F, warning = F} 
c(1, "a")
```

En este caso, el vector contiene un número y un caracter. No obstante, la coersión del carácter en **R** es mayor que la del número. 

### Generación de números aleatorios {-}

Para los entornos de programación en estadística y econometría, es vital disponer de buenos generadores de números aleatorios (random number generators o RNG), en particular para permitir a los usuarios realizar estudios de Monte Carlo. El RNG de **R** admite varios algoritmos (se recomienda ver **?RNG** para mas detalles). Aquí, se describen algunos comandos importantes.

El RNG se fundamenta en una “semilla aleatoria”, que es la base para la generación de números pseudoaleatorios. Configurando la semilla a un valor específico usando la función **set.seed()**, las simulaciones se pueden reproducir exactamente igual por otros usuarios. Por ejemplo, usando la función **rnorm()** para generar números aleatorios con una distribución normal:

```{r, 78, echo = T, eval = T, message = F, warning = F} 
set.seed(123)
rnorm(2)

rnorm(2)

set.seed(123)
rnorm(2)
```

Otra función básica para extraer muestras aleatorias, con o sin reemplazo de un conjunto finito de valores, es **sample()**. El valor predeterminado es *draw* (sin reemplazo), un vector del mismo tamaño que su argumento de entrada; es decir, para calcular una permutación de la entrada como en:

```{r, 79, echo = T, eval = T, message = F, warning = F} 
sample(1:5)
sample(c("masculino", "femenino"), size = 5, replace = TRUE, prob = c(0.2, 0.8))
```

El segundo comando extrae una muestra de tamaño $5$, con reemplazo, de los valores "**masculino**" y "**femenino**", que se escogen con probabilidades 0.2 y 0.8, respectivamente.

Arriba, ya se usó la función **rnorm()** para extraer de una distribución normal. Dicha función pertenece a una familia más amplia de funciones que son todas de la forma **r*dist*()**, dónde *dist* correspondiente a una familia distinta de distribución probabilística:

- **norm**
- **unif**
- **binom**
- **pois**
- **t**
- **f**
- **chisq** 

Todas las funciones enlistadas toman el tamaño de la muestra **n** como su primer argumento junto con otros argumentos que controlan los parámetros de la distribución respectiva. Por ejemplo, **rnorm()** toma **mean** (media) y **sd** (desviación estandar) como argumentos adicionales, con $0$ y $1$ como sus correspondientes valores predeterminados. Sin embargo, estas no son las únicas funciones disponibles para distribuciones estadísticas. Normalmente, también existen **d*dist*()**, **p*dist*()** y **q*dist*()**, que implementan la densidad, la función de distribución de probabilidad acumulada y la función de cuantiles (función de distribución inversa), respectivamente.

### Control de flujo {-}

Como la mayoría de los lenguajes de programación, **R** proporciona estructuras de control estándar como las declaraciones **if**/**else**, **for** (para bucles) y **while** (para bucles). Todas ellas tienen en común que una expresión **expr** se evalúa, ya sea condicional a una determinada **cond** (para **if** y **while**) o para una secuencia de valores (para **for**). La expresion **expr** en sí mismo puede ser una expresión simple o una expresión compuesta; es decir, típicamente un conjunto de expresiones simples encerradas entre llaves **{...}**. A continuación se presentan algunos breves ejemplos que ilustran su uso (se recomienda ver **?Control** para mayor información):

Una declaración **if**/**else** es de la forma:

```{r, 80, echo = T, eval = F, message = F, warning = F} 
if(cond) {
    expr1
} else {
    expr2
}
```

Dónde **expr1** se evalúa si **cond** es **TRUE**, de lo contrario se evalúa **expr2**. La rama **else** se puede omitir si está vacía. Un ejemplo simple (si no muy significativo) es:

```{r, 81, echo = T, eval = T, message = F, warning = F}
x <- c(1.8, 3.14, 4, 88.169, 13)
if(rnorm(1) > 0) sum(x) else mean(x)
```

Donde la condición es el valor de un número aleatorio normal estándar, y la evaluación calcula la suma o la media del vector **x**. 

Tenga en cuenta que la condición **cond** solo puede ser de longitud $1$. Sin embargo, también existe una función **ifelse()** ofreciendo una
versión vectorizada; por ejemplo:

```{r, 82, echo = T, eval = T, message = F, warning = F}
ifelse(x > 4, sqrt(x), x^2)
```

Esto calcula la raíz cuadrada de los valores de **x** que son mayores que $4$ y el cuadrado para los restantes.

El bucle **for** se construye de manera similar, pero el argumento principal para **for()** es de tipo variable en secuencia. Para ilustrar su uso, se calculan recursivamente las primeras diferencias en el vector **x**.

```{r, 83, echo = T, eval = T, message = F, warning = F}
for(i in 2:5) {x[i] <- x[i] - x[i-1]}
x[-1]
```

Finalmente, un bucle **while** se ve bastante similar. El argumento para **while()** es una condición que puede cambiar en cada ejecución del bucle para que finalmente pueda convertirse en **FALSE**, como en:

```{r, 84, echo = T, eval = T, message = F, warning = F}
while(sum(x) < 100) {x <- 2 * x}
x
```

### Escritura de funciones {-}

Una de las características de **S** y **R** es que los usuarios se convierten naturalmente en desarrolladores. En otras palabras, crean variables u objetos que aplican a funciones de forma interactiva (ya sea para modificarlos o para crear otros objetos de interés), lo que es parte de cada sesión en **R**. Al hacerlo, a menudo surgen secuencias típicas de comandos que se ejecutan para diferentes conjuntos de valores de entrada. En lugar de repetir los mismos pasos "a mano", se pueden integrar fácilmente nuevas funciones. Un ejemplo simple es:

```{r, 85, echo = T, eval = T, message = F, warning = F}
cmeans <- function(X) {
    rval <- rep(0, ncol(X))
    for(j in 1:ncol(X)) {
        mysum <- 0
        for(i in 1:nrow(X)) mysum <- mysum + X[i,j]
        rval[j] <- mysum/nrow(X)
        }
    return(rval)
}
```

Esto crea la función **cmeans()** (¡deliberadamente incómoda!), que toma un argumento de matriz **X** y usa un doble bucle **for** para calcular primero la suma y luego la media en cada columna. El resultado se almacena en un vector **rval** (el valor de **return** o valor de retorno), que se devuelve después de completar ambos ciclos. Esta función se puede aplicar fácilmente a nuevos datos, como en:

```{r, 86, echo = T, eval = T, message = F, warning = F}
X <- matrix(1:20, ncol = 2)
cmeans(X)
```

Como era de esperar, produce el mismo resultado que la función incorporada **colMeans()**:

```{r, 87, echo = T, eval = T, message = F, warning = F}
colMeans(X)
```

La función **cmeans()** toma solo un argumento **X** que no tiene un valor predeterminado. Si el autor de una función desea establecer un valor predeterminado, esto se puede lograr fácilmente definiendo un función (**function**) con una lista de pares **name = expr**, donde
**name** es el argumento de la variable y **expr** es una expresión con el valor predeterminado. Si se omite este último, no se establece ningún valor predeterminado.

En lenguajes interpretados basados en matrices, como lo es **R**, los bucles suelen ser menos eficientes que los correspondientes cálculos vectorizados ofrecidos por el sistema. Por lo tanto, evitar bucles reemplazándolos con operaciones vectorizadas puede ahorrar tiempo de cálculo, especialmente cuando el número de iteraciones en el bucle puede aumentar. Para ilustrar el punto anterior, se generan $2 \times 10^6$ números aleatorios de la distribución normal estándar y se compara con la función incorporada **colMeans()** con la incómoda función **cmeans()**. Se emplea la función **system.time()**, que es útil para perfilar código:

```{r, 88, echo = T, eval = T, message = F, warning = F}
X <- matrix(rnorm(2*10^6), ncol = 2)
system.time(colMeans(X))
system.time(cmeans(X))
```

Claramente, el desempeño de **cmeans()** es vergonzoso y usar **colMeans()** se prefiere.

### Cálculos vectorizados {-}

Como se señaló anteriormente, los bucles se pueden evitar utilizando aritmética vectorizada. En el caso de **cmeans()**, la función calcula las medias de una matriz por columnas, sería útil calcular directamente las medias columna por columna utilizando la función incorporada **mean()**. De hecho, esta es la solución preferida. Utilizando las herramientas de las que se dispone hasta el momento, se podría proceder de la siguiente manera:

```{r, 89, echo = T, eval = T, message = F, warning = F}
cmeans2 <- function(X) {
    rval <- rep(0, ncol(X))
    for(j in 1:ncol(X)) rval[j] <- mean(X[,j])
    return(rval)
}
```

Esto elimina uno de los bucles **for** y solo recorre las columnas. El resultado es idéntico a las soluciones anteriores, pero el rendimiento es claramente mejor que el de **cmeans()**:

```{r, 90, echo = T, eval = T, message = F, warning = F}
system.time(cmeans2(X))
```

Sin embargo, el código de **cmeans2()** todavía parece un poco engorroso con el resto de bucles **for** -se puede escribir de forma mucho más compacta con la función **apply()**. Esto aplica funciones sobre los márgenes de una matriz y toma tres argumentos: 

1. La matriz.
2. El índice del margen.
3. La función que se va a evaluar. 

En este caso, la llamada a la función es:

```{r, 91, echo = T, eval = T, message = F, warning = F}
apply(X, 2, mean)
```

Porque se requieren las medias (usando la función **mean()**) sobre las columnas (es decir, la segunda dimensión) de **X**. El rendimiento de la función **apply()** a veces puede ser mejor que un bucle **for**. Sin embargo, en muchos casos ambos enfoques funcionan de manera bastante similar:

```{r, 92, echo = T, eval = T, message = F, warning = F}
system.time(apply(X, 2, mean))
```

Para resumir, lo anterior implica que:

1. Los cálculos por elementos deben evitarse si existen cálculos vectorizados disponibles.
2. Las soluciones optimizadas (si están disponibles) generalmente funcionan mejor que las soluciones genéricas **for** o **apply()**.
3. Los bucles se pueden escribir de forma más compacta utilizando la función **apply()**. 

De hecho, esto es tan común en **R** que existen varias alternativas a la función **apply()**; a saber, están disponibles las funciones **lapply()**, **tapply()**, y **sapply()**. El primero devuelve una lista, el segundo una tabla y el tercero intenta simplificar el resultado a un vector o matriz cuando sea posible (se recomienda consultar las páginas del manual correspondientes para obtener información más detallada y ejemplos).

### Palabras reservadas {-}

Como la mayoría de los lenguajes de programación, **R** tiene una serie de palabras reservadas que proporcionan las construcciones gramaticales básicas del idioma. Algunos de estos ya se han presentado anteriormente y algunos más se describen a continuación. Una lista casi completa de palabras reservadas en **R** se puede consultar mediante el comando **?Reserved**. Algunas palabras clave que siempre debe tener en mente son:

- if 
- else
- for
- in
- while
- repeat
- break
- next
- function
- TRUE
- FALSE
- NA
- NULL
- Inf
- NaN

Si se intenta utilizar alguno de estos nombres para designar variables o cualquier tipo de objeto, se producirá un error.

## Fórmulas {#Fórmulas}

Las fórmulas son construcciones que se utilizan en varios programas estadísticos para especificar modelos. En **R**, Los objetos de la clase fórmula se pueden usar para almacenar descripciones simbólicas de relaciones entre variables, como el operador **~** en la formación de una fórmula:

```{r, 93, echo = T, eval = T, message = F, warning = F}
f <- y ~ x
class(f)
```

Hasta ahora, esta es solo una descripción sin ningún significado concreto. El resultado depende completamente de la función que evalúa esta fórmula. En **R**, la expresión anterior normalmente significa "Y es explicado por X". Dichas interfaces de fórmulas son convenientes para especificar, entre otras cosas, gráficos o relaciones de regresión. Por ejemplo, en el siguiente código primero se crean las variables **Y** y **X**, posteriormente se genera un diagrama de dispersión de **Y** contra **X** y finalmente se ajusta el correspondiente modelo de regresión lineal simple con pendiente $3.01$ e intersección $2.00$.

```{r, 94, echo = T, eval = T, message = F, warning = F}
# crear las variables
x <- seq(from = 0, to = 10, by = 0.5)
y <- 2 + 3 * x + rnorm(21)
```

```{r, 95, eval = T, message = F, warning = F, fig.align='center'}
# diagrama de dispersión simple de Y vs. X
plot(y ~ x)

# modelo de regresión
lm(y ~ x)
```

Para especificar modelos de regresión, el lenguaje de fórmulas es mucho más rico que el descrito anteriormente y se basa en una notación simbólica sugerida por Wilkinson y Rogers (1973) en la literatura estadística. Por ejemplo, al usar **lm()**, **log(y) ~ x1 + x2** especifica una regresión lineal de **log(y)** con dos regresores, **x1** y **x2**. y una constante implícitamente definida. 

## Gestión de datos en R {#GDR}

En **R**, un marco de datos corresponde a lo que otros paquetes estadísticos llaman una matriz de datos o un conjunto de datos. Por lo general, es una matriz que consta de una lista de vectores y/o factores de idéntica longitud, lo que produce un formato rectangular donde las columnas corresponden a variables y las filas a observaciones. 

### Creación desde cero {-}

Se puede crear un conjunto de datos simple artificialmente, con tres variables llamadas "uno", "dos" y "tres", mediante el uso de:

```{r, 96, echo = T, eval = T, message = F, warning = F}
mis_datos <- data.frame(uno = 1:10, dos = 11:20, tres = 21:30)
```

Alternativamente, **mis_datos** se puede crear usando:

```{r, 97, echo = T, eval = T, message = F, warning = F}
mis_datos <- as.data.frame(matrix(1:30, ncol = 3))
names(mis_datos) <- c("uno", "dos", "tres")
```

El código primero crea una matriz de tamaño $10 \times 3$ que posteriormente se coacciona en un marco de datos y cuyos nombres de variable se cambian finalmente a "uno", "dos" y "tres".

Se debe tener en cuenta que se puede utilizar la misma sintaxis tanto para consultar como para modificar los nombres en un marco de datos. Además, vale la pena reiterar que aunque un marco de datos se puede coaccionar a partir de una matriz como se indicó anteriormente, se representa internamente como una lista.

### Selección de subconjuntos {-}

Es posible acceder a un subconjunto de variables (es decir, columnas) a través de **[** o **$**, donde este último solo puede extraer una sola variable. Por tanto, la segunda variable (**dos**) se puede seleccionar a través de:

```{r, 98, echo = T, eval = T, message = F, warning = F}
mis_datos$dos
mis_datos[, "dos"]
mis_datos[, 2]
```

En todos los casos, el objeto devuelto es un vector simple; es decir, los atributos del marco de datos se eliminan (por defecto). 

Para simplificar el acceso a las variables en un determinado conjunto de datos, se pueden adjuntar los nombres mediante la función **attach()**. Técnicamente, esto significa que el conjunto de datos adjunto se agrega a la ruta de **search()** y, por ende, las variables contenidas en este conjunto de datos se pueden encontrar cuando su nombre se usa en un comando. Se puede poner en práctica lo anterior mediante:

```{r, 99, echo = T, eval = T, message = F, warning = F}
# mean(dos) por sí solo muestra un error. En consecuencia,
attach(mis_datos)
mean(dos)
detach(mis_datos)
```

Los marcos de datos deben adjuntarse con cuidado; en particular, se debe prestar atención a no adjuntar varios marcos de datos con los mismos nombres de columna o tener una variable con el mismo nombre en el entorno global, ya que es probable que esto genere confusión. Para evitar adjuntar y desadjuntar un conjunto de datos para un solo comando, la función **with()** puede ser útil, como en:

```{r, 100, echo = T, eval = T, message = F, warning = F}
with(mis_datos, mean(dos))
```

A menudo es necesario trabajar con subconjuntos de un marco de datos; es decir, utilizar solo observaciones seleccionadas (*= filas*) y/o variables (*= columnas*). Esto se puede hacer nuevamente a través de **[** o, más convenientemente, usando el comando **subset()**, cuyos principales argumentos son un marco de datos del que se tomará el subconjunto y una declaración lógica que define los elementos a seleccionar. Por ejemplo:

```{r, 101, echo = T, eval = T, message = F, warning = F}
mis_datos.sub <- subset(mis_datos, dos <= 16, select = -dos)
```

El código anterior toma todas las observaciones cuyo valor de la segunda variable (**dos**) no excedan de $16$ (se sabe que existen seis observaciones con esta propiedad) y, además, todas las variables a excepción de **dos** están seleccionadas.

### Importar y exportar {-}

Para exportar marcos de datos en formato de texto sin formato, la función **write.table()** se puede emplear:

```{r, 102, echo = T, eval = F, message = F, warning = F}
write.table(mis_datos, file = "mis_datos.txt", col.names = TRUE)
```

Crea un archivo de texto **mis_datos.txt** en el directorio de trabajo actual. Si este conjunto de datos se va a utilizar de nuevo, en otra sesión, se puede importar utilizando:

```{r, 103, echo = T, eval = F, message = F, warning = F}
nuevos_datos <- read.table("mis_datos.txt", header = TRUE)
```

La función **read.table()** devuelve un objeto de la clase "marco de datos" (**data.frame**), que luego se asigna al nuevo objeto **nuevos_datos**. Configurando **col.names = TRUE**, los nombres de las columnas están escritos en la primera línea de **mis_datos.txt** y, por lo cual, se establece **header = TRUE** al leer el archivo de nuevo. La función **write.table()** es bastante flexible y permite especificar el símbolo de separación y el separador decimal, entre otras propiedades del archivo a escribir, de modo que se pueden producir varios formatos basados en texto, incluidos valores separados por tabuladores o comas. 

Dado que este último es un formato popular para intercambiar datos (ya que puede ser leído y escrito por muchos programas de hojas de cálculo, incluido *Microsoft Excel*), las interfaces de conveniencia **read.csv()** y **write.csv()** están disponibles. De manera similar, **read.csv2()** y **write.csv2()** proporcionar exportación e importación de valores separados por punto y coma, un formato que se usa típicamente en sistemas que emplean la coma (y no el punto) como separador decimal. Además, existe una función más elemental, denominada **scan()**, para datos que no se ajustan al diseño matricial requerido por **read.table()**.^[Un ejemplo son las respectivas páginas del manual de **R** y el manual "R Data Import/Export” (R Development Core Team 2021c para más detalles).

También es posible guardar los datos en el formato binario interno de **R**, por convención con extensión *.RData* o *.rda*. El siguiente comando guarda los datos en formato binario en **R**. 

```{r, 104, echo = T, eval = F, message = F, warning = F}
save(mis_datos, file = "mis_datos.rda")
```

Los archivos binarios se pueden cargar usando:

```{r, 105, echo = T, eval = F, message = F, warning = F}
load("mis_datos.rda")
```

En contraste con **read.table()**, esto no devuelve un solo objeto. En su lugar, hace que todos los objetos almacenados en **mis_datos.rda** estén directamente disponible en el entorno actual. La ventaja de usar archivos *.rda* es que varios objetos de **R** se pueden almacenar (en realidad varios objetos arbitrarios de **R** se pueden almacenar, incluidas funciones o modelos ajustados), sin pérdida de información.

Todos los conjuntos de datos del paquete **AER** se suministran en este formato binario (vaya a la carpeta `~/AER/data`en la biblioteca de **R** para comprobarlo). Dado que son parte de un paquete, su acceso se hace mucho más fácil usando **data()** (que en este caso establece la correspondiente llamada **load()**). Por tanto, el siguiente código carga el marco de datos *Journals* del paquete **AER** (almacenado en el archivo **~/AER/data/Journals.rda**).^[El conjunto de datos utilizado en el Ejemplo 1 de los *Ejemplos introductorios a sesiones típicas en **R***. 

```{r, 106, echo = T, eval = T, message = F, warning = F}
data("Journals", package = "AER")
```

Si el argumento **package** se omite, todos los paquetes que se encuentran actualmente en la ruta de búsqueda son verificados para proporcionar el conjunto de datos *Journals*.

### Leer y escribir formatos binarios extranjeros {-}

**R** también puede leer y escribir una serie de formatos binarios de otros propietarios, en particular archivos *S-PLUS*, *SPSS*, *SAS*, *Stata*, *Minitab*, *Systat* y *dBase*, usando la función que proporciona el paquete **foreign** ( parte de un estándar de instalación de **R**). La mayoría de los comandos están diseñados para ser similares a **read.table()** y **write.table()**. Por ejemplo, para los archivos de *Stata*, ambos **read.dta()** y **write.dta()** están disponibles y se pueden utilizar para crear un archivo **Stata** que contenga **mis_datos**.

```{r, 107, echo = T, eval = F, message = F, warning = F}
library("foreign")
write.dta(mis_datos, file = "mis_datos.dta")
```

Los archivos se pueden leer en **R** a través de:

```{r, 108, echo = T, eval = F, message = F, warning = F}
mis_datos <- read.dta("mis_datos.dta")
```

Consulte la documentación del paquete **foreign** para mayor información.

### Interacción con el sistema de archivos y manipulaciones de cadenas {-}

En los párrafos anteriores fue necesaria cierta interacción con el sistema de archivos para leer y escribir archivos de datos. **R** posee una rica funcionalidad para interactuar con archivos externos y comunicarse con el sistema operativo. Esto esta más allá del alcance de este curso, pero me gustaría proporcionar al lector interesado algunos consejos que pueden servir como base para lecturas adicionales.

Los archivos disponibles en un directorio o carpeta se pueden consultar a través de la función **dir()** y también copiar (usando **file.copy()**) o eliminar (usando **file.remove()**) independiente del sistema operativo. Por ejemplo, el archivo *Stata* creado anteriormente se puede eliminar de nuevo desde **R** vía:

```{r, 109, echo = T, eval = F, message = F, warning = F}
file.remove("mis_datos.dta")
```

Otros comandos (potencialmente dependientes del sistema) se pueden enviar como cadenas al sistema operativo usando **system()**. Consulte las respectivas páginas del manual para obtener más información y ejemplos resueltos.

Anteriormente, se discutió cómo los objetos de datos (especialmente los marcos de datos) se pueden escribir como archivos en varios formatos. Más allá de eso, a menudo se desean guardar comandos o salida en archivos de texto. Una posibilidad para lograr esto es usar **sink()**, que puede dirigir la salida a una conexión **file()** en la que se podrán escribir las cadenas con **cat()**. En algunas situaciones **writeLines()** es más conveniente para esto. Además, **dump()** puede crear representaciones de texto de objetos en **R** y escribirlos en una conexión **file()**.

A veces, es necesario manipular las cadenas antes de crear una salida. **R** también proporciona una funcionalidad rica y flexible para esto. Las tareas típicas incluyen dividir cadenas (**strsplit()**) y/o pegarlas juntas (**paste()**). Para combinar y reemplazar patrones, **grep()** y **gsub()** están disponibles, que también admiten expresiones regulares. Para combinar texto y valores variables, **sprintf()** es útil.

### Factores {-}

Los factores son una extensión de los vectores diseñados para almacenar información categórica. Los ejemplos econométricos típicos de variables categóricas incluyen género, afiliación sindical o etnia. En muchos paquetes de software, estos se crean utilizando una codificación numérica (por ejemplo, $0$ para los hombres y $1$ para mujeres); a veces, especialmente en entornos de regresión, una sola variable categórica se almacena en varias de estas variables ficticias (o dummy) si existen más de dos categorías.

En **R**, las variables categóricas deben especificarse como factores. Por ejemplo, primero se crea un vector codificado ficticio con un cierto patrón y luego se transforma en un factor usando **factor()**:

```{r, 110, echo = T, eval = T, message = F, warning = F}
g <- rep(0:1, c(2, 4))
g <- factor(g, levels = 0:1, labels = c("masculino", "femenino"))
g
```

La terminología es que un factor tiene un conjunto de niveles, digamos *k* niveles. Internamente, un factor de *k-niveles* consta de dos elementos: 

1. Un vector de enteros entre $1$ y *k*.
2. Un vector de caracteres, de longitud *k*, que contiene cadenas con las etiquetas correspondientes.

Arriba, se creó el factor a partir de un vector entero; alternativamente, podría haberse construido a partir de otros vectores numéricos, de caracteres o lógicos. La información ordinal también se puede almacenar en un factor estableciendo el argumento **ordered = TRUE** al llamar a la función **factor()**.

La ventaja de este enfoque es que **R** sabe cuándo una determinada variable es categórica y puede elegir los métodos apropiados automáticamente. Por ejemplo, las etiquetas se pueden usar en la salida impresa, se pueden elegir diferentes métodos de resumen y trazado, así como se pueden calcular codificaciones de contraste (por ejemplo, variables ficticias) en regresiones lineales. Tenga en cuenta que para estas acciones el orden de los niveles puede ser importante.

### Valores faltantes {-}

Muchos conjuntos de datos contienen observaciones para las cuales ciertas variables no están disponibles. El software econométrico necesita formas de lidiar con esto. En **R**, tales valores perdidos se codifican como **N/A** ( para "no disponible" o **n**ot **a**vailable). Todos los cálculos estándar en **N/A** se vuelven **N/A**.

Es necesario tener especial cuidado al leer datos que utilizan una codificación diferente. Por ejemplo, al preparar el paquete **AER**, se pueden encontrar varios conjuntos de datos que empleaban $999$ para los valores perdidos. Si un archivo **mis_datos.txt** contiene valores perdidos codificados de esta manera, se pueden convertir a **N/A** usando el argumento **na.strings** al leer el archivo:

```{r, 111, echo = T, eval = F, message = F, warning = F}
newdata <- read.table("mis_datos.txt", na.strings = "-999")
```

Para consultar si ciertas observaciones son **N/A** o no, se debe usar la función **is.na()**.

## Orientación a objetos {#OO}

De manera un tanto vaga, la *Programación Orientada a Objetos (POO)* se refiere a un paradigma de programación en el que los usuarios/desarrolladores pueden crear objetos de una determinada "clase" (que deben tener una determinada estructura) y luego aplicar "métodos" para determinadas "funciones genéricas" a dichos objetos. Un simple ejemplo en **R** es la función **summary()**, que es una función genérica que elige, dependiendo de la clase de su argumento, el método de resumen definido para dicha clase. Por ejemplo, para el vector numérico **x** y el factor **g** usado arriba:

```{r, 112, echo = T, eval = T, message = F, warning = F}
x <- c(1.8, 3.14, 4, 88.169, 13)
g <- factor(rep(c(0, 1), c(2, 4)), levels = c(0, 1), labels = c("male", "female"))
```

La llamada a la función summary() produce diferentes tipos de resultados:

```{r, 113, echo = T, eval = T, message = F, warning = F}
summary(x)
summary(g)
```

Para el vector numérico **x**, se informa un resumen de cinco números (es decir, el mínimo, el máximo, la mediana, el primer y tercer cuartil) junto con la media, mientras que para el factor **g** devuelve una tabla de frecuencias simple. Esto muestra que **R** tiene diferente métodos disponibles para **summary()** dependiendo de los tipos de clases (en particular, sabe que un resumen de cinco números no es sensato para las variables categóricas). En **R**, cada objeto tiene una clase que se puede consultar usando la función **class()**:

```{r, 114, echo = T, eval = T, message = F, warning = F}
class(x)
class(g)
```

La clase se utiliza internamente para llamar al método apropiado para una función genérica. De echo, **R** ofrece varios paradigmas de orientación a objetos. La instalación básica viene con dos sistemas POO diferentes, generalmente llamados *S3* (Chambers y Hastie 1992) y *S4* (Chambers 1998). La *S3* es mucho más simple, utilizando un mecanismo de distribución basado en una convención de nomenclatura para los métodos. La *S4* es más sofisticada y más cercana a otros conceptos de programación orientada a objetos utilizados en informática, pero también requiere más disciplina y experiencia. Para la mayoría de las tareas, *S3* es suficiente y, por tanto, es el único sistema *POO* (brevemente) discutido aquí.

En *S3*, una función genérica se define como una función con una determinada lista de argumentos y luego una llamada **UseMethod()** con el nombre de la función genérica. Por ejemplo, imprimir la función **summary()** revela su definición:

```{r, 115, echo = T, eval = T, message = F, warning = F}
summary
```

Se necesita un primer **objeto**, como argumento obligatorio, más un número arbitrario de argumentos adicionales pasados **...** a sus métodos. ¿Qué sucede si esta función se aplica a un objeto de la clase "**foo**"? En este contexto, **R** intenta aplicar la función **summary.foo()** si existiera. Si no, llamará **summary.default()** si existe tal método predeterminado (que es el caso de **summary()**). 

Además, los objetos en **R** pueden tener un vector de clases (por ejemplo, **c("foo", "bar")**, lo que implica que esos objetos son de clase "**foo**" heredado de "**bar**”). En este caso, **R** hace los primeros intentos de aplicar **summary.foo()**, entonces (si esto no existe) **summary.bar()**, y luego (si ambos no existen) **summary.default()**. Todos los métodos que son actualmente definidos para una función genérica se pueden consultar usando **methods()**; por ejemplo, **methods(summary)** devolverá una lista (larga) de métodos para todo tipo de clases diferentes. 

Entre los métodos disponibles a partir de las clases, existe un método **summary.factor()**, que se usa cuando se llama a **summary(g)**. Sin embargo, no existe **summary.numeric()**; por ende, **summary(x)** es manejado por **summary.default()**. Como no se recomienda llamar a métodos directamente, algunos métodos están marcados como no visibles para el usuario y no se pueden llamar directamente (fácilmente). Sin embargo, incluso para los métodos visibles, se hace hincapié en que en la mayoría de las situaciones es claramente preferible su uso; por ejemplo, **summary(g)** en vez de **summary.factor(g)**.

Para ilustrar lo fácil que es definir una clase y algunos métodos para ella, se considera un ejemplo simple. Se creará un objeto de clase "**normsample**”, que contiene una muestra de una distribución normal y luego se define el método **summary()** que informa la media empírica y la desviación estándar para dicha muestra. Primero, se codifica un creador de clases simple. En principio, podría tener cualquier nombre, pero a menudo se llaman como la propia clase:

```{r, 116, echo = T, eval = T, message = F, warning = F}
normsample <- function(n, ...) {
    rval <- rnorm(n, ...)
    class(rval) <- "normsample"
    return(rval)
    }
```

Esta función toma un argumento requerido **n** ( el tamaño de la muestra) y otros argumentos **...**, que se transmiten a **rnorm()**, la función para generar números aleatorios normales. Además del tamaño de la muestra, se necesitan más argumentos (la media y desviación estándar). Se recomienda consultar **?rnorm**. Después de la generación del vector de números aleatorios normales, se le asigna la clase "normsample" Y luego se prueba:

```{r, 117, echo = T, eval = T, message = F, warning = F}
set.seed(123)
x <- normsample(10, mean = 5)
class(x)
```

Para definir un método **summary()**, se crea una función **summary.normsample()** que se ajusta a la lista de argumentos del genérico (aunque **...** no se usa aquí) y calcula el tamaño de la muestra, la media empírica y la desviación estándar.

```{r, 118, echo = T, eval = T, message = F, warning = F}
summary.normsample <- function(object, ...) {
    rval <- c(length(object), mean(object), sd(object))
    names(rval) <- c("sample size","mean","standard deviation")
    return(rval)
    }
```

En consecuencia, llamando a **summary(x)** se puede obtener automáticamente el nuevo método **summary()** y se produce el resultado deseado:

```{r, 119, echo = T, eval = T, message = F, warning = F}
summary(x)
```

Otras funciones genéricas con métodos para la mayoría de clases estándar en **R** son **print()**, **plot()** y **str()**, que imprimen, trazan y resumen la estructura de un objeto, respectivamente.

## Gráficos en **R** {#GR}

No es casualidad que las primeras publicaciones sobre *S* y **R**, como Becker y Chambers (1984) e Ihaka y Gentleman (1996), se titulan “S: Un entorno interactivo para análisis de datos y gráficos” y “R: Un lenguaje para análisis de datos y gráficos”, respectivamente. **R** de hecho tiene gráficos poderosos.

Aquí, se presentan brevemente los gráficos "convencionales" implementados en la base de **R**. **R** también viene con un motor gráfico nuevo e incluso más flexible, llamado *grid* (Murrell 2005), que proporciona la base para una implementación de gráficos en **R** del tipo "trellis" (Cleveland 1993) en el paquete **lattice** (Sarkar 2002), pero estos están más allá del alcance de este curso. Una excelente descripción de los gráficos en **R** se da en Murrell(2005).

### La función plot() {-}

La función básica es el método predeterminado **plot()**. Es una función genérica y tiene métodos para muchos objetos, incluidos marcos de datos, series de tiempo y modelos lineales ajustados. A continuación, se describe el valor predeterminado del método **plot()**, que puede crear varios tipos de diagramas de dispersión, pero muchas otras explicaciones se extienden a diversos métodos, así como a otras funciones de trazado de alto nivel.

El diagrama de dispersión (*scatterplot*) es probablemente la representación gráfica más común en estadística. Un diagrama de dispersión de **Y** vs. **X** está disponible haciendo uso de la función **plot(x, y)**. A modo de ilustración, se vuelve a utilizar el conjunto de datos *Journals* del paquete **AER**, tomado de Stock y Watson (2007). Como se señaló antes, los datos brindan alguna información sobre las suscripciones a revistas de economía en bibliotecas estadounidenses para el año 2000. El archivo contiene $180$ observaciones (las revistas) sobre $10$ variables, entre ellas el número de suscripciones a bibliotecas (**subs**), el precio de suscripción a la biblioteca (**price**),
y el número total de citas de la revista (**citations**). 

Aquí, interesa la relación entre el número de suscripciones y el precio por cita. El siguiente fragmento de código deriva la variable requerida **citeprice** y grafica el número de suscripciones a la biblioteca en logaritmos:

```{r, 120, eval = T, message = F, warning = F, fig.align='center'} 
# cargar los datos
data("Journals", package = "AER")

# crear la variable "citeprice"
Journals$citeprice <- Journals$price/Journals$citations

# crear el gráfico
attach(Journals)
plot(log(subs), log(citeprice))
rug(log(subs))
rug(log(citeprice), side = 2)
detach(Journals)
```
La función **rug()** agrega pequeñas barras, visualizando así las distribuciones marginales de las variables, a lo largo de uno o ambos ejes de una gráfica existente. La gráfica resultante tiene marcas en los ejes horizontal y vertical. Una forma alternativa de especificar **plot(x, y)** es utilizar el método de fórmula de **plot()**; es decir, **plot(y ~ x)**. Esto conduce al mismo diagrama de dispersión, pero tiene la ventaja de que se puede especificar el argumento **data**. Por lo tanto, se puede evitar adjuntar y desadjuntar el marco de datos:

```{r, 121, eval = T, message = F, warning = F, fig.align='center'} 
plot(log(subs) ~ log(citeprice), data = Journals)
```

### Parámetros gráficos {-}

Todo esto parece engañosamente simple, pero el resultado puede modificarse de muchas formas. Por ejemplo, **plot()** toma un argumento **type** que controla los puntos o **p**oints (**type = "p"**, de manera predeterminada), las líneas o **l**ines (**type = "l"**), ambas cosas o **b**oth (**type = "b"**) y los escalones de la escalera o **s**teps (**type = "s"**), con lo que se pueden generan más tipos de gráficos. La anotación se puede modificar cambiando las etiquetas del título principal (**main**), el eje **x** (**xlab**) o el eje **y** (**ylab**). Se recomienda ver **?plot** para más detalles.

Además, existen varias docenas de parámetros gráficos (ver **?par** para ver la lista completa) que se pueden modificar configurándolos con **par()** o sustituyéndolos a la función **plot()**. Una lista selectiva de argumentos para **par()**:

| Argumento        | Descripción                                                    |
|:-----------------|:---------------------------------------------------------------|
| axes             | ¿Deberían dibujarse los ejes?                                  |
| bg               | Color de fondo                                                 |
| cex              | Tamaño de un punto o símbolo                                   |
| col              | Color                                                          |
| las              | Orientación de las etiquetas de los ejes                       |
| lty, lwd         | Tipo de línea y ancho de línea                                 |
| main, sub        | Título y subtítulo                                             |
| mar              | Tamaño de los márgenes                                         |
| mfcol, mfrow     | Matriz que define el diseño para varios gráficos en un gráfico |
| pch              | Símbolo de trazado                                             |
| type             | Tipos (ver texto)                                              |
| xlab, ylab       | Etiquetas de eje                                               |
| xlim, ylim       | Rangos de ejes                                                 |
| xlog, ylog, log  | Escalas logarítmicas                                           |

Table: (\#tab:argumentos) Parámetros gráficos

No se puede explicar todos estos aquí, pero se deben destacar algunos parámetros importantes: 

- **col** establece el (los) color (es).
- **ylim** Y **xlim** ajustaN los rangos de trazado. 
- Si se trazan puntos, **pch** puede modificar el carácter de la trama y **cex** la extensión del carácter. 
- Si se trazan líneas, **lty** y **lwd** especifican el tipo de línea y el ancho, respectivamente. 
- El tamaño de las etiquetas, marcas de eje, entre otros, se puede cambiar escribiendo el argumentos **cex** como **cex.lab** y **cex.axis**.

Una breve lista de argumentos para **par()** se proporciona en la Tabla 1.1. Esto es solo la punta del iceberg, por ende, se introducirán más parámetros gráficos a medida que se avance en el curso.

Como ejemplo simple, se puede probar:

```{r, 122, eval = T, message = F, warning = F, fig.align='center'} 
plot(log(subs) ~ log(citeprice), 
    data = Journals, 
    pch = 20,
    col = "blue", 
    ylim = c(0, 8), 
    xlim = c(-7, 4),
    main = "Suscripciones a bibliotecas")
```

Esto produce círculos sólidos (**pch = 20**) en lugar de los abiertos predeterminados, dibujados en azul, y existen rangos más amplios en las direcciones de **X** y **Y**. Igualmente, se agregó un título principal.

También es posible agregar más capas a un gráfico. Por lo tanto, se pueden agregar más características con **lines()**, **points()**, **text()** y **legend()**, que aportan a una trama existente lo que sugieren sus nombres. Por ejemplo, **, text(-3.798, 5.846, "Econometrica", pos = 2)**, coloca una cadena de caracteres en la ubicación indicada (es decir, a la izquierda del punto). En los análisis de regresión, a menudo se desea agregar una línea de regresión a un diagrama de dispersión. Como se vio antes, esto se logra usando **abline(a, b)**, dónde **a** es la intersección y **b** es la pendiente.

En este punto, no parece haber una gran necesidad de todo esto; sin embargo, la mayoría de los usuarios requieren un control preciso de las presentaciones visuales en algún momento, especialmente cuando se necesitan gráficos con calidad de publicación. Me abstengo de presentar ejemplos artificiales jugando con las opciones gráficas; en su lugar, se introducen variaciones de las pantallas estándar a medida que se vaya avanzando.

Por supuesto, existen muchas más funciones de trazado además del método predeterminado **plot()**. Por ejemplo, las visualizaciones estadísticas estándar como gráficos de barras, gráficos circulares, gráficos de caja, gráficos QQ o histogramas están disponibles en las funciones **barplot()**, **pie()**, **boxplot()**, **qqplot()** e **hist()**, respectivamente. Resulta instructivo correr el código **demo("graphics")** para obtener una descripción general de las impresionantes comodidades gráficas de **R**.

### Exportación de gráficos {-}

En el uso interactivo, los gráficos generalmente se escriben en una ventana de gráficos para que puedan inspeccionarse directamente. Sin embargo, después de completar un análisis, normalmente se quieren guardar los gráficos resultantes (por ejemplo, para su publicación en
un informe, artículo de revista o tesis). Para los usuarios de *Microsoft Windows* y *Microsoft Word*, una opción simple consiste en "copiarlos y pegarlos" en el documento de *Microsoft Word*.

Para otros programas, como LATEX, es preferible exportar el gráfico a un archivo externo. Para ello, existen varios dispositivos gráficos en los que se puede escribir los gráficos. Los dispositivos que están disponibles en todas las plataformas incluyen los formatos vectoriales *PostScript* y *PDF*; otros dispositivos, como los formatos de mapa de bits *PNG* y *JPEG* y el formato vectorial *WMF*, solo están disponibles si el sistema los admite (consulte **?Devices** para mas detalles). 

Todos funcionan de la misma manera: 

1. Se abre el dispositivo; por ejemplo, la función **pdf()** abre el dispositivo *PDF*.
2. Se ejecutan los comandos que crean el gráfico.
3. El dispositivo se cierra mediante **dev.off()**. 

Un ejemplo simple de creación de un gráfico en un dispositivo *PDF* es:

```{r, 123, echo = T, eval = F, message = F, warning = F, fig.align='center'} 
pdf("mi_archivo.pdf", height = 5, width = 6)
plot(1:20, pch = 1:20, col = 1:20, cex = 2)
dev.off()
```

```{r, 124, echo = F, eval = T, message = F, warning = F, fig.align='center'} 
plot(1:20, pch = 1:20, col = 1:20, cex = 2)
```

Esto crea el archivo *PDF* **mi_archivo.pdf** en el directorio de trabajo actual, que contiene el gráfico generado al llamar a la función **plot()**. Por cierto, el gráfico ilustra algunos de los parámetros discutidos anteriormente: 

- Muestra los primeros 20 símbolos de trazado.
- Todos los símbolos de trazado se encuentran en tamaño doble. 
- **R** enumera un conjunto de colores con los que se puede trabajar. Los primeros ocho colores son negro, rojo, verde, azul, turquesa, violeta, amarillo y gris. Desde el color nueve en adelante, este vector simplemente se recicla.

Alternativamente para abrir, imprimir y cerrar un dispositivo, también es posible imprimir un trazado existente en la ventana gráfica en un dispositivo usando **dev.copy()** y **dev.print()** (se recomienda consultar las páginas de los manuales correspondiente para obtener más información).

### Anotación matemática de parcelas {-}

Una característica que se suma particularmente a las fortalezas **R** cuando se trata de gráficos con calidad de publicación es su capacidad para agregar anotaciones matemáticas a los gráficos (Murrell e Ihaka 2000). Una comando **S** que contiene una expresión matemática puede pasar como una función de trazado sin ser evaluada; es decir, se procesa como una anotación del gráfico creado. Los lectores familiarizados con LATEX no tendrán dificultades para adaptarse a la sintaxis empleada (para obtener más detalles, consulte **?plotmath** y **demo("plotmath")**). Como ejemplo, el siguiente gráfico proporciona la densidad de una distribución normal estándar (proporcionada por la funcióon **dnorm()** en **R**), incluyendo su definición matemática:

$$
f(x)=\frac{1}{\sigma \sqrt{2 \pi}} e^{-\frac{(x-\mu)^{2}}{2 \sigma^{2}}}
$$

Se obtiene a través de:

```{r, 125, eval = T, message = F, warning = F, fig.align='center'} 
# gráfico de la densidad de la distribución normal estándar
curve(dnorm, 
    from = -5, 
    to = 5, 
    col = "slategray", 
    lwd = 3,
    main = "Densidad de la distribución normal estándar")

# expresión matemática
text(-5, 
    0.3, 
    expression(f(x) == frac(1, sigma ~~sqrt(2*pi)) ~~ e^{-frac((x - mu)^2, 2*sigma^2)}), 
    adj = 0)
```

La función **curve()** traza la función de densidad **dnorm()**, mientras que la función **text()** se usa para agregar **expression()** que contiene la fórmula de la trama. Este ejemplo concluye la breve introducción a los gráficos en **R**.

## Análisis exploratorio de datos con **R** {#AEDR}

En esta sección, se ilustrarán brevemente algunas técnicas estándar de análisis exploratorio de datos. Para los lectores que buscan una introducción más detallada a las estadísticas básicas utilizando **R** se recomienda consultar a Dalgaard (2002). Reconsiderando los datos **CPS1985** tomados de Berndt (1991). Después de hacer que los datos estén disponibles a través de **data()**, alguna información básica puede ser consultada mediante la función estructura **str()**:

```{r, 126, echo = T, eval = T, message = F, warning = F}
data("CPS1985", package = "AER")
str(CPS1985)
```

Esto revela que el objeto "marco de datos" (**data.frame**) comprende 533 observaciones sobre 11 variables, incluida la variable numérica sueldo (**wage**), las variables enteras experiencia (**experience**), educación (**education**) y edad (**age**), así como siete factores, cada uno de los cuales comprende de dos a seis niveles.

En lugar de utilizar la vista de tipo lista que proporciona **str()**, a menudo es útil inspeccionar la parte superior (o la parte inferior) de un marco de datos en su representación rectangular. Para ello, existen las funciones de conveniencia **head()** y **tail()**, devolviendo (por defecto) la primera y las últimas seis filas, respectivamente. Por lo tanto:

```{r, 127, echo = T, eval = T, message = F, warning = F}
head(CPS1985)
tail(CPS1985)
```

Otra forma útil de obtener una descripción general rápida de un conjunto de datos es utilizar el método para marcos de datos (**data.frame**) de la función **summary()**, que proporciona un resumen para cada una de las variables. Como el tipo de resumen depende de la clase de la variable respectiva, más adelante se inspeccionarán los métodos **summary()** por separado para varios tipos de variables en **CPS1985**. Por tanto, la salida de **summary(CPS1985)** se omite aquí.

Como los datos de **CPS1985** se emplean repetidamente en lo siguiente, se evitan comandos largos como **CPS1985$education** adjuntando el conjunto de datos. Además, para compactar la producción posterior, se abrevian dos niveles de ocupación de "**technical**" a "**techn**" y de "**management**" a "**mgmt**".

```{r, 128, echo = T, eval = T, message = F, warning = F}
levels(CPS1985$occupation)[c(2, 6)] <- c("techn", "mgmt")
attach(CPS1985)
```

Ahora se puede acceder a las variables por sus nombres. Se procede a ilustrar el análisis exploratorio de variables únicas y pares de variables, distinguiendo entre variables numéricas, factores y combinaciones de los mismos. Comenzamos con el tipo más simple, una sola variable numérica.

### Una variable numérica {-}

Primero se vera la distribución de salarios en la muestra:

```{r, 129, echo = T, eval = T, message = F, warning = F}
summary(wage)
```

Esto proporciona el resumen de los *cinco números de Tukey* más el salario medio. La media y la mediana también se podrían haber obtenido utilizando:

```{r, 130, echo = T, eval = T, message = F, warning = F}
mean(wage)
median(wage)
```

Asimismo, **fivenum()** calcula el resumen de cinco números. Similarmente, **min()** y **max()** habrían dado el mínimo y el máximo. Las cantidades arbitrarias se pueden calcular mediante la función **quantile()**.

Para las medidas de propagación o disperción, existen las funciones:

```{r, 131, echo = T, eval = T, message = F, warning = F}
var(wage)
sd(wage)
```

Las funciones devuelven la varianza y la desviación estándar, respectivamente.

Los resúmenes gráficos también son útiles. Para variables numéricas como **wage**, las visualizaciones de densidad (a través de histogramas o suavizado de kernel) y los diagramas de caja son adecuados. Los diagramas de caja se considerarán a continuación en relación con los paneles de dos variables obtenidas a través de:

```{r, 132, eval = T, message = F, warning = F, fig.align='center'} 
# histogramas de salarios
hist(wage, freq = FALSE)
```

```{r, 133, eval = T, message = F, warning = F, fig.align='center'} 
# logaritmos con densidad superpuesta
hist(log(wage), freq = FALSE)
lines(density(log(wage)), col = 4)
```

El código anterior muestra las densidades de sueldo y su logaritmo (es decir, áreas bajo curvas iguales a $1$, resultante de **freq = FALSE**; de lo contrario, se habrían representado frecuencias absolutas). Otros argumentos permiten un ajuste fino de la selección de las rupturas en el histograma. Por ejemplo, en el histograma del panel derecho se agregó una estimación de la densidad del kernel obtenida usando **density()**. Claramente, la distribución de los logaritmos está menos sesgada que la de los datos brutos. Se debe tener en cuenta que **density()** solo calcula las coordenadas de densidad y no proporciona una gráfica; por lo tanto, la estimación se agrega a través de **lines()**

### Una variable categórica {-}

Para datos categóricos, no tiene sentido calcular medias y varianzas; en su lugar, se necesita una tabla que indique las frecuencias con las que ocurren las categorías. Si **R** dice que una determinada variable es categórica (al convertirla en un "**factor**”), elige automáticamente un resumen apropiado:

```{r, 134, echo = T, eval = T, message = F, warning = F}
summary(occupation)
```

Esto también podría haber sido calculado por **table(occupation)**. Si se desean frecuencias relativas (porcentajes) en lugar de absolutas (números), existe la función **prop.table()**:

```{r, 135, echo = T, eval = T, message = F, warning = F}
tab <- table(occupation)
prop.table(tab)
```

Las variables categóricas generalmente se visualizan mejor mediante gráficos de barras. Si se van a destacar las mayorías, los gráficos circulares también pueden ser útiles. Por lo tanto:

```{r, 136, eval = T, message = F, warning = F, fig.align='center'} 
# gráfico de barras de ocupación 
barplot(tab)
```

```{r, 137, eval = T, message = F, warning = F, fig.align='center'} 
# gráfico circular de ocupación
pie(tab)
```

El código anterior proporciona el gráfico de barras y gráfico circular de **occupation**. Se debe tener en cuenta que ambas funciones esperan las frecuencias tabuladas como entrada. Además, llamando **occupation** es equivalente a **barplot(table(occupation))**, dado que es una variable categórica.

### Dos variables categóricas {-}

La relación entre dos variables categóricas generalmente se resume en una tabla de contingencia. Esto puede ser creado por **xtabs()**, una función con una interfaz de fórmula, o por **table()**, una función que toma un número arbitrario de variables para la tabulación cruzada (y no solo una como se muestra arriba).

Se consideran los factores ocupación (**occupation**) y género (**gender**) para ilustrar el punto anterior:

```{r, 138, echo = T, eval = T, message = F, warning = F}
xtabs(~ gender + occupation, data = CPS1985)
```

La tabla anterior puede ser creada de forma equivalente por **table(gender, occupation)**. Una visualización simple es un diagrama de mosaico (Hartigan y Kleiner 1981; Friendly 1994), que puede verse como una generalización de diagramas de barras apilados. El gráfico que se muestra a continuación (también conocido como "gráfico de columna vertebral", es una variante de la pantalla de mosaico estándar), obtenido a través de:

```{r, 139, eval = T, message = F, warning = F, fig.align='center'} 
# gráfico de mosaico (gráfico de columna vertebral) de género versus ocupación.
plot(gender ~ occupation, data = CPS1985)
```

El gráfico muestra que la proporción de hombres y mujeres cambia considerablemente a lo largo de los niveles de ocupación. Además del sombreado que resalta la distribución condicional de género dada la ocupación, los anchos de las barras visualizan la distribución marginal de ocupación, lo que indica que hay comparativamente muchos trabajadores y pocos vendedores.

### Dos variables numéricas {-}

Se ejemplifica el análisis exploratorio de la relación entre dos variables numéricas utilizando sueldo (**wage**) y educación (**education**). Una medida de resumen para dos variables numéricas es el coeficiente de correlación, implementado mediante la función **cor()**. Sin embargo, el coeficiente de correlación estándar (Pearson) no es necesariamente significativo para variables positivas y muy sesgadas como sueldo (**wage**). Por lo tanto, también se calcula una variante no paramétrica, la correlación de Spearman, que está disponible en **cor()** como una opción mediante los métodos:

```{r, 140, echo = T, eval = T, message = F, warning = F}
cor(log(wage), education)
cor(log(wage), education, method = "spearman")
```

Ambas medidas son virtualmente idénticas e indican solo una pequeña cantidad de correlación aquí, vea también el diagrama de dispersión correspondiente:

```{r, 141, eval = T, message = F, warning = F, fig.align='center'} 
# diagrama de dispersión de salarios (en logaritmos) versus educación
plot(log(wage) ~ education)
```

### Una variable numérica y una categórica {-}

Es común tener variables tanto numéricas como categóricas en un marco de datos. Por ejemplo, aquí se tiene sueldo (**wage**) y género (**gender**), y puede haber cierto interés en la distribución de sueldo por género. Una función adecuada para resúmenes numéricos es **tapply()**. Se aplica, para una variable numérica como su primer argumento y una (lista de) variable(s) categórica(s) como su segundo argumento, la función especificada como el tercer argumento. Por lo tanto, los salarios medios condicionados al género están disponibles utilizando:

```{r, 142, echo = T, eval = T, message = F, warning = F}
tapply(log(wage), gender, mean)
```

Usando comandos similares, se pueden obtener medidas descriptivas o incluso resúmenes completos (simplemente reemplace **mean** por **summary**) se pueden calcular. 

Las presentaciones gráficas adecuadas son diagramas de caja paralelos y diagramas de cuantiles-cuantiles (QQ), se muestran a continuación. Recuerde que una gráfica de caja (o “gráfica de caja y bigotes”) es un resumen gráfico burdo de una distribución empírica. El cuadro indica "bisagras" (aproximadamente los cuartiles superior e inferior) y la mediana. 

Los “bigotes” (líneas) indican las observaciones más grandes y más pequeñas que se encuentran a una distancia de $1.5$ veces el tamaño de la caja desde la bisagra más cercana ($1.5$ desviaciones estandar). Cualquier observación que caiga fuera de este rango se muestra por separado y puede ser considerada extrema o atípica (en una muestra aproximadamente normal). Se debe tener en cuenta que existen varias variantes de diagramas de caja en la literatura.

Los comandos **plot(y ~ x)** y **boxplot(y ~ x)** producen la misma gráfica paralela de boxplot si **X** es un "**factor**”; por lo tanto:

```{r, 143, eval = T, message = F, warning = F, fig.align='center'} 
# diagrama de caja de salarios estratificados por género.
plot(log(wage) ~ gender)
```

El gráfico muestra que las formas generales de ambas distribuciones son bastante similares y que los hombres disfrutan de una ventaja sustancial, especialmente en el rango medio. La última característica también se destaca por el gráfico QQ resultante de:

```{r, 144, eval = T, message = F, warning = F, fig.align='center'} 
# diagrama QQ de salarios estratificados por género
mwage <- subset(CPS1985, gender == "male")$wage
fwage <- subset(CPS1985, gender == "female")$wage

qqplot(mwage, 
    fwage, 
    xlim = range(wage), 
    ylim = range(wage),
    xaxs = "i", 
    yaxs = "i", 
    xlab = "male", 
    ylab = "female")

abline(0, 1)
```

En el gráfico casi todos los puntos están por debajo de la diagonal (correspondientes a distribuciones idénticas en ambas muestras). Esto indica que, para la mayoría de los cuantiles, los salarios de los hombres suelen ser más altos que los de las mujeres.

Se termina esta sección separando los datos:

```{r, 145, eval = T, message = F, warning = F, fig.align='center'} 
detach(CPS1985)
```

### Ejercicios {-}

1. Cree una matriz cuadrada, digamos $A$, con entradas $a_{ii} = 2$, $i = 2$, $. . .$ , $n - 1$, $a_{11} = a_{nn} = 1$, $a_{i,i+1} = a_{i,i-1} = 1$ y $a_{ij} = 0$. (¿Dónde ocurre esta matriz en econometría?)

2. "**PARADE**" es la revista dominical que complementa la edición de los domingos o fines de semana de unos $500$ diarios de los Estados Unidos de América (se encuentra en el paquete **AER**). Una característica anual importante es un artículo que proporciona información sobre unos $120$-$150$ ciudadanos estadounidenses seleccionados “al azar”, indicando su profesión, ciudad natal y estado, así como sus ingresos anuales. 
El conjunto de datos **Parade2005** contienen la versión de 2005, modificada por una variable que indica el estatus de celebridad (motivada por un sobremuestreo sustancial de celebridades en estos datos). Para el conjunto de datos **Parade2005**:

(a) Determine las ganancias medias en California. Explique el resultado.
(b) Determine el número de personas que residen en Idaho (¿Qué dice esto sobre el conjunto de datos?).
(c) Determine la media y la mediana de los ingresos de las celebridades. Explique el resultado. 
(d) Obtenga un diagrama de caja de **log(earnings)** estratificado por **celebrity**. Explique el resultado.

3. Para el conjunto de datos **Parade2005** del ejercicio anterior, obtenga una densidad estimada de kernel de las ganancias para el conjunto de datos completo. Será necesario transformar los datos a logaritmos (¿por qué?). Comente el resultado. Asegúrese de probar algunos argumentos para **density()**, en particular, el complemento ancho de banda (**bw**).

4. Considere el conjunto de datos **CPS1988**, tomados de Bierens y Ginther (2001) (estos datos se utilizarán para estimar una ecuación de ganancias en próximos capítulos).

(a) Obtenga diagramas de dispersión del logaritmo del salario real (**wage**) versus experiencia (**experience**) y versus educación (**education**). 
(b) De hecho, **education** corresponde a años de escolaridad y, por ende, toma solo un número limitado de valores. Transformar educación en un factor y obtener diagramas de caja paralelos de la variable **wage** estratificada por los niveles de **education**. Repetir para **experience**.
(c) El conjunto de datos contiene cuatro factores adicionales: **ethnicity**, **smsa**, **region** y **parttime**. Obtenga visualizaciones gráficas adecuadas de **log(wage)** versus cada uno de estos factores.

<!--chapter:end:Capitulo_02.Rmd-->

# Teoría de la probabilidad {#TP}

```{r, echo = F}
options(knitr.duplicate.label = "allow")
```

```{r, 146, child="_setup.Rmd"}
```

```{r, 147, eval=knitr::opts_knit$get("rmarkdown.pandoc.to") == "html", results='asis', echo=FALSE}
cat('<hr style="background-color:#03193b;height:2px">')
```

Este capítulo revisa algunos conceptos básicos de la teoría de probabilidad y demuestra cómo se pueden aplicar en **R**.

La mayoría de las funciones estadísticas que tienen base en **R** se recopilan en el paquete **stats**. Dicho paquete proporciona funciones simples que calculan medidas descriptivas y facilitan los cálculos que involucran una variedad de distribuciones de probabilidad. También contiene rutinas más sofisticadas que, por ejemplo, permiten al usuario estimar una gran cantidad de modelos fundamentados en los mismos datos o ayudan a realizar estudios de simulación extensos. **stats** es parte de la distribución que tiene base en **R**, lo que significa que está instalado de forma predeterminada, por lo que no es necesario ejecutar `install.packages ("stats")` o `library ("stats")`. En este caso simplemente se necesita ejecutar `library(help ="stats")` en la consola para ver la documentación y una lista completa de todas las funciones reunidas en **stats**. Para la mayoría de los paquetes, existe una documentación que se puede ver en *RStudio*. La documentación se puede invocar usando el operador **?**, por ejemplo, al ejecutar `?stats` la documentación del paquete **stats** se muestra en la pestaña de ayuda del panel inferior derecho.

En lo que sigue, la perspectiva se centra en (algunas de) las distribuciones de probabilidad que maneja **R** y muestra cómo usar las funciones relevantes para resolver problemas simples. De ese modo, se actualizan algunos conceptos básicos de la teoría de la probabilidad. Entre otras cosas, aprenderá a dibujar números aleatorios, a calcular densidades, probabilidades, cuantiles y similares. Como se verá, es muy conveniente confiar en las rutinas o scripts que se mostrarán a continuación.

## Variables aleatorias y distribuciones de probabilidad
 
Resulta de vital importancia repasar brevemente algunos conceptos básicos de la teoría de la probabilidad.

- Los resultados mutuamente excluyentes de un proceso aleatorio se denominan simplemente *resultados*. "Mutuamente excluyente" implica que sólo se puede observar uno de los posibles resultados.
- La *probabilidad* de un resultado como se refiere a la proporción en que el resultado ocurre a largo plazo; es decir, si el experimento se repite muchas veces.
- El conjunto de todos los resultados posibles de una variable aleatoria se denomina *espacio muestral*.
- Un *evento* es un subconjunto del espacio muestral y consta de uno o más resultados.

Estas ideas están unificadas en un concepto llamado *variable aleatoria* que es un resumen numérico de resultados aleatorios. Las variables aleatorias pueden ser *discretas* o *continuas*.

- Las variables aleatorias discretas tienen resultados discretos, por ejemplo, $0$ y $1$ (números enteros).
- Una variable aleatoria continua puede tomar un continuo de valores posibles, por ejemplo, $0.5$ y $1.25$ (números decimales).

### Distribuciones de probabilidad de variables aleatorias discretas {-}

Un ejemplo típico de una variable aleatoria discreta $D$ es el resultado de lanzar un dado: en términos de un experimento aleatorio, esto no es más que seleccionar al azar una muestra de tamaño $1$ de un conjunto de números que son resultados mutuamente excluyentes. Aquí, el espacio muestral es $\{1,2,3,4,5,6\}$ y se puede pensar en muchos otros eventos, por ejemplo, "el resultado observado se puede encuentra entre $2$ y $5$". 

Una función básica para extraer muestras aleatorias de un conjunto específico de elementos es la función **sample()**, consulte `?Sample`. Se puede usar para simular el resultado aleatorio de una tirada de dados. ¡Se tiran los dados!

```{r, 148, echo = T, eval = T, message = F, warning = F} 
sample(1:6, 1) 
```

La distribución de probabilidad (DP) de una variable aleatoria discreta es la lista de todos los valores posibles de la variable y sus probabilidades, que suman $1$. La función de distribución de probabilidad (FDP) acumulada da la probabilidad de que la variable aleatoria sea menor o igual a un valor particular.

Para la tirada de dados, la distribución de probabilidad (DP) y la distribución de probabilidad acumulada (DPA) se resumen en la Tabla \@ref(tab:pdist).

| Resultado                 |  1  |  2  |  3  |  4  |  5  |  6  |
|---------------------------|:---:|:---:|:---:|:---:|:---:|:---:|
| Probabilidad              | 1/6 | 1/6 | 1/6 | 1/6 | 1/6 | 1/6 |
| Probabilidad Acumulada    | 1/6 | 2/6 | 3/6 | 4/6 | 5/6 |  1  |

Table: (\#tab:pdist) Función de Distribución de Probabilidad (FDP) y Función de Distribución de Probabilidad Acumulada (FDPA) de una tirada de dados

Se puede graficar fácilmente ambas funciones usando **R**. Dado que la probabilidad es igual a $1/6$ para cada resultado, se configura el vector **Probabilidad** usando la función **rep()** que replica un valor dado un número específico de veces.

```{r, 149, eval = T, message = F, warning = F, fig.align='center', fig.pos="h"} 
# generar el vector de probabilidades 
Probabilidad <- rep(1/6, 6) 

# graficar las probabilidades 
plot(Probabilidad,
     xlab = "Resultados",
     main = "Distribución de Probabilidad (DP)") 
```

Para la distribución de probabilidad acumulada, se necesitan las probabilidades acumuladas; es decir, se necesitan las sumas acumuladas del vector **Probabilidad**. Dichas sumas se pueden calcular usando **cumsum()**.

```{r, 150, echo = T, eval = T, message = F, warning = F, fig.align='center'} 
# generar el vector de probabilidades acumuladas 
Probabilidad_acumulada <- cumsum(Probabilidad) 

# graficar las probabilidades
plot(Probabilidad_acumulada, 
     xlab = "Resultados",
     main = "Distribución de Probabilidad Acumulada (DPA)") 
```

### Ensayos de Bernoulli {-}

El conjunto de elementos de los que **sample()** extrae resultados no tiene por qué consistir solo en números. También se podría simular el lanzamiento de una moneda con los resultados $H$ (cara) y $T$ (cruz).

```{r, 151, echo = T, eval = T, message = F, warning = F} 
sample(c("H", "T"), 1) 
```

El resultado de un solo lanzamiento de la moneda es una variable aleatoria con distribución de *Bernoulli*; es decir, una variable con dos posibles resultados distintos.

Imagine que está a punto de lanzar una moneda $10$ veces seguidas y se pregunta qué tan probable es que termine con $5$ caras. Este es un ejemplo típico de lo que se llama un *experimento de Bernoulli*, ya que consta de $n = 10$ ensayos de Bernoulli que son independientes entre sí y se está interesado en la probabilidad de observar $k = 5$ éxitos de $H$ que ocurren con probabilidad $p = 0.5$ (asumiendo que la moneda no tiene truco, una moneda justa) en cada prueba. Tenga en cuenta que aquí no importa el orden de los resultados.

Es un [resultado bien conocido](https://en.wikipedia.org/wiki/Binomial_distribution) que el número de éxitos $k$ en un experimento de Bernoulli sigue una distribución binomial. Se denota esto como

$$k \sim B(n,p).$$

La probabilidad de observar $k$ éxitos en el experimento $B(n, p)$ viene dada por

$$f(k)=P(k)=\begin{pmatrix}n\\ k \end{pmatrix} \cdot p^k \cdot (1-p)^{n-k}=\frac{n!}{k!(n-k)!} \cdot p^k \cdot (1-p)^{n-k}$$

con el coeficiente binomial $\begin{pmatrix}n\\ k \end{pmatrix}$.

En **R**, se pueden resolver problemas como el anterior mediante la función **dbinom()** que calcula $P(k\vert n, p)$, la probabilidad de la distribución binomial dados los parámetros **x** ($k$), **tamaño** ($n$) y **prob** ($p$), consulte `?dbinom`. Se calcula $P(k=5\vert n = 10, p = 0.5)$ (se escribe en corto como $P(k=5)$.)

```{r, 152, echo = T, eval = T, message = F, warning = F} 
dbinom(x = 5,
       size = 10,
       prob = 0.5) 
```

Se concluye que $P(k=5)$, la probabilidad de observar cara $k = 5$ veces cuando se lanza una moneda justa $n = 10$ veces es de aproximadamente $24,6 \%$.

Ahora suponga que se está interesado en $P(4 \leq k \leq 7)$; es decir, la probabilidad de observando éxitos de $4$, $5$, $6$ o $7$ para $B(10, 0.5)$. Esto se puede calcular proporcionando un vector como argumento **x** en la escritura de **dbinom()** y resumiendo usando **sum()**.

```{r, 153, echo = T, eval = T, message = F, warning = F} 
# calcular P(4 <= k <= 7) usando 'dbinom()'
sum(dbinom(x = 4:7, size = 10, prob = 0.5))
```

Un enfoque alternativo es usar **pbinom()**, que es la función de distribución, en específico, de la distribución binomial para calcular $$P(4 \leq k \leq 7) = P(k \leq 7) - P(k\leq3 ).$$

```{r, 154, echo = T, eval = T, message = F, warning = F}
# calcular P(4 <= k <= 7) usando 'pbinom()'
pbinom(size = 10, prob = 0.5, q = 7) - pbinom(size = 10, prob = 0.5, q = 3) 
```

La distribución de probabilidad de una variable aleatoria discreta no es más que una lista de todos los resultados posibles que pueden ocurrir y sus respectivas probabilidades. En el ejemplo del lanzamiento de una moneda, se tienen $11$ posibles resultados para $k$.

```{r, 155, echo = T, eval = T, message = F, warning = F}
# configurar el vector de posibles resultados
k <- 0:10
k
```

Por lo tanto, para visualizar la función de distribución de probabilidad de $k$ se puede hacer lo siguiente:

```{r, 156, echo = T, eval = T, message = F, warning = F, fig.align='center'} 
# asignar las probabilidades
Probabilidad <- dbinom(x = k,
                      size = 10, 
                      prob = 0.5)

# graficar los resultados contra sus probabilidades
plot(x = k, 
     y = Probabilidad,
     main = "Función de Distribución de Probabilidad (FDP)") 
```

De manera similar, se puede graficar la función de distribución acumulativa de $k$ ejecutando el siguiente fragmento de código:

```{r, 157, echo = T, eval = T, message = F, warning = F, fig.align='center'} 
# calcular probabilidades acumuladas
Probabilidad <- pbinom(q = k, 
               size = 10, 
               prob = 0.5)

# graficar las probabilidades acumuladas
plot(x = k, 
     y = Probabilidad,
     main = "Función de Distribución de Probabilidad Acumulada (FDPA)") 
```

### Valor esperado, media y varianza {-}

El valor esperado de una variable aleatoria es, en términos generales, el valor promedio a largo plazo de sus resultados cuando el número de ensayos repetidos es grande. Para una variable aleatoria discreta, el valor esperado se calcula como un promedio ponderado de sus posibles resultados, por lo que las ponderaciones son las probabilidades relacionadas. Esto se formaliza en el concepto clave 2.1.

```{r, 158, eval = my_output == "html", results='asis', echo=FALSE, purl=FALSE}
cat('<div class = "keyconcept" id="KC2.1"> 
<h3 class = "right"> Concepto clave 2.1 </h3> 
<h3 class= "left"> Valor esperado y media </h3> 

<p> 
Suponga que la variable aleatoria $Y$ toma $k$ valores posibles, $y_1, \\dots, y_k$, donde $y_1$ denota el primer valor, $y_2$ denota el segundo valor, y así sucesivamente, y que la probabilidad que $Y$ toma $y_1$ es $p_1$, la probabilidad de que $Y$ tome $y_2$ es $p_2$ y así sucesivamente. El valor esperado de $Y$, $E(Y)$ se define como

$$ E(Y) = y_1 p_1 + y_2 p_2 + \\cdots + y_k p_k = \\sum_{i=1}^k y_i p_i $$

donde la notación $\\sum_{i=1}^k y_i p_i$ implica "la suma de $y_i$ $p_i$ para $i$ desde $1$ a $k$". El valor esperado de $Y$ también se llama la media de $Y$ o la expectativa de $Y$ y se denota por $\\mu_Y$.
</p> 
</div>')
```

```{r, 159, eval = my_output == "latex", results='asis', echo=FALSE, purl=FALSE}
cat('\\begin{keyconcepts}[Valor esperado y media]{2.1}
Suponga que la variable aleatoria $Y$ toma $k$ valores posibles, $y_1, \\ puntos, y_k$, donde $y_1$ denota el primer valor, $y_2$ denota el segundo valor, y así sucesivamente, y que la probabilidad que $Y$ toma $y_1$ es $p_1$, la probabilidad de que $Y$ tome $y_2$ es $p_2$ y así sucesivamente. El valor esperado de $Y$, $E(Y)$ se define como

$$ E(Y) = y_1 p_1 + y_2 p_2 + \\cdots + y_k p_k = \\sum_{i=1}^k y_i p_i $$

donde la notación $\\sum_{i=1}^k y_i p_i$ significa \\"la suma de $y_i$ $p_i$ para $i$ desde $1$ hasta $k$ \\". El valor esperado de $Y$ también se denomina media de $Y$ o la expectativa de $Y$ y se denota por $\\ mu_Y$.
\\end{keyconcepts}')
```

En el ejemplo de los dados, la variable aleatoria, $D$ digamos, toma $6$ valores posibles $d_1 = 1, d_2 = 2, \dots, d_6 = 6$. Suponiendo un dado justo, cada uno de los resultados de $6$ ocurre con una probabilidad de $1/6$. Por lo tanto, es fácil calcular el valor exacto de $E(D)$ a mano:

$$ E(D) = 1/6 \sum_{i=1}^6 d_i = 3.5 $$

$E(D)$ es simplemente el promedio de los números naturales de $1$ a $6$ ya que todos los pesos $p_i$ son $1/6$. Esto se puede calcular fácilmente usando la función **mean()** que calcula la media aritmética de un vector numérico.

```{r, 160, echo = T, eval = T, message = F, warning = F} 
# calcular la media de números naturales del 1 al 6
mean(1:6)
```

Un ejemplo de muestreo con reemplazo es tirar un dado tres veces seguidas.

```{r, 161, eval = T, message = F, warning = F} 
# sembrar la semilla para la reproducibilidad
set.seed(1)

# tira un dado tres veces seguidas
sample(1:6, 3, replace = T)
```

Tenga en cuenta que cada llamada de `sample (1: 6, 3, replace = T)` da un resultado diferente, ya que se dibuja con reemplazo al azar. Para permitirle reproducir los resultados de los cálculos que involucran números aleatorios, se usará `set.seed()` para configurar el generador de números aleatorios de R en un estado específico. Debe verificar que realmente funcione: ¡Establezca la semilla en su sesión R en 1 y verifique que obtenga los mismos tres números aleatorios!

```{block2, randomseed, type='rmdknit'}
Las secuencias de números aleatorios generados por R son números pseudoaleatorios; es decir, no son "verdaderamente" aleatorios sino que se aproximan a las propiedades de las secuencias de números aleatorios. Dado que esta aproximación es suficientemente buena para los propósitos del presete trabajo, piense en los números pseudoaleatorios como números aleatorios a lo largo de este curso.

En general, las secuencias de números aleatorios se generan mediante funciones denominadas "generadores de números pseudoaleatorios" (GNP). El GNP en R funciona realizando alguna operación sobre un valor determinista. Generalmente, este valor es el número anterior generado por el GNP. Sin embargo, la primera vez que se usa el GNP, no existe un valor previo. Una "semilla" es el primer valor de una secuencia de números --- inicializa la secuencia. Cada valor semilla corresponderá a una secuencia de valores diferente. En R, se puede establecer una semilla usando <tt>set.seed()</tt>.

Esto es conveniente para el presente curso:

Si se proporciona la misma semilla dos veces, se obtiene la misma secuencia de números dos veces. Por lo tanto, establecer una semilla antes de ejecutar el código R que involucra números aleatorios hace que el resultado sea reproducible.
```

Por supuesto, también se podría considerar un número mucho mayor de pruebas, por ejemplo, $10000$. Al hacerlo, no tendría sentido simplemente imprimir los resultados en la consola: por defecto **R** muestra hasta $1000$ entradas de vectores grandes y omite el resto (pruébelo). Observar los números no revela mucho. En su lugar, se calcula el promedio de la muestra de los resultados usando **mean()** y viendo si el resultado se acerca al valor esperado $E(D)=3.5$.

```{r, 162, eval = T, message = F, warning = F} 
# sembrar la semilla para la reproducibilidad
set.seed(1)

# calcular la media muestral de 10000 tiradas de dados
mean(sample(1:6, 
           10000, 
           replace = T))
```

Se encuentra que la media muestral está bastante cerca del valor esperado. Este resultado se discutirá en el Capítulo \@ref(MADPM) con más detalle.

Otras medidas que se encuentran con frecuencia son la varianza y la desviación estándar. Ambas son medidas de la *dispersión* de una variable aleatoria.

```{r, 163, eval = my_output == "html", results='asis', echo=FALSE, purl=FALSE}
cat('<div class = "keyconcept" id="KC2.2">
<h3 class = "right"> Concepto clave 2.2 </h3> 
<h3 class= "left"> Varianza y desviación estándar </h3> 

<p> 
La varianza de la variable aleatoria discreta $Y$, denotada $\\sigma^2_Y$, es

$$ \\sigma^2_Y = \\text{Var}(Y) = E\\left[(Y-\\mu_y)^2\\right] = \\sum_{i=1}^k (y_i - \\mu_y)^2 p_i $$

La desviación estándar de $Y$ es $\\sigma_Y$, la raíz cuadrada de la varianza. Las unidades de la desviación estándar son las mismas que las unidades de $Y$.
</p> 
</div>')
```

```{r, 164, eval = my_output == "latex", results='asis', echo=FALSE, purl=FALSE}
cat('\\begin{keyconcepts}[Varianza y desviación estándar]{2.2}
La varianza de la \\textit{variable aleatoria discreta} $Y$, denotada $\\sigma^2_Y$, es

$$ \\sigma^2_Y = \\text{Var}(Y) = E\\left[(Y-\\mu_Y)^2\\right] = \\sum_{i=1}^k (y_i - \\mu_Y)^2 p_i $$

La desviación estándar de $Y$ es $\\sigma_Y$, la raíz cuadrada de la varianza. Las unidades de la desviación estándar son las mismas que las unidades de $Y$.
\\end{keyconcepts}')
```

La varianza, como se define en el Concepto clave 2.2, siendo una cantidad de población, *no se* implementa como una función en R. En su lugar, se tiene la función **var()** que calcula la *varianza de la muestra*

$$ s^2_Y = \frac{1}{n-1} \sum_{i=1}^n (y_i - \overline{y})^2. $$

Resulta importante recordar que $s^2_Y$ es diferente de la llamada *varianza poblacional* de una variable aleatoria discreta $Y$,

$$ \text{Var}(Y) = \frac{1}{N} \sum_{i=1}^N (y_i - \mu_Y)^2 $$

ya que mide cómo las observaciones de $n$ en la muestra se dispersan alrededor del promedio de la muestra $\overline{y}$. En cambio, $\text{Var}(Y)$ mide la dispersión de toda la población ($N$ miembros) alrededor de la media de la población $\mu_Y$. La diferencia se vuelve clara cuando se mira el ejemplo de lanzamiento de dados. Por $D$ se tiene

$$ \text{Var}(D) = 1/6 \sum_{i=1}^6 (d_i - 3.5)^2 = 2.92  $$

que es obviamente diferente del resultado de $s^2$ calculado por **var()**.

```{r, 165, echo = 1, eval = T, message = F, warning = F} 
var(1:6)
```

La varianza muestral calculada por **var()** es un *estimador* de la varianza poblacional. Se puede verificar esto usando el widget a continuación.

```{r, 166, echo=FALSE, results='asis', purl=FALSE}
write_html(playground = T)
```

### Distribuciones de probabilidad de variables aleatorias continuas {-}

Dado que una variable aleatoria continua toma un continuo de valores posibles, no se puede usar el concepto de distribución de probabilidad como se usa para las variables aleatorias discretas. En cambio, la distribución de probabilidad de una variable aleatoria continua se resume mediante su *función de densidad de probabilidad* (FDP).

La función de distribución de probabilidad acumulada (DPA) para una variable aleatoria continua se define como en el caso discreto. Por lo tanto, la DPA de una variable aleatoria continua establece la probabilidad de que la variable aleatoria sea menor o igual a un valor particular.

Para completar, se presentan revisiones de los conceptos clave 2.1 y 2.2 para el caso continuo.

```{r, 167, eval = my_output == "html", results='asis', echo=FALSE, purl=FALSE}
cat('<div class = "keyconcept" id="KC2.3"> 
<h3 class = "right"> Concepto clave 2.3 </h3> 
<h3 class= "left"> Probabilidades, valor esperado y varianza de una variable aleatoria continua </h3> 

<p> 
Sea $f_Y(y)$ la función de densidad de probabilidad de $Y$. La probabilidad de que $Y$ caiga entre $a$ y $b$ donde $a < b$ es

$$ P(a \\leq Y \\leq b) = \\int_a^b f_Y(y) \\mathrm{d}y. $$

Además se tiene que $P(-\\infty \\leq Y \\leq \\infty) = 1$ y, por lo tanto, $\\int_{-\\infty}^{\\infty} f_Y(y) \\mathrm{d}y = 1$.

En cuanto al caso discreto, el valor esperado de $Y$ es el promedio ponderado de probabilidad de sus valores. Debido a la continuidad, se usan integrales en lugar de sumas. El valor esperado de $Y$ se define como

$$ E(Y) =  \\mu_Y = \\int y f_Y(y) \\mathrm{d}y. $$

La varianza es el valor esperado de $(Y - \\mu_Y)^2$. Así se tiene

$$\\text{Var}(Y) =  \\sigma_Y^2 = \\int (y - \\mu_Y)^2 f_Y(y) \\mathrm{d}y.$$ 
</p> 
</div>')
```

```{r, 168, eval = my_output == "latex", results='asis', echo=FALSE, purl=FALSE}
cat('
\\begin{keyconcepts}[Probabilidades\\comma Valor esperado y varianza de una variable aleatoria continua]{2.3}
Sea $f_Y(y)$ la función de densidad de probabilidad de $Y$. La probabilidad de que $Y$ caiga entre $a$ y $b$ donde $a < b$ es

$$ P(a \\leq Y \\leq b) = \\int_a^b f_Y(y) \\mathrm{d}y. $$

Además se tiene que $P(-\\infty \\leq Y \\leq \\infty) = 1$ y, por lo tanto, $\\int_{-\\infty}^{\\infty} f_Y(y) \\mathrm{d}y = 1$.

En cuanto al caso discreto, el valor esperado de $Y$ es el promedio ponderado de probabilidad de sus valores. Debido a la continuidad, se usan integrales en lugar de sumas. El valor esperado de $Y$ se define como

$$ E(Y) =  \\mu_Y = \\int y f_Y(y) \\mathrm{d}y. $$

La varianza es el valor esperado de $(Y - \\mu_Y)^2$. Así se tiene

$$\\text{Var}(Y) =  \\sigma_Y^2 = \\int (y - \\mu_Y)^2 f_Y(y) \\mathrm{d}y.$$ 
\\end{keyconcepts}')
```

Se analiza un ejemplo:

Considere la variable aleatoria continua $X$ con FDP

$$ f_X(x) = \frac{3}{x^4}, x>1. $$

- Se puede mostrar analíticamente que la integral de $f_X (x)$ sobre la línea real es igual a $1$.

\begin{align}
 \int f_X(x) \mathrm{d}x =&  \int_{1}^{\infty} \frac{3}{x^4} \mathrm{d}x \\
  =& \lim_{t \rightarrow \infty} \int_{1}^{t} \frac{3}{x^4} \mathrm{d}x \\
  =& \lim_{t \rightarrow \infty}  -x^{-3} \rvert_{x=1}^t \\
  =& -\left(\lim_{t \rightarrow \infty}\frac{1}{t^3} - 1\right) \\
  =& 1
\end{align}

- La expectativa de $X$ se puede calcular de la siguiente manera:

\begin{align}
 E(X) = \int x \cdot f_X(x) \mathrm{d}x =&  \int_{1}^{\infty} x \cdot \frac{3}{x^4} \mathrm{d}x \\
  =& - \frac{3}{2} x^{-2} \rvert_{x=1}^{\infty} \\
  =& -\frac{3}{2} \left( \lim_{t \rightarrow \infty} \frac{1}{t^2} - 1 \right) \\
  =& \frac{3}{2}
\end{align}

- Se debe tener en cuenta que la varianza de $X$ se puede expresar como $\text{Var}(X) = E(X^2) - E(X)^2$. Dado que $E(X)$ se ha calculado en el paso anterior, se busca $E(X^2)$:

\begin{align}
 E(X^2)= \int x^2 \cdot f_X(x) \mathrm{d}x =&  \int_{1}^{\infty} x^2 \cdot \frac{3}{x^4} \mathrm{d}x \\
  =& -3 x^{-1} \rvert_{x=1}^{\infty} \\
  =& -3 \left( \lim_{t \rightarrow \infty} \frac{1}{t} - 1 \right) \\
  =& 3
\end{align}

Así que se ha demostrado que el área bajo la curva es igual a uno, que la expectativa es  $E(X)=\frac{3}{2}$ y se encontró que la varianza es $\text{Var}(X) = \frac{3}{4}$. Sin embargo, esto fue tedioso y, como veremos, un enfoque analítico no es aplicable para algunas FDP, por ejemplo, si las integrales no tienen soluciones de forma cerrada.

Afortunadamente, **R** también permite encontrar fácilmente los resultados derivados anteriormente. La herramienta que se usan para esto es la función **integrate()**. Primero, se tienen que definir las funciones para las que se quieren calcular integrales como funciones **R**; es decir, la FDP $f_X(x)$ así como las expresiones $x\cdot f_X(x)$ y $x^2\cdot f_X(x)$.

```{r, 169, echo = T, eval = T, message = F, warning = F}
# definir funciones
f <- function(x) 3 / x^4
g <- function(x) x * f(x)
h <- function(x) x^2 * f(x)
```

A continuación, se usa **integrate()** y se establecen los límites superior e inferior de integración en $1$ y $\infty$ usando argumentos **lower** y **upper**. De forma predeterminada, **integrate()** imprime el resultado junto con una estimación del error de aproximación en la consola. Sin embargo, el resultado no es un valor numérico con el que se puedan hacer más cálculos fácilmente. Para obtener solo un valor numérico de la integral, se necesita usar el operador **\$** junto con **value**. El operador **\$** se usa para extraer elementos por nombre de un objeto de tipo **list**.

```{r, 170, echo = T, eval = T, message = F, warning = F}
# calcular el área bajo la curva de densidad
area <- integrate(f, 
                 lower = 1, 
                 upper = Inf)$value
area 

# calcular E(X)
EX <- integrate(g,
                lower = 1,
                upper = Inf)$value
EX

# calcular Var(X)
VarX <- integrate(h,
                  lower = 1,
                  upper = Inf)$value - EX^2 
VarX
```

Aunque existe una amplia variedad de distribuciones, las que se encuentran con mayor frecuencia en econometría son las distribuciones normal, chi-cuadrado, Student $t$ y $F$. Por lo tanto, se discutiran algunas funciones básicas **R** que permiten hacer cálculos que involucran densidades, probabilidades y cuantiles de estas distribuciones.

Cada distribución de probabilidad que maneja **R** tiene cuatro funciones básicas cuyos nombres consisten en un prefijo seguido de un nombre raíz. Se tiene como ejemplo la distribución normal. El nombre raíz de las cuatro funciones asociadas con la distribución normal es **norm**. Los cuatro prefijos son:

- **d** para "densidad" - función de probabilidad / función de densidad de probabilidad
- **p** para "probabilidad" - función de distribución acumulativa
- **q** para "cuantil" - función cuantil (función de distribución acumulativa inversa)
- **r** para "aleatorio" - generador de números aleatorios

Así, para la distribución normal se tienen las funciones **R** **dnorm()**, **pnorm()**, **qnorm()** y **rnorm()**.

### La distribución normal {-}

La distribución de probabilidad probablemente más importante considerada aquí es la distribución normal. Esto se debe sobre todo al papel especial de la distribución normal estándar y al teorema del límite central, que se tratará en breve. Las distribuciones normales son simétricas y en forma de campana. Una distribución normal se caracteriza por su media $\mu$ y su desviación estándar $\sigma$, expresada de manera concisa por $\mathcal{N}(\mu,\sigma^2)$. La distribución normal tiene el FDP:

\begin{align}
f(x) = \frac{1}{\sqrt{2 \pi} \sigma} \exp{-(x - \mu)^2/(2 \sigma^2)}.
\end{align}

Para la distribución normal estándar se tiene $\mu = 0$ y $\sigma = 1$. Las variantes normales estándar a menudo se indican con $Z$. Por lo general, la FDP normal estándar se indica con $\phi$ y la FDPA normal estándar se indica con $\Phi$. Por eso,

$$ \phi(c) = \Phi'(c) \ \ , \ \ \Phi(c) = P(Z \leq c) \ \ , \ \ Z \sim \mathcal{N}(0,1).$$ 

Tenga en cuenta que la notación X $\sim$ Y se lee como "X se distribuye como Y". En **R**, se puede obtener convenientemente densidades de distribuciones normales usando la función **dnorm()**. Es momento de dibujar una gráfica de la función de densidad normal estándar usando **curve()** junto con **dnorm()**.

```{r, 171, echo = T, eval = T, message = F, warning = F, fig.align='center'} 
# dibujar un gráfico de la FDP N(0,1)
curve(dnorm(x),
      xlim = c(-3.5, 3.5),
      ylab = "Densidad", 
      main = "Función de densidad normal estándar") 
```

Se puede obtener la densidad en diferentes posiciones pasando un vector por **dnorm()**.

```{r, 172, echo = T, eval = T, message = F, warning = F}
# calcular la densidad en x = -1.96, x = 0 y x = 1.96
dnorm(x = c(-1.96, 0, 1.96))
```

Similar a la FDP, se puede trazar la FDPA normal estándar usando **curve()**. Se podría usar **dnorm()** para esto, pero es mucho más conveniente confiar en **pnorm()**.

```{r, 173, echo = T, eval = T, message = F, warning = F, fig.align='center'}
# graficar la FDPA normal estándar
curve(pnorm(x), 
      xlim = c(-3.5, 3.5), 
      ylab = "Probabilidad", 
      main = "Función de distribución acumulativa normal estándar")
```

También se puede usar **R** para calcular la probabilidad de eventos asociados con una variable normal estándar.

Suponiendo que se está interesado en $P(Z\leq 1.337)$. Para alguna variable aleatoria continua $Z$ en $[-\infty, \infty]$ con densidad $g(x)$ se tendría que determinar $G(x)$, la anti-derivada de $g(x)$ así que 

$$ P(Z \leq 1.337 ) = G(1.337) = \int_{-\infty}^{1.337} g(x) \mathrm{d}x.  $$

Si $Z \sim \mathcal{N}(0,1)$, se tiene $g(x)=\phi(x)$. No existe una solución analítica para la integral anterior. Afortunadamente, **R** ofrece buenas aproximaciones. El primer enfoque hace uso de la función **integrate()** que permite resolver problemas de integración unidimensionales utilizando un método numérico. Para esto, primero se define la función de la que se quiere calcular la integral como una función **R** **f**. En el ejemplo, **f** es la función de densidad normal estándar y, por lo tanto, toma un solo argumento **x**. Siguiendo la definición de $\phi(x)$ se define **f** como

```{r, 174, echo = T, eval = T, message = F, warning = F} 
# definir la FDP normal estándar como una función R
f <- function(x) {
  1/(sqrt(2 * pi)) * exp(-0.5 * x^2)
}
```

Se debe comprobar si esta función calcula densidades normales estándar pasando un vector.

```{r, 175, echo = T, eval = T, message = F, warning = F}
# definir un vector de reales
quants <- c(-1.96, 0, 1.96)

# calcular densidades
f(quants)

# comparar con los resultados producidos por 'dnorm()'
f(quants) == dnorm(quants)
```

Los resultados producidos por **f()** son, de hecho, equivalentes a los dados por **dnorm()**.

A continuación, se llama a **integrate()** en **f()** y se especifican los argumentos **lower** y **upper**, los límites inferior y superior de integración.

```{r, 176, echo = T, eval = T, message = F, warning = F}
# integrar f()
integrate(f, 
          lower = -Inf, 
          upper = 1.337)
```

Se encuentra que la probabilidad de observar $Z \leq 1.337$ es aproximadamente $90.94\% $.

Una segunda y mucho más conveniente forma es usar la función **pnorm()**, la función de distribución acumulativa normal estándar.

```{r, 177, echo = T, eval = T, message = F, warning = F} 
# calcular la probabilidad usando pnorm()
pnorm(1.337)
```

El resultado coincide con el resultado del enfoque utilizando **integrate()**.

Es momento de analizar algunos ejemplos adicionales:

Un resultado comúnmente conocido es que $95\%$ de la masa de probabilidad de una normal estándar se encuentra en el intervalo $[-1.96, 1.96]$; es decir, en una distancia de aproximadamente $2$ desviaciones estándar de la media. Se puede confirmar esto fácilmente calculando $$ P(-1.96 \leq Z \leq 1.96) = 1-2\times P(Z \leq -1.96) $$ debido a la simetría de la FDP normal estándar. Gracias a **R**, se puede abandonar la tabla de la FDPA normal estándar que se encuentra en muchos otros libros de texto y, en su lugar, resolver esto rápidamente usando **pnorm()**.

```{r, 178, echo = T, eval = T, message = F, warning = F} 
# calcula la probabilidad
1 - 2 * (pnorm(-1.96)) 
```

Para hacer afirmaciones sobre la probabilidad de observar resultados de $Y$ en algún rango específico es conveniente estandarizar primero como se muestra en el Concepto clave 2.4.

```{r, 179, eval = my_output == "html", results='asis', echo=FALSE, purl=FALSE}
cat('<div class = "keyconcept" id="KC2.4">
<h3 class = "right"> Concepto clave 2.4 </h3> 
<h3 class = "left"> Calcular probabilidades que involucran variables aleatorias normales </h3>

<p>
Suponga que $Y$ se distribuye normalmente con la media $\\mu$ y la varianza $\\sigma^2$:

$$Y \\sim \\mathcal{N}(\\mu, \\sigma^2)$$ 

Entonces $Y$ se estandariza restando su media y dividiendo por su desviación estándar: 

$$ Z = \\frac{Y -\\mu}{\\sigma} $$ 

Sea que $c_1$ y $c_2$ denotan dos números en los que $c_1 < c_2$ y más $d_1 = (c_1 - \\mu)/\\sigma$ y $d_2 = (c_2 - \\mu)/\\ sigma$. Luego

\\begin{align*} 
P(Y \\leq c_2) =& \\, P(Z \\leq d_2) = \\Phi(d_2), \\\\ 
P(Y \\geq c_1) =& \\, P(Z \\geq d_1) = 1 - \\Phi(d_1), \\\\ 
P(c_1 \\leq Y \\leq c_2) =& \\, P(d_1 \\leq Z \\leq d_2) = \\Phi(d_2) - \\Phi(d_1). 
\\end{align*}
</p> 
</div>')
```

```{r, 180, eval = my_output == "latex", results='asis', echo=FALSE, purl=FALSE}
cat('\\begin{keyconcepts}[Calcular probabilidades que involucran variables aleatorias normales]{2.4}
Suponga que $Y$ se distribuye normalmente con la media $\\mu$ y la varianza $\\sigma^2$:

$$Y \\sim \\mathcal{N}(\\mu, \\sigma^2)$$ 

Entonces $Y$ se estandariza restando su media y dividiendo por su desviación estándar: 

$$ Z = \\frac{Y -\\mu}{\\sigma} $$ 

Sea que $c_1$ y $c_2$ denotan dos números en los que $c_1 < c_2$ y más $d_1 = (c_1 - \\mu)/\\sigma$ y $d_2 = (c_2 - \\mu)/\\ sigma$. Luego

\\begin{align*} 
P(Y \\leq c_2) =& \\, P(Z \\leq d_2) = \\Phi(d_2), \\\\ 
P(Y \\geq c_1) =& \\, P(Z \\geq d_1) = 1 - \\Phi(d_1), \\\\ 
P(c_1 \\leq Y \\leq c_2) =& \\, P(d_1 \\leq Z \\leq d_2) = \\Phi(d_2) - \\Phi(d_1). 
\\end{align*}
\\end{keyconcepts}')
```

Ahora considere una variable aleatoria $Y$ con $Y \sim \mathcal{N}(5, 25)$. Las funciones de **R** que utilizan la distribución normal pueden realizar la estandarización. Si se está interesado en $P(3 \leq Y \leq 4)$ se puede usar **pnorm()** y ajustar por una media y/o una desviación estándar que se desvíe de $\mu = 0$ y $\sigma = 1$ especificando los argumentos **mean** y **sd**, respectivamente. **Atención**: ¡El argumento **sd** requiere la desviación estándar, no la varianza!

```{r, 181, echo = T, eval = T, message = F, warning = F} 
pnorm(4, mean = 5, sd = 5) - pnorm(3, mean = 5, sd = 5) 
```

Una extensión de la distribución normal en un entorno univariante es la distribución normal multivariante. La FDP conjunta de dos variables normales aleatorias $X$ y $Y$ viene dada por

\begin{align}
\begin{split}
g_{X,Y}(x,y) =& \, \frac{1}{2\pi\sigma_X\sigma_Y\sqrt{1-\rho_{XY}^2}} \\ 
\cdot & \, \exp \left\{ \frac{1}{-2(1-\rho_{XY}^2)} \left[ \left( \frac{x-\mu_x}{\sigma_X} \right)^2 - 2\rho_{XY}\left( \frac{x-\mu_X}{\sigma_X} \right)\left( \frac{y-\mu_Y}{\sigma_Y} \right) + \left( \frac{y-\mu_Y}{\sigma_Y} \right)^2 \right]  \right\}.
\end{split} (\#eq:bivnorm)
\end{align}

La ecuación \@ref(eq:bivnorm) contiene la FDP normal bivariado. Es un poco difícil obtener información a partir de esta complicada expresión. En su lugar, se considera el caso especial en el que $X$ y $Y$ son variables aleatorias normales estándar no correlacionadas con densidades $f_X(x)$ y $f_Y(y)$ con distribución normal conjunta. Entonces se tienen los parámetros $\sigma_X = \sigma_Y = 1$, $\mu_X = \mu_Y = 0$ (debido a la normalidad estándar marginal) y $\rho_{XY} = 0$ (debido a la independencia). La densidad conjunta de $X$ y $Y$ se convierte en:

$$ g_{X,Y}(x,y) = f_X(x) f_Y(y) = \frac{1}{2\pi} \cdot \exp \left\{ -\frac{1}{2} \left[x^2 + y^2 \right]  \right\}, \tag{2.2}  $$ 

la FDP de la distribución normal estándar bivariada. El siguiente widget proporciona un gráfico tridimensional interactivo de (<a href="#mjx-eqn-2.2">2.2</a>).

```{r, 182, echo=F, purl=FALSE}
library("knitr")
library("usethis")
library("devtools")
url<-"https://plot.ly/~mca_unidue/22.embed?width=550&height=550?showlink=false" 
plotly_iframe <- paste("<center><iframe scrolling='no' seamless='seamless' style='border:none' src='", url, 
    "/800/1200' width='600' height='400'></iframe></center>", sep = "")
```

`r I(plotly_iframe)`

Al mover el cursor sobre el gráfico, puede ver que la densidad es invariante en rotación; es decir, la densidad en $(a, b)$ depende únicamente de la distancia de $(a, b)$ al origen: geométricamente, regiones de igual densidad son los bordes de círculos concéntricos en el plano $XY$, centrados en $(\mu_X = 0, \mu_Y = 0)$.

La distribución normal tiene algunas características notables. Por ejemplo, para dos variables distribuidas normalmente conjuntamente $X$ y $Y$, la función de expectativa condicional es lineal: se puede mostrar que 

$$ E(Y\vert X) = E(Y) + \rho \frac{\sigma_Y}{\sigma_X} (X - E(X)). $$

El widget interactivo a continuación ofrece datos de una muestra bivariada estándar distribuida normalmente junto con la función de expectativa condicional $E(Y\vert X)$ y las densidades marginales de $X$ y $Y$. Todos los elementos se ajustan en consecuencia a medida que varían los parámetros.

```{r, 183, echo=FALSE, results='asis', purl=FALSE}
if (my_output=="html"){
  cat('
<center>
<iframe height="880" width="770" frameborder="0" scrolling="no" src="DCL/Normal_Bivariado.html"></iframe>
</center>
')
} else {
  cat("\\begin{center}\\textit{Esta parte interactiva del curso solo está disponible en la versión HTML.}\\end{center}")
}
```

<a name="chisquare"></a>

### La distribución chi-cuadrado {-}

La distribución chi-cuadrado es otra distribución relevante en econometría. A menudo es necesario cuando se prueban tipos especiales de hipótesis que se encuentran con frecuencia cuando se trabaja con modelos de regresión.

La suma de las variables aleatorias distribuidas normales estándar independientes de $M$ al cuadrado sigue una distribución de chi-cuadrado con $M$ grados de libertad:

\begin{align*}
Z_1^2 + \dots + Z_M^2 = \sum_{m=1}^M Z_m^2 \sim \chi^2_M \ \ \text{with} \ \ Z_m \overset{i.i.d.}{\sim} \mathcal{N}(0,1) (\#eq:chisq)
\end{align*}

Una variable aleatoria distribuida $\chi^2$ con $M$ grados de libertad tiene expectativa $M$, moda en $M-2$ para $M \geq 2$ y varianza $2 \cdot M$. Por ejemplo, para

$$ Z_1,Z_2,Z_3 \overset{i.i.d.}{\sim} \mathcal{N}(0,1) $$

se sostiene que

$$ Z_1^2+Z_2^2+Z_3^2 \sim \chi^2_3. \tag{2.3} $$

Usando el código a continuación, se puede mostrar la FDP y la FDPA de una variable aleatoria $\chi^2_3$ en un solo gráfico. Esto se logra estableciendo el argumento **add = TRUE** en la segunda llamada de **curve()**. Además, se ajustan los límites de ambos ejes usando **xlim** y **ylim** y se eligen diferentes colores para que ambas funciones se distingan mejor. La trama se completa agregando una leyenda con la ayuda de **legend()**.

```{r, 184, echo = T, eval = T, message = F, warning = F, fig.align='center'} 
# graficar el FDP
curve(dchisq(x, df = 3), 
      xlim = c(0, 10), 
      ylim = c(0, 1), 
      col = "blue",
      ylab = "",
      main = "F.D.P y F.D.P.A de la distribución chi-cuadrado, M = 3")

# Agregar la FDPA al gráfico
curve(pchisq(x, df = 3), 
      xlim = c(0, 10), 
      add = TRUE, 
      col = "red")

# agregar una leyenda al gráfico
legend("topleft", 
       c("FDP", "FDPA"), 
       col = c("blue", "red"), 
       lty = c(1, 1))
```

Dado que los resultados de una variable aleatoria distribuida $\chi^2_M$ son siempre positivos, el soporte de la FDP y FDPA relacionadas es $\mathbb{R}_{\geq0}$.

Como la expectativa y la varianza dependen (¡únicamente!) de los grados de libertad, la forma de la distribución cambia drásticamente si se varía el número de normales estándar cuadradas que se resumen. Dicha relación a menudo se describe superponiendo densidades para diferentes $M$, consulte el siguiente <a href="https://en.wikipedia.org/wiki/Chi-squared_distribution"> artículo de Wikipedia </a>.

Se reproduce esto aquí trazando la densidad de la distribución $\chi_1^2$ en el intervalo $[0, 15]$ con **curve()**. En el siguiente paso, se recorren los grados de libertad $M = 2, ..., 7$ y se agrega una curva de densidad para cada $M$ al gráfico. También se ajusta el color de la línea para cada iteración del ciclo estableciendo **col = M**. Por último, se agrega una leyenda que muestra los grados de libertad y los colores asociados.

```{r, 185, echo = T, eval = T, message = F, warning = F, fig.align='center'} 
# graficar la densidad para M = 1
curve(dchisq(x, df = 1), 
      xlim = c(0, 15), 
      xlab = "x", 
      ylab = "Densidad", 
      main = "Variables aleatorias distribuidas de chi-cuadrado")

# agregar densidades para M = 2, ..., 7 al gráfico usando un bucle 'for()'
for (M in 2:7) {
  curve(dchisq(x, df = M),
        xlim = c(0, 15), 
        add = T, 
        col = M)
}

# agrega una leyenda
legend("topright", 
       as.character(1:7), 
       col = 1:7 , 
       lty = 1, 
       title = "F.D.")
```

Al aumentar los grados de libertad, la distribución se desplaza hacia la derecha (la moda se vuelve más grande) y aumenta la dispersión (la varianza de la distribución aumenta).

### La distribución t de Student {-#thetdist}

<a name="tdist"></a>

Sea $Z$ una variable normal estándar, $W$ una variable aleatoria $\chi^2_M$ y suponiendo además que $Z$ y $W$ son independientes. Entonces se sostiene que

$$ \frac{Z}{\sqrt{W/M}} =:X \sim t_M $$

y $X$ sigue una distribución *$t$ de Student* (o simplemente distribución $t$) con $M$ grados de libertad.

Similar a la distribución $\chi^2_M$, la forma de una distribución $t_M$ depende de $M$. Las distribuciones $t$ son simétricas, en forma de campana y se ven similares a una distribución normal, especialmente cuando $M$ es grande. Esto no es una coincidencia: para un $M$ suficientemente grande, la distribución $t_M$ puede aproximarse mediante la distribución normal estándar. Esta aproximación funciona razonablemente bien para $M\geq 30$. Como se ilustrará más adelante mediante un pequeño estudio de simulación, la distribución $t_{\infty}$ *es la distribución normal estándar*.

En $t_{\infty}$ una variable aleatoria distribuida $X$ tiene una expectativa si $M > 1$ y tiene una variación si $M > 2$.

\begin{align}
  E(X) =& 0, \ M>1 \\
  \text{Var}(X) =& \frac{M}{M-2}, \ M>2
\end{align}

Es momento de graficar algunas distribuciones de $t$ con diferentes $M$ y compararlas con la distribución normal estándar.

```{r, 186, echo = T, eval = T, message = F, warning = F, fig.align='center'} 
# graficar la densidad normal estándar
curve(dnorm(x), 
      xlim = c(-4, 4), 
      xlab = "x", 
      lty = 2, 
      ylab = "Densidad", 
      main = "Densidades de distribuciones t")

# graficar la densidad t para M = 2
curve(dt(x, df = 2), 
      xlim = c(-4, 4), 
      col = 2, 
      add = T)

# graficar la densidad t para M = 4
curve(dt(x, df = 4), 
      xlim = c(-4, 4), 
      col = 3, 
      add = T)

# graficar la densidad t para M = 25
curve(dt(x, df = 25), 
      xlim = c(-4, 4), 
      col = 4, 
      add = T)

# agrega una leyenda
legend("topright", 
       c("N(0, 1)", "M=2", "M=4", "M=25"), 
       col = 1:4, 
       lty = c(2, 1, 1, 1))
```

El gráfico ilustra lo que se ha dicho en el párrafo anterior: a medida que aumentan los grados de libertad, la forma de la distribución $t$ se acerca a la de una curva de campana normal estándar. Ya para $M = 25$ se encuentra poca diferencia con la densidad normal estándar. Si $M$ es pequeño, se encuentra que la distribución tiene colas más pesadas que una normal estándar; es decir, tiene una forma de campana "más gruesa".

### La distribución F {-}

Otra razón de variables aleatorias importante para los econometristas es la razón de dos variables aleatorias independientes distribuidas $\chi^2$ que se dividen por sus grados de libertad $M$ y $n$. La cantidad

$$ \frac{W/M}{V/n} \sim F_{M,n} \ \ \text{with} \ \ W \sim \chi^2_M \ \ , \ \ V \sim \chi^2_n $$

sigue una distribución $F$ con grados de libertad del numerador $M$ y grados de libertad del denominador $n$, denotado $F_{M,n}$. La distribución fue derivada por primera vez por George Snedecor, pero recibió su nombre en honor a [Sir Ronald Fisher](https://en.wikipedia.org/wiki/Ronald_Fisher).

Por definición, el soporte de la FDP y FDPA de una variable aleatoria distribuida $F_{M,n}$ es $\mathbb{R}_{\geq0}$.

Suponiendo que se tiene una variable aleatoria $Y$ distribuida $F$ con grados de libertad de numerador $3$ y grados de libertad de denominador $14$ y se está interesado en $P(Y \geq 2)$. Esto se puede calcular con la ayuda de la función **pf()**. Al establecer el argumento **lower.tail** en **FALSE**, se debe asegurar que **R** calcula $1- P(Y \leq 2)$; es decir, la masa de probabilidad en la cola derecha de $2$.

```{r, 187, echo = T, eval = T, message = F, warning = F}
pf(2, df1 = 3, df2 = 14, lower.tail = F)
```

Se puede visualizar dicha probabilidad dibujando una gráfica de lineal de la densidad relacionada y agregar un sombreado de color con **polygon()**.

```{r, 188, echo = T, eval = T, message = F, warning = F, fig.align='center'}
# definir vectores de coordenadas para los vértices del polígono
x <- c(2, seq(2, 10, 0.01), 10)
y <- c(0, df(seq(2, 10, 0.01), 3, 14), 0)

# graficar la densidad de F_{3, 14}
curve(df(x ,3 ,14), 
      ylim = c(0, 0.8), 
      xlim = c(0, 10), 
      ylab = "Densidad",
      main = "Función de densidad")

# graficar el polígono
polygon(x, y, col = "orange")
```

La distribución $F$ está relacionada con muchas otras distribuciones. Un caso especial importante encontrado en econometría surge si los grados de libertad del denominador son grandes de tal manera que la distribución $F_{M,n}$ puede aproximarse mediante la distribución $F_{M,\infty}$ que resulta ser simplemente la distribución de una variable aleatoria $\chi^2_M$ dividida por sus grados de libertad $M$,

$$ W/M \sim F_{M,\infty} \ \ , \ \ W \sim \chi^2_M. $$

```{r, 189, echo=FALSE, results='asis', purl=FALSE}
if (my_output=="html"){
  cat('
<iframe src="DCL/playground.html" frameborder="0" scrolling="no" style="width:100%;height:340px"></iframe>
')
}  
```

## Muestreo aleatorio y distribución de promedios muestrales {#MADPM}

Para aclarar la idea básica del muestreo aleatorio, volviendo al ejemplo de tirar los dados:

Suponga que se tirando los dados $n$ veces. Esto significa que se está interesado en los resultados aleatorios de $Y_i$, $i = 1, ..., n$ que se caracterizan por la misma distribución. Dado que estos resultados se seleccionan al azar, son *variables aleatorias* en sí mismas y sus resultados diferirán cada vez que se extraiga una muestra; es decir, cada vez que se tiren los dados $n$ veces. Además, cada observación se extrae aleatoriamente de la misma población; es decir, los números de $1$ hasta $6$, y su distribución individual es la misma. Por tanto, $Y_1, \dots, Y_n$ se distribuyen de forma idéntica.

Además, se sabe que el valor de cualquiera de los $Y_i$ no proporciona ninguna información sobre el resto de los resultados. En el ejemplo, sacar un seis como la primera observación en la muestra no altera las distribuciones de $Y_2, \dots , Y_n$: Todos los números tienen la misma probabilidad de ocurrir. Esto significa que todos los $Y_i$ también se distribuyen de forma independiente. Por tanto, $Y_1, \dots, Y_n$ son independientes e idénticamente distribuidos (*i.i.d*).

El ejemplo de los dados usa este esquema de muestreo más simple. Por eso se llama *muestreo aleatorio simple*. Este concepto se resume en el Concepto clave 2.5.

```{r, 190, eval = my_output == "html", results='asis', echo=FALSE, purl=FALSE}
cat('
<div class = "keyconcept" id="KC2.5">
<h3 class = "right"> Concepto clave 2.5 </h3> 
<h3 class = "left"> Muestreo aleatorio simple y Variables aleatorias independientes e idénticamente distribuidas (i.i.d) </h3>
<p>
En un muestreo aleatorio simple, $n$ objetos se extraen al azar de una población. Es igualmente probable que cada objeto termine en la muestra. Se denota el valor de la variable aleatoria $Y$ para el objeto $i^{th}$ dibujado al azar como $Y_i$. Dado que todos los objetos tienen la misma probabilidad de ser tomados y la distribución de $Y_i$ es la misma para todos los $i$, los $Y_i, \\dots, Y_n$ son independientes e idénticamente distribuidos (i.i.d.). Esto significa que la distribución de $Y_i$ es la misma para todos los $i=1,\\dots,n$ y $Y_1$ se distribuyen independientemente de $Y_2, \\dots, Y_n$ y $Y_2$ se distribuyen independientemente de $Y_1, Y_3, \\dots, Y_n$ y así sucesivamente.
</p> 
</div>')
```

```{r, 191, eval = my_output == "latex", results='asis', echo=FALSE, purl=FALSE}
cat('\\begin{keyconcepts}[Muestreo aleatorio simple e i.i.d. Variables aleatorias]{2.5}
En un muestreo aleatorio simple, $n$ objetos se extraen al azar de una población. Es igualmente probable que cada objeto termine en la muestra. Se denota el valor de la variable aleatoria $Y$ para el objeto $i^{th}$ dibujado al azar como $Y_i$. Dado que todos los objetos tienen la misma probabilidad de ser tomados y la distribución de $Y_i$ es la misma para todos los $i$, los $Y_i, \\dots, Y_n$ son independientes e idénticamente distribuidos (i.i.d.). Esto significa que la distribución de $Y_i$ es la misma para todos los $i=1,\\dots,n$ y $Y_1$ se distribuyen independientemente de $Y_2, \\dots, Y_n$ y $Y_2$ se distribuyen independientemente de $Y_1, Y_3, \\dots, Y_n$ y así sucesivamente.
\\end{keyconcepts}')
```

¿Qué sucede si se consideran funciones de los datos de la muestra? Considere, una vez más, el ejemplo de lanzar un dado dos veces seguidas. Una muestra ahora consta de dos extracciones aleatorias independientes del conjunto $\{1,2,3,4,5,6\}$. Es evidente que cualquier función de estas dos variables aleatorias también es aleatoria; por ejemplo, su suma. Convénzase ejecutando el siguiente código varias veces.

```{r, 192, echo = T, eval = T, message = F, warning = F} 
sum(sample(1:6, 2, replace = T))
```

Claramente, esta suma, llamada $S$, es una variable aleatoria, dado que depende de sumandos extraídos aleatoriamente. Para este ejemplo, se pueden enumerar completamente todos los resultados y, por lo tanto, escribir la distribución de probabilidad teórica de la función de los datos de la muestra $S$:

En esta situación se está enfrentando a $6^2 = 36$ pares posibles. Esos pares son:

\begin{align*}
  &(1,1)	(1,2)	(1,3)	(1,4)	(1,5)	(1,6) \\ 
  &(2,1)	(2,2)	(2,3)	(2,4)	(2,5)	(2,6) \\ 
  &(3,1)	(3,2)	(3,3)	(3,4)	(3,5)	(3,6) \\ 
  &(4,1)	(4,2)	(4,3)	(4,4)	(4,5)	(4,6) \\ 
  &(5,1)	(5,2)	(5,3)	(5,4)	(5,5)	(5,6) \\ 
  &(6,1)	(6,2)	(6,3)	(6,4)	(6,5)	(6,6)
\end{align*}

Por lo tanto, los posibles resultados para $S$ son:

$$ \left\{ 2,3,4,5,6,7,8,9,10,11,12 \right\} . $$

Enumeración de rendimientos de los resultados:

\begin{align}
  P(S) = 
  \begin{cases} 
    1/36, \ & S = 2 \\ 
    2/36, \ & S = 3 \\
    3/36, \ & S = 4 \\
    4/36, \ & S = 5 \\
    5/36, \ & S = 6 \\
    6/36, \ & S = 7 \\
    5/36, \ & S = 8 \\
    4/36, \ & S = 9 \\
    3/36, \ & S = 10 \\
    2/36, \ & S = 11 \\
    1/36, \ & S = 12
  \end{cases}
\end{align}

También se puede calcular $E(S)$ y $\text{Var}(S)$ como se indica en el Concepto clave 2.1 y el Concepto clave 2.2.

```{r, 193, echo = T, eval = T, message = F, warning = F} 
# Vector de resultados
S <- 2:12

# Vector de probabilidades
PS <- c(1:6, 5:1) / 36

# Expectativa de S
ES <- sum(S * PS)
ES

# Varianza de S
VarS <- sum((S - c(ES))^2 * PS)
VarS
```

Entonces se conoce la distribución de $S$. También es evidente que su distribución difiere considerablemente de la distribución marginal; es decir, la distribución del resultado de una sola tirada de dados, $D$. Se puede visualizar esto usando gráficos de barras.

```{r, 194, echo = T, eval = T, message = F, warning = F, fig.align='center'} 
# divide el área de trazado en una fila con dos columnas
par(mfrow = c(1, 2))

# graficar la distribución de S
barplot(PS, 
        ylim = c(0, 0.2), 
        xlab = "S", 
        ylab = "Probabilidad", 
        col = "steelblue", 
        space = 0, 
        main = "Suma de dos lanzamientos")

# graficar la distribución de D
probability <- rep(1/6, 6)
names(probability) <- 1:6

barplot(probability, 
        ylim = c(0, 0.2), 
        xlab = "D", 
        col = "steelblue", 
        space = 0, 
        main = "Resultado de un solo lanzamiento")
```

Muchos procedimientos econométricos tratan con promedios de datos muestreados. Por lo general, se asume que las observaciones se extraen al azar de una población desconocida más grande. Como se demostró para la función de muestra $S$, calcular el promedio de una muestra aleatoria tiene el efecto de que el promedio es una variable aleatoria en sí misma. Dicha variable aleatoria, a su vez, tiene una distribución de probabilidad, denominada distribución de muestreo. Por tanto, el conocimiento sobre la distribución muestral del promedio es fundamental para comprender el desempeño de los procedimientos econométricos.

El *promedio de la muestra* de una muestra de $n$ observaciones $Y_1, \dots, Y_n$ es

$$ \overline{Y} = \frac{1}{n} \sum_{i=1}^n Y_i = \frac{1}{n} (Y_1 + Y_2 + \cdots + Y_n). $$

$\overline{Y}$ también se denomina media muestral.

### Media y varianza de la media muestral {-}

Suponga que $Y_1,\dots,Y_n$ son i.i.d. y se denota $\mu_Y$ y $\sigma_Y^2$ como la media y la varianza de $Y_i$. Entonces se tiene que:

$$ E(\overline{Y}) = E\left(\frac{1}{n} \sum_{i=1}^n Y_i \right) = \frac{1}{n} E\left(\sum_{i=1}^n Y_i\right) = \frac{1}{n} \sum_{i=1}^n E\left(Y_i\right) = \frac{1}{n} \cdot n \cdot \mu_Y = \mu_Y    $$

y

\begin{align*}
  \text{Var}(\overline{Y}) =& \text{Var}\left(\frac{1}{n} \sum_{i=1}^n Y_i \right) \\
  =& \frac{1}{n^2} \sum_{i=1}^n \text{Var}(Y_i) + \frac{1}{n^2} \sum_{i=1}^n \sum_{j=1, j\neq i}^n \text{cov}(Y_i,Y_j) \\
  =& \frac{\sigma^2_Y}{n} \\
  =& \sigma_{\overline{Y}}^2.
\end{align*}

El segundo sumando desaparece desde $\text{cov}(Y_i,Y_j)=0$ para $i\neq j$  debido a la independencia. En consecuencia, la desviación estándar de la media muestral viene dada por 

$$\sigma_{\overline{Y}} = \frac{\sigma_Y}{\sqrt{n}}.$$

Vale la pena mencionar que estos resultados se mantienen independientemente de la distribución subyacente de $Y_i$.

#### La distribución de muestreo de $\overline{Y}$ cuando $Y$ se distribuye normalmente {-}

Si $Y_1,\dots,Y_n$ son i.i.d. se extrae de una distribución normal con media $\mu_Y$ y varianza $\sigma_Y^2$, lo siguiente es válido para su promedio de muestra $\overline{Y}$:

$$ \overline{Y} \sim \mathcal{N}(\mu_Y, \sigma_Y^2/n) \tag{2.4} $$

Por ejemplo, si una muestra $Y_i$ con $i=1,\dots,10$ se extrae de una distribución normal estándar con media $\mu_Y = 0$ y varianza $\sigma_Y^2=1$ se sigue que:

$$ \overline{Y} \sim \mathcal{N}(0,0.1).$$

Se puede usar la instalación de generación de números aleatorios de **R** para verificar el resultado. La idea básica es simular los resultados de la distribución real de $\overline{Y}$ extrayendo repetidamente muestras aleatorias de 10 observaciones de la distribución  $\mathcal{N}(0,1)$ y calculando sus respectivos promedios. Si se hace esto para un gran número de repeticiones, el conjunto de datos simulados de promedios debería reflejar con bastante precisión la distribución teórica de $\overline{Y}$ si el resultado teórico se cumple.

El enfoque esbozado anteriormente es un ejemplo de lo que se conoce comúnmente como *Simulación de Monte Carlo* o *Experimento de Monte Carlo*. Para realizar esta simulación en **R** se debe proceder de la siguiente manera:

1. Elegir un tamaño de muestra **n** y el número de muestras que se extraerán, **reps**.
2. Utilizar la función **replicate()** junto con **rnorm()** para extraer **n** observaciones de la distribución normal estándar **rep** veces.

    **Nota**: El resultado de **replicate()** es una matriz con dimensiones **n** $\times$ **rep**. Contiene las muestras extraídas como *columnas*.
    
3. Calcular las medias de la muestra utilizando **colMeans()**. Esta función calcula la media de cada columna; es decir, de cada muestra y devuelve un vector.

```{r, 195, echo = T, eval = T, message = F, warning = F} 
# establecer el tamaño de la muestra y el número de muestras
n <- 10
reps <- 10000

# realizar muestreo aleatorio
muestras <- replicate(reps, rnorm(n)) # 10 x 10000 sample matrix

# calcular medias de muestra
muestra.promedios <- colMeans(muestras)
```

Luego se termina con un vector de promedios muestrales (medias muestrales). Se puede verificar la propiedad vectorial de **muestra.promedios**:

```{r, 196, echo = T, eval = T, message = F, warning = F} 
# comprobar que 'muestra.promedios' es un vector
is.vector(muestra.promedios) 

# imprimir las primeras entradas en la consola
head(muestra.promedios)
```

Un enfoque sencillo para examinar la distribución de datos numéricos univariados es trazarlos como un histograma y compararlos con alguna distribución conocida o supuesta. De forma predeterminada, **hist()** da un histograma de frecuencia; es decir, un gráfico de barras donde las observaciones se agrupan en rangos, también llamados bins. La ordenada informa el número de observaciones que caen en cada uno de los contenedores. En cambio, se quiere que informe estimaciones de densidad con fines de comparación. Esto se logra estableciendo el argumento **freq = FALSE**. El número de bins se ajusta mediante el argumento **breaks**.

Usando **curve()**, se superpone el histograma con una línea roja, la densidad teórica de una variable aleatoria $\mathcal{N}(0, 0.1)$ . Recuerde usar el argumento **add = TRUE** para agregar la curva al gráfico actual. De lo contrario, **R** abrirá un nuevo dispositivo gráfico y descartará el gráfico anterior.^[*Sugerencia:* **T** y **F** son alternativas para **TRUE** y **FALSE**.]

```{r, 197, echo = T, eval = T, message = F, warning = F, fig.align='center'}
# Grafique el histograma de densidad
hist(muestra.promedios, 
     ylim = c(0, 1.4), 
     col = "steelblue" , 
     freq = F, 
     breaks = 20)

# Superponga la distribución teórica de los promedios de la muestra en la parte superior del histograma
curve(dnorm(x, sd = 1/sqrt(n)), 
      col = "red", 
      lwd = "2", 
      add = T)
```

La distribución de muestreo de $\overline{Y}$ es, de hecho, muy cercana a la de una distribución $\mathcal{N}(0, 0.1)$, por lo que la simulación de Monte Carlo respalda la afirmación teórica.

Analizando otro ejemplo en que el uso del muestreo aleatorio simple en una configuración de simulación ayuda a verificar un resultado bien conocido. Como se discutió antes, la distribución [Chi-cuadrado](#chi-cuadrado) con $M$ grados de libertad surge como la distribución de la suma de $M$ variables aleatorias independientes distribuidas de forma normalmente estándar al cuadrado.

Para visualizar el argumento expresado en la ecuación (<a href="#mjx-eqn-2.3">2.3</a>), se procede de manera similar al ejemplo anterior:

1. Elija los grados de libertad, **DF** (Degrees of Freedom) y el número de muestras que se extraerán *repeticiones*.
2. Graficar las **reps** de las muestras aleatorias de tamaño **DF** de la distribución normal estándar usando **replicate()**.
3. Para cada muestra, potencie los resultados al cuadrado y súmelos en columnas. Almacene los resultados.

Nuevamente, se produce una estimación de densidad para la distribución subyacente a los datos simulados usando un histograma de densidad y se superpone con un gráfico lineal de la función de densidad teórica de la distribución $\chi^2_3$.

```{r, 198, echo = T, eval = T, message = F, warning = F, fig.align='center'} 
# número de repeticiones
reps <- 10000

# establecer grados de libertad de una distribución chi-cuadrada
DF <- 3 

# muestra 10000 vectores de columna à 3 N (0,1) R.V.S
Z <- replicate(reps, rnorm(DF)) 

# columna sumas de cuadrados
X <- colSums(Z^2)

# histograma de columnas de sumas de cuadrados
hist(X, 
     freq = F, 
     col = "steelblue", 
     breaks = 40, 
     ylab = "Densidad", 
     main = "")

# agregar densidad teórica
curve(dchisq(x, df = DF), 
      type = 'l', 
      lwd = 2, 
      col = "red", 
      add = T)
```

### Aproximaciones de muestras grandes para distribuciones de muestreo {-}

Las distribuciones muestrales consideradas en la última sección juegan un papel importante en el desarrollo de métodos econométricos. Existen dos enfoques principales para caracterizar las distribuciones muestrales: Un enfoque "exacto" y un enfoque "aproximado".

El enfoque exacto tiene como objetivo encontrar una fórmula general para la distribución muestral, que se mantiene para cualquier tamaño de muestra $n$. A esto se le llama *distribución exacta* o *distribución de muestra finita*. En los ejemplos anteriores de lanzamiento de dados y variantes normales, se ha tratado con funciones de variables aleatorias cuyas distribuciones de muestra son *exactamente conocidas* en el sentido de que se pueden escribir como expresiones analíticas. Sin embargo, esto no siempre es posible. Para $\overline{Y}$, el resultado (<a href="#mjx-eqn-2.4">2.4</a>) indica que la normalidad de $Y_i$  implica la normalidad de $\overline{Y}$ (se demuestra esto para el caso especial de $Y_i \overset{i.i.d.}{\sim} \mathcal{N}(0,1)$ con $n=10$  usando un estudio de simulación que involucra un muestreo aleatorio simple). Desafortunadamente, la distribución *exacta* de $\overline{Y}$ es generalmente desconocida y, a menudo, difícil de derivar (o incluso imposible de rastrear) si se descarta la suposición de que $Y_i$ tiene una distribución normal.

Por lo tanto, como se puede adivinar por su nombre, el enfoque "aproximado" tiene como objetivo encontrar una aproximación a la distribución muestral donde se requiere que el tamaño de la muestra $n$ sea grande. Una distribución que se utiliza como una aproximación de muestra grande a la distribución muestral también se denomina *distribución asintótica*. Esto se debe al hecho de que la distribución asintótica *es  la distribución de muestreo* para $n \rightarrow \infty$; es decir, la aproximación se vuelve exacta si el tamaño de la muestra llega al infinito. Sin embargo, la diferencia entre la distribución muestral y la distribución asintótica es insignificante para tamaños de muestra moderados o incluso pequeños, por lo que las aproximaciones que utilizan la distribución asintótica son útiles.

En esta sección se discurtirán dos resultados bien conocidos que se utilizan para aproximar distribuciones muestrales y, por lo tanto, constituyen herramientas clave en la teoría econométrica: la *ley de los grandes números* y el *teorema del límite central*. La ley de los números grandes establece que en muestras grandes, $\overline{Y}$ está cerca de $\mu_Y$ con alta probabilidad. El teorema del límite central dice que la distribución muestral de los promedios muestrales estandarizados; es decir, $(\overline{Y} - \mu_Y)/\sigma_{\overline{Y}}$ tiene una distribución asintóticamente normal. Es particularmente interesante que ambos resultados no dependen de la distribución de $Y$. En otras palabras, al no poder describir la complicada distribución muestral de $\overline{Y}$ si $Y$ no es normal, las aproximaciones de este último utilizando el teorema del límite central simplifican enormemente el desarrollo y la aplicabilidad de los procedimientos econométricos. Este es un componente clave que subyace a la teoría de la inferencia estadística para modelos de regresión. Ambos resultados se resumen en el Concepto clave 2.6 y el Concepto clave 2.7.

```{r, 199, eval = my_output == "html", results='asis', echo=FALSE, purl=FALSE}
cat('
<div class = "keyconcept" id="KC2.6">
<h3 class = "right"> Concepto clave 2.6 </h3> 
<h3 class = "left"> Convergencia en probabilidad, consistencia y la ley de los números grandes </h3>
<p>
El promedio de la muestra $\\overline{Y}$ converge en probabilidad a $\\mu_Y$: $\\overline{Y}$ es *consistente* para $\\mu_Y$ si la probabilidad de que $\\overline{Y}$ está en el rango $(\\mu_Y - \\epsilon)$ a $(\\mu_Y + \\epsilon)$ se vuelve arbitrario cerca de $1$ a medida que $n$ aumenta para cualquier $\\epsilon > 0$. Se escribe esto como:

$$ P(\\mu_Y-\\epsilon \\leq \\overline{Y} \\leq \\mu_Y + \\epsilon) \\rightarrow 1, \\, \\epsilon > 0 \\text{ as } n\\rightarrow\\infty. $$

Considere las variables aleatorias distribuidas de forma independiente e idéntica $Y_i, i=1,\\dots,n$ con expectativa $E(Y_i)=\\mu_Y$ y varianza $\\text{Var}(Y_i)=\\sigma^2_Y$. Bajo la condición de que $\\sigma^2_Y< \\infty$; es decir, los valores atípicos grandes son poco probables, la ley de los números grandes establece que

$$ \\overline{Y} \\xrightarrow[]{p} \\mu_Y. $$

La siguiente aplicación simula una gran cantidad de lanzamientos de monedas (puede establecer el número de intentos usando el control deslizante) con una moneda justa. Asimismo, calcula la fracción de caras observadas para cada lanzamiento adicional. El resultado es una ruta aleatoria que, como lo establece la ley de los números grandes, muestra una tendencia a acercarse al valor de $0.5$ a medida que $n$ crece.

<iframe height="570" width="800" frameborder="0" scrolling="no" src="DCL/Lanzamiento_Moneda.html"></iframe>
</p> 
</div>')
```

```{r, 200, eval = my_output == "latex", results='asis', echo=FALSE, purl=FALSE}
cat('\\begin{keyconcepts}[Convergencia en la probabilidad\\comma La coherencia y la ley de los números grandes]{2.6}
El promedio de la muestra $\\overline{Y}$ converge en probabilidad a $\\mu_Y$: $\\overline{Y}$ es *consistente* para $\\mu_Y$ si la probabilidad de que $\\overline{Y}$ está en el rango $(\\mu_Y - \\epsilon)$ a $(\\mu_Y + \\epsilon)$ se vuelve arbitrario cerca de $1$ a medida que $n$ aumenta para cualquier $\\epsilon > 0$. Se escribe esto como:

$$ P(\\mu_Y-\\epsilon \\leq \\overline{Y} \\leq \\mu_Y + \\epsilon) \\rightarrow 1, \\, \\epsilon > 0 \\text{ as } n\\rightarrow\\infty. $$

Considere las variables aleatorias distribuidas de forma independiente e idéntica $Y_i, i=1,\\dots,n$ con expectativa $E(Y_i)=\\mu_Y$ y varianza $\\text{Var}(Y_i)=\\sigma^2_Y$. Bajo la condición de que $\\sigma^2_Y< \\infty$; es decir, los valores atípicos grandes son poco probables, la ley de los números grandes establece que

$$ \\overline{Y} \\xrightarrow[]{p} \\mu_Y. $$

La siguiente aplicación simula una gran cantidad de lanzamientos de monedas (puede establecer el número de intentos usando el control deslizante) con una moneda justa. Asimismo, calcula la fracción de caras observadas para cada lanzamiento adicional. El resultado es una ruta aleatoria que, como lo establece la ley de los números grandes, muestra una tendencia a acercarse al valor de $0.5$ a medida que $n$ crece.
\\begin{center}
\\textit{Esta aplicación interactiva solo está disponible en la versión HTML.}
\\end{center}
\\end{keyconcepts}')
```

El enunciado central de la ley de los grandes números es que, en condiciones bastante generales, la probabilidad de obtener un promedio de muestra $\overline{Y}$ que esté cerca de $\mu_Y$ es alta si se tiene un tamaño de muestra grande.

Al considerar el ejemplo de lanzar repetidamente una moneda donde $Y_i$ es el resultado del lanzamiento de la moneda  $i^{th}$. $Y_i$ es una variable aleatoria distribuida de Bernoulli con $p$ la probabilidad de observar la cara

$$ P(Y_i) = \begin{cases} p, & Y_i = 1 \\ 1-p, & Y_i = 0 \end{cases} $$

donde $p = 0.5$ como se asume una moneda justa. Es sencillo demostrar que

$$ \mu_Y = p = 0.5. $$

Sea $R_n$ la proporción de caras en los primeros $n$ lanzamientos,

$$ R_n = \frac{1}{n} \sum_{i=1}^n Y_i. \tag{2.5}$$

De acuerdo con la ley de los números grandes, la proporción observada de caras converge en probabilidad a $\mu_Y = 0.5$, la probabilidad de lanzar cara en un *solo* lanzamiento de moneda, 

$$ R_n \xrightarrow[]{p} \mu_Y=0.5 \ \ \text{as} \ \ n \rightarrow \infty.$$  

Este resultado es ilustrado por la aplicación interactiva en el Concepto clave 2.6. Ahora se debe demostrar cómo replicar esto usando **R**.

El procedimiento es el siguiente:

1. Muestra de **N** observaciones de la distribución de Bernoulli, por ejemplo, usar **sample()**.

2. Calcular la proporción de caras $R_n$ como en (<a href="#mjx-eqn-2.5">2.5</a>). Una forma de lograr esto es llamar a **cumsum()** en el vector de observaciones **Y** para obtener su suma acumulada y luego dividir por el número respectivo de observaciones.

Se continua trazando la ruta y también agregando una línea discontinua para la probabilidad de referencia $p = 0.5$.

```{r, 201, eval = T, message = F, warning = F, fig.align='center'} 
# sembrar semilla
set.seed(1)

# establecer el número de lanzamientos de monedas y simular
N <- 30000
Y <- sample(0:1, N, replace = T)

# calcular R_n para 1: N
S <- cumsum(Y)
R <- S/(1:N)

# graficar el camino
plot(R, 
     ylim = c(0.3, 0.7), 
     type = "l", 
     col = "steelblue", 
     lwd = 2, 
     xlab = "n", 
     ylab = "R_n",
     main = "Cuota convergente de caras en el lanzamiento repetido de monedas")

# agregar una línea discontinua para R_n = 0.5
lines(c(0, N), 
      c(0.5, 0.5), 
      col = "darkred", 
      lty = 2, 
      lwd = 1)
```

Hay varias cosas que decir sobre esta trama.

- El gráfico azul muestra la proporción observada de caras al lanzar una moneda $n$ veces.

- Dado que $Y_i$ son variables aleatorias, $R_n$ también es una variable aleatoria. La ruta representada es solo una de las muchas realizaciones posibles de $R_n$, ya que está determinada por las observaciones de $30000$ muestreadas de la distribución de Bernoulli.

- Si el número de lanzamientos de moneda $n$ es pequeño, la proporción de caras puede ser cualquier cosa menos cercana a su valor teórico, $\mu_Y = 0.5$. Sin embargo, a medida que se incluyen más y más observaciones en la muestra, se encuentra que la trayectoria se estabiliza en el vecindario de $0.5$. El promedio de múltiples ensayos muestra una clara tendencia a converger a su valor esperado a medida que aumenta el tamaño de la muestra, tal como lo afirma la ley de los grandes números.

```{r, 202, eval = my_output == "html", results='asis', echo=FALSE, purl=FALSE}
cat('
<div class = "keyconcept" id="KC2.7">
<h3 class = "right"> Concepto clave 2.7 </h3> 
<h3 class = "left"> El teorema del límite central </h3>
<p>
Suponga que $Y_1,\\dots,Y_n$ son variables aleatorias independientes y distribuidas de forma idéntica con expectativa $E(Y_i)=\\mu_Y$ y varianza $\\text{Var}(Y_i)=\\sigma^2_Y$ donde $0<\\sigma^2_Y<\\infty$. El Teorema del Límite Central (TLC) establece que, si el tamaño de la muestra $n$ llega al infinito, la distribución del promedio muestral estandarizado

$$ \\frac{\\overline{Y} - \\mu_Y}{\\sigma_{\\overline{Y}}} = \\frac{\\overline{Y} - \\mu_Y}{\\sigma_Y/\\sqrt{n}} \\ $$

se aproxima arbitrariamente bien por la distribución normal estándar.

La siguiente aplicación demuestra el TLC para el promedio de la muestra de variables aleatorias distribuidas normalmente con una media de $5$ y una varianza de $25^2$. Se pueden comprobar las siguientes propiedades:\\newline

+ La distribución del promedio de la muestra es normal.
+ A medida que aumenta el tamaño de la muestra, la distribución de $\\overline{Y}$ se ajusta alrededor de la media real de $5$.
+ La distribución del promedio muestral estandarizado está cerca de la distribución normal estándar para grandes $n$.

<iframe height="620" width="800" frameborder="0" scrolling="no" src="DCL/Distribución_Muestral.html"></iframe>
</p> 
</div>')
```

```{r, 203, eval = my_output == "latex", results='asis', echo=FALSE, purl=FALSE}
cat('\\begin{keyconcepts}[El teorema del límite central]{2.7}
Suponga que $Y_1,\\dots,Y_n$ son variables aleatorias independientes y distribuidas de forma idéntica con expectativa $E(Y_i)=\\mu_Y$ y varianza $\\text{Var}(Y_i)=\\sigma^2_Y$ donde $0<\\sigma^2_Y<\\infty$. El Teorema del Límite Central (TLC) establece que, si el tamaño de la muestra $n$ llega al infinito, la distribución del promedio muestral estandarizado

$$ \\frac{\\overline{Y} - \\mu_Y}{\\sigma_{\\overline{Y}}} = \\frac{\\overline{Y} - \\mu_Y}{\\sigma_Y/\\sqrt{n}} \\ $$

se aproxima arbitrariamente bien por la distribución normal estándar.

La siguiente aplicación demuestra el TLC para el promedio de la muestra de variables aleatorias distribuidas normalmente con una media de $5$ y una varianza de $25^2$. Se pueden comprobar las siguientes propiedades:\\newline

\\begin{itemize}
\\item La distribución del promedio de la muestra es normal.
\\item A medida que aumenta el tamaño de la muestra, la distribución de $\\overline{Y}$ se ajusta alrededor de la media real de $5$.
\\item La distribución del promedio muestral estandarizado está cerca de la distribución normal estándar para grandes $n$.
\\end{itemize}
\\vspace{0.5cm}
\\begin{center}
\\textit{Esta aplicación interactiva solo está disponible en la versión HTML.}
\\end{center}

\\end{keyconcepts}
')
```

Según el TLC, la distribución de la media muestral $\overline{Y}$ de las variables aleatorias distribuidas de Bernoulli $Y_i$, $i=1,...,n$, está bien aproximada por la distribución normal con parámetros $\mu_Y=p=0.5$ y $\sigma^2_{Y} = p(1-p)/n = 0.25/n$ para $n$ grandes. En consecuencia, para la media muestral estandarizada, se llega a la conclusión de que 

$$\frac{\overline{Y} - 0.5}{0.5/\sqrt{n}} \tag{2.6}$$ 

debería estar bien aproximado por la distribución normal estándar $\mathcal{N}(0,1)$. Se emplea otro estudio de simulación para demostrar esto gráficamente. La idea es la siguiente.

Extraiga una gran cantidad de muestras aleatorias, suponiendo $10000$, de tamaño $n$ de la distribución de Bernoulli y calcular los promedios de las muestras. Estandarizar los promedios como se muestra en (<a href="#mjx-eqn-2.6">2.6</a>). A continuación, visualizar la distribución de los promedios muestrales estandarizados generados por medio de un histograma y comparar con la distribución normal estándar. Repetir esto para diferentes tamaños de muestra $n$ para ver cómo el aumento del tamaño de muestra $n$ afecta la distribución simulada de los promedios.

En **R**, se puede dar cuenta de esto de la siguiente manera:

1. Comenzar por definir que las siguientes cuatro figuras generadas posteriormente se dibujarán en una matriz $2\times2$ de manera que puedan compararse fácilmente. Esto se hace llamando a `par(mfrow = c(2, 2))` antes de generar las figuras.

2. Definir el número de repeticiones **reps** como $10000$ y crear un vector de tamaños de muestra llamado **sample.sizes**. Considerar muestras de tamaños de $5$, $20$, $75$ y $100$.

3. A continuación, combinar dos bucles **for()** para simular los datos y graficar las distribuciones. El ciclo interno genera muestras aleatorias de $10000$, cada una de las cuales consta de **n** observaciones que se extraen de la distribución de Bernoulli, y calcula los promedios estandarizados. El ciclo externo ejecuta el ciclo interno para los diferentes tamaños de muestra **n** y produce una gráfica para cada iteración.

```{r, 204, echo = T, eval = T, message = F, warning = F, cache=T, fig.align='center'} 
# subdividir el panel de la trama en una matriz de 2 por 2
par(mfrow = c(2, 2))

# establecer el número de repeticiones y los tamaños de muestra
reps <- 10000
sample.sizes <- c(5, 20, 75, 100)

# sembrar la semilla para la reproducibilidad
set.seed(123)

# bucle externo (bucle sobre los tamaños de muestra)
  for(n in sample.sizes){
    samplemean <- rep(0, reps) # inicializar el vector de medias muestrales
    stdsamplemean <- rep(0, reps) # inicializar el vector de medias muestrales estandarizadas

# bucle interno (bucle sobre repeticiones)   
    for(i in 1:reps){
      x <- rbinom(n, 1, 0.5)
      samplemean[i] <- mean(x)
      stdsamplemean[i] <- sqrt(n)*(mean(x) - 0.5)/0.5
    }
    
# graficar el histograma y superponer la densidad N(0,1) en cada iteración  
    hist(stdsamplemean, 
         col = "steelblue", 
         freq = FALSE, 
         breaks = 40,
         xlim = c(-3, 3), 
         ylim = c(0, 0.8), 
         xlab = paste("n =", n), 
         main = "")
    
    curve(dnorm(x), 
          lwd = 2, 
          col = "darkred", 
          add = TRUE)
  }  
```

Se puede ver que la distribución muestral simulada del promedio estandarizado tiende a desviarse fuertemente de la distribución normal estándar si el tamaño de la muestra es pequeño, por ejemplo, para $n = 5$ y $n = 10$. Sin embargo, a medida que crece $n$, los histogramas se acercan a la distribución normal estándar. La aproximación funciona bastante bien, vea $n = 100$.

```{r, 205, echo=FALSE, results='asis', purl=FALSE}
write_html(playground = T)
```

## Ejercicios {#Ejercicios-2}

```{r, 206, echo=FALSE, results='asis', purl=FALSE}
if (my_output=="html"){
  cat('
<div  class = "DCexercise">

#### 1. Muestreo {-}

Suponga que usted es el hada de la lotería en una lotería semanal, donde se extraen $6$ de $49$ *números únicos*.

**Instrucciones:**

  + Trazar los números ganadores de esta semana.

<iframe src="DCL/ex2_1.html" frameborder="0" scrolling="no" style="width:100%;height:360px"></iframe>

**Sugerencias:**

  + Puede usar la función <tt>sample()</tt> para trazar números aleatorios, vea <tt>?Sample</tt>.

  + El conjunto de elementos a muestrear desde aquí es $\\{1,...,49\\}$.

</div>')
} else {
  cat('\\begin{center}\\textit{Esta parte interactiva del curso solo está disponible en la versión HTML.}\\end{center}')
}
```

```{r, 207, echo=FALSE, results='asis', purl=FALSE}
if (my_output=="html"){
  cat('
<div  class = "DCexercise">

#### 2. Función de densidad de probabilidad {-}

Considere una variable aleatoria $X$ con función de densidad de probabilidad (FDP)

$$f_X(x)=\\frac{x}{4}e^{-x^2/8},\\quad x\\geq 0.$$

**Instrucciones:**

  + Definir la FDP desde arriba como una función <tt>f()</tt>. <tt>exp(a)</tt> calcular $e^a$.

  + Comprobar si la función que se ha definido es realmente una FDP.

<iframe src="DCL/ex2_2.html" frameborder="0" scrolling="no" style="width:100%;height:360px"></iframe>

**Sugerencias:**

  + Usar <tt>function(x) {...}</tt> para definir una función que toma el argumento <tt>x</tt>.

  + Para que <tt>f()</tt> sea una FDP, su integral en todo el dominio tiene que ser igual a 1: $\\int_0^\\infty f_X(x)\\mathrm{d}x=1$.

  + La función <tt>integrate()</tt> realiza la integración. Debe especificar la función a integrar, así como los límites superior e inferior de integración. Estos se pueden establecer en $[- \\infty, \\infty]$ estableciendo los argumentos correspondientes en <tt>-Inf</tt> e <tt>Inf</tt>. Puede acceder al valor numérico de la integral calculada agregando <tt>$value</tt>. Consulte <tt>?Integral</tt> para obtener una descripción detallada de la función.

</div>
')}
```

```{r, 208, echo=FALSE, results='asis', purl=FALSE}
if (my_output=="html"){
  cat('
<div  class = "DCexercise">

#### 3. Valor esperado y variación {-}

En este ejercicio debe calcular el valor esperado y la varianza de la variable aleatoria $X$ considerada en el ejercicio anterior.

La FDP <tt>f()</tt> del ejercicio anterior está disponible en su entorno de trabajo.

**Instrucciones:**

  + Definir una función adecuada <tt>ex()</tt> que se integre al valor esperado de $X$.

  + Calcular el valor esperado de $X$. Almacene el resultado en <tt>expected_value</tt>.

  + Definir una función adecuada <tt>ex2()</tt> que se integre al valor esperado de $ X^2 $.

  + Calcular la varianza de $X$. Almacene el resultado en <tt>variance</tt>.

<iframe src="DCL/ex2_3.html" frameborder="0" scrolling="no" style="width:100%;height:340px"></iframe>

**Sugerencias:**

  + El valor esperado de $X$ se define como $E(X)=\\int_0^\\infty xf_X(x)dx$.

  + El valor de una integral calculado por <tt>integrate()</tt> se puede obtener a través de <tt>\\$value</tt>.

  + La varianza de $X$ se define como $Var(X)=E(X^2)-E(X)^2$, donde $E(X^2)=\\int_0^\\infty x^2f_X(x)\\mathrm{d}x$.

</div>')}
```

```{r, 209, echo=FALSE, results='asis', purl=FALSE}
if (my_output=="html"){
  cat('
<div  class = "DCexercise">

#### 4. Distribución normal estándar I {-}

Sea $Z\\sim\\mathcal{N}(0, 1)$.

**Instrucciones:**

  + Calcular $\\phi(3)$, es decir, el valor de la densidad normal estándar en $c=3$.

<iframe src="DCL/ex2_4.html" frameborder="0" scrolling="no" style="width:100%;height:340px"></iframe>

**Sugerencias:**

  + Valores de $\\phi(\\cdot)$ se puede calcular usando <tt>dnorm()</tt>. Tenga en cuenta que por defecto <tt>dnorm()</tt> usa <tt>mean = 0</tt> y <tt>sd = 1</tt> por lo que no es necesario establecer los argumentos correspondientes cuando desee obtener valores de densidad de la distribución normal estándar.

</div>')}
```

```{r, 210, echo=FALSE, results='asis', purl=FALSE}
if (my_output=="html") {
  cat('
<div  class = "DCexercise">

#### 5. Distribución normal estándar II {-}

Sea $Z\\sim\\mathcal{N}(0, 1)$.

**Instrucciones:**

  + Calcular $P(|Z|\\leq 1.64)$ usando la función <tt>pnorm()</tt>.

<iframe src="DCL/ex2_5.html" frameborder="0" scrolling="no" style="width:100%;height:340px"></iframe>

**Sugerencias:**

  + $P(|Z|\\leq z) = P(-z \\leq Z \\leq z)$.

  + Probabilidades de la forma $P(a \\leq Z \\leq b)$ se puede calcular como $P(Z\\leq b)-P(Z\\leq a)=F_Z(b)-F_Z(a)$ with $F_Z(\\cdot)$ la función de distribución acumulativa (FDPA) de $Z$. Alternativamente, puede aprovechar la simetría de la distribución normal estándar.

</div>')}
```

```{r, 211, echo=FALSE, results='asis', purl=FALSE}
if (my_output=="html"){
  cat('
<div  class = "DCexercise">

#### 6. Distribución normal I {-}

Sea $Y\\sim\\mathcal{N}(5, 25)$.

**Instrucciones:**

  + Calcular el cuantil del 99% de la distribución dada, es decir, encontrar $y$ tal que $\\Phi(\\frac{y-5}{5})=0.99$.

<iframe src="DCL/ex2_6.html" frameborder="0" scrolling="no" style="width:100%;height:340px"></iframe>

**Sugerencias:**

  + Se pueden calcular cuantiles de la distribución normal utilizando la función <tt>qnorm()</tt>.

  + Además del cuantil a calcular, se debe especificar la media y la desviación estándar de la distribución. Esto se hace mediante los argumentos <tt>mean</tt> y <tt>sd</tt>. Se debe tener en cuenta que <tt>sd</tt> establece la desviación estándar, no la varianza.

  + <tt>sqrt(a)</tt> devuelve la raíz cuadrada del argumento numérico <tt>a</tt>.

</div>')}
```

```{r, 212, echo=FALSE, results='asis', purl=FALSE}
if (my_output=="html"){
  cat('
<div  class = "DCexercise">

#### 7. Distribución normal II {-}

Sea $Y\\sim\\mathcal{N}(2, 12)$.

**Instrucciones:**

  + Generar $10$ números aleatorios a partir de esta distribución.

<iframe src="DCL/ex2_7.html" frameborder="0" scrolling="no" style="width:100%;height:340px"></iframe>

**Sugerencias:**

  + Usar <tt>rnorm()</tt> para extraer números aleatorios de una distribución normal.

  + Además del número de sorteos, se debe especificar la media y la desviación estándar de la distribución. Esto se puede hacer a través de los argumentos <tt>mean</tt> y <tt>sd</tt>. Se debe tener en cuenta que <tt>sd</tt> requiere la desviación estándar, no la varianza.

</div>')}
```

```{r, 213, echo=FALSE, results='asis', purl=FALSE}
if (my_output=="html"){
  cat('
<div  class = "DCexercise">

#### 8. Distribución chi-cuadrado I {-}

Sea $W\\sim\\chi^2_{10}$.

**Instrucciones:**

  + Trazar la FDP correspondiente usando <tt>curve()</tt>. Especificar el rango de valores x como $[0,25]$ a través del argumento <tt>xlim</tt>.

<iframe src="DCL/ex2_8.html" frameborder="0" scrolling="no" style="width:100%;height:340px"></iframe>

**Sugerencias:**

  + <tt>curve()</tt> espera una función y sus parámetros como argumentos (aquí <tt>dchisq()</tt> y los grados de libertad <tt>df</tt>).

  + El rango de valores x en <tt>xlim</tt> se puede pasar como un vector de límites de intervalo.

</div>')}
```

```{r, 214, echo=FALSE, results='asis', purl=FALSE}
if (my_output=="html"){
  cat('
<div  class = "DCexercise">

#### 9. Distribución de chi-cuadrado II {-}

Sea $X_1$ y $X_2$ ser dos variables aleatorias independientes normalmente distribuidas con $\\mu=0$ y $\\sigma^2=15$.

**Instrucciones:**

  + Calcular $P(X_1^2+X_2^2>10)$.

<iframe src="DCL/ex2_9.html" frameborder="0" scrolling="no" style="width:100%;height:340px"></iframe>

**Sugerencias:**

  + Se debe tener en cuenta que $X_1$ y $X_2$ no son $\\mathcal{N}(0,1)$, pero $\\mathcal{N}(0,15)$ repartido. Por lo tanto, debe escalar adecuadamente. Luego puede usar <tt>pchisq()</tt> para calcular la probabilidad.
  + El argumento <tt>lower.tail</tt> puede resultar útil.

</div>')}
```

```{r, 215, echo=FALSE, results='asis', purl=FALSE}
if (my_output=="html"){
  cat('
<div  class = "DCexercise">

#### 10. Distribución t de Student I {-}

Sea $X\\sim t_{10000}$ y $Z\\sim\\mathcal{N}(0,1)$.

**Instrucciones:**

  + Calcular el cuantil $95\\%$ de ambas distribuciones. ¿Que notaste?

<iframe src="DCL/ex2_10.html" frameborder="0" scrolling="no" style="width:100%;height:340px"></iframe>

**Sugerencias:**

  + Puede usar <tt>qt()</tt> y <tt>qnorm()</tt> para calcular cuantiles de las distribuciones dadas.

  + Para la distribución $t$ se deben especificar los grados de libertad <tt>df</tt>.

</div>')}
```

```{r, 216, echo=FALSE, results='asis', purl=FALSE}
if (my_output=="html"){
  cat('
<div  class = "DCexercise">

#### 11. Distribución t de Student II {-}

Sea $X\\sim t_1$. Una vez que se haya inicializado la sesión, se verá el gráfico de la función de densidad de probabilidad (FDP) correspondiente.

**Instrucciones:**

  + Generar números aleatorios de $1000$ a partir de esta distribución y asígnar a la variable <tt>x</tt>.

  + Calcular la media muestral de <tt>x</tt>. ¿Puede explicar el resultado?

<iframe src="DCL/ex2_11.html" frameborder="0" scrolling="no" style="width:100%;height:340px"></iframe>

**Sugerencias:**

  + Puede usar <tt>rt()</tt> para dibujar números aleatorios de una distribución t.

  + Se debe tener en cuenta que la distribución t está completamente determinada a través de los grados de libertad. Especificarlos mediante el argumento <tt>df</tt>.

  + Para calcular la media muestral de un vector, se puede usar la función <tt>mean()</tt>.

</div>')}
```

```{r, 217, echo=FALSE, results='asis', purl=FALSE}
if (my_output=="html"){
  cat('
<div  class = "DCexercise">

#### 12. F Distribución I {-}

Sea $Y\\sim F(10, 4)$.

**Instrucciones:**

  + Graficar la función cuantil de la distribución dada usando la función <tt>curve()</tt>.

<iframe src="DCL/ex2_12.html" frameborder="0" scrolling="no" style="width:100%;height:340px"></iframe>

**Sugerencias:**

  + <tt>curve()</tt> expectativa de la función con sus respectivos parámetros (aquí: grados de libertad <tt>df1</tt> y <tt>df2</tt>) como argumento.

</div>')}
```

```{r, 218, echo=FALSE, results='asis', purl=FALSE}
if (my_output=="html"){
  cat('
<div  class = "DCexercise">

#### 13. F Distribución II {-}

Sea $Y\\sim F(4,5)$.

**Instrucciones:**

  + Calcular $P(1<Y<10)$ mediante la integración de la FDP.

<iframe src="DCL/ex2_13.html" frameborder="0" scrolling="no" style="width:100%;height:340px"></iframe>

**Sugerencias:**

  + Además de proporcionar la función que se va a integrar, se deben especificar los límites superior e inferior de integración.

  + Los parámetros adicionales de la distribución (aquí <tt>df1</tt> y <tt>df2</tt>) también deben pasarse *dentro* de la llamada de <tt>integrate()</tt>.

  + El valor de la integral se puede obtener mediante <tt>\\$value</tt>.

</div>')}
```

<!--chapter:end:Capitulo_03.Rmd-->

# Revisión de estadística con R {#RER}

```{r, echo = F}
options(knitr.duplicate.label = "allow")
```

```{r, 219, child="_setup.Rmd"}
```

```{r, 220, eval=knitr::opts_knit$get("rmarkdown.pandoc.to") == "html", results='asis', echo=FALSE}
cat('<hr style="background-color:#03193b;height:2px">')
```

Esta sección revisa conceptos estadísticos importantes:

- Estimación de parámetros poblacionales desconocidos.

- Evaluación de la hipótesis.

- Intervalos de confianza.

Estos métodos se utilizan mucho en econometría. Se discutirán en el contexto simple de inferencia sobre una media poblacional desconocida, así como de diferentes aplicaciones en **R**. Dichas aplicaciones en **R** encuentran fundamento en los siguientes paquetes que no forman parte de la versión base de **R**:

+ **readxl** - permite importar datos de *Excel* a **R**.

+ **dplyr** - proporciona una gramática flexible para la manipulación de datos.

+ **MASS** - una colección de funciones para estadísticas aplicadas.

Asegúrese de que estén instalados antes de continuar e intentar replicar los ejemplos. La forma más segura de hacerlo es verificando si el siguiente fragmento de código se ejecuta sin errores.

```{r, 221, warning=FALSE, message=FALSE, eval=FALSE}
library(dplyr)
library(MASS)
library(readxl)
```

## Estimación de la media poblacional

```{r, 222, eval = my_output == "html", results='asis', echo=F, purl=F}
cat('
<div class = "keyconcept" id="KC3.1">
<h3 class = "right"> Concepto clave 3.1 </h3>          
<h3 class = "left"> Estimadores y estimaciones </h3>

Los *estimadores* son funciones extraídas de una muestra de datos que parten de una población desconocida. Las *estimaciones* son valores numéricos calculados por estimadores basados en los datos de la muestra. Los estimadores son variables aleatorias porque son funciones de datos *aleatorios*. Las estimaciones son números no aleatorios.

</div>
')
```

```{r, 223, eval = my_output == "latex", results='asis', echo=F, purl=F}
cat('\\begin{keyconcepts}[Estimadores y estimaciones]{3.1}
Los \\textit{estimadores} son funciones extraídas de una muestra de datos que parten de una población desconocida. Las \\textit{estimaciones} son valores numéricos calculados por estimadores basados en los datos de la muestra. Los estimadores son variables aleatorias porque son funciones de datos \\textit{aleatorios}. Las estimaciones son números no aleatorios.
\\end{keyconcepts}')
```

Piense en alguna variable económica, por ejemplo, los ingresos por hora de los graduados universitarios, denotados por $Y$. Suponga que se está interesado en $\mu_Y$ la media de $Y$. Para calcular exactamente $\mu_Y$ se tendrían que entrevistar a todos los graduados que trabajan en el sistema económico. Simplemente no se puede hacer esto debido a limitaciones de tiempo y costos. Sin embargo, se puede extraer una muestra aleatoria de $n$ i.i.d. observaciones $Y_1, \dots, Y_n$ y estimar $\mu_Y$ usando uno de los estimadores más simples en el sentido del Concepto Clave 3.1 que uno pueda imaginar, es decir,

$$ \overline{Y} = \frac{1}{n} \sum_{i=1}^n Y_i, $$

la media muestral de $Y$. Por otra parte, se podría usar un estimador aún más simple para $\mu_Y$: la primera observación de la muestra, $Y_1$. ¿Es $Y_1$ un buen estimador? Por ahora, asuma que

$$ Y \sim \chi_{12}^2 $$

lo cual no es demasiado irracional ya que los ingresos por hora no son negativos y se espera que muchas ganancias por hora estén en un rango de $5€\,$ a $15€$. Además, es común que las distribuciones de ingresos estén sesgadas hacia la derecha, una propiedad de la distribución $\chi^2_{12}$. 

```{r, 224, fig.align='center'}
# graficar la distribución chi_12^2
curve(dchisq(x, df=12), 
      from = 0, 
      to = 40, 
      ylab = "Densidad", 
      xlab = "Ganancias por hora en euros")
```

Ahora se extraerá una muestra de $n = 100$ observaciones y se tomará la primera observación $Y_1$ como una estimación de $\mu_Y$

```{r, 225, fig.align='center'}
# sembrar la semilla para la reproducibilidad
set.seed(1)

# muestra de la distribución chi_12^2, usar solo la primera observación
rsamp <- rchisq(n = 100, df = 12)
rsamp[1]
```

El estimado de $8.26$ no está muy lejos de $\mu_Y = 12$ pero es algo intuitivo que se podría hacer algo mejor: el estimador $Y_1$ descarta mucha información y su varianza es la varianza de la población:

$$ \text{Var}(Y_1) = \text{Var}(Y) = 2 \cdot 12 = 24 $$

Esto nos lleva a la siguiente pregunta: ¿Qué es un estimador *bueno* de un parámetro desconocido en primer lugar? Esta cuestión se aborda en los Conceptos clave 3.2 y 3.3.

```{r, 226, eval = my_output == "html", results='asis', echo=F, purl=F}
cat('
<div class = "keyconcept" id="KC3.2">
<h3 class = "right"> Concepto clave 3.2 </h3>          
<h3 class = "left"> Sesgo, consistencia y eficiencia </h3>

Las características deseables de un estimador incluyen insesgabilidad, consistencia y eficiencia.

**Insesgabilidad:**

Si la media de la distribución muestral de algún estimador $\\hat\\mu_Y$ para la media poblacional $\\mu_Y$ es igual a $\\mu_Y$,

$$ E(\\hat\\mu_Y) = \\mu_Y, $$

el estimador es imparcial para $\\mu_Y$. El *sesgo* de $\\hat\\mu_Y$ entonces es $0$:

$$ E(\\hat\\mu_Y) - \\mu_Y = 0$$

**Consistencia:**

Se quiere que la incertidumbre del estimador $\\mu_Y$ disminuya a medida que aumenta el número de observaciones en la muestra. Más precisamente, se quiere que la probabilidad de que la estimación $\\hat\\mu_Y$ caiga dentro de un pequeño intervalo alrededor del valor real $\\mu_Y$ se acerque cada vez más a $1$ a medida que $n$ crece. Se escribe esto como

$$ \\hat\\mu_Y \\xrightarrow{p} \\mu_Y. $$

**Varianza y eficiencia:**

Se quiere que el estimador sea eficiente. Suponga que se tienen dos estimadores, $\\hat\\mu_Y$ y $\\overset{\\sim}{\\mu}_Y$ para un tamaño de muestra dado $n$ se sigue que

$$ E(\\hat\\mu_Y) = E(\\overset{\\sim}{\\mu}_Y) = \\mu_Y $$

pero

$$\\text{Var}(\\hat\\mu_Y) < \\text{Var}(\\overset{\\sim}{\\mu}_Y).$$

Entonces se prefiere usar $\\hat\\mu_Y$, ya que tiene una variación menor que $\\overset{\\sim}{\\mu}_Y$, lo que significa que $\\hat\\mu_Y$ es más *eficiente* en el uso de la información proporcionada por las observaciones en la muestra.

</div>
')
```

```{r, 227, eval = my_output == "latex", results='asis', echo=F, purl=F}
cat('\\begin{keyconcepts}[Sesgo\\comma consistencia y eficiencia]{3.2}
Las características deseables de un estimador incluyen insesgabilidad, consistencia y eficiencia.\\newline

\\textbf{Insesgabilidad::}

Si la media de la distribución muestral de algún estimador $\\hat\\mu_Y$ para la media poblacional $\\mu_Y$ es igual a $\\mu_Y$,

$$ E(\\hat\\mu_Y) = \\mu_Y, $$

el estimador es imparcial para $\\mu_Y$. El \\textit{sesgo} de $\\hat\\mu_Y$ entonces es $0$:

$$ E(\\hat\\mu_Y) - \\mu_Y = 0$$

\\textbf{Consistencia:}

Se quiere que la incertidumbre del estimador $\\mu_Y$ disminuya a medida que aumenta el número de observaciones en la muestra. Más precisamente, se quiere que la probabilidad de que la estimación $\\hat\\mu_Y$ caiga dentro de un pequeño intervalo alrededor del valor real $\\mu_Y$ se acerque cada vez más a $1$ a medida que $n$ crece. Se escribe esto como

$$ \\hat\\mu_Y \\xrightarrow{p} \\mu_Y. $$

\\textbf{Varianza y eficiencia:}

Se quiere que el estimador sea eficiente. Suponga que se tienen dos estimadores, $\\hat\\mu_Y$ y $\\overset{\\sim}{\\mu}_Y$ para un tamaño de muestra dado $n$ se sigue que

$$ E(\\hat\\mu_Y) = E(\\overset{\\sim}{\\mu}_Y) = \\mu_Y $$

pero

$$\\text{Var}(\\hat\\mu_Y) < \\text{Var}(\\overset{\\sim}{\\mu}_Y).$$

Entonces se prefiere usar $\\hat\\mu_Y$, ya que tiene una variación menor que $\\overset{\\sim}{\\mu}_Y$, lo que significa que $\\hat\\mu_Y$ es más \\textit{eficiente} en el uso de la información proporcionada por las observaciones en la muestra.
\\end{keyconcepts}')
```

## Propiedades de la media muestral {#PMM}

```{block2, consistency, type='rmdknit'}
Una forma más precisa de expresar la consistencia de un estimador $\hat\mu$ para un parámetro $\mu$ es

$$ P(|\hat{\mu} - \mu|<\epsilon) \xrightarrow[n \rightarrow \infty]{p} 1 \quad \text{for any}\quad\epsilon>0.$$

Esta expresión implica que la probabilidad de observar una desviación del valor verdadero de $\mu$ que es menor que algún $\epsilon > 0$ arbitrario converge a $1$ a medida que $n$ crece. La consistencia no requiere imparcialidad.
```

Para examinar las propiedades de la media muestral como estimador de la media poblacional correspondiente, considere el siguiente ejemplo en **R**.

Se genera una población **pop** que consta de observaciones $Y_i$, $i=1,\dots,10000$ que se originan en una distribución normal con media $\mu = 10$ y varianza $\sigma^2 = 1$.

Para investigar el comportamiento del estimador $\hat{\mu} = \bar{Y}$ se pueden extraer muestras aleatorias de esta población y calcular $\bar{Y}$ para cada una de ellas. Esto se hace fácilmente haciendo uso de la función **replicate()**. El argumento **expr** se evalúa **n** veces. En este caso, se extraen muestras de tamaños $n = 5$ y $n = 25$, se calculan las medias muestrales y se repite esto exactamente $N = 25000$ veces.

Para fines de comparación, se almacenan los resultados para el estimador $Y_1$, la primera observación en una muestra para una muestra de tamaño $5$, por separado.

```{r, 228, echo = T, eval = T, message = F, warning = F}
# generar una población ficticia
pop <- rnorm(10000, 10, 1)

# muestra de la población y estimación de la media
est1 <- replicate(expr = mean(sample(x = pop, size = 5)), n = 25000)

est2 <- replicate(expr = mean(sample(x = pop, size = 25)), n = 25000)

fo <- replicate(expr = sample(x = pop, size = 5)[1], n = 25000)
```

Comprobar que **est1** y **est2** son vectores de longitud $25000$:

```{r, 229}
# comprobar si el tipo de objeto es vector
is.vector(est1)
is.vector(est2)

# comprobar la longitud
length(est1)
length(est2)
```

El fragmento de código a continuación produce una gráfica de las distribuciones muestrales de los estimadores $\bar{Y}$ y $Y_1$ sobre la base de las muestras de $25000$ en cada caso. También se grafica la función de densidad de la distribución $\mathcal{N}(10,1)$.

```{r, 230, echo = T, eval = T, message = F, warning = F, fig.align='center'}
# graficar la densidad de la estimación Y_1
plot(density(fo), 
      col = "green", 
      lwd = 2,
      ylim = c(0, 2),
      xlab = "Estimados",
      main = "Distribuciones muestrales de estimadores insesgados")

# agregar la estimación de densidad para la distribución de la media muestral con n = 5 a la gráfica
lines(density(est1), 
     col = "steelblue", 
     lwd = 2, 
     bty = "l")

# agregar la estimación de densidad para la distribución de la media muestral con n = 25 a la gráfica
lines(density(est2), 
      col = "red2", 
      lwd = 2)

# agregar una línea vertical en el parámetro verdadero
abline(v = 10, lty = 2)

# agregar la densidad N(10, 1) a la gráfica
curve(dnorm(x, mean = 10), 
     lwd = 2,
     lty = 2,
     add = T)

# agregar una leyenda
legend("topleft",
       legend = c("N(10,1)",
                  expression(Y[1]),
                  expression(bar(Y) ~ n == 5),
                  expression(bar(Y) ~ n == 25)
                  ), 
       lty = c(2, 1, 1, 1), 
       col = c("black","green", "steelblue", "red2"),
       lwd = 2)
```

Primero, *todas* las distribuciones de muestreo (representadas por líneas continuas) se centran alrededor de $\mu = 10$. Esto es evidencia de la *imparcialidad* de $Y_1$, $\overline{Y}_{5}$ y $\overline{Y}_{25}$. Por supuesto, la densidad teórica $\mathcal{N}(10,1)$ también se centra en $10$.

A continuación, se observa la extensión de las distribuciones muestrales. Varias cosas son dignas de mención:

- La distribución de muestreo de $Y_1$ (curva verde) rastrea la densidad de la distribución $\mathcal{N}(10,1)$ (línea discontinua negra) bastante de cerca. De hecho, la distribución muestral de $Y_1$ es la distribución $\mathcal{N}(10,1)$. Esto es menos sorprendente si se tiene en cuenta que el estimador $Y_1$ no hace más que informar una observación que se selecciona al azar de una población con distribución $\mathcal{N}(10,1)$. Por lo tanto, $Y_1 \sim \mathcal{N}(10,1)$. Se debe tener en cuenta que este resultado no depende del tamaño de la muestra $n$: la distribución muestral de $Y_1$ *es siempre* la distribución de la población, sin importar el tamaño de la muestra. $Y_1$ es una buena estimación de $\mu_Y$, pero se puede hacer mejor.

- Ambas distribuciones de muestreo de $\overline{Y}$ muestran menos dispersión que la distribución de muestreo de $Y_1$. Lo cual implica que $\overline{Y}$ tiene una variación menor que $Y_1$. En vista de los Conceptos clave 3.2 y 3.3, se encuentra que $\overline{Y}$ es un estimador más eficiente que $Y_1$. De hecho, esto es válido para todos los $n > 1$.

- $\overline{Y}$ muestra un comportamiento que ilustra la coherencia (ver Concepto clave 3.2). Las densidades azul y roja están mucho más concentradas alrededor de $\mu=10$ que la verde. A medida que aumenta el número de observaciones de $1$ a $5$, la distribución muestral se ajusta en torno al parámetro verdadero. Al aumentar el tamaño de la muestra a $25$, este efecto se vuelve más evidente. Esto implica que la probabilidad de obtener estimaciones cercanas al valor real aumenta con $n$. Esto también se refleja en los valores estimados de la función de densidad cercanos a $10$: cuanto mayor es el tamaño de la muestra, mayor es el valor de la densidad.

Se recomienda que siga adelante y modifique el código. Pruebe diferentes valores para el tamaño de la muestra y vea cómo cambia la distribución muestral de $\overline{Y}$.

#### $\overline{Y}$ es el estimador de mínimos cuadrados de $\mu_Y$ {-}

Suponga que tiene algunas observaciones $Y_1,\dots,Y_n$ en $Y \sim \mathcal{N}(10,1)$ (que se desconoce) y le gustaría encontrar un estimador $m$ que prediga las observaciones también como sea posible. Por bueno se quiere decir elegir $m$ de manera que la desviación al cuadrado total entre el valor predicho y los valores observados sea pequeña. Matemáticamente, esto significa que se quiere encontrar un $m$ que minimice

\begin{equation}
  \sum_{i=1}^n (Y_i - m)^2. (\#eq:sqm)
\end{equation}

Piense en $Y_i - m$ como el error cometido al predecir $Y_i$ por $m$. También se podría minimizar la suma de las desviaciones absolutas de $m$, pero minimizar la suma de las desviaciones al cuadrado es matemáticamente más conveniente (y conduce a un resultado diferente). Es por eso que el estimador que se está buscando se llama *estimador de mínimos cuadrados*. $m = \overline{Y}$, la media de la muestra, es este estimador.

Se puede mostrar esto generando una muestra aleatoria y graficando \@ref(eq:sqm) como una función de $m$.

```{r, 231, fig.align='center'}
# definir la función y vectorizarla
sqm <- function(m) {
 sum((y-m)^2)
}
sqm <- Vectorize(sqm)

# extraer una muestra aleatoria y calcular la media
y <- rnorm(100, 10, 1)
mean(y)
```

```{r, 232, fig.align='center'}
# graficar la función objetivo
curve(sqm(x), 
      from = -50, 
      to = 70,
      xlab = "m",
      ylab = "sqm(m)")

# agregar una línea vertical en la media (y)
abline(v = mean(y), 
       lty = 2, 
       col = "darkred")

# agregar anotación en la media (y)
text(x = mean(y), 
     y = 0, 
     labels = paste(round(mean(y), 2)))
```

Observe que \@ref(eq:sqm) es una función cuadrática, por lo que solo hay un mínimo. La gráfica muestra que este mínimo se encuentra exactamente en la media muestral de los datos muestrales.

```{block2, vecfunction, type='rmdknit'}
Algunas funciones en <tt>R</tt> solo pueden interactuar con funciones que toman un vector como entrada y evalúan el cuerpo de la función en cada entrada del vector, por ejemplo <tt>curve()</tt>. A estas funciones se les da el nombre de funciones vectorizadas y, a menudo, es una buena idea escribir las funciones vectorizadas por usted mismo, aunque esto es engorroso en algunos casos. Tener una función vectorizada en <tt>R</tt> nunca es un inconveniente ya que estas funciones sirven tanto para valores únicos, así como para vectores.    

Se puede ver la función <tt>sqm()</tt>, que no está vectorizada:

<tt>
sqm <- function(m) {  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; sum((y-m)^2)   #cuerpo de la función    
}
</tt>
  
Proporcionar, por ejemplo, <tt>c (1,2,3)</tt> como argumento <tt>m</tt> causaría un error, ya que entonces la operación <tt>ym</tt> no es válida: los vectores <tt>y</tt> y <tt>m</tt> son de dimensiones incompatibles. Es por eso que no se puede usar <tt>sqm()</tt> junto con <tt>curve()</tt>.

Aquí entra en juego <tt>Vectorize()</tt>. Genera una versión vectorizada de una función no vectorizada.
```

#### ¿Por qué es importante el muestreo aleatorio? {-}

Hasta ahora, se asume (a veces implícitamente) que los datos observados $Y_1, \dots, Y_n$ son el resultado de un proceso de muestreo que satisface el supuesto de muestreo aleatorio simple. Esta suposición a menudo se cumple al estimar la media de una población usando $\overline{Y}$. Si este no es el caso, las estimaciones pueden estar sesgadas.

Volviendo a **pop**, la población ficticia de observaciones de $10000$ y calcular la media de la población $\mu_{\texttt{pop}}$:

```{r, 233}
# calcular la media poblacional de pop
mean(pop)
```

A continuación, se toman muestras de $10$ observaciones de **pop** con **sample()** y se estima $\mu_{\texttt{pop}}$ usando $\overline{Y}$ repetidamente. Sin embargo, ahora se usa un esquema de muestreo que se desvía del muestreo aleatorio simple: en lugar de asegurar que cada miembro de la población tenga la misma probabilidad de terminar en una muestra, se asigna una probabilidad más alta de ser muestreada a las observaciones más pequeñas de $2500$ de la población estableciendo el argumento **prob** en un vector adecuado de ponderaciones de probabilidad:

```{r, 234}
# simular resultados para la media muestral cuando la suposición de variables aleatorias i.i.d. falla
est3 <-  replicate(n = 25000, 
                   expr = mean(sample(x = sort(pop), 
                                      size = 10, 
                                      prob = c(rep(4, 2500), rep(1, 7500)))))

# calcular la media muestral de los resultados
mean(est3)
```

A continuación, se traza la distribución muestral de $\overline{Y}$ para el caso de no i.i.d. y se compara con la distribución muestral cuando la suposición de i.i.d. se mantiene.

```{r, 235, fig.align='center'}
# distribución muestral de la media muestral, i.i.d. sostiene, n = 25
plot(density(est2), 
      col = "steelblue",
      lwd = 2,
      xlim = c(8, 11),
      xlab = "Estimados",
      main = "Cuando la suposición de i.i.d. falla")

# distribución muestral de la media muestral, i.i.d. falla, n = 25
lines(density(est3),
      col = "red2",
      lwd = 2)

# agrega una leyenda
legend("topleft",
       legend = c(expression(bar(Y)[n == 25]~", i.i.d. falla"),
                  expression(bar(Y)[n == 25]~", i.i.d. se mantiene")
                  ), 
       lty = c(1, 1), 
       col = c("red2", "steelblue"),
       lwd = 2)
```

Aquí, el fracaso de la suposición de i.i.d. implica que, en promedio, se ha *subestimado* $\mu_Y$ usando $\overline{Y}$: la distribución correspondiente de $\overline{Y}$ se desplaza hacia la izquierda. En otras palabras, $\overline{Y}$ es un estimador *sesgado* para $\mu_Y$ si la suposición de i.i.d. no se sostiene.

## Pruebas de hipótesis relativas a la media de la población

En esta sección, se revisan brevemente los conceptos de la prueba de hipótesis y se discute cómo realizar pruebas de hipótesis en **R**. El objetivo en hacer inferencias sobre una media poblacional desconocida.

#### Acerca de las hipótesis y las pruebas de hipótesis {-}

En una prueba de significancia se quiere aprovechar la información contenida en una muestra como evidencia a favor o en contra de una hipótesis. Esencialmente, las hipótesis son preguntas simples que pueden responderse con un "sí" o un "no". En una prueba de hipótesis, normalmente se trata con dos hipótesis diferentes:

- La *hipótesis nula*, denotada $H_0$, es la hipótesis que se está interesado en probar.

- Debe haber una *hipótesis alternativa*, denotada $H_1$, la hipótesis que se cree que se cumple si se rechaza la hipótesis nula.

La hipótesis nula de que la media poblacional de $Y$ es igual al valor $\mu_{Y,0}$ se escribe como

$$ H_0: E(Y) = \mu_{Y,0}. $$

A menudo, la hipótesis alternativa elegida es la más general,

$$ H_1: E(Y) \neq \mu_{Y,0}, $$

lo que implica que $E(Y)$ puede ser cualquier cosa menos el valor de la hipótesis nula. Esto se llama una alternativa *de dos caras*.

En aras de la brevedad, solo se consideran alternativas de dos caras en las secciones siguientes de este apartado.

### El valor p (p-Value) {-}

Suponga que la hipótesis nula es *verdadera*. El valor $p$ es la probabilidad de extraer datos y observar un estadístico de prueba correspondiente que sea al menos tan adverso a lo que se establece bajo la hipótesis nula, como el estadístico de prueba realmente calculado usando los datos de la muestra.

En el contexto de la media poblacional y la media muestral, esta definición puede expresarse matemáticamente de la siguiente manera:

\begin{equation}
p \text{-value} = P_{H_0}\left[ \lvert \overline{Y} - \mu_{Y,0} \rvert > \lvert \overline{Y}^{act} - \mu_{Y,0} \rvert \right] (\#eq:pvalue)
\end{equation}

En \@ref(eq:pvalue), $\overline{Y}^{act}$ es la media muestral de los datos disponibles (un valor). Para calcular el valor $p$ como en \@ref(eq:pvalue), se requiere el conocimiento sobre la distribución muestral de $\overline{Y}$ (una variable aleatoria) cuando la hipótesis nula es verdadera (la *distribución nula*). Sin embargo, en la mayoría de los casos se desconoce la distribución muestral y, por tanto, la distribución nula de $\overline{Y}$. Afortunadamente, el TLC (ver Concepto clave 2.7) permite la aproximación de muestra grande 

$$ \overline{Y} \approx \mathcal{N}(\mu_{Y,0}, \, \sigma^2_{\overline{Y}}) \ \ , \ \ \sigma^2_{\overline{Y}} = \frac{\sigma_Y^2}{n}, $$

asumiendo que la hipótesis nula $H_0: E(Y) = \mu_{Y, 0}$ es cierta. Con algo de álgebra se sigue para una $n$ grande que

$$ \frac{\overline{Y} - \mu_{Y,0}}{\sigma_Y/\sqrt{n}} \sim \mathcal{N}(0,1). $$

Entonces, en muestras grandes, el valor $p$ se puede calcular *sin* conocimiento de la distribución muestral exacta de $\overline{Y}$ usando la aproximación normal anterior.

### Cálculo del valor p cuando se conoce la desviación estándar {-}

Por ahora, se supone que se conoce $\sigma_{\overline{Y}}$. Entonces, se puede reescribir \@ref(eq:pvalue) como:

\begin{align}
p \text{-value} =& \, P_{H_0}\left[ \left\lvert \frac{\overline{Y} - \mu_{Y,0}}{\sigma_{\overline{Y}}} \right\rvert > \left\lvert \frac{\overline{Y}^{act} - \mu_{Y,0}}{\sigma_{\overline{Y}}} \right\rvert \right] \\
=& \, 2 \cdot \Phi \left[ - \left\lvert \frac{\overline{Y}^{act} - \mu_{Y,0}}{\sigma_{\overline{Y}}}  \right\rvert\right].  (\#eq:pvaluenorm1)
\end{align}

El valor $p$ es el área en las colas de la distribución $\mathcal{N}(0,1)$ que se encuentra más allá de

\begin{equation}
\pm \left\lvert \frac{\overline{Y}^{act} - \mu_{Y,0}}{\sigma_{\overline{Y}}} \right\rvert (\#eq:pvaluenorm2)
\end{equation}

Ahora usando **R** para visualizar lo que se indica en \@ref(eq:pvaluenorm1) y \@ref(eq:pvaluenorm2). El siguiente fragmento de código replica la Figura 3.1.

```{r, 236, fig.align='center'}
# graficar la densidad normal estándar en el intervalo [-4,4]
curve(dnorm(x),
      xlim = c(-4, 4),
      main = "Calcular un valor p",
      yaxs = "i",
      xlab = "z",
      ylab = "",
      lwd = 2,
      axes = "F")

# agregar eje x
axis(1, 
     at = c(-1.5, 0, 1.5), 
     padj = 0.75,
     labels = c(expression(-frac(bar(Y)^"act"~-~bar(mu)[Y,0], sigma[bar(Y)])),
                0,
                expression(frac(bar(Y)^"act"~-~bar(mu)[Y,0], sigma[bar(Y)]))))

# sombrear la región del valor p/2 en la cola izquierda
polygon(x = c(-6, seq(-6, -1.5, 0.01), -1.5),
        y = c(0, dnorm(seq(-6, -1.5, 0.01)),0), 
        col = "steelblue")

# sombrear la región del valor p/2 en la cola derecha
polygon(x = c(1.5, seq(1.5, 6, 0.01), 6),
        y = c(0, dnorm(seq(1.5, 6, 0.01)), 0), 
        col = "steelblue")
```

### Varianza de muestra, desviación estándar de muestra y error estándar {-}

Si se desconoce $\sigma^2_Y$, debe estimarse. Esto se puede hacer usando la varianza de la muestra:

\begin{equation}
s_Y^2 = \frac{1}{n-1} \sum_{i=1}^n (Y_i - \overline{Y})^2.
\end{equation}

Además

\begin{equation}
s_Y = \sqrt{\frac{1}{n-1} \sum_{i=1}^n (Y_i - \overline{Y})^2}
\end{equation}

es un estimador adecuado para la desviación estándar de $Y$. En **R**, $s_Y$ se implementa en la función **sd()**, ver `?Sd`.

Usando **R** se puede ilustrar que $s_Y$ es un estimador consistente para $\sigma_Y$, es decir

$$ s_Y \overset{p}{\longrightarrow} \sigma_Y. $$

La idea aquí es generar una gran cantidad de muestras $Y_1,\dots,Y_n$ donde, $Y\sim \mathcal{N}(10, 9)$ digamos, estimar $\sigma_Y$ usando $s_Y$ e investigar cómo la distribución de $s_Y$ cambia a medida que $n$ aumenta.

```{r, 237, fig.align='center'}
# vector de tamaños de muestra
n <- c(10000, 5000, 2000, 1000, 500)

# observaciones de muestra, estimando usando 'sd()' y graficar las distribuciones estimadas
sq_y <- replicate(n = 10000, expr = sd(rnorm(n[1], 10, 3)))
plot(density(sq_y),
     main = expression("Distribuciones de muestreo o" ~ s[Y]),
     xlab = expression(s[y]),
     lwd = 2)

for (i in 2:length(n)) {
  sq_y <- replicate(n = 10000, expr = sd(rnorm(n[i], 10, 3)))
  lines(density(sq_y), 
        col = i, 
        lwd = 2)
}

# agrega una leyenda
legend("topleft",
       legend = c(expression(n == 10000),
                  expression(n == 5000),
                  expression(n == 2000),
                  expression(n == 1000),
                  expression(n == 500)), 
       col = 1:5,
       lwd = 2)
```

El gráfico muestra que la distribución de $s_Y$ se ajusta alrededor del valor real $\sigma_Y = 3$ a medida que aumenta $n$.

La función que estima la desviación estándar de un estimador se llama *error estándar del estimador*. El concepto clave 3.4 resume la terminología en el contexto de la media muestral.

```{r, 238, eval = my_output == "html", results='asis', echo=F, purl=F}
cat('
<div class = "keyconcept" id="KC3.4">
<h3 class = "right"> Concepto clave 3.4 </h3>          
<h3 class = "left"> El error estándar de $\\overline{Y}$ </h3>
Tomar una muestra i.i.d. $Y_1, \\dots, Y_n$. La media de $Y$ se estima consistentemente mediante $\\overline{Y}$, la media muestral de $ Y_i $. Dado que $\\overline{Y}$ es una variable aleatoria, tiene una distribución de muestreo con varianza $\\frac{\\sigma_Y^2}{n}$.

El error estándar de $\\overline{Y}$, denotado $SE(\\overline{Y})$ es un estimador de la desviación estándar de $\\overline{Y}$:

$$ SE(\\overline{Y}) = \\hat\\sigma_{\\overline{Y}} = \\frac{s_Y}{\\sqrt{n}} $$

El signo de intercalación (\\^) sobre $\\sigma$ indica que $\\hat\\sigma_{\\overline{Y}}$ es un estimador de $\\sigma_{\\overline{Y}}$.
</div>
')
```

```{r, 239, eval = my_output == "latex", results='asis', echo=F, purl=F}
cat('\\begin{keyconcepts}[El error estándar de $\\overline{Y}$]{3.4}
Tomar una muestra i.i.d. $Y_1, \\dots, Y_n$. La media de $Y$ se estima consistentemente mediante $\\overline{Y}$, la media muestral de $ Y_i $. Dado que $\\overline{Y}$ es una variable aleatoria, tiene una distribución de muestreo con varianza $\\frac{\\sigma_Y^2}{n}$.

El error estándar de $\\overline{Y}$, denotado $SE(\\overline{Y})$ es un estimador de la desviación estándar de $\\overline{Y}$:

$$ SE(\\overline{Y}) = \\hat\\sigma_{\\overline{Y}} = \\frac{s_Y}{\\sqrt{n}} $$

El signo de intercalación (\\^) sobre $\\sigma$ indica que $\\hat\\sigma_{\\overline{Y}}$ es un estimador de $\\sigma_{\\overline{Y}}$.
\\end{keyconcepts}
')
```

Como ejemplo para respaldar el Concepto clave 3.4, considere una muestra de $n = 100$ i.i.d. observaciones de la variable distribuida de Bernoulli $Y$ con probabilidad de éxito $p=0.1$. Entonces $E(Y)=p=0.1$ y $\text{Var}(Y)=p(1-p)$. $E(Y)$ se puede estimar mediante $\overline{Y}$, que luego tiene una variación

$$ \sigma^2_{\overline{Y}} = p(1-p)/n = 0.0009 $$

y desviación estándar

$$ \sigma_{\overline{Y}} = \sqrt{p(1-p)/n} = 0.03. $$

En este caso, el error estándar de $\overline{Y}$  puede estimarse mediante

$$ SE(\overline{Y}) = \sqrt{\overline{Y}(1-\overline{Y})/n}. $$

Se comprueba si $\overline{Y}$ y $SE(\overline{Y})$ estiman los valores verdaderos respectivos, en promedio.

```{r, 240}
# extraiga 10000 muestras de tamaño 100 y estime la media de Y y
# estimar el error estándar de la media muestral

mean_estimates <- numeric(10000)
se_estimates <- numeric(10000)

for (i in 1:10000) {
  
  s <- sample(0:1, 
              size = 100,  
              prob = c(0.9, 0.1),
              replace = T)
  
  mean_estimates[i] <- mean(s)
  se_estimates[i] <- sqrt(mean(s) * (1 - mean(s)) / 100)

}

mean(mean_estimates)
mean(se_estimates)
```

Ambos estimadores parecen no tener sesgos para los parámetros verdaderos. De hecho, esto es cierto para la media de la muestra, pero no para $SE(\overline{Y})$. Sin embargo, ambos estimadores son *consistentes* para los parámetros verdaderos.

### Cálculo del valor p cuando la desviación estándar es desconocida {-}

Cuando se desconoce $\sigma_Y$, el valor de $p$ para una prueba de hipótesis sobre $\mu_Y$ usando $\overline{Y}$  se puede calcular reemplazando $\sigma_{\overline{Y}}$ en \@ref(eq:pvaluenorm1) por el error estándar $SE(\overline{Y}) = \hat\sigma_{\overline{Y}}$. Luego,

$$ p\text{-value} = 2\cdot\Phi\left(-\left\lvert \frac{\overline{Y}^{act}-\mu_{Y,0}}{SE(\overline{Y})} \right\rvert \right). $$

Esto se hace fácilmente en **R**:

```{r, 241}
# muestra y estimación, calcula el error estándar
samplemean_act <- mean(
  sample(0:1, 
         prob = c(0.9, 0.1), 
         replace = T, 
         size = 100))

SE_samplemean <- sqrt(samplemean_act * (1 - samplemean_act) / 100)

# hipótesis nula
mean_h0 <- 0.1

# calcular el valor p
pvalue <- 2 * pnorm(- abs(samplemean_act - mean_h0) / SE_samplemean)
pvalue
```

Más adelante en el curso, se encontrarán enfoques más convenientes para obtener estadísticos $t$ y valores $p$ usando **R**.

### La estadística t {-}

En la prueba de hipótesis, el promedio de la muestra estandarizada

\begin{equation}
t = \frac{\overline{Y} - \mu_{Y,0}}{SE(\overline{Y})} (\#eq:tstat)
\end{equation}

se llama un estadístico $t$. Dicho estadístico de $t$ juega un papel importante en la prueba de hipótesis sobre $\mu_Y$. Es un ejemplo destacado de estadística de prueba.

Implícitamente, ya se ha calculado un estadístico $t$ para $\overline{Y}$ en el fragmento de código anterior.

```{r, 242}
# calcular un estadístico t para la media de la muestra
tstatistic <- (samplemean_act - mean_h0) / SE_samplemean
tstatistic
```

Usando **R** se puede ilustrar que si $\mu_{Y,0}$ es igual al valor verdadero, es decir, si la hipótesis nula es verdadera, \@ref(eq:tstat) es aproximadamente $\mathcal{N}(0,1)$ distribuido cuando $n$ es grande.

```{r, 243}
# preparar un vector vacío para estadísticas t
tstatistics <- numeric(10000)

# establecer tamaño de muestra
n <- 300

# simular 10000 estadísticos t
for (i in 1:10000) {
  
  s <- sample(0:1, 
              size = n,  
              prob = c(0.9, 0.1),
              replace = T)
  
  tstatistics[i] <- (mean(s)-0.1)/sqrt(var(s)/n)
  
}
```

En la simulación anterior, se estimó la varianza de $Y_i$ usando **var(s)**. Esto es más general que `mean(s)*(1-mean(s))`, ya que esta última requiere que los datos estén distribuidos por Bernoulli y que se sepa esto.

```{r, 244, fig.align='center'}
# graficar densidad y comparar con la densidad N(0,1)
plot(density(tstatistics),
     xlab = "Estadístico t",
     main = "Distribución estimada del estadístico t cuando n = 300",
     lwd = 2,
     xlim = c(-4, 4),
     col = "steelblue")

# N(0,1) densidad (discontinua)
curve(dnorm(x), 
      add = T, 
      lty = 2, 
      lwd = 2)
```

A juzgar por el gráfico, la aproximación normal funciona razonablemente bien para el tamaño de muestra elegido. Esta aproximación normal ya se ha utilizado en la definición del valor $p$, ver \@ref(eq:tstat).

### Prueba de hipótesis con un nivel de significancia preespecificado {-}

```{r, 245, eval = my_output == "html", results='asis', echo=F, purl=F}
cat('
<div class = "keyconcept" id="KC3.5">
<h3 class = "right"> Concepto clave 3.5 </h3>          
<h3 class = "left"> La terminología de la prueba de hipótesis </h3>

En la prueba de hipótesis, son posibles dos tipos de errores:

1. La hipótesis nula *es* rechazada aunque es cierta (error de tipo I)

2. La hipótesis nula *no se* rechaza aunque es falsa (error de tipo II)

El **nivel de significancia** de la prueba es la probabilidad de cometer un error de tipo I, que se está dispuesto a aceptar de antemano. Por ejemplo, usando un nivel de significancia preespecificado de $0.05$, se rechaza la hipótesis nula si y solo si el valor $p$ es menor que $0.05$. El nivel de significancia se elige antes de realizar la prueba.

Un procedimiento equivalente es rechazar la hipótesis nula si el estadístico de prueba observado es, en términos de valor absoluto, mayor que el **valor crítico** del estadístico de prueba. El valor crítico está determinado por el nivel de significancia elegido y define dos conjuntos de valores separados que se denominan **región de aceptación** y **región de rechazo**. La región de aceptación contiene todos los valores de la estadística de prueba para los que la prueba no rechaza, mientras que la región de rechazo contiene todos los valores para los que la prueba sí rechaza.

El **valor $p$** es la probabilidad de que, en un muestreo repetido bajo las mismas condiciones, se observe un estadístico de prueba que proporcione tanta evidencia contra la hipótesis nula como el estadístico de prueba realmente observado.

La probabilidad real de que la prueba rechace la verdadera hipótesis nula se denomina **tamaño de la prueba**. En un entorno ideal, el tamaño es igual al nivel de significancia.

La probabilidad de que la prueba rechace correctamente una hipótesis nula falsa se llama **potencia**. 

</div>
')
```

```{r, 246, eval = my_output == "latex", results='asis', echo=F, purl=F}
cat('\\begin{keyconcepts}[La terminología de la prueba de hipótesis]{3.5}

En la prueba de hipótesis, son posibles dos tipos de errores:\\newline

\\begin{enumerate}
\\item La hipótesis nula \\textit{es} rechazada aunque es cierta (error de tipo I)
\\item La hipótesis nula \\textit{no se} rechaza aunque es falsa (error de tipo II) 
\\end{enumerate}\\vspace{0.5cm}

El \\textit{nivel de significancia} de la prueba es la probabilidad de cometer un error de tipo I, que se está dispuesto a aceptar de antemano. Por ejemplo, usando un nivel de significancia preespecificado de $0.05$, se rechaza la hipótesis nula si y solo si el valor $p$ es menor que $0.05$. El nivel de significancia se elige antes de realizar la prueba.\\newline

Un procedimiento equivalente es rechazar la hipótesis nula si el estadístico de prueba observado es, en términos de valor absoluto, mayor que el \\textit{valor crítico} del estadístico de prueba. El valor crítico está determinado por el nivel de significancia elegido y define dos conjuntos de valores separados que se denominan \\textit{región de aceptación} y \\textit{región de rechazo}. La región de aceptación contiene todos los valores de la estadística de prueba para los que la prueba no rechaza, mientras que la región de rechazo contiene todos los valores para los que la prueba sí rechaza.\\newline

El \\textit{valor $p$} es la probabilidad de que, en un muestreo repetido bajo las mismas condiciones, se observe un estadístico de prueba que proporcione tanta evidencia contra la hipótesis nula como el estadístico de prueba realmente observado.\\newline

La probabilidad real de que la prueba rechace la verdadera hipótesis nula se denomina \\textit{tamaño de la prueba}. En un entorno ideal, el tamaño es igual al nivel de significancia.\\newline

La probabilidad de que la prueba rechace correctamente una hipótesis nula falsa se llama \\textit{potencia}. 

\\end{keyconcepts}
')
```

Reconsidere el **valor p** calculado más arriba:

```{r, 247}
# comprobar si el valor p < 0.05
pvalue < 0.05
```

La condición no se cumple por lo que no se rechaza correctamente la hipótesis nula.

Cuando se trabaja con un estadístico $t$ en su lugar, es equivalente a aplicar la siguiente regla:

$$ \text{Rechazar } H_0 \text{ si } \lvert t^{act} \rvert > 1.96 $$

Se rechaza la hipótesis nula al nivel de significancia de $5\%$ si la estadística de $t$ calculada se encuentra más allá del valor crítico de 1.96 en términos de valor absoluto. $1.96$ es el cuantil $0.975$ de la distribución normal estándar.

```{r, 248}
# comprobar el valor crítico
qnorm(p = 0.975)

# compruebar si la hipótesis nula se rechaza utilizando el estadístico t calculado más arriba
abs(tstatistic) > 1.96
```

Al igual que con el valor $p$, no se puede rechazar la hipótesis nula utilizando el estadístico $t$ correspondiente. El Concepto clave 3.6 resume el procedimiento de realizar una prueba de hipótesis bilateral sobre la media poblacional $E(Y)$.

```{r, 249, eval = my_output == "html", results='asis', echo=F, purl=F}
cat('
<div class = "keyconcept" id="KC3.6">
<h3 class = "right"> Concepto clave 3.6 </h3>          
<h3 class = "left"> Prueba de la hipótesis $E(Y) = \\mu_{Y,0}$ contra la alternativa $E(Y) \\neq \\mu_{Y,0}$ </h3>

1. Estimar $\\mu_{Y}$ usando $\\overline{Y}$ y calculando el $SE(\\overline{Y})$, error estándar de $\\overline{Y}$.

2. Calcular el estadístico $t$.

3. Calcular el valor $p$ y rechazar la hipótesis nula en el nivel de significancia $5\\%$ si el valor $p$ es menor que $0.05$ o, de manera equivalente, si 

$$ \\left\\lvert t^{act} \\right\\rvert > 1.96. $$
</div>

')
```

```{r, 250, eval = my_output == "latex", results='asis', echo=F, purl=F}
cat('\\begin{keyconcepts}[Probar la hipótesis $E(Y) = \\mu_{Y,0}$ contra la alternativa $E(Y) \\neq \\mu_{Y,0}$]{3.6}
\\begin{enumerate}
\\item Estimar $\\mu_{Y}$ usando $\\overline{Y}$ y calculando el $SE(\\overline{Y})$, error estándar de $\\overline{Y}$.
\\item Calcular el estadístico $t$.
\\item Calcular el valor $p$ y rechazar la hipótesis nula en el nivel de significancia $5\\%$ si el valor $p$ es menor que $0.05$ o, de manera equivalente, si 

$$ \\left\\lvert t^{act} \\right\\rvert > 1.96. $$
\\end{enumerate}
\\end{keyconcepts}
')
```

### Alternativas unilaterales {-}

A veces interesa probar si la media es mayor o menor que algún valor hipotético bajo el nulo. Para ceñirse al curso, se toma la presunta brecha salarial entre los trabajadores con buena educación y los menos educados. Dado que se anticipó que existe tal diferencial, una alternativa relevante (a la hipótesis nula de que no existe un diferencial salarial) es que los individuos bien educados ganan más; es decir, que el salario promedio por hora para este grupo, $\mu_Y$ es *mayor* que $\mu_{Y, 0}$, el salario promedio de los trabajadores con menos educación que se asume que se conoce aquí por simplicidad (la sección \@ref(CMDP) analiza cómo probar la equivalencia de medias poblacionales desconocidas).

Este es un ejemplo de una *prueba del lado derecho* y el par de hipótesis se elige para ser

$$ H_0: \mu_Y = \mu_{Y,0} \ \ \text{vs} \ \ H_1: \mu_Y > \mu_{Y,0}. $$

Se rechaza la hipótesis nula si el estadístico de prueba calculado es mayor que el valor crítico $1.64$, el equivalente de $0.95$ de la distribución $\mathcal{N}(0,1)$. Esto asegura que $1-0.95=5\%$ masa de probabilidad permanece en el área a la derecha del valor crítico. Como antes, se puede visualizar esto en **R** usando la función **polygon()**.

```{r, 251, fig.align='center'}
# graficar la densidad normal estándar en el dominio [-4,4]
curve(dnorm(x),
      xlim = c(-4, 4),
      main = "Región de rechazo de una prueba del lado derecho",
      yaxs = "i",
      xlab = "Estadístico t",
      ylab = "",
      lwd = 2,
      axes = "F")

# Agregar el eje x
axis(1, 
     at = c(-4, 0, 1.64, 4), 
     padj = 0.5,
     labels = c("", 0, expression(Phi^-1~(.95)==1.64), ""))

# Sombrear la región de rechazo en la cola izquierda
polygon(x = c(1.64, seq(1.64, 4, 0.01), 4),
        y = c(0, dnorm(seq(1.64, 4, 0.01)), 0), 
        col = "darkred")
```

De manera análoga, para la prueba del lado izquierdo se tiene

$$ H_0: \ mu_Y = \ mu_ {Y, 0} \ \ \ text {vs.} \ \ H_1: \ mu_Y <\ mu_ {Y, 0}. $$ 

El nulo se rechaza si el estadístico de prueba observado no alcanza el valor crítico que, para una prueba en el nivel de significancia de $0.05$, está dado por $-1.64$, el equivalente de $0.05$ -cuantil de la distribución $\mathcal{N}(0,1)$. $5\%$ masa de probabilidad se encuentra a la izquierda del valor crítico.

Es sencillo adaptar el fragmento de código anterior al caso de una prueba del lado izquierdo. Solo se tiene que ajustar el sombreado de color y las marcas de graduación.

```{r, 252, fig.align='center'}
# graficar la densidad normal estándar en el dominio [-4,4]
curve(dnorm(x),
      xlim = c(-4, 4),
      main = "Región de rechazo de una prueba del lado izquierdo",
      yaxs = "i",
      xlab = "Estadístico t",
      ylab = "",
      lwd = 2,
      axes = "F")

# Agregar eje x
axis(1, 
     at = c(-4, 0, -1.64, 4), 
     padj = 0.5,
     labels = c("", 0, expression(Phi^-1~(.05)==-1.64), ""))

# Región de rechazo de sombra en la cola derecha
polygon(x = c(-4, seq(-4, -1.64, 0.01), -1.64),
        y = c(0, dnorm(seq(-4, -1.64, 0.01)), 0), 
        col = "darkred")
```

## Intervalos de confianza para la media de la población

Como se enfatizó anteriormente, nunca se calcula el valor *exacto* de la media poblacional de $Y$ usando una muestra aleatoria. Sin embargo, se pueden calcular los intervalos de confianza para la media de la población. En general, un intervalo de confianza para un parámetro desconocido es una receta que, en muestras repetidas, produce intervalos que contienen el parámetro verdadero con una probabilidad preespecificada, el *nivel de confianza*. Los intervalos de confianza se calculan utilizando la información disponible en la muestra. Dado que esta información es el resultado de un proceso aleatorio, los intervalos de confianza son variables aleatorias en sí mismas.

El Concepto clave 3.7 muestra cómo calcular los intervalos de confianza para la media poblacional desconocida $E(Y)$.

```{r, 253, eval = my_output == "html", results='asis', echo=F, purl=F}
cat('
<div class = "keyconcept" id="KC3.7">
<h3 class = "right"> Concepto clave 3.7 </h3>          
<h3 class = "left"> Intervalos de confianza para la media poblacional </h3>

Un intervalo de confianza de $95\\%$ para $\\mu_Y$ es una *variable aleatoria* que contiene el verdadero $\\mu_Y$ en $95\\%$ de todas las muestras aleatorias posibles. Cuando $n$ es grande, se puede usar la aproximación normal. Entonces, $99\\%$,  $95\\%$, $90\\%$ los intervalos de confianza son:

\\begin{align}
&99\\%\\text{ intervalo de confianza para } \\mu_Y = \\left[ \\overline{Y} \\pm 2.58 \\times SE(\\overline{Y}) \\right], \\\\
&95\\%\\text{ intervalo de confianza para } \\mu_Y = \\left[\\overline{Y} \\pm 1.96 \\times SE(\\overline{Y}) \\right], \\\\
&90\\%\\text{ intervalo de confianza para } \\mu_Y = \\left[ \\overline{Y} \\pm 1.64 \\times SE(\\overline{Y}) \\right].
\\end{align}

Estos intervalos de confianza son conjuntos de hipótesis nulas que no se pueden rechazar en una prueba de hipótesis bilateral con el nivel de confianza dado.

Ahora considerar las siguientes declaraciones.

1. En muestreo repetido, el intervalo

$$ \\left[ \\overline{Y} \\pm 1.96 \\times SE(\\overline{Y}) \\right] $$

cubre el valor real de $\\mu_Y$ con una probabilidad de $95\\%$.

2. Se ha calculado $\\overline{Y} = 5.1$ y $SE(\\overline{Y})=2.5$ por lo que el intervalo

$$ \\left[ 5.1  \\pm 1.96 \\times 2.5 \\right] = \\left[0.2,10\\right] $$ 

cubre el valor real de $\\mu_Y$ con una probabilidad de $95\\%$.

Si bien 1. es correcto (esto está en línea con la definición anterior), 2. está incorrecto y ninguno de profesionista quiere leer una oración de este tipo en un trabajo final, examen escrito o similar, creealo.

La diferencia es que, mientras que 1. es la definición de una variable aleatoria, 2. es un posible *resultado* de dicha variable aleatoria, por lo que no tiene sentido hacer ninguna afirmación probabilística al respecto. ¡O el intervalo calculado cubre $\\mu_Y$ *o no*!

</div>
')
```

```{r, 254, eval = my_output == "latex", results='asis', echo=F, purl=F}
cat('\\begin{keyconcepts}[Intervalos de confianza para la media poblacional]{3.7}

Un intervalo de confianza de $95\\%$ para $\\mu_Y$ es una \\texttt{variable aleatoria} que contiene el verdadero $\\mu_Y$ en $95\\%$ de todas las muestras aleatorias posibles. Cuando $n$ es grande, se puede usar la aproximación normal. Entonces, $99\\%$,  $95\\%$, $90\\%$ los intervalos de confianza son:\\newline

\\begin{align}
&99\\%\\text{ intervalo de confianza para } \\mu_Y = \\left[ \\overline{Y} \\pm 2.58 \\times SE(\\overline{Y}) \\right], \\\\
&95\\%\\text{ intervalo de confianza para } \\mu_Y = \\left[\\overline{Y} \\pm 1.96 \\times SE(\\overline{Y}) \\right], \\\\
&90\\%\\text{ intervalo de confianza para } \\mu_Y = \\left[ \\overline{Y} \\pm 1.64 \\times SE(\\overline{Y}) \\right].
\\end{align}

Estos intervalos de confianza son conjuntos de hipótesis nulas que no se pueden rechazar en una prueba de hipótesis bilateral con el nivel de confianza dado.\\newline

Ahora considerar las siguientes declaraciones.\\newline

\\begin{enumerate}
\\item En muestreo repetido, el intervalo
$$ \\left[ \\overline{Y} \\pm 1.96 \\times SE(\\overline{Y}) \\right] $$
cubre el valor real de $\\mu_Y$ con una probabilidad de $95\\%$.

\\item Se ha calculado $\\overline{Y} = 5.1$ y $SE(\\overline{Y})=2.5$ por lo que el intervalo
$$ \\left[5.1  \\pm 1.96 \\times 2.5 \\right] = \\left[0.2,10\\right] $$ 
cubre el valor real de $\\mu_Y$ con una probabilidad de $95\\%$.
\\end{enumerate}\\vspace{0.5cm}

Si bien 1. es correcto (esto está en línea con la definición anterior), 2. está incorrecto y ninguno de profesionista quiere leer una oración de este tipo en un trabajo final, examen escrito o similar, creealo.

La diferencia es que, mientras que 1. es la definición de una variable aleatoria, 2. es un posible \\textit{resultado} de dicha variable aleatoria, por lo que no tiene sentido hacer ninguna afirmación probabilística al respecto. ¡O el intervalo calculado cubre $\\mu_Y$ \\textit{o no}!

\\end{keyconcepts}
')
```

En **R**, probar hipótesis sobre la media de una población sobre la base de una muestra aleatoria es muy fácil debido a funciones como **t.test()** del paquete **stats**. Produce un objeto del tipo **list**. Afortunadamente, una de las formas más sencillas de usar **t.test()** es cuando se desea obtener un intervalo de confianza de $95\%$ para alguna media poblacional. Comenzando por generar algunos datos aleatorios y llamando a **t.test()** junto con **ls()** para obtener un desglose de los componentes de salida.

```{r, 255}
# sembrar semilla
set.seed(1)

# generar algunos datos de muestra
sampledata <- rnorm(100, 10, 10)

# comprobar el tipo de resultado producido por t.test
typeof(t.test(sampledata))

# mostrar los elementos de la lista producidos por t.test
ls(t.test(sampledata))
```

Aunque se seinforman muchos elementos, por el momento solo interesa calcular un conjunto de confianza de $95\%$ para la media.

```{r, 256}
t.test(sampledata)$"conf.int"
```

Esto indica que el intervalo de confianza de $95\%$ es

$$ \left[9.31, 12.87\right]. $$

En este ejemplo, el intervalo calculado obviamente cubre el verdadero $\mu_Y$ que se sabe que es $10$.

Resulta importante echar un vistazo a toda la salida estándar producida por **t.test()**.

```{r, 257}
t.test(sampledata)
```

Se puede ver que **t.test()** no solo calcula un intervalo de confianza de $95\%$ sino que automáticamente realiza una prueba de significancia bilateral de la hipótesis $H_0: \mu_Y = 0$ al nivel de $5\%$ e informa los parámetros relevantes de la misma: la hipótesis alternativa, la media estimada, el estadístico $t$ resultante, los grados de libertad de la distribución $t$ subyacente (**t.test()** utiliza realizar la aproximación normal) y el valor $p$ correspondiente. ¡Esto es muy conveniente!

En este ejemplo, se llegó a la conclusión de que la media poblacional *es significativamente* diferente de $0$ (lo cual es correcto) al nivel de $5\%$, ya que $\mu_Y = 0$ no es un elemento del $95\%$ intervalo de confianza

$$ 0 \not\in \left[9.31,12.87\right]. $$

Se llega a un resultado equivalente cuando se usa la regla de rechazo del valor $p$, ya que

$$ p\text{-value} = 2.2\cdot 10^{-16} \ll 0.05. $$

## Comparación de medias de diferentes poblaciones {#CMDP}

Suponga que está interesado en las medias de dos poblaciones diferentes, denotadas como $\mu_1$ y $\mu_2$. Más específicamente, interesa si estas medias poblacionales son diferentes entre sí y planear usar una prueba de hipótesis para verificar esto sobre la base de datos de muestra independientes de ambas poblaciones. Un par de hipótesis adecuadas es

\begin{equation}
H_0: \mu_1 - \mu_2 = d_0 \ \ \text{vs.} \ \ H_1: \mu_1 - \mu_2 \neq d_0 (\#eq:hypmeans)
\end{equation}

donde $d_0$ denota la diferencia hipotética de medias (entonces $d_0 = 0$ cuando las medias son iguales, bajo la hipótesis nula). En el curso se enseña que $H_0$ se puede probar con el estadístico $t$

\begin{equation}
t=\frac{(\overline{Y}_1 - \overline{Y}_2) - d_0}{SE(\overline{Y}_1 - \overline{Y}_2)} (\#eq:tstatmeans)
\end{equation}

donde 

\begin{equation}
SE(\overline{Y}_1 - \overline{Y}_2) = \sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}.
\end{equation}

Esto se denomina prueba $t$ de dos muestras. Para $n_1$ y $n_2$ grandes, \@ref(eq:tstatmeans) es normal estándar bajo la hipótesis nula. De manera análoga a la prueba simple $t$, se pueden calcular intervalos de confianza para la verdadera diferencia en las medias poblacionales:

$$ (\overline{Y}_1 - \overline{Y}_2) \pm 1.96 \times SE(\overline{Y}_1 - \overline{Y}_2) $$

es un intervalo de confianza de $95\%$ para $d$. <br> En **R**, las hipótesis como en \@ref(eq:hypmeans) también se pueden probar con **t.test()**. Se debe tener en cuenta que **t.test()** elige $d_0 = 0$ de forma predeterminada. En consecuenci, esto se puede cambiar configurando el argumento **mu**.

El siguiente fragmento de código demuestra cómo realizar una prueba $t$ de dos muestras en **R** utilizando datos simulados.

```{r, 258}
# establecer semilla aleatoria
set.seed(1)

# extraer datos de dos poblaciones diferentes con la misma media
sample_pop1 <- rnorm(100, 10, 10)
sample_pop2 <- rnorm(100, 10, 20)

# realizar una prueba t de dos muestras
t.test(sample_pop1, sample_pop2)
```

Se ha encontrado que la prueba $t$ de dos muestras no rechaza la hipótesis nula (verdadera) de que $d_0 = 0$.

## Una aplicación a la brecha de género en los ingresos {#UABGI}

En esta sección se analiza cómo reproducir los resultados presentados en el estudio *La brecha de género en los ingresos de los graduados universitarios en los Estados Unidos*.

Para reproducir los resultados, se deben descargar los datos de replicación que están alojados en Pearson y se pueden descargar mediante el siguiente [enlace](https://wps.pearsoned.com/wps/media/objects/11422/11696965/datasets3e /datasets/cps_ch3.xlsx). El archivo contiene datos que van desde $1992$ hasta $2008$ y las ganancias se expresan en precios de $2008$.

Existen varias formas de importar los archivos **.xlsx** a **R**. La sugerencia es la función **read_excel()** del paquete **readxl** [@R-readxl]. El paquete no forma parte de la versión base de **R** y debe instalarse manualmente.

```{r, 259, message=F, warning=F}
# cargar el paquete 'readxl'
library(readxl)
```

Ahora está listo para importar el conjunto de datos. ¡Asegúrese de utilizar la ruta correcta para importar el archivo descargado! En el presente ejemplo, el archivo se guarda en una subcarpeta del directorio de trabajo llamada **Datos**. Si no está seguro de cuál es su directorio de trabajo actual, use **getwd()**, vea también **?Getwd**. Esto le dará la ruta que apunta al lugar donde **R** está buscando archivos con los que trabajar.

```{r, 260, message = F, warning = F,}
# importar los datos a R
cps <- read_excel(path = "data/cps_ch3.xlsx")
```

A continuación, instale y cargue el paquete **dyplr** [@R-dplyr]. Dicho paquete proporciona algunas funciones útiles que simplifican mucho la manipulación de datos. Hace uso del operador **%>%** (mejor conocido en la ciencia de datos como tubería).

```{r, 261, message=F, warning=F}
# cargar el paquete 'dplyr'
library(dplyr)
```

Primero, obtenga una descripción general del conjunto de datos. Luego, use **%>%** y algunas funciones del paquete **dplyr** para agrupar las observaciones por género y año, así como para calcular estadísticas descriptivas para ambos grupos.

```{r, 262, echo=T, eval=T, fig.align='center'}
# obtener una descripción general de la estructura de datos
head(cps)

# agrupar los datos por género y año y calcular la media, la desviación estándar
# y número de observaciones para cada grupo
avgs <- cps %>% 
        group_by(a_sex, year) %>% 
        summarise(mean(ahe08), 
                  sd(ahe08), 
                  n())

# imprimir los resultados en la consola
print(avgs)
```

Con el operador de tubería **%>%** simplemente se encadenan diferentes funciones de **R** que producen entradas y salidas compatibles. En el código anterior, se tomó el conjunto de datos **cps** y se usó como entrada para la función **group_by()**. La salida de **group_by()** se utiliza posteriormente como entrada para **summarise()** y así sucesivamente.

Ahora que se han calculado las estadísticas de interés para ambos géneros, se puede investigar cómo evoluciona la brecha en los ingresos entre ambos grupos con el tiempo.

```{r, 263, echo=T}
# dividir el conjunto de datos por género
male <- avgs %>% dplyr::filter(a_sex == 1) 

female <- avgs %>% dplyr::filter(a_sex == 2)

# cambiar el nombre de las columnas de ambas divisiones
colnames(male)   <- c("Sexo", "Año", "Y_bar_m", "s_m", "n_m")
colnames(female) <- c("Sexo", "Año", "Y_bar_f", "s_f", "n_f")

# estimar brechas de género, calcular errores estándar e intervalos de confianza para todas las fechas
gap <- male$Y_bar_m - female$Y_bar_f

gap_se <- sqrt(male$s_m^2 / male$n_m + female$s_f^2 / female$n_f)

gap_ci_l <- gap - 1.96 * gap_se

gap_ci_u <- gap + 1.96 * gap_se

result <- cbind(male[,-1], female[,-(1:2)], gap, gap_se, gap_ci_l, gap_ci_u)

# imprimir los resultados en la consola
print(result, digits = 3)
```

Se observan prácticamente los mismos resultados que los presentados en el artículo. Las estadísticas calculadas sugieren que *existe una brecha de género en los ingresos*. Se debe tener en cuenta que se puede rechazar la hipótesis nula de que la brecha es cero para todos los períodos. Además, las estimaciones de la brecha y los límites de los intervalos de confianza de $95\%$ indican que la brecha ha sido bastante estable en el pasado reciente.

## Diagramas de dispersión, covarianza de muestra y correlación de muestra

Un diagrama de dispersión representa datos bidimensionales, por ejemplo $n$ observación en $X_i$ y $Y_i$, por puntos en un sistema de coordenadas. Es muy fácil generar gráficos de dispersión usando la función **plot()** en **R**. Es momento de generar algunos datos artificiales sobre la edad y los ingresos de los trabajadores y trazar un gráfico.

```{r, 264, fig.align='center'}
# establecer semilla aleatoria
set.seed(123)

# generar conjunto de datos
X <- runif(n = 100, 
           min = 18, 
           max = 70)

Y <- X + rnorm(n=100, 50, 15)

# graficar observaciones
plot(X, 
     Y, 
     type = "p",
     main = "Una gráfica de dispersión de X e Y",
     xlab = "Edad",
     ylab = "Ganancias",
     col = "steelblue",
     pch = 19)
```

El gráfico muestra una correlación positiva entre la edad y los ingresos. Esto está en consonancia con la noción de que los trabajadores de más edad ganan más que los que se incorporaron recientemente a la población activa.

#### Covarianza y correlación de muestra {-}

A estas alturas debería estar familiarizado con los conceptos de varianza y covarianza. Si no es así:

+ La varianza es una medida de dispersión definida como la esperanza del cuadrado de la desviación de una variable aleatoria respecto a su media. La varianza tiene como valor mínimo 0 y puede verse muy influida por los valores atípicos; por tanto, no se aconseja su uso cuando las distribuciones de las variables aleatorias tienen colas pesadas.

+ La covarianza es una medida de variabilidad entre dos variables. Implica que el aumento del valor de una variable se corresponde con el aumento del valor de otra, lo que da como resultado una covarianza positiva. Cuando el valor de una variable aumenta, mientras que el valor de otra disminuye, se dice que la covarianza es negativa. La covarianza muestra la relación lineal entre ambas variables, aunque la magnitud de la covarianza es difícil de interpretar. En este sentido, la correlación es la versión normalizada de la covarianza.

Al igual que la varianza, covarianza y correlación de dos variables son propiedades que se relacionan con la distribución de probabilidad conjunta (desconocida) de las variables. Se puede estimar la covarianza y la correlación mediante estimadores adecuados utilizando una muestra $(X_i, Y_i)$, $i = 1, \dots, n$.

La covarianza de la muestra

$$ s_{XY} = \frac{1}{n-1} \sum_{i=1}^n (X_i - \overline{X})(Y_i - \overline{Y}) $$

es un estimador de la varianza poblacional de $X$ y $Y$, mientras que la correlación muestral

$$ r_{XY} = \frac{s_{XY}}{s_Xs_Y} $$

se puede utilizar para estimar la correlación poblacional, una medida estandarizada de la fuerza de la relación lineal entre $X$ y $Y$. 

En cuanto a la varianza y la desviación estándar, estos estimadores se implementan como funciones **R** en el paquete **stats**. Se pueden usar para estimar la covarianza poblacional y la correlación poblacional de los datos artificiales sobre edad e ingresos.

```{r, 265}
# calcular la covarianza de muestra de X e Y
cov(X, Y)

# calcular la correlación de la muestra entre X e Y
cor(X, Y)

# una forma equivalente de calcular la correlación de la muestra
cov(X, Y) / (sd(X) * sd(Y))
```

Las estimaciones indican que $X$ y $Y$ tienen una correlación moderada.

El siguiente fragmento de código usa la función **mvnorm()** del paquete **MASS** [@R-MASS] para generar datos de muestra bivariados con diferentes grados de correlación.

```{r, 266, fig.align='center'}
library(MASS)

# establecer semilla aleatoria
set.seed(1)

# correlación positiva (0,81)
example1 <- mvrnorm(100,
                    mu = c(0, 0), 
                    Sigma = matrix(c(2, 2, 2, 3), ncol = 2),
                    empirical = TRUE)

# correlación negativa (-0,81)
example2 <- mvrnorm(100,
                    mu = c(0, 0), 
                    Sigma = matrix(c(2, -2, -2, 3), ncol = 2),
                    empirical = TRUE)

# sin correlación
example3 <- mvrnorm(100,
                    mu = c(0, 0), 
                    Sigma = matrix(c(1, 0, 0, 1), ncol = 2),
                    empirical = TRUE)

# sin correlación (relación cuadrática)
X <- seq(-3, 3, 0.01)
Y <- - X^2 + rnorm(length(X))

example4 <- cbind(X, Y)

# dividir el área de la gráfica como una matriz de 2 por 2
par(mfrow = c(2, 2))

# plot datasets
plot(example1, col = "steelblue", pch = 20, xlab = "X", ylab = "Y", 
     main = "Correlación = 0.81")

plot(example2, col = "steelblue", pch = 20, xlab = "X", ylab = "Y", 
     main = "Correlación = -0.81")

plot(example3, col = "steelblue", pch = 20, xlab = "X", ylab = "Y", 
     main = "Correlación = 0")

plot(example4, col = "steelblue", pch = 20, xlab = "X", ylab = "Y", 
     main = "Correlación = 0")
```

## Ejercicios {#Ejercicios-3}

```{r, 267, echo=F, purl=F, results='asis'}
if (my_output=="html") {
  cat('
<div  class = "DCexercise">

#### 1. Sesgado ... {-}

Considere el siguiente estimador alternativo para $\\mu_Y$, la media de $Y_i$

$$\\widetilde{Y}=\\frac{1}{n-1}\\sum\\limits_{i=1}^n Y_i$$

En este ejercicio se ilustrara que este estimador es un estimador sesgado para $\\mu_Y$.

**Instrucciones:**

  + Definir una función <tt>Y_tilde</tt> que implemente el estimador anterior.
  
  + Dibujar al azar 5 observaciones de la distribución $\\mathcal{N}(10, 25)$ y calcular una estimación usando <tt>Y_tilde()</tt>. Repetir este procedimiento 10000 veces y almacenar los resultados en <tt>est_biased</tt>.
  
  + Graficar un histograma de <tt>est_biased</tt>.
  
  + Agregar una línea vertical roja en $\\mu = 10$ usando la función <tt>abline()</tt>.
  
<iframe src="DCL/ex3_1.html" frameborder="0" scrolling="no" style="width:100%;height:400px"></iframe>

**Sugerencias:**

  + Para calcular la suma de un vector se puede usar <tt>sum()</tt>, para obtener la longitud de un vector puede usar <tt>length()</tt>.
  
  + Utilizar la función <tt>replicate()</tt> para calcular repetidamente estimaciones de muestras aleatorias. Con los argumentos <tt>expr</tt> y <tt>n</tt> se puede especificar la operación y la frecuencia con la que debe replicarse.
  
  + Se puede trazar un histograma con la función <tt>hist()</tt>.
  
  + El punto en el eje x así como el color de la línea vertical se pueden especificar mediante los argumentos <tt>v</tt> y <tt>col</tt>.

</div>')
} else {
  cat('\\begin{center}\\textit{Esta parte interactiva del curso solo está disponible en la versión HTML.}\\end{center}')
}
```

```{r, 268, echo=F, purl=F, results='asis'}
if (my_output=="html") {
  cat('
<div  class = "DCexercise">

#### 2. ... pero estimador consistente {-}

Considere nuevamente el estimador del ejercicio anterior. Está disponible en su entorno como la función <tt>Y_tilde()</tt>. Se le solicita que realice el mismo procedimiento que en el ejercicio anterior. Sin embargo, esta vez, aumente el número de observaciones para extraer de 5 a 1000.

¿Que se puede notar? ¿Qué se puede decir sobre este estimador?

**Instrucciones:**

  + Dibujar al azar 1000 observaciones de la distribución $\\mathcal{N}(10, 25)$ y calcular una estimación de la media usando <tt>Y_tilde()</tt>. Repetir este procedimiento 10000 veces y almacenar los resultados en <tt>est_consistent</tt>.
  
  + Graficar un histograma de <tt>est_consistent</tt>.
  
  + Agregar una línea vertical roja en $\\mu = 10$ usando la función <tt>abline()</tt>.
  
<iframe src="DCL/ex3_2.html" frameborder="0" scrolling="no" style="width:100%;height:340px"></iframe>

**Sugerencias:**

  + Utilizar la función <tt>replicate()</tt> para calcular estimaciones de muestras aleatorias extraídas repetidamente. Usando los argumentos <tt>expr</tt> y <tt>n</tt> especificar la operación y con qué frecuencia se replicará.
  
  + Se puede graficar un histograma con la función <tt>hist()</tt>.
  
  + La posición en el eje x así como el color de la línea vertical se pueden especificar mediante los argumentos <tt>v</tt> y <tt>col</tt>.

</div>')}
```

```{r, 269, echo=F, purl=F, results='asis'}
if (my_output=="html") {
  cat('
<div  class = "DCexercise">

#### 3. Eficiencia de un estimador {-}

En este ejercicio se quiere ilustrar el resultado de que la media muestral:

$$\\hat{\\mu}_Y=\\sum\\limits_{i=1}^{n}a_iY_i$$ 

con el esquema de ponderación igual $a_i=\\frac{1}{n}$ por $i=1,...,n$ es el mejor estimador lineal insesgado (AZUL) de $\\mu_Y$.

Como alternativa, considere el estimador:

$$\\tilde{\\mu}_Y=\\sum\\limits_{i=1}^{n}b_iY_i$$

donde $b_i$ da a las primeras $\\frac{n}{2}$ observaciones una ponderación más alta que las segundas $\\frac{n}{2}$ observaciones (asumiendo que $n$ es par por simplicidad). 

El vector de pesos <tt>w</tt> ya se ha definido y está disponible en su entorno de trabajo.

**Instrucciones:**

  + Verificar que $\\tilde{\\mu}$ sea un estimador imparcial de $\\mu_Y$, la media de $Y_i$.
  
  + Implementar el estimador alternativo de $\\mu_Y$ como una función <tt>mu_tilde()</tt>.

  + Extraer al azar 100 observaciones de la distribución $\\mathcal{N}(5, 10)$ y calcular estimaciones con ambos estimadores. Repetir este procedimiento 10000 veces y almacenar los resultados en <tt>est_bar</tt> y <tt>est_tilde</tt>.
  
  + Calcular las varianzas muestrales de <tt>est_bar</tt> y <tt>est_tilde</tt>. ¿Qué puedes decir sobre ambos estimadores?
  
<iframe src="DCL/ex3_3.html" frameborder="0" scrolling="no" style="width:100%;height:420px"></iframe>

**Sugerencias:**

  + Para que $\\tilde{\\mu}$ sea un estimador imparcial, todos los pesos deben sumar 1.
  
  + Utilizar la función <tt>replicate()</tt> para calcular estimaciones de muestras extraídas repetidamente. Con los argumentos <tt>expr</tt> y <tt>n</tt> se puede especificar la operación y con qué frecuencia se replica.

  + Se puede usar <tt>var()</tt> para calcular la varianza de la muestra.

</div>')}
```

```{r, 270, echo=F, purl=F, results='asis'}
if (my_output=="html") {
  cat('
<div  class = "DCexercise">

#### 4. Prueba de hipótesis --- estadístico $t$ {-}

Considere nuevamente el conjunto de datos de (CPS) \\@ref(UABGI). El conjunto de datos <tt>cps</tt> está disponible en su entorno de trabajo.

Suponga que las ganancias medias por hora (en precios de 2012) <tt>ahe12</tt> superan los 23.50 $\\$/h$ y se desea probar esta hipótesis a un nivel de significancia de $\\alpha = 0.05$. Por favor haga lo siguiente:

**Instrucciones:**

  + Calcular la estadística de prueba a mano y asígnarla a <tt>tstat</tt>.
  
  + Utilizar <tt>tstat</tt> para aceptar o rechazar la hipótesis nula. Hacerlo utilizando la aproximación normal.
  
<iframe src="DCL/ex3_4.html" frameborder="0" scrolling="no" style="width:100%;height:340px"></iframe>

**Sugerencias:**

  + Probar $H_0:\\mu_{Y_{ahe}}\\leq 23.5$ frente a $H_1:\\mu_{Y_{ahe}}>23.5$. Es decir, realizar una prueba del lado derecho.
  
  + La estadística $t$ se define como $\\frac{\\bar{Y}-\\mu_{Y,0}}{s_{Y}/\\sqrt{n}}$ donde $s_Y$ denota la varianza de la muestra.

  + Para decidir si la hipótesis nula es aceptada o rechazada, puede comparar la estadística $t$ con el respectivo cuantil de la distribución normal estándar. Utilice operadores lógicos.

</div>')}
```

```{r, 271, echo=F, purl=F, results='asis'}
if (my_output=="html") {
  cat('
<div  class = "DCexercise">

#### 5. Prueba de hipótesis --- valor $p$ {-}

Reconsiderar la situación de prueba del ejercicio anterior. El conjunto de datos <tt>cps</tt> y el vector <tt>tstat</tt> están disponibles en su entorno de trabajo.

En lugar de usar el estadístico $t$ como criterio de decisión, también se puede usar el valor $p$. Ahora hacer lo siguiente:

**Instrucciones:**

  + Calcular el valor $p$ a mano y asignarlo a <tt>pval</tt>.
  
  + Utilizar <tt>pval</tt> para aceptar o rechazar la hipótesis nula.
  
<iframe src="DCL/ex3_5.html" frameborder="0" scrolling="no" style="width:100%;height:340px"></iframe>

**Sugerencias:**

  + El valor $p$ para una prueba del lado derecho se puede calcular como $p=P(t>t^{act}|H_0)$.
  
  + Se rechaza la hipótesis nula si $p<\\alpha$. Se usan operadores lógicos para verificar esto.

</div>')}
```

```{r, 272, echo=F, purl=F, results='asis'}
if (my_output=="html") {
  cat('
<div  class = "DCexercise">

#### 6. Prueba de hipótesis --- Una muestra prueba $t$ {-}

En los dos últimos ejercicios se discutieron dos formas de realizar una prueba de hipótesis. Estos enfoques son algo engorrosos de aplicar a mano, por lo que <tt>R</tt> proporciona la función <tt>t.test()</tt>. Hace la mayor parte del trabajo automáticamente. <tt>t.test()</tt> proporciona el estadístico $t$, valor $p$ e incluso intervalos de confianza (más sobre esto último en ejercicios posteriores). Se debe tener en cuenta que <tt>t.test()</tt> usa la distribución $t$ en lugar de la distribución normal, que se vuelve importante cuando el tamaño de la muestra es pequeño.

El conjunto de datos <tt>cps</tt> y la variable <tt>pval</tt> del ejercicio 3.4 están disponibles en su entorno de trabajo.

**Instrucciones:**

  + Realizar la prueba de hipótesis de ejercicios anteriores usando la función <tt>t.test()</tt>.
  
  + Extraer es estadístico $t$ y el valor $p$ de la lista creada por <tt>t.test()</tt>. Asígnarlos a las variables <tt>tstat</tt> y <tt>pvalue</tt>.
  
  + Verificar que el uso de la aproximación normal sea válido calculando la diferencia entre ambos valores de $p$.
  
<iframe src="DCL/ex3_6.html" frameborder="0" scrolling="no" style="width:100%;height:340px"></iframe>

**Sugerencias:**

  + El tipo de prueba así como la hipótesis nula se pueden especificar mediante los argumentos <tt>alternative</tt> y <tt>mu</tt>.
  
  + La estadística $t$ y el valor $p$ se pueden obtener mediante <tt>\\$statistic</tt> y <tt>\\$p.value</tt>, respectivamente.

</div>')}
```

```{r, 273, echo=F, purl=F, results='asis'}
if (my_output=="html") {
  cat('
<div  class = "DCexercise">

#### 7. Prueba de hipótesis --- Dos muestras prueba $t$ {-}

Considere los niveles máximos anuales del mar en Port Pirie (Australia Meridional) y Fremantle (Australia Occidental) durante los últimos 30 años.

Las observaciones están disponibles como vectores <tt>portpirie</tt> y <tt>fremantle</tt> en su entorno de trabajo.

**Instrucciones:**

  + Pruebe si existe una diferencia significativa en los niveles máximos anuales del mar a un nivel de significancia de $\\alpha = 0.05$.

<iframe src="DCL/ex3_7.html" frameborder="0" scrolling="no" style="width:100%;height:340px"></iframe>

**Sugerencias:**

  + Probar $H_0:\\mu_{P}-\\mu_{F}=0$  frente a $H_1:\\mu_{P}-\\mu_{F}\\ne 0$. Es decir, realizar una prueba $t$ de dos muestras.
  
  + Para una prueba $t$ de dos muestras, la función <tt>t.test()</tt> espera dos vectores que contengan los datos.

</div>')}
```

```{r, 274, echo=F, purl=F, results='asis'}
if (my_output=="html") {
  cat('
<div  class = "DCexercise">

#### 8. Intervalo de confianza {-}

Reconsidar la situación de prueba con respecto a los niveles máximos anuales del mar en Port Pirie y Fremantle.

Las variables <tt>portpirie</tt> y <tt>fremantle</tt> vuelven a estar disponibles en su entorno de trabajo.

**Instrucciones:**

  + Construir un intervalo de confianza de $95\\%$ - para la diferencia en los niveles del mar usando <tt>t.test()</tt>.

<iframe src="DCL/ex3_8.html" frameborder="0" scrolling="no" style="width:100%;height:340px"></iframe>

**Sugerencia:**

  + La función <tt>t.test()</tt> calcula un intervalo de confianza de $95\\%$ por defecto. Esto es accesible a través de <tt>$conf.int</tt>.

</div>')}
```

```{r, 275, echo=F, purl=F, results='asis'}
if (my_output=="html") {
  cat('
<div  class = "DCexercise">

#### 9. (Co)varianza y correlación I {-}

Considerar una muestra aleatoria $(X_i, Y_i)$ para $i = 1, ..., 100$.

Los vectores respectivos <tt>X</tt> e <tt>Y</tt> están disponibles en su entorno de trabajo.

**Instrucciones:**

  + Calcular la varianza de $X$ usando la función <tt>cov()</tt>.
  
  + Calcular la covarianza de $X$ y $Y$.
  
  + Calcular la correlación entre $X$ y $Y$.

<iframe src="DCL/ex3_9.html" frameborder="0" scrolling="no" style="width:100%;height:340px"></iframe>

**Sugerencias:**

  + La varianza es un caso especial de covarianza.
  
  + <tt>cov()</tt> y <tt>cor()</tt> esperan un vector para cada variable.

</div>')}
```

```{r, 276, echo=F, purl=F, results='asis'}
if (my_output=="html") {
  cat('
<div  class = "DCexercise">

#### 10. (Co)varianza y correlación II {-}

En este ejercicio se quieren examinar las limitaciones de la correlación como medida de dependencia.

Una vez que se haya inicializado la sesión, verá la gráfica de 100 realizaciones de dos variables aleatorias $X$ y $Y$.

Las observaciones respectivas están disponibles en los vectores <tt>X</tt> e <tt>Y</tt> en su entorno de trabajo.

**Instrucciones:**

  + Calcular la correlación entre $X$ y $Y$. Interpretar su resultado de manera crítica.

<iframe src="DCL/ex3_10.html" frameborder="0" scrolling="no" style="width:100%;height:340px"></iframe>

**Sugerencia:**

  + <tt>cor()</tt> espera un vector para cada variable.

</div>')}
```

<!--chapter:end:Capitulo_04.Rmd-->

# Regresión lineal con un regresor {#RLR}

```{r, echo = F}
options(knitr.duplicate.label = "allow")
```

```{r, 277, child="_setup.Rmd"}
```

```{r, 278, eval=knitr::opts_knit$get("rmarkdown.pandoc.to") == "html", results='asis', echo=FALSE}
cat('<hr style="background-color:#03193b;height:2px">')
```

El presente capítulo presenta los conceptos básicos de la regresión lineal y muestra cómo realizar análisis de regresión en **R**. En la regresión lineal, el objetivo es modelar la relación entre una variable dependiente $Y$ y una o más variables explicativas denotadas por $X_1, X_2, \dots, X_k$. En la siguiente parte del curso, y a lo largo de todo el capítulo, se estudiará el concepto de regresión lineal simple. En la regresión lineal simple, solo hay una variable explicativa $X_1$.

Si, por ejemplo, una escuela reduce el tamaño de sus clases contratando nuevos maestros; es decir, la escuela reduce $X_1$, la proporción de estudiantes por maestro en las clases aumentará, ¿cómo afectaría esto a $Y$ (el desempeño de los estudiantes involucrados en una prueba estandarizada)? Con la regresión lineal, no solo se puede examinar si la proporción alumno-maestro *tiene un impacto* en los resultados de la prueba, sino que también se puede aprender sobre la *dirección* y la *fuerza* de este efecto.

Los siguientes paquetes son necesarios para reproducir el código presentado en este capítulo:

+ **AER**: Que acompaña al Libro *Econometría aplicada con R* @kleiber2008 y proporciona funciones y conjuntos de datos útiles.

+ **MASS**: Una colección de funciones para estadística aplicadas.

Asegúrese de que estén instalados antes de continuar e intentar replicar los ejemplos. La forma más segura de hacerlo es verificando si el siguiente fragmento de código se ejecuta sin errores:

```{r, 279, warning=FALSE, message=FALSE, eval=FALSE}
library(AER)
library(MASS)
```

## Regresión lineal simple

Para comenzar con un ejemplo sencillo, considere las siguientes combinaciones de puntaje promedio de exámenes y la proporción promedio de estudiantes por maestro en algunos distritos escolares ficticios.

```{r kable, 280, echo=F, purl=F,, eval=my_output=="html"}
library(knitr)
library(kableExtra)

frame <- data.frame(
  TestScore = c(680, 640, 670, 660, 630, 660, 635),
  STR = c(15, 17, 19, 20, 22, 23.5, 25)
         )
rownames(frame) <- 1:7

t(frame) %>% kable("latex", booktabs = T) %>%
kable_styling(latex_options = "striped")
```

```{r, 281, echo=F, purl=F,, eval=my_output=="latex"}
library(knitr)
library(kableExtra)

frame <- data.frame(
  TestScore = c(680, 640, 670, 660, 630, 660, 635),
  STR = c(15, 17, 19, 20, 22, 23.5, 25)
         )
rownames(frame) <- 1:7

kable(t(frame), "latex", booktabs = T) %>% kable_styling(position = "center")
```

Para trabajar con estos datos en **R**, se comienza generando dos vectores: uno para las proporciones alumno-maestro (**STR**) y otro para los puntajes de las pruebas (**TestScore**), ambos contienen los datos de la tabla de arriba. 

```{r, 282}
# Crear datos de muestra
STR <- c(15, 17, 19, 20, 22, 23.5, 25)
TestScore <- c(680, 640, 670, 660, 630, 660, 635) 

# Imprimir datos de muestra
STR
TestScore
```

Para construir un modelo de regresión lineal simple, se plantea la hipótesis de que la relación entre la variable dependiente y la independiente es lineal, formalmente: 

$$ Y = b \cdot X + a. $$

Por ahora, suponga que la función que relaciona la puntuación de la prueba y la proporción alumno-maestro entre sí es $$TestScore = 713 - 3 \times STR.$$

Siempre es una buena idea visualizar los datos con los que trabaja. Aquí, es adecuado usar **plot()** para producir un diagrama de dispersión con **STR** en el eje $x$ y **TestScore** en el eje $y$. Simplemente llamando a **plot(y_variable ~ x_variable)** donde **y_variable** y **x_variable** son marcadores de posición para los vectores de observaciones que se quiere trazar. Además, es posible que se desee agregar una relación sistemática a la trama. Para dibujar una línea recta, **R** proporciona la función **abline()**. Solo se tiene que llamar a la función con el argumento **a** (que representan la intersección) y **b** (que representa la pendiente) después de ejecutar **plot()** para agregar la línea al gráfico.

El siguiente código es clave:

```{r, 283, echo=TRUE, fig.align='center', cache=TRUE}
# crear un diagrama de dispersión de los datos
plot(TestScore ~ STR)

# agregar la relación sistemática a la trama
abline(a = 713, b = -3)
```

Se encuentra que la línea no toca ninguno de los puntos aunque se afirma que representa la relación sistemática. La razón de esto es la aleatoriedad. La mayoría de las veces existen influencias adicionales que implican que no existe una relación bivariada entre las dos variables.

Para tener en cuenta dichas diferencias entre los datos observados y la relación sistemática, se amplía el modelo de arriba con un *término de error* $u$ que captura efectos aleatorios adicionales. Dicho de otra manera, $u$ representa todas las diferencias entre la línea de regresión y los datos observados reales. Además de la pura aleatoriedad, estas desviaciones también podrían surgir de errores de medición o, como se discutirá más adelante, podrían ser la consecuencia de dejar de lado otros factores que son relevantes para explicar la variable dependiente.

¿Qué otros factores son plausibles en el ejemplo? Por un lado, los puntajes de las pruebas pueden depender de la calidad de los profesores y los antecedentes de los estudiantes. También es posible que en algunas clases los alumnos tuvieran suerte en los días de prueba y así consiguieran puntuaciones más altas. Por ahora, se resumiran tales influencias por un componente aditivo:

$$ TestScore = \beta_0 + \beta_1 \times STR + \text{other factors} $$

Por supuesto, esta idea es muy general, ya que se puede extender fácilmente a otras situaciones que se pueden describir con un modelo lineal. Por tanto, el modelo de regresión lineal básico con el que se trabajará es

$$ Y_i = \beta_0 + \beta_1 X_i + u_i. $$

El Concepto clave 4.1 resume la terminología del modelo de regresión lineal simple.   

```{r, 284, eval = my_output == "html", results='asis', echo=F, purl=F}
cat('
<div class = "keyconcept" id="KC4.1">
<h3 class = "right"> Concepto clave 4.1 </h3>          
<h3 class = "left"> Terminología para el modelo de regresión lineal con un regresor único </h3>

<p> El modelo de regresión lineal es

$$Y_i = \\beta_0 + \\beta_1 X_i + u_i$$

dónde

- el índice $i$ corre sobre las observaciones, $i = 1, \\dots, n$.
- $Y_i$ es la *variable dependiente*, la *regresiva*, o simplemente la *variable de la izquierda*.
- $X_i$ es la *variable independiente*, el *regresor*, o simplemente la *variable de la derecha*.
- $Y = \\beta_0 + \\beta_1 X$ es la *línea de regresión de población* también llamada *función de regresión de población*.
- $\\beta_0$ es la *intersección* de la línea de regresión de la población.
- $\\beta_1$ es la *pendiente* de la línea de regresión de la población.
- $u_i$ es el *término de error*.
</p>
</div>
')
```

```{r, 285, eval = my_output == "latex", results='asis', echo=F, purl=F}
cat('\\begin{keyconcepts}[Terminología para el modelo de regresión lineal con un regresor único]{4.1}
El modelo de regresión lineal es

$$Y_i = \\beta_0 + \\beta_1 X_i + u_i$$

\\begin{itemize}
\\item el índice $i$ corre sobre las observaciones, $i = 1, \\dots, n$.
\\item $Y_i$ es la \\textit{variable dependiente}, la \\textit{regresiva}, o simplemente la \\textit{variable de la izquierda}.
\\item $X_i$ es la \\textit{variable independiente}, el \\textit{regresor}, o simplemente la \\textit{variable de la derecha}.
\\item $Y = \\beta_0 + \\beta_1 X$ es la \\textit{línea de regresión de población} también llamada \\textit{función de regresión de población}.
\\item $\\beta_0$ es la \\textit{intersección} de la línea de regresión de la población.
\\item $\\beta_1$ es la \\textit{pendiente} de la línea de regresión de la población.
\\item $u_i$ es el \\textit{término de error}.
\\end{itemize}
\\end{keyconcepts}
')
```

## Estimación de los coeficientes del modelo de regresión lineal

En la práctica, se desconocen la intersección $\beta_0$ y la pendiente $\beta_1$ de la línea de regresión de la población. Por lo tanto, se deben emplear datos para estimar ambos parámetros desconocidos. A continuación, se utilizará un ejemplo del mundo real para demostrar cómo se logra esto. Se quieren relacionar los resultados de las pruebas con la proporción de alumnos por maestro medida en las escuelas de California. El puntaje de la prueba es el promedio de todo el distrito de puntajes de lectura y matemáticas para los estudiantes de quinto grado. Nuevamente, el tamaño de la clase se mide como el número de estudiantes dividido por el número de maestros (la proporción de estudiantes por maestro). En cuanto a los datos, el conjunto de datos de las escuelas de California (**CASchools**) viene con un paquete **R** llamado **AER**, un acrónimo de [Econometría aplicada con R](https: //cran.r -project.org/web/packages/AER/AER.pdf) [@R-AER]. Después de instalar el paquete con **install.packages("AER")** y adjuntarlo con **library(AER)** el conjunto de datos se puede cargar usando la función **data()**.

```{r, 286, message=FALSE, warning=FALSE, eval=5:8}
# instalar el paquete AER (una vez)
install.packages("AER")

# cargar el paquete AER 
library(AER)   

# cargar el conjunto de datos en el espacio de trabajo
data(CASchools) 
```

Una vez que se ha instalado un paquete, está disponible para su uso en otras ocasiones cuando se invoca con **library()** --- ¡no es necesario ejecutar **install.packages()** de nuevo!

Es interesante saber con qué tipo de objeto se está tratando. En consecuencia, **class()** devuelve la clase de un objeto. Dependiendo de la clase de un objeto, algunas funciones (por ejemplo, **plot()** y **summary()**) se comportan de manera diferente.

Comprobar la clase del objeto **CASchools**.

```{r, 287}
class(CASchools)
```

Resulta que **CASchools** es de la clase **data.frame**, que es un formato conveniente para trabajar, especialmente para realizar análisis de regresión.

Con la ayuda de **head()** se obtiene una primera descripción general de los datos. Esta función muestra solo las primeras 6 filas del conjunto de datos, lo que evita una salida de consola sobrecargada. 

```{block2, note-text, type='rmdnote'}
Presionar <tt>ctrl + L</tt> para borrar la consola. Este comando elimina cualquier código que haya escrito y ejecutado por usted o impreso en la consola por las funciones <tt>R</tt>. La buena noticia es que todo lo demás queda intacto. No pierde variables definidas, entre otros, lo que incluye el historial del código. Todavía es posible recuperar comandos <tt>R</tt> ejecutados previamente usando las teclas arriba y abajo. Si está trabajando en *RStudio*, presione <tt>ctrl + Arriba</tt> en su teclado (<tt>CMD + Arriba</tt> en una Mac) para revisar una lista de comandos ingresados previamente.
```

```{r, 288}
head(CASchools)
```

Se encuentra que el conjunto de datos consta de muchas variables y que la mayoría de ellas son numéricas.

Por cierto: Una alternativa a **class()** y **head()** es **str()** que se deduce de la palabra 'estructura' en inglés y ofrece una descripción general completa del objeto. ¡Intenténtelo!

Volviendo a **CASchools**, las dos variables que interesan (es decir, el puntaje promedio de la prueba y la proporción alumno-maestro) *no* están incluidas. Sin embargo, es posible calcular ambos a partir de los datos proporcionados. Para obtener las proporciones de estudiantes por maestro, simplemente se divide el número de estudiantes por el número de maestros. La puntuación media de la prueba es la media aritmética de la puntuación de la prueba de lectura y la puntuación de la prueba de matemáticas. El siguiente fragmento de código muestra cómo se pueden construir las dos variables como vectores y cómo se añaden a **CASchools**.

```{r, 289}
# calcular STR y agregarlo a CASchools
CASchools$STR <- CASchools$students/CASchools$teachers 

# calcular TestScore y añadirlo a CASchools
CASchools$score <- (CASchools$read + CASchools$math)/2     
```

Si se ejecuta **head(CASchools)** nuevamente, se encontrarían las dos variables de interés como columnas adicionales llamadas **STR** y **score** (¡marque esto!).

Resulta necesario hablar de la distribución de los puntajes de las pruebas y la proporción de alumnos por maestro. Existen varias funciones que pueden usarse para producir dichos resultados, por ejemplo:

- **mean()** (calcula la media aritmética de los números proporcionados),

- **sd()** (calcula la desviación estándar de la muestra),

- **cuantile()** (devuelve un vector de los cuantiles de muestra especificados para los datos).

El siguiente fragmento de código muestra cómo lograr esto. Primero, se calculan las estadísticas de resumen en las columnas **STR** y **score** de **CASchools**. Para obtener un buen resultado, recopilando las medidas en un **data.frame** llamado **DistributionSummary**.

```{r Table 4.1, results='hold'}
# calcular promedios de muestra de STR y score
avg_STR <- mean(CASchools$STR) 
avg_score <- mean(CASchools$score)

# calcular las desviaciones estándar de la muestra de STR y score
sd_STR <- sd(CASchools$STR) 
sd_score <- sd(CASchools$score)

# configurar un vector de percentiles y calcular los cuantiles
quantiles <- c(0.10, 0.25, 0.4, 0.5, 0.6, 0.75, 0.9)
quant_STR <- quantile(CASchools$STR, quantiles)
quant_score <- quantile(CASchools$score, quantiles)

# recopilar todo en un marco de datos (data.frame)
DistributionSummary <- data.frame(Average = c(avg_STR, avg_score), 
                                  StandardDeviation = c(sd_STR, sd_score), 
                                  quantile = rbind(quant_STR, quant_score))

# imprimir el resumen en la consola
DistributionSummary
```

En cuanto a los datos de muestra, usando **plot()** se pueden detectar características en los datos, como valores atípicos que son más difíciles de descubrir al observar simples números. Esta vez se agregan algunos argumentos adicionales a la llamada de **plot()**.

El primer argumento en la llamada de **plot()**, **score ~ STR**, es nuevamente una fórmula que establece variables en el eje y y el eje x. Sin embargo, esta vez las dos variables no se guardan en vectores separados sino que son columnas de **CASchools**. Por lo tanto, **R** no los encontraría sin que el argumento **data** se haya especificado correctamente. El arguemnto **data** debe estar de acuerdo con el nombre del **data.frame** al que pertenecen las variables, en este caso **CASchools**. Se utilizan más argumentos para cambiar la apariencia del gráfico: mientras **main** agrega un título, **xlab** y **ylab** agregan etiquetas personalizadas a ambos ejes.

```{r, 290, fig.align='center'}
plot(score ~ STR, 
     data = CASchools,
     main = "Diagrama de dispersión de TestScore y STR", 
     xlab = "STR (X)",
     ylab = "Resultado de la prueba (Y)")
```

La gráfica de dispersión muestra todas las observaciones sobre la proporción alumno-maestro y la puntuación de la prueba. Se puede ver que los puntos están fuertemente dispersos y que las variables están correlacionadas negativamente. Es decir, se espera observar puntuaciones más bajas en las pruebas en clases más grandes.

La función **cor()** (consultar **?Cor** para obtener más información) se puede utilizar para calcular la correlación entre dos vectores *numéricos*.

```{r, 291}
cor(CASchools$STR, CASchools$score)
```

Como ya sugiere el diagrama de dispersión, la correlación es negativa pero bastante débil.

La tarea a la que se deben enfrentar ahora los economistas es encontrar la línea que mejor se ajuste a los datos. Por supuesto, se podría simplemente seguir con la inspección gráfica y el análisis de correlación y luego seleccionar la línea que mejor se ajuste a ojo de buen cubero. Sin embargo, esto sería bastante subjetivo: Diferentes observadores dibujarían diferentes líneas de regresión. Por este motivo, se está interesado en las técnicas menos arbitrarias. Esta técnica viene dada por la estimación de mínimos cuadrados ordinarios (MCO).

### El estimador de mínimos cuadrados ordinarios {-}

El estimador de MCO elige los coeficientes de regresión de manera que la línea de regresión estimada sea lo más "cercana" posible a los puntos de datos observados. Aquí, la cercanía se mide por la suma de los errores al cuadrado cometidos al predecir $Y$ dado $X$. Sea $b_0$ y $b_1$ algunos estimadores de $\beta_0$ y $\beta_1$. Entonces, la suma de los errores de estimación al cuadrado se puede expresar como: 

$$ \sum^n_{i = 1} (Y_i - b_0 - b_1 X_i)^2. $$

El estimador MCO en el modelo de regresión simple es el par de estimadores para la intersección y la pendiente que minimiza la expresión anterior. La derivación de los estimadores MCO para ambos parámetros se resumen en el Concepto clave 4.2.

```{r, 292, eval = my_output == "html", results='asis', echo=F, purl=F}
cat('
<div class = "keyconcept" id="KC4.2">
<h3 class = "right"> Concepto clave 4.2 </h3>          
<h3 class = "left"> Estimador de MCO, valores pronosticados y residuos </h3>
<p> Los estimadores MCO de la pendiente $\\beta_1$ y la intersección $\\beta_0$ en el modelo de regresión lineal simple son:

\\begin{align}
  \\hat\\beta_1 & = \\frac{ \\sum_{i = 1}^n (X_i - \\overline{X})(Y_i - \\overline{Y}) } { \\sum_{i=1}^n (X_i - \\overline{X})^2},  \\\\
  \\\\
  \\hat\\beta_0 & =  \\overline{Y} - \\hat\\beta_1 \\overline{X}. 
\\end{align}

Los valores predichos por MCO $\\widehat{Y}_i$ y los residuos $\\hat{u}_i$ son:

\\begin{align}
  \\widehat{Y}_i & =  \\hat\\beta_0 + \\hat\\beta_1 X_i,\\\\
  \\\\
  \\hat{u}_i & =  Y_i - \\widehat{Y}_i. 
\\end{align}

La intersección estimada $\\hat{\\beta}_0$, el parámetro de pendiente $\\hat{\\beta}_1$ y los residuos $\\left(\\hat{u}_i\\right)$ son calculado a partir de una muestra de $n$ observaciones de $X_i$ y $Y_i$, $i$, $...$, $n$. Estas son *estimaciones* de la intersección de la población desconocida $\\left(\\beta_0 \\right)$, pendiente $\\left(\\beta_1\\right)$ y término de error $(u_i)$.
</p>

Las fórmulas presentadas anteriormente pueden no ser muy intuitivas a primera vista. La siguiente aplicación interactiva tiene como objetivo ayudarlo a comprender la mecánica de los MCO. Puede agregar observaciones haciendo clic en el sistema de coordenadas donde los datos están representados por puntos. Una vez que hay dos o más observaciones disponibles, la aplicación calcula una línea de regresión usando MCO y algunas estadísticas que se muestran en el panel derecho. Los resultados se actualizan a medida que agrega más observaciones al panel izquierdo. Un doble clic restablece la aplicación; es decir, se eliminan todos los datos.

<iframe height="410" width="900" frameborder="0" scrolling="no" src="DCL/Regresión_simple.html"></iframe>

</div>')
```

```{r, 293, eval = my_output == "latex", results='asis', echo=F, purl=F}
cat('\\begin{keyconcepts}[El estimador de MCO, valores pronosticados y residuos]{4.2}
Los estimadores MCO de la pendiente $\\beta_1$ y la intersección $\\beta_0$ en el modelo de regresión lineal simple son:

\\begin{align*}
  \\hat\\beta_1 & = \\frac{ \\sum_{i = 1}^n (X_i - \\overline{X})(Y_i - \\overline{Y}) } { \\sum_{i=1}^n (X_i - \\overline{X})^2},  \\\\
  \\hat\\beta_0 & =  \\overline{Y} - \\hat\\beta_1 \\overline{X}. 
\\end{align*}

Los valores predichos por MCO $\\widehat{Y}_i$ y los residuos $\\hat{u}_i$ son:

\\begin{align*}
  \\widehat{Y}_i & =  \\hat\\beta_0 + \\hat\\beta_1 X_i,\\\\
  \\hat{u}_i & =  Y_i - \\widehat{Y}_i. 
\\end{align*}

La intersección estimada $\\hat{\\beta}_0$, el parámetro de pendiente $\\hat{\\beta}_1$ y los residuos $\\left(\\hat{u}_i\\right)$ son calculado a partir de una muestra de $n$ observaciones de $X_i$ y $Y_i$, $i$, $...$, $n$. Estas son *estimaciones* de la intersección de la población desconocida $\\left(\\beta_0 \\right)$, pendiente $\\left(\\beta_1\\right)$ y término de error $(u_i)$.

\\end{keyconcepts}
')
```

Existen muchas formas posibles de calcular $\hat{\beta}_0$ y $\hat{\beta}_1$ en **R**. Por ejemplo, se podrían implementar las fórmulas presentadas en el Concepto clave 4.2 con dos de las funciones más básicas de **R**: **mean()** y **sum()**. Antes de hacerlo, se *adjunta* el conjunto de datos **CASchools**.

```{r, 294, echo=-1}
rm(STR)
attach(CASchools) # permite utilizar las variables contenidas en CASchools directamente

# calcular beta_1_hat
beta_1 <- sum((STR - mean(STR)) * (score - mean(score))) / sum((STR - mean(STR))^2)

# calcular beta_0_hat
beta_0 <- mean(score) - beta_1 * mean(STR)

# imprimir los resultados en la consola
beta_1
beta_0
```

```{block2, attach, type='rmdknit'}
Llamar a <tt>attach (CASchools)</tt> permite direccionar una variable contenida en <tt>CASchools</tt> por su nombre: ya no es necesario utilizar el operador <tt>$</tt> junto con el conjunto de datos: <tt>R</tt> puede evaluar el nombre de la variable directamente.

<tt>R</tt> usa el objeto en el entorno del usuario si este objeto comparte el nombre de la variable contenida en una base de datos adjunta. Sin embargo, es una mejor práctica usar siempre nombres distintivos para evitar tales (aparentemente) ambivalencias.
```

<br>

**¡Observe que se abordan las variables contenidas en el conjunto de datos adjunto <tt>CASchools</tt> directamente durante el resto de este capítulo!**

Por supuesto, existen aún más formas manuales de realizar estas tareas. Dado que MCO es una de las técnicas de estimación más utilizadas, **R**, por supuesto, ya contiene una función incorporada llamada **lm()** (**l** inear **m** odel) que se puede utilizar para realizar análisis de regresión.

El primer argumento de la función a especificar es, similar a **plot()**, la fórmula de regresión con la sintaxis básica **y ~ x** donde **y** es la variable dependiente y **x** la variable explicativa. El argumento **data** determina el conjunto de datos que se utilizará en la regresión. En este punto, es momento de examinar un ejemplo donde se analice la relación entre los puntajes de las pruebas y el tamaño de las clases. El siguiente código usa **lm()** para obtener los resultados.

```{r, 295}
# estimar el modelo y asignar el resultado a linear_model
linear_model <- lm(score ~ STR, data = CASchools)

# imprimir la salida estándar del objeto lm estimado en la consola
linear_model
```

Agregar la línea de regresión estimada al gráfico. Esta vez también se amplian los rangos de ambos ejes estableciendo los argumentos **xlim** y **ylim**.

```{r, 296, fig.align='center'}
# graficar los datos
plot(score ~ STR, 
     data = CASchools,
     main = "Diagrama de dispersión de TestScore y STR", 
     xlab = "STR (X)",
     ylab = "Resultado de la prueba (Y)",
     xlim = c(10, 30),
     ylim = c(600, 720))

# agrega la línea de regresión
abline(linear_model) 
```

¿Notaste que esta vez, no se pasaron los parámetros de intersección y pendiente a **abline**? Si llama a **abline()** en un objeto de clase **lm** que solo contiene un regresor, **R** grafica la línea de regresión automáticamente.

## Medidas de ajuste

Después de ajustar un modelo de regresión lineal, una pregunta natural es qué tan bien describe el modelo los datos. Visualmente, esto equivale a evaluar si las observaciones están estrechamente agrupadas alrededor de la línea de regresión. Tanto el *coeficiente de determinación* como el *error estándar de la regresión* miden qué tan bien se ajusta la línea de regresión MCO a los datos.

### El coeficiente de determinación {-}

$R^2$, el *coeficiente de determinación*, es la fracción de la varianza muestral de $Y_i$ que se explica por $X_i$. Matemáticamente, $R^2$ se puede escribir como la razón entre la suma de cuadrados explicada y la suma total de cuadrados. La *suma de cuadrados explicada* ($SCE$) es la suma de las desviaciones cuadradas de los valores predichos $\hat{Y_i}$, del promedio de $Y_i$. La *suma total de cuadrados* ($STC$) es la suma de las desviaciones cuadradas de $Y_i$ de su promedio. Así se tiene:

\begin{align}
  SCE & =  \sum_{i = 1}^n \left( \hat{Y_i} - \overline{Y} \right)^2,   \\
  STC & =  \sum_{i = 1}^n \left( Y_i - \overline{Y} \right)^2,   \\
  R^2 & = \frac{SCE}{STC}.
\end{align}

Como $STC = SCE + SRC$ también se puede escribir como:

$$ R^2 = 1- \frac{SRC}{STC} $$ 

donde $SRC$ es la suma de los residuos al cuadrado, una medida de los errores cometidos al predecir $Y$ por $X$. La $SRC$ se define como

$$ SRC = \sum_{i=1}^n \hat{u}_i^2. $$

$R^2$ se encuentra entre $0$ y $1$. Es fácil ver que un ajuste perfecto; es decir, que no se cometan errores al ajustar la línea de regresión, implica $R^2 = 1$ ya que entonces se tiene $SRC = 0$. Por el contrario, si nuestra línea de regresión estimada no explica ninguna variación en $Y_i$, se tiene $SCE = 0$ y, en consecuencia, $R^2 = 0$.

### El error estándar de la regresión {-}

El *error estándar de la regresión* ($EER$) es un estimador de la desviación estándar de los residuos $\hat{u}_i$. Como tal, mide la magnitud de una desviación típica de la línea de regresión; es decir, la magnitud de un residuo típico.

$$ SER = s_{\hat{u}} = \sqrt{s_{\hat{u}}^2} \ \ \ \text{donde} \ \ \ s_{\hat{u} }^2 = \frac{1}{n-2} \sum_{i = 1}^n \hat{u}^2_i = \frac{SSR}{n - 2} $$

Se debe recordar que los $u_i$ son *no observados*. Es por eso que se usan sus contrapartes estimadas, los residuos $\hat{u}_i$, en su lugar. El error estándar de la regresión $EER$ es el valor que muestra la diferencia entre los valores reales y los estimados de una regresión. Es utilizado para valorar si existe una correlación entre la regresión y los valores medidos. Muchos autores prefieren este dato a otros como el coeficiente de correlación lineal, ya que el error estándar se mide en las mismas unidades que los valores que se estudian. 

### Aplicación a los datos de la prueba de puntuación {-}

Ambas medidas de ajuste se pueden obtener utilizando la función **summary()** con un objeto **lm** proporcionado como único argumento. Mientras que la función **lm()** solo imprime los coeficientes estimados en la consola, **summary()** proporciona información adicional predefinida como $R^2$ y $EER$ de la regresión.

```{r, 297}
mod_summary <- summary(linear_model)
mod_summary
```

El $R^2$ en la salida llama *R cuadrada múltiple* y tiene un valor de $0.051$. Por tanto, $5.1\%$ de la varianza de la variable dependiente $score$ se explica por la variable explicativa $STR$. Es decir, la regresión explica poco de la varianza en $score$, y gran parte de la variación en los puntajes de las pruebas permanece sin explicación.

El $EER$ se llama *error estándar residual* y equivale a $18.58$. La unidad del $EER$ es la misma que la unidad de la variable dependiente. Es decir, en promedio, la desviación del puntaje de prueba alcanzado real y la línea de regresión es de $18.58$ puntos.

Ahora, se verifica si **summary()** usa las mismas definiciones para $R^2$ y $EER$ que se usan cuando se calculan manualmente.

```{r, 298}
# calcular R^2 manualmente
SSR <- sum(mod_summary$residuals^2)
STC <- sum((score - mean(score))^2)
R2 <- 1 - SSR/STC

# imprimir el valor en la consola
R2

# calcular EER manualmente
n <- nrow(CASchools)
EER <- sqrt(SSR / (n-2))

# imprimir el valor en la consola
EER
```

Se encuentra que los resultados coinciden. Se debe tener en cuenta que los valores proporcionados por **summary()** se redondean a dos lugares decimales.

## Supuestos de mínimos cuadrados {#SMC}

MCO funciona bien en una amplia variedad de circunstancias diferentes. Sin embargo, existen algunas suposiciones que deben cumplirse para asegurar que las estimaciones se distribuyan normalmente en muestras grandes (se discute esto en el Capítulo \@ref(DMEMCO).

```{r, 299, eval = my_output == "html", results='asis', echo=F, purl=F}
cat('
<div class = "keyconcept" id="KC4.3">
<h3 class = "right"> Concepto clave 4.3 </h3>          
<h3 class = "left"> Los supuestos de mínimos cuadrados </h3>
<p> 
$$Y_i = \\beta_0 + \\beta_1 X_i + u_i \\text{, } i = 1,\\dots,n$$

donde

1. El término de error $u_i$ tiene una media condicional cero dada $X_i$: $E(u_i|X_i) = 0$.
2. $(X_i,Y_i), i = 1,\\dots,n$ son extractos independientes e idénticamente distribuidos (i.i.d.) de su distribución conjunta.
3. Los valores atípicos grandes son poco probables: $X_i$ y $Y_i$ tienen momentos finitos distintos de cero.
</p>
</div>
')
```

```{r, 300, eval = my_output == "latex", results='asis', echo=F, purl=F}
cat('\\begin{keyconcepts}[Los supuestos de mínimos cuadrados]{4.3}
$$Y_i = \\beta_0 + \\beta_1 X_i + u_i \\text{, } i = 1,\\dots,n$$

donde

\\begin{enumerate}
\\item El término de error $u_i$ tiene una media condicional cero dada $X_i$: $E(u_i|X_i) = 0$.
\\item $(X_i,Y_i), i = 1,\\dots,n$ son extractos independientes e idénticamente distribuidos (i.i.d.) de su distribución conjunta.
\\item Los valores atípicos grandes son poco probables: $X_i$ y $Y_i$ tienen momentos finitos distintos de cero.
\\end{enumerate}
\\end{keyconcepts}
')
```

### Supuesto 1: El término de error tiene una media condicional de cero {-}

Esto significa que, independientemente del valor que se elija para $X$, el término de error $u$ no debe mostrar ningún patrón sistemático y debe tener una media de $0$.
Considere el caso de que, incondicionalmente, $E(u) = 0$, pero para valores bajos y altos de $X$, el término de error tiende a ser positivo y para valores de rango medio de
$X$ el error tiende a ser negativo. Se puede usar R para construir tal ejemplo. Para hacerlo, se generan datos propios utilizando los generadores de números aleatorios integrados de **R**.

Se usarán las siguientes funciones:

* **runif()** - genera números aleatorios distribuidos uniformemente
* **rnorm()** - genera números aleatorios distribuidos normalmente
* **predecir()** - realiza predicciones basadas en los resultados de funciones de ajuste del modelo como **lm()**
* **lines()** - agrega segmentos de línea a un gráfico existente

Se comienza creando un vector que contenga valores que se distribuyan uniformemente en el intervalo $[-5, 5]$. Esto se puede hacer con la función **runif()**. También se necesita simular el término de error. Para esto, se generan números aleatorios normalmente distribuidos con una media igual a $0$ y una varianza de $1$ usando **rnorm()**. Los valores de $Y$ se obtienen como una función cuadrática de los valores de $X$ y del error.

Después de generar los datos, se estima tanto un modelo de regresión simple como un modelo cuadrático que también incluye el regresor $X^2$ (este es un modelo de regresión múltiple, consulte el Capítulo \@ref(MRVR)). Finalmente, se grafican los datos simulados y se agrega la línea de regresión estimada de un modelo de regresión simple, así como las predicciones hechas con un modelo cuadrático para comparar el ajuste gráficamente.

```{r, 301, fig.align='center'}
# establecer una semilla para que los resultados sean reproducibles
set.seed(321)

# simular los datos
X <- runif(50, min = -5, max = 5)
u <- rnorm(50, sd = 1)  

# la verdadera relación
Y <- X^2 + 2 * X + u                

# estimar un modelo de regresión simple
mod_simple <- lm(Y ~ X)

# predecir usando un modelo cuadrático
prediction <- predict(lm(Y ~ X + I(X^2)), data.frame(X = sort(X)))

# graficar los resultados
plot(Y ~ X)
abline(mod_simple, col = "red")
lines(sort(X), prediction)
```

El gráfico muestra qué se entiende por $E(u_i|X_i) = 0$ y por qué no es válido para el modelo lineal:

Usando el modelo cuadrático (representado por la curva negra) se ve que no existe desviaciones sistemáticas de la observación de la relación predicha. Es creíble que no se viole la suposición cuando se emplea tal modelo. Sin embargo, al usar un modelo de regresión lineal simple, se ve que la suposición probablemente se viola ya que $E(u_i|X_i)$ varía con $X_i$.

### Supuesto 2: Datos independientes e idénticamente distribuidos  {-}

La mayoría de los esquemas de muestreo utilizados para recopilar datos de poblaciones producen muestras i.i.d. Por ejemplo, se podría usar el generador de números aleatorios de **R** para seleccionar al azar las identificaciones de los estudiantes de la lista de inscripción de una universidad y registrar la edad $X$ y los ingresos $Y$ de los estudiantes correspondientes. Este es un ejemplo típico de muestreo aleatorio simple y garantiza que todos los $(X_i, Y_i)$ se extraigan al azar de la misma población.

Un ejemplo destacado en que la suposición de i.i.d. no se cumple es en los datos de series de tiempo donde se tienen observaciones en la misma unidad a lo largo del tiempo. Por ejemplo, al tomar $X$ como el número de trabajadores en una empresa de producción a lo largo del tiempo. Debido a las transformaciones comerciales, la empresa recorta puestos de trabajo periódicamente por una participación específica, pero también hay algunas influencias no deterministas que se relacionan con la economía, la política, entre otrs. Con **R** se puede simular fácilmente un proceso de este tipo y graficarlo.

Se comienza la serie con un total de 5000 trabajadores y se simula la reducción del empleo con un proceso autorregresivo que exhibe un movimiento descendente en el largo plazo y tiene errores normalmente distribuidos:^[Ver Capítulo \@ref(IRSTP) para más información sobre autorregresión procesos y análisis de series de tiempo en general.]

$$ employment_t = -5 + 0.98 \cdot employment_{t-1} + u_t $$
 
```{r, 302, fig.align="center"}
# sembrar semilla
set.seed(123)

# generar un vector de fecha
Date <- seq(as.Date("1951/1/1"), as.Date("2000/1/1"), "years")

# inicializar el vector de empleo
X <- c(5000, rep(NA, length(Date)-1))

# generar observaciones de series de tiempo con influencias aleatorias
for (i in 2:length(Date)) {
  
    X[i] <- -50 + 0.98 * X[i-1] + rnorm(n = 1, sd = 200)
    
}

# trazar los resultados
plot(x = Date, 
     y = X, 
     type = "l", 
     col = "steelblue", 
     ylab = "Trabajadores", 
     xlab = "Tiempo")
```

Es evidente que las observaciones sobre el número de empleados no pueden ser independientes en este ejemplo: el nivel de empleo de hoy está correlacionado con el nivel de empleo de mañana. Por lo tanto, se viola la suposición de i.i.d.

### Supuesto 3: Los valores atípicos grandes son poco probables {-}

Es fácil pensar en situaciones en las que pueden ocurrir observaciones extremas; es decir, observaciones que se desvían considerablemente del rango habitual de datos. Estas observaciones se denominan valores atípicos. Técnicamente hablando, el supuesto 3 requiere que $X$ y $Y$ tengan una curtosis finita.^[La curtosis de una variable estadística/aleatoria es una característica de la forma de su distribución de frecuencias/probabilidad. Según la concepción clásica, una curtosis grande implica una mayor concentración de valores de la variable tanto muy cerca de la media de la distribución (pico) como muy lejos de ella (colas), al tiempo que existe una relativamente menor frecuencia de valores intermedios. Esto explica una forma de la distribución de frecuencias/probabilidad con colas más gruesas, con un centro más apuntado y una menor proporción de valores intermedios entre el pico y colas. Una mayor curtosis no implica una mayor varianza, ni viceversa.].

Los casos comunes en los que se quiere excluir o (si es posible) corregir dichos valores atípicos son cuando aparentemente son errores tipográficos, errores de conversión o errores de medición. Incluso si parece que las observaciones extremas se han registrado correctamente, es aconsejable excluirlas antes de estimar un modelo, ya que MCO adolece de *sensibilidad a valores atípicos*.

¿Qué significa esto? Se puede demostrar que las observaciones extremas reciben una gran ponderación en la estimación de los coeficientes de regresión desconocidos cuando se utiliza MCO. Por lo tanto, los valores atípicos pueden dar lugar a estimaciones de los coeficientes de regresión muy distorsionadas. Para tener una mejor impresión de este problema, considere la siguiente aplicación donde se han colocado algunos datos de muestra en $X$ y $Y$ que están altamente correlacionados. La relación entre $X$ y $Y$ parece explicarse bastante bien por la línea de regresión trazada: todos los puntos de datos blancos se encuentran cerca de la línea de regresión roja y se tiene $R^2 = 0.92$.

Ahora continúe y agregue una observación adicional en, digamos, $(18, 2)$. Esta observación claramente es un caso atípico. El resultado es bastante sorprendente: la línea de regresión estimada difiere mucho de la que se considera que se ajusta bien a los datos. ¡La pendiente está fuertemente sesgada a la baja y $R^2$ disminuyó a solo $29\%$! 

<br>

Haga doble clic dentro del sistema de coordenadas para restablecer la aplicación. Siéntase libre de experimentar. Elija diferentes coordenadas para el valor atípico o agregue otras adicionales.

<iframe height="410" width="900" frameborder="0" scrolling="no" src="DCL/Valor_atípico.html"></iframe>

En el siguiente código se usan datos de muestra generados usando las funciones de números aleatorios de **R**: **rnorm()** y **runif()**. Se estiman dos modelos de regresión simple, uno basado en el conjunto de datos original y otro usando un conjunto modificado donde una observación cambia para ser un valor atípico y luego se grafican los resultados. Para comprender el código completo, debe estar familiarizado con la función **sort()** que ordena las entradas de un vector numérico en orden ascendente.

```{r, 303, fig.align='center'}
# sembrar semilla
set.seed(123)

# generar los datos
X <- sort(runif(10, min = 30, max = 70))
Y <- rnorm(10 , mean = 200, sd = 50)
Y[9] <- 2000

# ajustar modelo con valor atípico
fit <- lm(Y ~ X)

# ajuste del modelo sin valores atípicos
fitWithoutOutlier <- lm(Y[-9] ~ X[-9])

# graficar los resultados
plot(Y ~ X)
abline(fit)
abline(fitWithoutOutlier, col = "red")
```

## La distribución muestral del estimador de MCO {#DMEMCO}

Como $\hat{\beta}_0$ y $\hat{\beta}_1$ se calculan a partir de una muestra, los estimadores en sí mismos son variables aleatorias con una distribución de probabilidad, --- la denominada distribución muestral de los estimadores --- describe los valores que podrían tomar en diferentes muestras. Aunque la distribución muestral de $\hat\beta_0$ y $\hat\beta_1$  puede ser complicada cuando el tamaño de la muestra es pequeño y generalmente cambia con el número de observaciones, $n$, es posible, siempre que se cumplan los supuestos discutidos sean válidos, para hacer ciertas declaraciones sobre lo que se mantiene para todos los $n$. En particular

$$ E(\hat{\beta}_0) = \beta_0 \ \ \text{and} \ \  E(\hat{\beta}_1) = \beta_1,$$

es decir, $\hat\beta_0$ y $\hat\beta_1$  son estimadores no sesgados de $\beta_0$ y $\beta_1$, los parámetros verdaderos. Si la muestra es lo suficientemente grande, según el teorema del límite central, la distribución muestral *conjunta* de los estimadores está bien aproximada por la distribución normal bivariada (<a href="#mjx-eqn-2.1">2.1</a>). Esto implica que las distribuciones marginales también son normales en muestras grandes. Los datos básicos sobre las distribuciones de muestras grandes de $\hat\beta_0$ y $\hat\beta_1$ se presentan en el Concepto clave 4.4.

```{r, 304, eval = my_output == "html", results='asis', echo=F, purl=F}
cat('
<div class = "keyconcept" id="KC4.4">
<h3 class = "right"> Concepto clave 4.4 </h3>          
<h3 class = "left"> Distribución de muestra grande de $\\hat\\beta_0$ y $\\hat\\beta_1$ </h3>

<p> 
Si se cumplen los supuestos de mínimos cuadrados del Concepto clave 4.3, entonces en muestras grandes $\\hat\\beta_0$ y $\\hat\\beta_1$ tienen una distribución muestral normal conjunta. La distribución normal de muestra grande de $\\hat\\beta_1$ es $\\mathcal{N}(\\beta_1, \\sigma^2_{\\hat\\beta_1})$, donde la varianza de la distribución, $\\sigma^2_{\\hat\\beta_1}$, es

\\begin{align}
\\sigma^2_{\\hat\\beta_1} = \\frac{1}{n} \\frac{Var \\left[ \\left(X_i - \\mu_X \\right) u_i  \\right]}  {\\left[  Var \\left(X_i \\right)  \\right]^2}. (\\#eq:olsvar1)
\\end{align}

La distribución normal de muestra grande de $\\hat\\beta_0$ es $\\mathcal{N}(\\beta_0, \\sigma^2_{\\hat\\beta_0})$ con

\\begin{align}
\\sigma^2_{\\hat\\beta_0} =  \\frac{1}{n} \\frac{Var \\left( H_i u_i \\right)}{ \\left[  E \\left(H_i^2  \\right)  \\right]^2 } \\ , \\ \\text{donde} \\ \\ H_i = 1 - \\left[ \\frac{\\mu_X} {E \\left( X_i^2\\right)} \\right] X_i. (\\#eq:olsvar2)
\\end{align}

La siguiente simulación interactiva genera continuamente muestras aleatorias $(X_i, Y_i)$ de $200$ observaciones donde $E(Y\\vert X) = 100 + 3X$, estima un modelo de regresión simple, almacena la estimación de la pendiente $\\beta_1$ y visualiza la distribución de los $\\widehat{\\beta}_1$ observados hasta ahora usando un histograma. La idea aquí es que para un gran número de $\\widehat{\\beta}_1$, el histograma da una buena aproximación de la distribución muestral del estimador. Al disminuir el tiempo entre dos iteraciones de muestreo, queda claro que la forma del histograma se aproxima a la forma de campana característica de una distribución normal centrada en la pendiente real de $3$.

<iframe height="470" width="700" frameborder="0" scrolling="no" src="DCL/Distribución_de_regresión_de_muestras_pequeñas.html"></iframe>

(Haga doble clic en el histograma para reiniciar la simulación.)

</p>
</div>
')
```

```{r, 305, eval = my_output == "latex", results='asis', echo=F, purl=F}
cat('\\begin{keyconcepts}[Distribución de muestra grande de $\\hat\\beta_0$ y $\\hat\\beta_1$]{4.4}

Si se cumplen los supuestos de mínimos cuadrados del Concepto clave 4.3, entonces en muestras grandes $\\hat\\beta_0$ y $\\hat\\beta_1$ tienen una distribución muestral normal conjunta. La distribución normal de muestra grande de $\\hat\\beta_1$ es $\\mathcal{N}(\\beta_1, \\sigma^2_{\\hat\\beta_1})$, donde la varianza de la distribución, $\\sigma^2_{\\hat\\beta_1}$, es

\\begin{equation}
\\sigma^2_{\\hat\\beta_1} = \\frac{1}{n} \\frac{Var \\left[ \\left(X_i - \\mu_X \\right) u_i  \\right]}  {\\left[  Var \\left(X_i \\right)  \\right]^2}.
\\end{equation}

La distribución normal de muestra grande de $\\hat\\beta_0$ es $\\mathcal{N}(\\beta_0, \\sigma^2_{\\hat\\beta_0})$ con

\\begin{equation}
\\sigma^2_{\\hat\\beta_0} =  \\frac{1}{n} \\frac{Var \\left( H_i u_i \\right)}{ \\left[  E \\left(H_i^2  \\right)  \\right]^2 } \\ , \\ \\text{donde} \\ \\ H_i = 1 - \\left[ \\frac{\\mu_X} {E \\left( X_i^2\\right)} \\right] X_i.
\\end{equation}

La siguiente simulación interactiva genera continuamente muestras aleatorias $(X_i, Y_i)$ de $200$ observaciones donde $E(Y\\vert X) = 100 + 3X$, estima un modelo de regresión simple, almacena la estimación de la pendiente $\\beta_1$ y visualiza la distribución de los $\\widehat{\\beta}_1$ observados hasta ahora usando un histograma. La idea aquí es que para un gran número de $\\widehat{\\beta}_1$, el histograma da una buena aproximación de la distribución muestral del estimador. Al disminuir el tiempo entre dos iteraciones de muestreo, queda claro que la forma del histograma se aproxima a la forma de campana característica de una distribución normal centrada en la pendiente real de $3$.\\vspace{0.5cm}

\\begin{center}\\textit{Esta parte interactiva del curso solo está disponible en la versión HTML.}\\end{center}

\\end{keyconcepts}
')
```

### Estudio de simulación 1 {-}

También se puede verificar si las afirmaciones del Concepto clave 4.4 realmente son válidas mediante **R**. Para esto, primero se construye una población de observaciones de $100 000$ en total. Para hacer esto, se necesitan valores para la variable independiente $X$, para el término de error $u$ y para los parámetros $\beta_0$ y $\beta_1$. Con estos combinados en un modelo de regresión simple, se calcula la variable dependiente $Y$. 

<br> 

En el ejemplo, se generan los números $X_i$, $i = 1$, ..., $100000$ extrayendo una muestra aleatoria de una distribución uniforme en el intervalo $[0, 20]$. La realización de los términos de error $u_i$ se extraen de una distribución normal estándar con parámetros $\mu = 0$ y $\sigma^2 = 100$ (se debe tener en cuenta que **rnorm()** requiere $\sigma$ como entrada para el argumento **sd**, consultar **?rnorm**). Además, se elige $\beta_0 = -2$ y $\beta_1 = 3.5$, por lo que el modelo verdadero es

$$ Y_i = -2 + 3.5 \cdot X_i. $$

Finalmente, se almacenan los resultados en un marco de datos (data.frame):

```{r, 306}
# simular datos
N <- 100000
X <- runif(N, min = 0, max = 20)
u <- rnorm(N, sd = 10)

# regresión de población
Y <- -2 + 3.5 * X + u
population <- data.frame(X, Y)
```

De ahora en adelante, se consideran los datos generados previamente como la población real (que por supuesto sería *desconocida* en una aplicación del mundo real, de lo contrario no habría razón para extraer una muestra aleatoria en primer lugar). El conocimiento sobre la población real y la verdadera relación entre $Y$ y $X$ se puede utilizar para verificar las afirmaciones hechas en el Concepto clave 4.4.

Primero, se calculan las verdaderas varianzas $\sigma^2_{\hat{\beta}_0}$ y $\sigma^2_{\hat{\beta}_1}$ para una muestra extraída al azar de tamaño $n = 100$:

```{r, 307, fig.align='center'}
# establecer el tamaño de la muestra
n <- 100

# calcular la varianza de beta_hat_0
H_i <- 1 - mean(X) / mean(X^2) * X
var_b0 <- var(H_i * u) / (n * mean(H_i^2)^2 )

# calcular la varianza de hat_beta_1
var_b1 <- var( ( X - mean(X) ) * u ) / (100 * var(X)^2)
```

```{r, 308}
# imprimir variaciones en la consola
var_b0
var_b1
```

Suponga ahora que no conoce los valores verdaderos de $\beta_0$ y $\beta_1$ y que no es posible observar a toda la población. Sin embargo, se puede observar una muestra aleatoria de $n$ observaciones. Entonces, no sería posible calcular los parámetros verdaderos, pero se podrían obtener estimaciones de $\beta_0$ y $\beta_1$ a partir de los datos de muestra utilizando MCO. Sin embargo, se sabe que estas estimaciones son resultado de variables aleatorias en sí mismas, ya que las observaciones se extraen aleatoriamente de la población. El Concepto clave 4.4 describe sus distribuciones para grandes $n$. Cuando se extrae una sola muestra de tamaño $n$, no es posible hacer ninguna afirmación sobre estas distribuciones. Las cosas cambian si se repite el esquema de muestreo muchas veces y se calculan las estimaciones para cada muestra: usando este procedimiento, se simulan los resultados de las respectivas distribuciones.

Para lograr esto en R, se emplea el siguiente enfoque:

- Se asigna el número de repeticiones, suponiendo $10000$, a **repeticiones** y luego se inicializa una matriz **ajuste** donde las estimaciones obtenidas en cada iteración de muestreo se almacenan en filas. Por lo tanto, **ajuste** tiene que ser una matriz de dimensiones **repeticiones** $\times2$.

- En el siguiente paso, se extraen **repeticiones** de muestras aleatorias de tamaño **n** de la población y se obtienen las estimaciones de MCO para cada muestra. Los resultados se almacenan como entradas de fila en la matriz de resultados **ajuste**. Esto se hace usando un bucle **for()**.

- Por último, se estiman las varianzas de ambos estimadores utilizando los resultados muestreados y se grafican los histogramas de estos últimos. También se agrega una gráfica de las funciones de densidad que pertenecen a las distribuciones que se derivan del Concepto clave 4.4. La función **bquote()** se usa para obtener expresiones matemáticas en los títulos y etiquetas de ambos gráficos. Ver **?Bquote**.

```{r, 309, cache=T}
# establecer repeticiones y tamaño de muestra
n <- 100
reps <- 10000

# inicializar la matriz de resultados
fit <- matrix(ncol = 2, nrow = reps)

# muestreo de bucle y estimación de los coeficientes
for (i in 1:reps){
  
 sample <- population[sample(1:N, n), ]
 fit[i, ] <- lm(Y ~ X, data = sample)$coefficients
 
}

# calcular estimaciones de varianza utilizando resultados
var(fit[, 1])
var(fit[, 2])
```

```{r, 310, fig.align='center'}
# dividir el área de graficado como una matriz de 1 por 2
par(mfrow = c(1, 2))

# graficar histogramas de estimaciones beta_0
hist(fit[, 1],
     cex.main = 1,
     main = bquote(La ~ distribución ~ de ~ 10000 ~ estimaciones ~ beta[0]), 
     xlab = bquote(hat(beta)[0]), 
     freq = F)

# agregar una distribución verdadera a la gráfica
curve(dnorm(x, 
            -2, 
            sqrt(var_b0)), 
      add = T, 
      col = "darkred")

# grraficar histogramas de beta_hat_1
hist(fit[, 2],
    cex.main = 1,
     main = bquote(La ~ distribución ~ de ~ 10000 ~ estimaciones ~ beta[1]), 
     xlab = bquote(hat(beta)[1]), 
     freq = F)

# agregar una distribución verdadera a la gráfica
curve(dnorm(x, 
            3.5, 
            sqrt(var_b1)), 
      add = T, 
      col = "darkred")
```

Las estimaciones de varianza apoyan las afirmaciones realizadas en el Concepto clave 4.4, acercándose a los valores teóricos. Los histogramas sugieren que las distribuciones de los estimadores pueden aproximarse bien mediante las respectivas distribuciones normales teóricas establecidas en el Concepto clave 4.4.

### Estudio de simulación 2 {-}

Otro resultado implícito en el Concepto clave 4.4 es que ambos estimadores son consistentes; es decir, convergen en probabilidad a los parámetros verdaderos que interesan. Esto se debe a que son asintóticamente insesgados y sus varianzas convergen a $0$ a medida que $n$ aumentan. Se puede comprobar esto repitiendo la simulación anterior para una secuencia de tamaños de muestra crecientes. Esto significa que ya no se asigna el tamaño de la muestra sino un *vector* de tamaños de muestra: **n <- c(...)**. 

<br>

Viendo las distribuciones de $\beta_1$. La idea aquí es agregar una llamada adicional de **for()** al código. Esto se hace para recorrer el vector de tamaños de muestra **n**. Para cada uno de los tamaños de muestra, se realiza la misma simulación que antes, pero se traza una estimación de densidad para los resultados de cada iteración sobre **n**. Se puede observar que se tiene que cambiar **n** a **n[j]** en el ciclo interno para asegurar de que se use el elemento **j**$^{esimo}$ de **n**. En la simulación, se usan tamaños de muestra de $100, 250, 1000$ y $3000$. En consecuencia, se tiene un total de cuatro simulaciones distintas que utilizan diferentes tamaños de muestra.
 
```{r, 311, fig.align='center', cache=T}
# sembrar la semilla para la reproducibilidad
set.seed(1)

# establecer repeticiones y el vector de tamaños de muestra
reps <- 1000
n <- c(100, 250, 1000, 3000)

# inicializar la matriz de resultados
fit <- matrix(ncol = 2, nrow = reps)

# dividir el panel de la grafica en una matriz de 2 por 2
par(mfrow = c(2, 2))

# muestreo y graficado de bucles

# bucle exterior sobre n
for (j in 1:length(n)) {
  
  # bucle interno: muestreo y estimación de los coeficientes
  for (i in 1:reps){
    
    sample <- population[sample(1:N, n[j]), ]
    fit[i, ] <- lm(Y ~ X, data = sample)$coefficients
    
  }
  
  # dibujar estimaciones de densidad
  plot(density(fit[ ,2]), xlim=c(2.5, 4.5), 
       col = j, 
       main = paste("n=", n[j]), 
       xlab = bquote(hat(beta)[1]))
  
}
```

Se encontró que, a medida que aumenta $n$, la distribución de $\hat\beta_1$ se concentra alrededor de su media; es decir, su varianza disminuye. Dicho de otra manera, la probabilidad de observar estimaciones cercanas al valor real de $\beta_1 = 3.5$ aumenta a medida que aumenta el tamaño de la muestra. Se puede observar el mismo comportamiento si se analiza la distribución de $\hat\beta_0$ en su lugar.

### Estudio de simulación 3 {-}

Además, la varianza del estimador MCO para $\beta_1$ disminuye a medida que aumenta la varianza de $X_i$. En otras palabras, a medida que aumenta la cantidad de información proporcionada por el regresor; es decir, aumentando $Var(X)$, que se utiliza para estimar $\beta_1$, se tiene más confianza en que la estimación se acerca al valor real. (es decir, $Var(\hat\beta_1)$ disminuye). 

<br>

Se puede visualizar esto tomando muestras de las observaciones $(X_i, Y_i)$, $i = 1, \dots, 100$ de una distribución normal bivariada con 

$$E(X)=E(Y)=5,$$ 

$$Var(X)=Var(Y)=5$$ 

y

$$Cov(X,Y)=4.$$

Formalmente, esto se escribe como

\begin{align}
  \begin{pmatrix}
    X \\
    Y \\
  \end{pmatrix}
  \overset{i.i.d.}{\sim} & \ \mathcal{N} 
  \left[
    \begin{pmatrix}
      5 \\
      5 \\
    \end{pmatrix}, \ 
    \begin{pmatrix}
      5 & 4 \\
      4 & 5 \\
    \end{pmatrix}
  \right]. \tag{4.3}
\end{align}

Para realizar el muestreo aleatorio se utiliza la función **mvrnorm()** del paquete **MASS** [@R-MASS] que permite extraer muestras aleatorias de distribuciones normales multivariadas, ver **?Mvtnorm**. A continuación, se usa **subset()** para dividir la muestra en dos subconjuntos de modo que el primer conjunto, **set1**, consista en observaciones que cumplan la condición $\lvert X - \overline{X} \rvert > 1$ y el segundo conjunto, **set2**, incluya el resto de la muestra. Luego se grafican ambos conjuntos y se usan diferentes colores para distinguir las observaciones.

```{r, 312, fig.align = 'center', cache = T}
# carga el paquete MASS
library(MASS)

# sembrar la semilla para la reproducibilidad
set.seed(4)

# simular datos normales bivariados
bvndata <- mvrnorm(100, 
                mu = c(5, 5), 
                Sigma = cbind(c(5, 4), c(4, 5))) 

# asignar nombres de columna / convertir a data.frame
colnames(bvndata) <- c("X", "Y")
bvndata <- as.data.frame(bvndata)

# subconjunto de los datos
set1 <- subset(bvndata, abs(mean(X) - X) > 1)
set2 <- subset(bvndata, abs(mean(X) - X) <= 1)

# graficar ambos conjuntos de datos
plot(set1, 
     xlab = "X", 
     ylab = "Y", 
     pch = 19)

points(set2, 
       col = "steelblue", 
       pch = 19)
```

Está claro que las observaciones que están cerca del promedio muestral de $X_i$ tienen menos varianza que las que están más lejos. Ahora, si se tuviera que dibujar una línea con la mayor precisión posible a través de cualquiera de los dos conjuntos, es intuitivo que elegir las observaciones indicadas por los puntos negros; es decir, usar el conjunto de observaciones que tiene una varianza mayor que las azules, resultaría en una línea más precisa. Ahora, se usarán los MCO para estimar la pendiente y la intersección para ambos conjuntos de observaciones. Luego se grafican las observaciones junto con ambas líneas de regresión.

```{r, 313, fig.align='center'}
# estimar ambas líneas de regresión
lm.set1 <- lm(Y ~ X, data = set1)
lm.set2 <- lm(Y ~ X, data = set2)

# graficar las observaciones
plot(set1, xlab = "X", ylab = "Y", pch = 19)
points(set2, col = "steelblue", pch = 19)

# agregar ambas líneas a la gráfica
abline(lm.set1, col = "green")
abline(lm.set2, col = "red")
```

Evidentemente, la línea de regresión verde es mucho mejor para describir los datos muestreados de la distribución normal bivariada indicada en (<a href="#mjx-eqn-4.3">4.3</a>) que la línea roja. Este es un buen ejemplo para demostrar por qué se está interesado en una alta varianza del regresor $X$: más varianza en $X_i$ significa más información de la que se beneficia la precisión de la estimación.

## Ejercicios {#Ejercicios-4}

```{r, 314, echo=F, purl=F, results='asis'}
if (my_output=="html") {
  cat('
<div  class = "DCexercise">

#### 1. Tamaños de clases y puntajes de exámenes {-}

Un investigador desea analizar la relación entre el tamaño de la clase (medida por la proporción de alumnos por maestro) y el puntaje promedio de la prueba. Por lo tanto, mide ambas variables en clases diferentes de $10$ y obtiene los siguientes resultados.

<!--html_preserve-->

  <table>
      <tr>
        <td><b>Tamaño de la clase</b></td>
        <td>23</td>
        <td>19</td>
        <td>30</td>
        <td>22</td>
        <td>23</td>
        <td>29</td>
        <td>35</td>
        <td>36</td>
        <td>33</td>
        <td>25</td>
      </tr>
      <tr>
        <td><b>Resultado de la prueba</b></td>
        <td>430</td>
        <td>430</td>
        <td>333</td>
        <td>410</td>
        <td>390</td>
        <td>377</td>
        <td>325</td>
        <td>310</td>
        <td>328</td>
        <td>375</td>
      </tr>
    </table>

<!--/html_preserve-->

**Instrucciones:**

  + Crear los vectores <tt>cs</tt> (el tamaño de la clase) y <tt>ts</tt> (la puntuación de la prueba), que contengan las observaciones anteriores.

  + Dibujar un diagrama de dispersión de los resultados usando <tt>plot()</tt>.

<iframe src="DCL/ex4_1.html" frameborder="0" scrolling="no" style="width:100%;height:340px"></iframe>

</div>')
} else {
  cat('\\begin{center}\\textit{Esta parte interactiva del curso solo está disponible en la versión HTML.}\\end{center}')
}
```

```{r, 315, echo=F, purl=F, results='asis'}
if (my_output=='html') {
  cat('
<div  class = "DCexercise">

#### 2. Media, varianza, covarianza y correlación {-}

Los vectores <tt>cs</tt> y <tt>ts</tt> están disponibles en el entorno de trabajo (puede comprobar esto: escribiendo sus nombres en la consola y presionando enter).

**Instrucciones:**

  + Calcular la media, la varianza muestral y la desviación estándar muestral de <tt>ts</tt>.

  + Calcular la covarianza y el coeficiente de correlación para <tt>ts</tt> y <tt>cs</tt>.

<iframe src="DCL/ex4_2.html" frameborder="0" scrolling="no" style="width:100%;height:340px"></iframe>

**Sugerencia:** 

  + Utilizar las funciones <tt>R</tt> presentadas en este capítulo: <tt>mean()</tt>, <tt>sd()</tt>, <tt>cov()</tt>, <tt>cor()</tt> y <tt>var()</tt>.

</div>')}
```

```{r, 316, echo=F, purl=F, results='asis'}
if (my_output=="html") {
  cat('
<div  class = "DCexercise">

#### 3. Regresión lineal simple {-}

Los vectores <tt>cs</tt> y <tt>ts</tt> están disponibles en el entorno de trabajo.

**Instrucciones:**

  + La función <tt>lm()</tt> es parte del paquete <tt>AER</tt>. Adjunte el paquete usando <tt>library()</tt>.

  + Utilizar <tt>lm()</tt> para estimar el modelo de regresión $$TestScore_i = \\beta_0 + \\beta_1 STR_i + u_i.$$ Asignar el resultado a <tt>mod</tt>.

  + Obtener un resumen estadístico del modelo.

<iframe src="DCL/ex4_3.html" frameborder="0" scrolling="no" style="width:100%;height:340px"></iframe>

</div>')}
```

```{r, 317, echo=F, purl=F, results='asis'}
if (my_output=="html") {
  cat('
<div  class = "DCexercise">

#### 4. El objeto de modelo {-}

  Resulta importante obervar cómo se estructura un objeto de clase <tt>lm</tt>.

  Los vectores <tt>cs</tt> y <tt>ts</tt> así como el objeto modelo <tt>mod</tt> del ejercicio anterior están disponibles en su espacio de trabajo.

**Instrucciones:**

  + Usar <tt>class()</tt> para aprender sobre la clase del objeto <tt>mod</tt>.
  
  + <tt>mod</tt> es un objeto de tipo <tt>list</tt> con entradas con nombre. Verificar esto usando la función <tt>is.list()</tt>.
  
  + Vea qué información puede obtener de <tt>mod</tt> usando <tt>names()</tt>.

  + Leer una entrada arbitraria del objeto <tt>mod</tt> usando el operador <tt>$</tt>.

<iframe src="DCL/ex4_4.html" frameborder="0" scrolling="no" style="width:100%;height:340px"></iframe>

</div>')}
```

```{r, 318, echo=F, purl=F, results='asis'}
if (my_output=="html") {
  cat('
<div  class = "DCexercise">

#### 5. Graficar la línea de regresión {-}

  Se proporciona el código para el diagrama de dispersión en el <tt>script.R</tt>

**Instrucciones:**

  + Agregar la línea de regresión al diagrama de dispersión de algunos ejercicios anteriores.

  + El objeto <tt>mod</tt> está disponible en el entorno de trabajo.

<iframe src="DCL/ex4_5.html" frameborder="0" scrolling="no" style="width:100%; height:340px"> </iframe>

**Sugerencia:** 

  + Usar la función <tt>abline()</tt>.

</div>')}
```

```{r, 319, echo=F, purl=F, results='asis'}
if (my_output=="html") {
  cat('
<div  class = "DCexercise">

#### 6. Resumen de un objeto modelo {-}

Ahora leer y almacenar parte de la información contenida en la salida de <tt>summary()</tt>.

**Instrucciones:**

  + Asignar la salida de <tt>summary (mod)</tt> a la variable <tt>s</tt>.

  + Comprobar los nombres de entrada del objeto <tt>s</tt>.

  + Crear una nueva variable <tt>R2</tt> y asignar el $R^2$ de la regresión.

  + El objeto <tt>mod</tt> está disponible en el entorno de trabajo.

<iframe src="DCL/ex4_6.html" frameborder="0" scrolling="no" style="width:100%;height:340px"></iframe>

</div>')}
```

```{r, 320, echo=F, purl=F, results='asis'}
if (my_output=="html") {
  cat('
<div  class = "DCexercise">

#### 7. Coeficientes estimados {-}

La función <tt>summary()</tt> también proporciona información sobre la significancia estadística de los coeficientes estimados.

**Instrucciones:**

  + Extraer la matriz denominada $2\\times4$ con coeficientes estimados, errores estándar, $t$ -estadísticos y los correspondientes valores $p$- del resumen del modelo <tt>s</tt>. Guardar esta matriz en un objeto llamado <tt>coefs</tt>.

  + Los objetos <tt>mod</tt> y <tt>s</tt> están disponibles en su entorno de trabajo.

<iframe src="DCL/ex4_7.html" frameborder="0" scrolling="no" style="width:100%;height:340px"></iframe>

</div>')}
```

```{r, 321, echo=F, purl=F, results='asis'}
if (my_output=="html") {
  cat('
<div  class = "DCexercise">

#### 8. Dejar caer la intersección {-}

Hasta ahora, se han estimado modelos de regresión que consisten en una intersección y un regresor único. En este ejercicio aprenderá a especificar y estimar la regresión de un modelo sin intersección.

Se debe tener en cuenta que excluir la intersección de un modelo de regresión podría ser una práctica poco fiable en algunas aplicaciones, ya que esto impone que la función de expectativa condicional de la variable dependiente sea cero si el regresor es cero.

**Instrucciones:**

  + Averiguar cómo se debe especificar el argumento de la <tt>formula</tt> para una regresión de <tt>ts</tt> únicamente en <tt>cs</tt>; es decir, una regresión sin intercepto. ¡Google es tu amigo!

  + Estimar el modelo de regresión sin interceptar y almacenar el resultado en <tt>mod_ni</tt>.

  + Los vectores <tt>cs</tt>, <tt>ts</tt> y el objeto modelo <tt>mod</tt> de ejercicios anteriores están disponibles en el entorno de trabajo.

<iframe src="DCL/ex4_8.html" frameborder="0" scrolling="no" style="width:100%;height:340px"></iframe>

</div>')}
```

```{r, 322, echo=F, purl=F, results='asis'}
if (my_output=="html") {
  cat('
<div  class = "DCexercise">

#### 9. Resultado de la regresión: Sin caso constante {-}

En el ejercicio 8 se estimó un modelo sin intersección. La función de regresión estimada es

$$\\widehat{TestScore} = \\underset{(1.36)}{12.65} \\times STR.$$

**Instrucciones:**

  + Convénzase de que todo es como se indicó anteriormente: extraer la matriz de coeficientes del resumen de <tt>mod_ni</tt> y guardar en una variable llamada <tt>coef</tt>.

  + Los vectores <tt>cs</tt>, <tt>ts</tt> así como el objeto modelo <tt>mod_ni</tt> del ejercicio anterior están disponibles en el entorno de trabajo.

<iframe src="DCL/ex4_9.html" frameborder="0" scrolling="no" style="width:100%;height:340px"></iframe>

**Sugerencia:** 

  + Se puede acceder a la entrada de una lista con nombre usando el operador <tt>$</tt>.

</div>')}
```

```{r, 323, echo=F, purl=F, results='asis'}
if (my_output=="html") {
  cat('
<div  class = "DCexercise">

#### 10. Salida de regresión: Sin caso constante --- Ctd. {-}

En los Ejercicios 8 y 9 se ha tratado con un modelo sin intersección. La función de regresión estimada fue

$$\\widehat{TestScore_i} = \\underset{(1.36)}{12.65} \\times STR_i.$$

La matriz de coeficientes <tt>coef</tt> del ejercicio 9 contiene el coeficiente estimado en $STR$, su error estándar, el estadístico $t$ de la prueba de significancia y el valor $p$ correspondiente.

**Instrucciones:**

  + Imprimir el contenido de <tt>coef</tt> en la consola.
  
  + Convénzase de que el estadístico $t$ reportada es correcta: Usar las entradas de <tt>coef</tt> para calcular el estadístico $t$ y guardarlo en <tt>t_stat</tt>.

  + La matriz <tt>coef</tt> del ejercicio anterior está disponible en el entorno de trabajo.

<iframe src="DCL/ex4_10.html" frameborder="0" scrolling="no" style="width:100%;height:340px"></iframe>

**Sugerencias:** 

  + <tt>X[a, b]</tt> devuelve el elemento <tt>[a, b]</tt> de la matriz <tt>X</tt>.

  + El estadístico $t$ para una prueba de la hipótesis $H_0: \\beta_1 = 0$ se calcula como $$t = \\frac{\\hat{\\beta}_1}{SE(\\hat{\\beta}_1)}.$$

</div>')}
```

```{r, 324, echo=F, purl=F, results='asis'}
if (my_output=="html") {
  cat('
<div  class = "DCexercise">

#### 11. Dos regresiones, una gráfica {-}

Los dos modelos de regresión estimados de los ejercicios anteriores son

$$\\widehat{TestScore_i} = \\underset{(1.36)}{12.65} \\times STR_i$$

<center>y</center>

$$\\widehat{TestScore_i} = \\underset{(23.96)}{567.4272} \\underset{(0.85)}{-7.1501} \\times STR_i.$$

Se le proporciona la línea de código <tt>plot(cs, ts)</tt> que crea un diagrama de dispersión de <tt>ts</tt> y <tt>cs</tt>. ¡Se debe tener en cuenta que esta línea debe ejecutarse antes de llamar a <tt>abline()</tt>! Se pueden colorear las líneas de regresión usando; por ejemplo, <tt>col = "red"</tt> o <tt>col = "blue"</tt> como un argumento adicional para <tt>abline()</tt> para una mejor distinción.

Los vectores <tt>cs</tt> y <tt>ts</tt> así como los objetos de lista <tt>mod</tt> y <tt>mod_ni</tt> de ejercicios anteriores están disponibles en su trabajo. ambiente.

**Instrucciones:**

  + Generar un diagrama de dispersión de <tt>ts</tt> y <tt>cs</tt> y agregar las líneas de regresión estimadas de <tt>mod</tt> y <tt>mod_ni</tt>.

<iframe src="DCL/ex4_11.html" frameborder="0" scrolling="no" style="width:100%;height:340px"></iframe>

</div>')}
```

```{r, 325, echo=F, purl=F, results='asis'}
if (my_output=="html") {
  cat('
<div  class = "DCexercise">

#### 12. $STC$ y $SRC$ {-}

Si la inspección gráfica no ayuda, los investigadores recurren a técnicas analíticas para detectar si un modelo se ajusta bien o mejor a los datos disponibles que otro modelo.

Volviendo al modelo de regresión simple que incluye una intersección. La línea de regresión estimada para <tt>mod</tt> fue

$$\\widehat{TestScore_i} = 567.43 - 7.15 \\times STR_i, \\, R^2 = 0.8976, \\, SER=15.19.$$

Se puede comprobar esto como <tt>mod</tt> y los vectores <tt>cs</tt> y <tt>ts</tt> que están disponibles en el entorno de trabajo.

**Instrucciones:**

  + Calcular $SRC$, la suma de los residuos al cuadrado, y guardar en <tt>ssr</tt>.
  
  + Calcule $STC$, la suma total de cuadrados y guardar en <tt>STC</tt>.

<iframe src="DCL/ex4_12.html" frameborder="0" scrolling="no" style="width:100%;height:340px"></iframe>

</div>')}
```

```{r, 326, echo=F, purl=F, results='asis'}
if (my_output=="html") {
  cat('
<div  class = "DCexercise">

#### 13. El $R^2$ de un modelo de regresión {-}

El $R^2$ de la regresión guardada en <tt>mod</tt> es $0.8976$. Puede verificar esto ejecutando <tt>summary(mod)$r.squared</tt> en la consola a continuación.

Recordar la fórmula de $R^2$:

$$R^2 = \\frac{ESS}{STC} = 1 - \\frac{SSR}{STC}$$

Los objetos <tt>mod</tt>, <tt>STC</tt> y <tt>ssr</tt> del ejercicio anterior están disponibles en el entorno de trabajo.

**Instrucciones:**

  + Utilizar <tt>ssr</tt> y <tt>STC</tt> para calcular $R^2$ manualmente. *Redondear* el resultado a *cuatro* decimales y guárdalo en <tt>R2</tt>.

  + Utilizar el operador lógico <tt>==</tt> para comprobar si el resultado coincide con el valor mencionado anteriormente.

<iframe src="DCL/ex4_13.html" frameborder="0" scrolling="no" style="width:100%;height:340px"></iframe>

**Sugerencias:**

  + Se pueden redondear valores numéricos usando la función <tt>round()</tt>.

</div>')}
```

```{r, 327, echo=F, purl=F, results='asis'}
if (my_output=="html") {
  cat('
<div  class = "DCexercise">

#### 14. El error estándar de la regresión {-}

El error estándar de la regresión en el modelo de regresión simple es 

$$SER = \\frac{1}{n-2} \\sum_{i=1}^n \\widehat{u}_i^2 =\\sqrt{\\frac{SSR}{n-2}}.$$ 

$SER$ mide el tamaño de un residuo promedio que es una estimación de la magnitud de un error de regresión típico.

El objeto modelo <tt>mod</tt> y los vectores <tt>cs</tt> y </tt>ts</tt> están disponibles en su espacio de trabajo.

**Instrucciones:**

  + Usar <tt>summary()</tt> para obtener el $SER$ de la regresión <tt>ts</tt> en <tt>cs</tt> guardado en el objeto modelo <tt>mod</tt>. Guardar el resultado en la variable <tt>SER</tt>.

  + Use <tt>SER</tt> para calcular el $SSR$ y almacenarlo en <tt>SSR</tt>.

  + Comprobar que <tt>SSR</tt> es de hecho el $SSR$ comparando <tt>SSR</tt> con el resultado de <tt>sum(mod$residuals^2)</tt>

<iframe src="DCL/ex4_14.html" frameborder="0" scrolling="no" style="width:100%;height:340px"></iframe>

</div>')}
```

```{r, 328, echo=F, purl=F, results='asis'}
if (my_output=="html") {
  cat('
<div  class = "DCexercise">

#### 15. La matriz de covarianza estimada {-}

Como se discutió en el Capítulo \\@ref(SMC), los estimadores MCO $\\widehat{\\beta}_0$ y $\\widehat{\\beta}_1$ son funciones del término de error aleatorio. Por tanto, son variables aleatorias en sí mismas. Para dos o más variables aleatorias, sus covarianzas y varianzas se resumen mediante una *matriz de varianza-covarianza* (que a menudo se denomina simplemente *matriz de covarianza*). Tomando la raíz cuadrada de los elementos diagonales de la matriz de covarianza estimada se obtiene $SE(\\widehat\\beta_0)$ y $SE(\\widehat\\beta_1)$, los errores estándar de $\\widehat{\\beta}_0$ y $\\widehat{\\beta}_1$.

<tt>summary()</tt> calcula una estimación de la matriz. La entrada respectiva en la salida de resumen (recuerde que <tt>resumen()</tt> produce una lista) se llama <tt>cov.unscaled</tt>. El objeto modelo <tt>mod</tt> está disponible en su espacio de trabajo.

**Instrucciones:**

  + Utilizar <tt>summary()</tt> para obtener la estimación de la matriz de covarianza para la regresión de los puntajes de las pruebas en las proporciones alumno-maestro almacenadas en el objeto modelo <tt>mod</tt>. Guardar el resultado en <tt>cov_matrix</tt>.

  + Obtener los elementos diagonales de <tt>cov_matrix</tt>, calcular su raíz cuadrada y asignar el resultado a la variable <tt>SES</tt>.

<iframe src="DCL/ex4_15.html" frameborder="0" scrolling="no" style="width:100%;height:340px"></iframe>

**Sugerencia:** 

  + <tt>diag(A)</tt> devuelve un vector que contiene los elementos diagonales de la matriz <tt>A</tt>.

</div>')}
```

<!--chapter:end:Capitulo_05.Rmd-->

# Pruebas de hipótesis e intervalos de confianza en el modelo de regresión lineal simple {#PHICMRLS}

```{r, echo = F}
options(knitr.duplicate.label = "allow")
```

```{r, 329, child="_setup.Rmd"}
```

```{r, 330, eval=knitr::opts_knit$get("rmarkdown.pandoc.to") == "html", results='asis', echo=FALSE}
cat('<hr style="background-color:#03193b;height:2px">')
```

Este capítulo continúa con el tratamiento del modelo de regresión lineal simple. Las siguientes subsecciones discuten cómo se puede usar el conocimiento sobre la distribución muestral del estimador MCO para hacer declaraciones sobre su incertidumbre.

Estas subsecciones cubren los siguientes temas:

- Prueba de hipótesis sobre coeficientes de regresión.

- Intervalos de confianza para coeficientes de regresión.

- Regresión cuando $X$ es una variable ficticia.

- Heteroscedasticidad y Homoscedasticidad.

Los paquetes **AER** [@R-AER] y **scales** [@R-scales] son necesarios para la reproducción de los fragmentos de código presentados a lo largo de este capítulo. El paquete **scales** proporciona métodos genéricos adicionales de escalado de gráficos. Asegúrese de que ambos paquetes estén instalados antes de continuar. La forma más segura de hacerlo es verificando si el siguiente fragmento de código se ejecuta sin errores.

```{r, 331, warning=FALSE, message=FALSE, eval=FALSE}
library(AER)
library(scales)
```

## Prueba de hipótesis de dos lados respecto al coeficiente de la pendiente

Usando el hecho de que $\hat{\beta}_1$ se distribuye aproximadamente normalmente en muestras grandes (ver [Concepto clave 4.4](#La_distribución_muestral_del_estimador_de_MCO)), se pueden probar hipótesis sobre el valor verdadero $\beta_1$ como en el Capítulo \@ref(PMM).

```{r, 332, eval = my_output == "html", results='asis', echo=F, purl=F}
cat('
<div class = "keyconcept" id="KC5.1">
<h3 class = "right"> Concepto clave 5.1 </h3>
<h3 class = "left"> Forma general del estadístico $t$</h3>
Recuerde del Capítulo \\@ref(RER) que un estadístico $t$ general tiene la forma

$$ t = \\frac{\\text{valor estimado} - \\text{valor hipotético}}{\\text{error estándar del estimador}}.$$
</div>
')
```

```{r, 333, eval = my_output == "latex", results='asis', echo=F, purl=F}
cat('\\begin{keyconcepts}[Forma general del estadístico $t$]{5.1}
Recuerde del Capítulo \\@ref(RER) que un estadístico $t$ general tiene la forma

$$ t = \\frac{\\text{valor estimado} - \\text{valor hipotético}}{\\text{error estándar del estimador}}.$$
\\end{keyconcepts}
')
```

```{r, 334, eval = my_output == "html", results='asis', echo=F, purl=F}
cat('
<div class = "keyconcept" id="KC5.2">
<h3 class = "right"> Concepto clave 5.2 </h3>
<h3 class = "left"> Prueba de hipótesis sobre $\\beta_1$ </h3>

Para probar la hipótesis $H_0: \\beta_1 = \\beta_{1,0}$, se deben realizar los siguientes pasos:

1. Calcular el error estándar de $\\hat{\\beta}_1$, $SE(\\hat{\\beta}_1)$

$$ SE(\\hat{\\beta}_1) = \\sqrt{ \\hat{\\sigma}^2_{\\hat{\\beta}_1} } \\ \\ , \\ \\ 
  \\hat{\\sigma}^2_{\\hat{\\beta}_1} = \\frac{1}{n} \\times \\frac{\\frac{1}{n-2} \\sum_{i=1}^n (X_i - \\overline{X})^2 \\hat{u_i}^2 }{ \\left[ \\frac{1}{n} \\sum_{i=1}^n (X_i - \\overline{X})^2 \\right]^2}. $$

2. Calcular el estadístico $t$

$$ t = \\frac{\\hat{\\beta}_1 - \\beta_{1,0}}{ SE(\\hat{\\beta}_1) }. $$

3. Dada una alternativa de dos lados ($H_1:\\beta_1 \\neq \\beta_{1,0}$), se rechaza en el nivel $5\\%$ si $|t^{act}| > 1.96$ o, de manera equivalente, si el valor de $p$ es menor que $0.05$. 
<br>
Se debe recordar la definición del valor $p$:

  \\begin{align*}
    p \\text{-value} =& \\, \\text{Pr}_{H_0} \\left[ \\left| \\frac{ \\hat{\\beta}_1 - \\beta_{1,0} }{ SE(\\hat{\\beta}_1) } \\right| > \\left|        \\frac{ \\hat{\\beta}_1^{act} - \\beta_{1,0} }{ SE(\\hat{\\beta}_1) } \\right| \\right] \\\\
    =& \\, \\text{Pr}_{H_0} (|t| > |t^{act}|) \\\\
    \\approx& \\, 2 \\cdot \\Phi(-|t^{act}|)
  \\end{align*}  

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; La última transformación se debe a la aproximación normal para muestras grandes.

</div>
')
```

```{r, 335, eval = my_output == "latex", results='asis', echo=F, purl=F}
cat('\\begin{keyconcepts}[Prueba de hipótesis sobre $\\beta_1$]{5.2}

Para probar la hipótesis $H_0: \\beta_1 = \\beta_{1,0}$, se deben realizar los siguientes pasos:\\newline

\\begin{enumerate}
\\item Calcular el error estándar de $\\hat{\\beta}_1$, $SE(\\hat{\\beta}_1)$

$$ SE(\\hat{\\beta}_1) = \\sqrt{ \\hat{\\sigma}^2_{\\hat{\\beta}_1} } \\ \\ , \\ \\ 
  \\hat{\\sigma}^2_{\\hat{\\beta}_1} = \\frac{1}{n} \\times \\frac{\\frac{1}{n-2} \\sum_{i=1}^n (X_i - \\overline{X})^2 \\hat{u_i}^2 }{ \\left[ \\frac{1}{n} \\sum_{i=1}^n (X_i - \\overline{X})^2 \\right]^2}. $$

\\item Calcular el estadístico $t$

$$ t = \\frac{\\hat{\\beta}_1 - \\beta_{1,0}}{ SE(\\hat{\\beta}_1) }. $$

\\item Dada una alternativa de dos lados ($H_1:\\beta_1 \\neq \\beta_{1,0}$), se rechaza en el nivel $5\\%$ si $|t^{act}| > 1.96$ o, de manera equivalente, si el valor de $p$ es menor que $0.05$.\\newline

Se debe recordar la definición del valor $p$:

  \\begin{align*}
    p \\text{-value} =& \\, \\text{Pr}_{H_0} \\left[ \\left| \\frac{ \\hat{\\beta}_1 - \\beta_{1,0} }{ SE(\\hat{\\beta}_1) } \\right| > \\left|        \\frac{ \\hat{\\beta}_1^{act} - \\beta_{1,0} }{ SE(\\hat{\\beta}_1) } \\right| \\right] \\\\
    =& \\, \\text{Pr}_{H_0} (|t| > |t^{act}|) \\\\
    =& \\, 2 \\cdot \\Phi(-|t^{act}|)
  \\end{align*}  

La última transformación se debe a la aproximación normal para muestras grandes.
\\end{enumerate}
\\end{keyconcepts}
')
```

Al considerar nuevamente la regresión MCO almacenada en **linear_model** del Capítulo \@ref(RLR) que dio la línea de regresión

$$\widehat{TestScore} \ = \underset{(9.47)}{698.9} - \underset{(0.49)}{2.28} \times STR \ , \ R^2=0.051 \ , \ SER=18.6.$$

Copie y ejecute el siguiente fragmento de código si el objeto de modelo anterior no está disponible en su entorno de trabajo.

```{r, 336, message=F, warning=F}
# cargar el conjunto de datos `CASchools`
library(AER)
data(CASchools)

# agregar la proporción de alumnos por maestro
CASchools$STR <- CASchools$students/CASchools$teachers

# agregar puntaje promedio de prueba
CASchools$score <- (CASchools$read + CASchools$math)/2

# estimar el modelo
linear_model <- lm(score ~ STR, data = CASchools)          
```

Para probar una hipótesis sobre el parámetro de pendiente (el coeficiente de $STR$), se necesita $SE(\hat{\beta}_1)$, el error estándar del estimador puntual respectivo. Como es común en la literatura, los errores estándar se presentan entre paréntesis debajo de las estimaciones puntuales.

El Concepto clave 5.1 revela que es bastante engorroso calcular el error estándar y, por lo tanto, el estadístico $t$ a mano. La pregunta que debería hacerse ahora mismo es: ¿se pueden obtener estos valores con el mínimo esfuerzo utilizando **R**? Si se puede. Primero se usa **summary()** para obtener un resumen de los coeficientes estimados en **linear_model**.

**Nota**: A lo largo del curso, se informan errores estándar sólidos. Considere que es instructivo mantener las cosas simples al principio y, por lo tanto, comenzar con ejemplos simples que no permiten una inferencia sólida. Los errores estándar que son robustos a la heterocedasticidad se introducen en el Capítulo \@ref(HH) donde se demuestra cómo se pueden calcular usando **R**. En el Capítulo \@ref(EECD) tiene lugar una discusión de los errores estándar robustos de heterocedasticidad-autocorrelación.

```{r, 337, warning=F, message=F}
# imprimir el resumen de los coeficientes en la consola
summary(linear_model)$coefficients
```

La segunda columna del resumen de los coeficientes informa $SE(\hat\beta_0)$ y $SE(\hat\beta_1)$. Además, en la tercera columna **t value**, se encuentra el estadístico $t^{act}$ adecuado para las pruebas de hipótesis separadas $H_0: \beta_0=0$ y $H_0: \beta_1=0$. Además, la salida nos proporciona valores de $p$ correspondientes a ambas pruebas frente a las alternativas de dos caras $H_1:\beta_0\neq0$ respectivamente $H_1:\beta_1\neq0$ en la cuarta columna de la tabla.

Es momento de echar un vistazo más de cerca a la prueba de

$$H_0: \beta_1=0 \ \ \ vs. \ \ \ H_1: \beta_1 \neq 0.$$ 

Se tiene que 

$$ t^{act} = \frac{-2.279808 - 0}{0.4798255} \approx - 4.75. $$

¿Qué dice esto sobre la importancia del coeficiente estimado? Se rechaza la hipótesis nula en el nivel de significancia de $5\%$ ya que $|t^{act}| > 1.96$. Es decir, la estadística de prueba observada cae en la región de rechazo como $p\text{-value} = 2.78\cdot 10^{-6} < 0.05$. Se concluye que el coeficiente es significativamente diferente de cero. En otras palabras, se rechaza la hipótesis de que el tamaño de la clase *no influye* en los resultados de las pruebas de los estudiantes al nivel de $5\%$.

Tenga en cuenta que aunque la diferencia es insignificante en el presente caso, como se verá más adelante, **summary()** no realiza la aproximación normal, sino que calcula los valores de $p$ usando la distribución $t$ en su lugar. Generalmente, los grados de libertad de la distribución de $t$ supuesta se determinan de la siguiente manera:

$$ \text{DF} = n - k - 1 $$

donde $n$ es el número de observaciones utilizadas para estimar el modelo y $k$ es el número de regresores, excluyendo la intersección. En este caso, se tienen $n = 420$ observaciones y el único regresor es $STR$ entonces $k = 1$. La forma más sencilla de determinar los grados de libertad del modelo es

```{r, 338}
# determinar grados de libertad residuales
linear_model$df.residual
```

Por lo tanto, para la distribución muestral asumida de $\hat\beta_1$ se tienen

$$\hat\beta_1 \sim t_{418}$$

Tal que el valor $p$ para una prueba de significancia bilateral se puede obtener ejecutando el siguiente código:

```{r, 339}
2 * pt(-4.751327, df = 418)
```

El resultado está muy cerca del valor proporcionado por **summary()**. Sin embargo, dado que $n$ es suficientemente grande, también se podría usar la densidad normal estándar para calcular el valor de $p$:

```{r, 340}
2 * pnorm(-4.751327)
```

De hecho, la diferencia es insignificante. Estos hallazgos indican que, si $H_0: \beta_1 = 0$ es cierto y se tuviera que repetir todo el proceso de recopilar observaciones y estimar el modelo, ¡observar un $\hat\beta_1 \geq |-2.28|$ es muy poco probable!

Usando **R** se puede visualizar cómo se hace tal declaración cuando se usa la aproximación normal. Esto refleja los principios descritos en el siguiente código, aunque es algo más largo que los ejemplos habituales y parece poco atractivo, pero hay mucha repetición, dado que se agregan matices de color y anotaciones en ambas colas de la distribución normal. Se debe recordar ejecutar el código paso a paso para ver cómo se aumenta el gráfico con las anotaciones.

```{r, 341, fig.align='center'}
# Graficar la normal estándar en el soporte [-6,6]
t <- seq(-6, 6, 0.01)

plot(x = t, 
     y = dnorm(t, 0, 1), 
     type = "l", 
     col = "steelblue", 
     lwd = 2, 
     yaxs = "i", 
     axes = F, 
     ylab = "", 
     main = expression("Cálculo del valor p de una prueba bilateral cuando "~t^act~" = -4.75"), 
     cex.lab = 0.7,
     cex.main = 1)

tact <- -4.75

axis(1, at = c(0, -1.96, 1.96, -tact, tact), cex.axis = 0.7)

# sombrear las regiones críticas usando polygon():

# región crítica en la cola izquierda
polygon(x = c(-6, seq(-6, -1.96, 0.01), -1.96),
        y = c(0, dnorm(seq(-6, -1.96, 0.01)), 0), 
        col = 'orange')

# región crítica en cola derecha

polygon(x = c(1.96, seq(1.96, 6, 0.01), 6),
        y = c(0, dnorm(seq(1.96, 6, 0.01)), 0), 
        col = 'orange')

# Agregar flechas y textos que indiquen regiones críticas y el valor p
arrows(-3.5, 0.2, -2.5, 0.02, length = 0.1)
arrows(3.5, 0.2, 2.5, 0.02, length = 0.1)

arrows(-5, 0.16, -4.75, 0, length = 0.1)
arrows(5, 0.16, 4.75, 0, length = 0.1)

text(-3.5, 0.22, 
     labels = expression("0.025"~"="~over(alpha, 2)),
     cex = 0.7)
text(3.5, 0.22, 
     labels = expression("0.025"~"="~over(alpha, 2)),
     cex = 0.7)

text(-5, 0.18, 
     labels = expression(paste("-|",t[act],"|")), 
     cex = 0.7)
text(5, 0.18, 
     labels = expression(paste("|",t[act],"|")), 
     cex = 0.7)

# Agregar palos que indiquen valores críticos en el nivel 0.05, t^act y -t^act 
rug(c(-1.96, 1.96), ticksize  = 0.145, lwd = 2, col = "darkred")
rug(c(-tact, tact), ticksize  = -0.0451, lwd = 2, col = "darkgreen")
```

El valor $p$ es el área bajo la curva a la izquierda de $-4.75$ más el área bajo la curva a la derecha de $4.75$. Como ya se sabe por los cálculos anteriores, este valor es muy pequeño.

## Intervalos de confianza para coeficientes de regresión {#ICCR}

Como ya se sabe, las estimaciones de los coeficientes de regresión $\beta_0$ y $\beta_1$ están sujetas a incertidumbre de muestreo, consultar el Capítulo \@ref(RLR). Por lo tanto, *nunca* se estimará exactamente el valor real de dichos parámetros a partir de datos de muestra en una aplicación empírica. Sin embargo, se pueden construir intervalos de confianza para la intersección y el parámetro de pendiente.

Un intervalo de confianza de $95\%$ para $\beta_i$ tiene dos definiciones equivalentes:

- El intervalo es el conjunto de valores para los que no se puede rechazar una prueba de hipótesis al nivel de $5\% $.
- El intervalo tiene una probabilidad de $95\%$ de contener el valor real de $\beta_i$. Entonces, en el $95\%$ de todas las muestras que podrían extraerse, el intervalo de confianza cubrirá el valor real de $\beta_i$.

También se dice que el intervalo tiene un nivel de confianza de $95\%$. La idea del intervalo de confianza se resume en el Concepto clave 5.3.

```{r, 342, eval = my_output == "html", results='asis', echo=F, purl=F}
cat('
<div class = "keyconcept" id="KC5.3">
<h3 class = "right"> Concepto clave 5.3 </h3>
<h3 class = "left"> Un intervalo de confianza por $\\beta_i$ </h3>

Imagíne que pudiera extraer todas las muestras aleatorias posibles de un tamaño determinado. El intervalo que contiene el valor verdadero $\\beta_i$ en $95\\%$ de todas las muestras viene dado por la expresión:

$$ \\text{CI}_{0.95}^{\\beta_i} = \\left[ \\hat{\\beta}_i - 1.96 \\times SE(\\hat{\\beta}_i) \\, , \\, \\hat{\\beta}_i + 1.96 \\times SE(\\hat{\\beta}_i) \\right]. $$

De manera equivalente, este intervalo puede verse como el conjunto de hipótesis nulas para las que una prueba de hipótesis de dos lados $5\\%$ no es rechazada.
</div>
')
```

```{r, 343, eval = my_output == "latex", results='asis', echo=F, purl=F}
cat('\\begin{keyconcepts}[Un intervalo de confianza por $\\beta_i$]{5.3}

Imagíne que pudiera extraer todas las muestras aleatorias posibles de un tamaño determinado. El intervalo que contiene el valor verdadero $\\beta_i$ en $95\\%$ de todas las muestras viene dado por la expresión:

$$ \\text{CI}_{0.95}^{\\beta_i} = \\left[ \\hat{\\beta}_i - 1.96 \\times SE(\\hat{\\beta}_i) \\, , \\, \\hat{\\beta}_i + 1.96 \\times SE(\\hat{\\beta}_i) \\right]. $$

De manera equivalente, este intervalo puede verse como el conjunto de hipótesis nulas para las que una prueba de hipótesis de dos lados $5\\%$ no es rechazada.
\\end{keyconcepts}
')
```

### Estudio de simulación: Intervalos de confianza {-}

Para comprender mejor los intervalos de confianza, se realiza otro estudio de simulación. Por ahora, suponga que se tiene la siguiente muestra de $n = 100$ observaciones en una sola variable $Y$, donde

$$ Y_i \overset{i.i.d}{\sim} \mathcal{N}(5,25), \ i = 1, \dots, 100.$$

```{r, 344, fig.align='center'}
# establecer semillas para la reproducibilidad
set.seed(4)

# generar y graficar los datos de muestra
Y <- rnorm(n = 100, 
           mean = 5, 
           sd = 5)

plot(Y, 
     pch = 19, 
     col = "steelblue")
```

Se supone que los datos son generados por el modelo

$$ Y_i = \mu + \epsilon_i $$

donde $\mu$ es una constante desconocida y se sabe que $\epsilon_i \overset{i.i.d.}{\sim} \mathcal{N}(0,25)$. En este modelo, el estimador MCO para $\mu$ viene dado por 

$$ \hat\mu = \overline{Y} = \frac{1}{n} \sum_{i=1}^n Y_i, $$

es decir, el promedio de la muestra de $Y_i$. Además sostiene que:

$$ SE(\hat\mu) = \frac{\sigma_{\epsilon}}{\sqrt{n}} = \frac{5}{\sqrt{100}} $$

Un intervalo de confianza de $95\%$ de muestra grande para $\mu$ viene dado por:

\begin{equation} 
CI^{\mu}_{0.95} = \left[\hat\mu - 1.96 \times \frac{5}{\sqrt{100}} \ , \ \hat\mu + 1.96 \times \frac{5}{\sqrt{100}}  \right]. (\#eq:KI)
\end{equation}

Es bastante fácil calcular este intervalo en **R** a mano. El siguiente fragmento de código genera un vector con nombre que contiene los límites del intervalo:

```{r, 345}
cbind(CIlower = mean(Y) - 1.96 * 5 / 10, CIupper = mean(Y) + 1.96 * 5 / 10)
```

Sabiendo que $\mu = 5$, se puede ver que, para los datos del ejemplo, el intervalo de confianza cubre el valor real.

A diferencia de los ejemplos del mundo real, se puede usar **R** para comprender mejor los intervalos de confianza muestreando datos repetidamente, estimando $\mu$ y calculando el intervalo de confianza para $\mu$ como en \@ref(eq:KI).

El procedimiento es el siguiente:

- Inicializar los vectores **lower** y **upper** en los que se van a guardar los límites del intervalo simulado. Se quiere simular intervalos de $10000$ para que ambos vectores tengan esta longitud.
- Se usa un bucle **for()** para muestrear observaciones de $100$ de la distribución $\mathcal{N}(5,25)$ y calcular $\hat\mu$ así como los límites del intervalo de confianza en cada iteración del bucle.
- Por fin se une **lower** y **upper** en una matriz.

```{r, 346}
# sembrar semilla
set.seed(1)

# inicializar vectores de límites de intervalo superior e inferior
lower <- numeric(10000)
upper <- numeric(10000)

# muestreo / estimación / IC de bucle
for(i in 1:10000) {
  
  Y <- rnorm(100, mean = 5, sd = 5)
  lower[i] <- mean(Y) - 1.96 * 5 / 10
  upper[i] <- mean(Y) + 1.96 * 5 / 10
  
}

# unir vectores de límites de intervalo en una matriz
CIs <- cbind(lower, upper)
```

Según el Concepto clave 5.3, se espera que la fracción de los intervalos simulados de $10000$ guardados en la matriz **IC** que contienen el valor real $\mu = 5$ debe ser aproximadamente $95\%$. Se puede verificar esto fácilmente usando operadores lógicos.

```{r, 347}
mean(CIs[, 1] <= 5 & 5 <= CIs[, 2])
```

La simulación muestra que la fracción de intervalos que cubren $\mu = 5$; es decir, aquellos intervalos para los que $H_0: \mu = 5$ no se pueden rechazar, está cerca del valor teórico de $95\%$.

Trazar un gráfico de los primeros intervalos de confianza simulados de $100$ e indicar aquellos que *no* cubren el valor real de $\mu$. Hacer esto a través de líneas horizontales que representan los intervalos de confianza uno encima del otro.

```{r, 348, fig.align='center', fig.height=5, fig.width=4}
# identificar intervalos que no cubren mu
# (4 intervalos de 100)
ID <- which(!(CIs[1:100, 1] <= 5 & 5 <= CIs[1:100, 2]))

# inicializar la gráfica
plot(0, 
     xlim = c(3, 7), 
     ylim = c(1, 100), 
     ylab = "Sample", 
     xlab = expression(mu), 
     main = "Intervalos de confianza")

# configurar vector de color
colors <- rep(gray(0.6), 100)
colors[ID] <- "red"

# dibuja la línea de referencia en mu = 5
abline(v = 5, lty = 2)

# agregar barras horizontales que representen los IC
for(j in 1:100) {
  
  lines(c(CIs[j, 1], CIs[j, 2]), 
        c(j, j), 
        col = colors[j], 
        lwd = 2)
  
}
```

Para las primeras muestras de $100$, la hipótesis nula verdadera se rechaza en cuatro casos, por lo que estos intervalos no cubren $\mu = 5$. Se han indicado los intervalos que conducen a un rechazo del rojo nulo.

Volviendo al ejemplo de los resultados de las pruebas y el tamaño de las clases. El modelo de regresión del Capítulo \@ref(RLR) se almacena en **linear_model**. Una manera fácil de obtener intervalos de confianza de $95\%$ para $\beta_0$ y $\beta_1$, los coeficientes en **(intercepción)** y **STR**, es usar la función **confint()**. Solo se tiene que proporcionar un objeto de modelo ajustado como entrada para esta función. El nivel de confianza está establecido en $95\%$ de forma predeterminada, pero se puede modificar configurando el argumento **level**, consultar **?Confint**.

```{r, 349}
# calcular el intervalo de confianza del 95% para los coeficientes en 'linear_model'
confint(linear_model)
```

Comprobar si el cálculo se realiza como se espera para $\beta_1$, el coeficiente de **STR**.

```{r, 350}
# calcular el intervalo de confianza del 95% para los coeficientes en 'linear_model' a mano
lm_summ <- summary(linear_model)

c("lower" = lm_summ$coef[2,1] - qt(0.975, df = lm_summ$df[2]) * lm_summ$coef[2, 2],
  "upper" = lm_summ$coef[2,1] + qt(0.975, df = lm_summ$df[2]) * lm_summ$coef[2, 2])
```

Los límites superior e inferior coinciden. Se ha utilizado el $0.975$ -cuantil de la distribución $t_{418}$ para obtener el resultado exacto informado por **confint**. Evidentemente, este intervalo *no contiene el valor cero* que, como ya se ha visto en el apartado anterior, conduce al rechazar la hipótesis nula $\beta_{1,0} = 0$.

## Regresión cuando X es una variable binaria {#RCXVB}

En lugar de usar un regresor continuo $X$, de podría estar interesados en ejecutar la regresión

$Y_i = \beta_0 + \beta_1 D_i + u_i \tag{5.2}$

donde $D_i$ es una variable binaria, una llamada *variable ficticia* (*dummy variable*). Por ejemplo, se puede definir $D_i$ de la siguiente manera:

$D_i = \begin{cases}
        1 \ \ \text{si $STR$ en $i^{ésimo}$ distrito escolar < 20} \\
        0 \ \ \text{si $STR$ en $i^{ésimo}$ distrito escolar $\geq$ 20} \\
      \end{cases} \tag{5.3}$

El modelo de regresión ahora es

$TestScore_i = \beta_0 + \beta_1 D_i + u_i. \tag{5.4}$

Es momento de ver cómo se comportan estos datos en un diagrama de dispersión:

```{r, 351, eval=F}
# Cree la variable ficticia como se define arriba
CASchools$D <- CASchools$STR < 20

# Graficar los datos
plot(CASchools$D, CASchools$score,     # proporciona los datos que se trazarán
     pch = 20,                         # utilizar círculos rellenos como símbolos de la gráfica
     cex = 0.5,                        # establecer el tamaño de los símbolos de la gráfica en 0.5
     col = "Steelblue",                # establece el color de los símbolos en "Steelblue"
     xlab = expression(D[i]),          # establecer el título y los nombres de los ejes
     ylab = "Resultado de la prueba",
     main = "Regresión ficticia")
```

```{r, 352, fig.align='center', echo=F}
# crear la variable ficticia como se define arriba
CASchools$D <- CASchools$STR < 20

# estimar la regresión ficticia
dummy_model <- lm(score ~ D, data = CASchools)

# graficar los datos
plot(CASchools$D, CASchools$score, 
     pch = 20, cex = 0.5 , col = "Steelblue",
     xlab = expression(D[i]), ylab = "Resultado de la prueba",
     main = "Regresión ficticia")
#
points(CASchools$D, predict(dummy_model), col = "red", pch = 20)
```

Con $D$ como regresor, no es útil pensar en $\beta_1$ como un parámetro de pendiente, ya que $D_i \in \{0,1\}$; es decir, solo se observan dos valores discretos en lugar de un continuo de valores regresores. No hay una línea continua que represente la función de expectativa condicional $E(TestScore_i | D_i)$, teniendo en cuenta que esta función está definida únicamente para $x$ - en las posiciones $0$ y $1$.

Por tanto, la interpretación de los coeficientes en este modelo de regresión es la siguiente:

- $E(Y_i | D_i = 0) = \beta_0$, por lo que $\beta_0$ es la puntuación de prueba esperada en los distritos donde $D_i = 0$ donde $STR$ está por encima de $20$.

- $E(Y_i | D_i = 1) = \beta_0 + \beta_1$ o, usando el resultado anterior, $\beta_1 = E(Y_i | D_i = 1) - E(Y_i | D_i = 0)$. Por lo tanto, $\beta_1$ es *la diferencia en las expectativas específicas del grupo*; es decir, la diferencia en la puntuación esperada de la prueba entre los distritos con $STR<20$ y aquellos con $STR\geq20$.

Ahora se usará **R** para estimar un modelo de regresión ficticio según lo definido por las ecuaciones (<a href="#mjx-eqn-5.2">5.2</a>) y (<a href="#mjx-eqn-5.3">5.3</a>).

```{r, 353}
# estimar el modelo de regresión ficticia
dummy_model <- lm(score ~ D, data = CASchools)
summary(dummy_model)
```

```{block2, precision, type='rmdnote'}
<tt>summary()</tt> informa el valor $p$ de la prueba, que en este caso el coeficiente en <tt>(Intercepción)</tt> es cero para ser <tt><2e-16</tt>. Esta notación científica establece que el valor $p$ es menor que $\frac{2}{10^{16}}$, por lo que es un número muy pequeño. La razón de esto es que las computadoras no pueden manejar números pequeños arbitrarios. De hecho, $\frac{2}{10^{16}}$ es el número más pequeño posible con el que <tt>R</tt> puede trabajar.
```

El vector **CASchools\$D** tiene el tipo **logical** (para ver esto, usar **typeof (CASchools\$D)**) que es mostrado en la salida de **summary (modelo_dummy)**: la etiqueta **DTRUE** establece que todas las entradas **TRUE** están codificadas como **1** y todas las entradas **FALSE** se codifican como **0**. Por lo tanto, la interpretación del coeficiente **DTRUE** es como se indicó anteriormente para $\beta_1$.

Se puede ver como predecir el puntaje de prueba esperado en distritos con $STR < 20$ ($D_i = 1$) será $650.1 + 7.17 = 657.27$  mientras que los distritos con $STR \geq 20$ ($D_i = 0$) se espera que tenga un puntaje promedio de prueba de solo $650.1$.

Las predicciones específicas de grupo se pueden agregar al gráfico mediante la ejecución del siguiente fragmento de código.

```{r, 354, eval=F}
# agregar predicciones específicas de grupo a la gráfica
points(x = CASchools$D, 
       y = predict(dummy_model), 
       col = "red", 
       pch = 20)
```

Aquí se usa la función **predict()** para obtener estimaciones de las medias específicas del grupo. Los puntos rojos representan los promedios de estos grupos de muestra. En consecuencia, $\hat{\beta}_1 = 7.17$ puede verse como la diferencia en los promedios del grupo.

**summary(dummy_model)** también responde a la pregunta de si existe una diferencia estadísticamente significativa en las medias de los grupos. Esto, a su vez, apoyaría la hipótesis de que los estudiantes se desempeñan de manera diferente cuando se les enseña en clases pequeñas. Se puede evaluar esto mediante una prueba de dos colas de la hipótesis $H_0: \beta_1 = 0$. Convenientemente, el estadístico $t$ y el valor $p$ correspondiente para esta prueba se calculan mediante **summary()**.

Dado que el valor **t** $= 3.88 > 1.96$, se rechaza la hipótesis nula en el nivel de significancia de $5\%$. Se obtiene la misma conclusión cuando se usa el valor $p$, que reporta significancia hasta el nivel $0.00012\%$.

Como se hizo con **linear_model**, alternativamente se puede usar la función **confint()** para calcular un intervalo de confianza de $95\%$ para la verdadera diferencia en las medias y ver si el valor hipotético es un elemento de este conjunto de confianza.

```{r, 355}
# intervalos de confianza para coeficientes en el modelo de regresión ficticia
confint(dummy_model)
```

Se rechaza la hipótesis de que no existe diferencia entre las medias de los grupos en el nivel de significancia de $5\%$, ya que $\beta_{1,0} = 0$ se encuentra fuera de $[3.54, 10.8]$, el intervalo de confianza de $95\%$ para el coeficiente de $D$.

## Heteroscedasticidad y homocedasticidad {#HH}

Todas las inferencias hechas en los capítulos anteriores se basan en el supuesto de que la varianza del error no varía a medida que cambian los valores del regresor. Pero este no suele ser el caso en las aplicaciones empíricas.

```{r, 356, eval = my_output == "html", results='asis', echo=F, purl=F}
cat('
<div class = "keyconcept" id="KC5.4">
<h3 class = "right"> Concepto clave 5.4 </h3>          
<h3 class = "left"> Heteroscedasticidad y homocedasticidad </h3>
<br>


- El término de error del modelo de regresión es homocedástico si la varianza de la distribución condicional de $u_i$ dado $X_i$, $Var(u_i|X_i=x)$, es constante *para todas las observaciones* en la muestra:

$$ \\text{Var}(u_i|X_i=x) = \\sigma^2 \\ \\forall \\ i=1,\\dots,n. $$

- Si, en cambio, existe dependencia de la varianza condicional de $u_i$ con $X_i$, se dice que el término de error es heterocedástico. Luego se escribe:

$$ \\text{Var}(u_i|X_i=x) = \\sigma_i^2 \\ \\forall \\ i=1,\\dots,n. $$

- La homocedasticidad es un *caso especial* de heterocedasticidad. En conclusión, en los modelos de regresión lineales se dice que hay heterocedasticidad cuando la varianza de los errores no es igual en todas las observaciones realizadas.

</div>
')
```

```{r, 357, eval = my_output == "latex", results='asis', echo=F, purl=F}
cat('\\begin{keyconcepts}[Heteroscedasticidad y homocedasticidad]{5.4}
\\begin{itemize}

\\item El término de error del modelo de regresión es homocedástico si la varianza de la distribución condicional de $u_i$ dado $X_i$, $Var(u_i|X_i=x)$, es constante \\textit{para todas las observaciones} en la muestra:

$$ \\text{Var}(u_i|X_i=x) = \\sigma^2 \\ \\forall \\ i=1,\\dots,n. $$

\\item Si, en cambio, existe dependencia de la varianza condicional de $u_i$ con $X_i$, se dice que el término de error es heterocedástico. Luego se escribe:

$$ \\text{Var}(u_i|X_i=x) = \\sigma_i^2 \\ \\forall \\ i=1,\\dots,n. $$

\\item La homocedasticidad es un \\textit{caso especial} de heterocedasticidad. En conclusión, en los modelos de regresión lineales se dice que hay heterocedasticidad cuando la varianza de los errores no es igual en todas las observaciones realizadas.

\\end{itemize}
\\end{keyconcepts}
')
```

Para una mejor comprensión de la heterocedasticidad, se generan algunos datos heterocedásticos bivariados, se estima un modelo de regresión lineal y luego se usan diagramas de caja para representar las distribuciones condicionales de los residuos.

```{r, 358, fig.align='center', warning=FALSE}
# cargar el paquete de escalas para ajustar opacidades de color
library(scales)

# generar algunos datos heterocedásticos:

# establecer semillas para la reproducibilidad
set.seed(123) 

# configurar vector de x coordenadas
x <- rep(c(10, 15, 20, 25), each = 25)

# inicializar vector de errores
e <- c()

# muestrear 100 errores tales que la varianza aumenta con x
e[1:25] <- rnorm(25, sd = 10)
e[26:50] <- rnorm(25, sd = 15)
e[51:75] <- rnorm(25, sd = 20)
e[76:100] <- rnorm(25, sd = 25)

# configurar y
y <- 720 - 3.3 * x + e

# estimar el modelo
mod <- lm(y ~ x)

# graficar los datos
plot(x = x, 
     y = y, 
     main = "Un ejemplo de heterocedasticidad",
     xlab = "Proporción alumno-maestro",
     ylab = "Resultado de la prueba",
     cex = 0.5, 
     pch = 19, 
     xlim = c(8, 27), 
     ylim = c(600, 710))

# Agregar la línea de regresión al gráfico
abline(mod, col = "darkred")

# Agregar diagramas de caja a la trama
boxplot(formula = y ~ x, 
        add = TRUE, 
        at = c(10, 15, 20, 25), 
        col = alpha("gray", 0.4), 
        border = "black")
```

Se ha utilizado la función **formula** con el argumento **y ~ x** en **boxplot()** para especificar que se quiere dividir el vector **y** en grupos, de acuerdo con **x**. **boxplot(y ~ x)** genera un boxplot para cada uno de los grupos en **y** definido por **X**.

Para estos datos artificiales, está claro que las variaciones de error condicionales difieren. Específicamente, se observa que la varianza en los puntajes de las pruebas (y por lo tanto la varianza de los errores cometidos) *aumenta* con la proporción de alumnos por maestro.

### Un ejemplo del mundo real de heterocedasticidad {-}

Piense en el valor económico de la educación: Si no hubiera un valor agregado económico esperado para recibir educación universitaria, probablemente no estaría atendiendo la lectura del presente curso en este momento. Un punto de partida para verificar empíricamente tal relación es tener datos sobre las personas que trabajan. Más precisamente, se necesitan datos sobre salarios y educación de los trabajadores para estimar un modelo como

$$ wage_i = \beta_0 + \beta_1 \cdot education_i + u_i. $$

¿Qué se puede presumir de esta relación? Es probable que, en promedio, los trabajadores con mayor educación ganen más que los trabajadores con menos educación, por lo que se espera estimar una línea de regresión con pendiente ascendente. Además, parece plausible que los ingresos de los trabajadores mejor educados tengan una mayor dispersión que los de los trabajadores poco calificados: una educación sólida no es garantía para un salario alto, por lo que incluso los trabajadores altamente calificados aceptan trabajos de bajos ingresos. Sin embargo, es más probable que cumplan con los requisitos para los trabajos bien remunerados que los trabajadores con menos educación para quienes las oportunidades en el mercado laboral son mucho más limitadas.

Para verificar esto empíricamente, se pueden usar datos reales sobre ganancias por hora y el número de años de educación de los empleados. Estos datos se pueden encontrar en **CPSSWEducation**. Este conjunto de datos es parte del paquete **AER** y proviene de la Current Population Survey (CPS) que es realizada periódicamente por la [Oficina de Estadísticas Laborales](http://www.bls.gov/) en los Estados Unidos.

Los siguientes fragmentos de código demuestran cómo importar los datos a **R** y cómo producir un gráfico:

```{r, 359, fig.align='center'}
# cargar paquete y adjuntar datos
library(AER)
data("CPSSWEducation")
attach(CPSSWEducation)

# obtener una descripción general
summary(CPSSWEducation)

# estimar un modelo de regresión simple
labor_model <- lm(earnings ~ education)

# graficar observaciones y agregar la línea de regresión
plot(education, 
     earnings, 
     ylim = c(0, 150))

abline(labor_model, 
       col = "steelblue", 
       lwd = 2)
```

La gráfica revela que la media de la distribución de ingresos aumenta con el nivel de educación. Esto también está respaldado por un análisis formal: El modelo de regresión estimado almacenado en **labor_mod** muestra que existe una relación positiva entre los años de educación y los ingresos.

```{r, 360}
# imprimir el contenido de labor_model en la consola
labor_model
```

La ecuación de regresión estimada establece que, en promedio, un año adicional de educación aumenta los ingresos por hora de un trabajador en aproximadamente $\$ 1.47$. Una vez más se usa **confint()** para obtener un intervalo de confianza de $95\%$ para ambos coeficientes de regresión.

```{r, 361}
# calcular un intervalo de confianza del 95% para los coeficientes en el modelo
confint(labor_model)
```

Dado que el intervalo es $[1.33, 1.60]$, se puede rechazar la hipótesis de que el coeficiente de **educación** es cero en el nivel de $5\%$.

Además, el gráfico indica que hay heterocedasticidad: Si se asume que la línea de regresión es una representación razonablemente buena de la función media condicional $E(ganancias_i\vert educación_i)$, la dispersión de las ganancias por hora alrededor de esa función aumenta claramente con el nivel de la educación; es decir, aumenta la varianza de la distribución de los ingresos. En otras palabras: La varianza de los errores (los errores cometidos al explicar los ingresos por educación) aumenta con la educación, por lo que los errores de regresión son heterocedásticos.

Este ejemplo demuestra que el supuesto de homocedasticidad es dudoso en aplicaciones económicas. ¿Los economistas deberían preocuparse por la heterocedasticidad? Sí, deberían. Como se explica en la siguiente sección, la heterocedasticidad puede tener graves consecuencias negativas en la prueba de hipótesis, si se ignora.

### ¿Los economistas deberían preocuparse por la heterocedasticidad? {-}

Para responder a la pregunta de si deberían preocuparse por la presencia de heterocedasticidad, se debe considerar la varianza de $\hat\beta_1$ bajo el supuesto de homocedasticidad. En este caso se tiene:

$$ \sigma^2_{\hat\beta_1} = \frac{\sigma^2_u}{n \cdot \sigma^2_X} \tag{5.5} $$

que es una versión simplificada de la ecuación general (<a href="#mjx-eqn-4.1">4.1</a>) presentada en el Concepto clave 4.4; así como **summary()** estimaciones (<a href="#mjx-eqn-5.5">5.5</a>) por

$$ \overset{\sim}{\sigma}^2_{\hat\beta_1} = \frac{SER^2}{\sum_{i=1}^n (X_i - \overline{X})^2} \ \ \text{where} \ \ SER=\frac{1}{n-2} \sum_{i=1}^n \hat u_i^2. $$

Por lo tanto, **summary()** estima el error estándar *de solo homocedasticidad*:

$$\sqrt{ \overset{\sim}{\sigma}^2_{\hat\beta_1} } = \sqrt{ \frac{SER^2}{\sum_{i=1}^n(X_i - \overline{X})^2} }.$$

De hecho, este es un estimador para la desviación estándar del estimador $\hat{\beta}_1$ que es *inconsistente* para el valor verdadero $\sigma^2_{\hat\beta_1}$ cuando existe heterocedasticidad. La implicación es que el estadístico $t$ calculado a la manera del Concepto clave 5.1 no sigue una distribución normal estándar, incluso en muestras grandes. Este problema puede invalidar la inferencia cuando se utilizan las herramientas tratadas anteriormente para la prueba de hipótesis: Se debe tener cuidado al hacer afirmaciones sobre la importancia de los coeficientes de regresión sobre la base del estadístico $t$ calculado por **summary()** o intervalos de confianza producido por **confint()** si es dudoso que se mantenga el supuesto de homocedasticidad.

Ahora se usará **R** para calcular el error estándar de solo homocedasticidad para $\hat{\beta}_1$ en el modelo de regresión de puntaje de prueba **modelo_laboral** a mano y se verá que coincide con el valor producido por **summary()**.

```{r, 362}
# resumen del modelo de tienda en 'modelo'
model <- summary(labor_model)

# extraer el error estándar de la regresión del resumen del modelo
SER <- model$sigma

# calcular la variación en 'educación'
V <- (nrow(CPSSWEducation)-1) * var(education)

# calcular el error estándar del estimador del parámetro de pendiente e imprimirlo
SE.beta_1.hat <- sqrt(SER^2/V)
SE.beta_1.hat

# utilizar los operadores lógicos para ver si el valor calculado coincide con el proporcionado
# en mod$coefficients, redondear estimaciones a cuatro lugares decimales
round(model$coefficients[2, 2], 4) == round(SE.beta_1.hat, 4)
```

De hecho, los valores estimados son iguales.

### Cálculo de errores estándar robustos de heterocedasticidad {-}

Se otorga una estimación consistente de $\sigma_{\hat{\beta}_1}$ bajo heterocedasticidad cuando se usa el siguiente estimador *robusto*:

$SE(\hat{\beta}_1) = \sqrt{ \frac{1}{n} \cdot \frac{ \frac{1}{n} \sum_{i=1}^n (X_i - \overline{X})^2 \hat{u}_i^2 }{ \left[ \frac{1}{n} \sum_{i=1}^n (X_i - \overline{X})^2  \right]^2} } \tag{5.6}$

Las estimaciones de error estándar calculadas de esta manera también se conocen como [errores estándar de Eicker-Huber-White](https://en.wikipedia.org/wiki/Heteroscedasticity-consistent_standard_errors), el artículo más frecuentemente citado sobre esto es @white1980.

Puede resultar bastante engorroso hacer este cálculo a mano. Afortunadamente, existen ciertas funciones en R que cumplen con ese propósito. Uno conveniente llamado **vcovHC()** es parte del paquete **sandwich**.^[El paquete **sandwich** es una dependencia del paquete **AER**, lo que implica que se adjunta automáticamente si se carga **AER**.] Dicha función puede calcular una variedad de errores estándar. El presentado en (<a href="#mjx-eqn-5.6">5.6</a>) se calcula cuando el argumento **type** se establece en **"HC0"**. La mayoría de los ejemplos presentados en los libros de texto encuentran fundamento en una fórmula ligeramente diferente, que es la predeterminada en el paquete de estadísticas *STATA*:

\begin{align}
SE(\hat{\beta}_1)_{HC1} = \sqrt{ \frac{1}{n} \cdot \frac{ \frac{1}{n-2} \sum_{i=1}^n (X_i - \overline{X})^2 \hat{u}_i^2 }{ \left[ \frac{1}{n} \sum_{i=1}^n (X_i - \overline{X})^2  \right]^2}} (\#eq:hc1)
\end{align}

La diferencia es que se multiplica por $\frac{1}{n-2}$ en el numerador de \@ref(eq:hc1). Esta es una corrección de grados de libertad y fue considerada por @mackinnon1985. Para que **vcovHC()** use \@ref(eq:hc1), se tiene que establecer **type = "HC1"**.

Calcular ahora estimaciones robustas del error estándar para los coeficientes en **modelo_lineal**.

```{r, 363}
# calcular errores estándar robustos a la heterocedasticidad
vcov <- vcovHC(linear_model, type = "HC1")
vcov
```

La salida de **vcovHC()** es la matriz de varianza-covarianza de estimaciones de coeficientes. Se está interesado en la raíz cuadrada de los elementos diagonales de esta matriz; es decir, las estimaciones del error estándar.

```{block2, vcovmatrix, type='rmdknit'}
Cuando se tiene k > 1 regresores, escribir las ecuaciones para un modelo de regresión se vuelve muy complicado. Una forma más conveniente de denotar y estimar los llamados modelos de regresión múltiple (ver Capítulo \@ref(MRVR)) es usando álgebra matricial. Es por eso que funciones como <tt>vcovHC()</tt> producen matrices. En el modelo de regresión lineal simple, las varianzas y covarianzas de los estimadores se pueden recopilar en la matriz simétrica de varianza-covarianza.

\begin{equation}
\text{Var}
  \begin{pmatrix}
    \hat\beta_0 \\
    \hat\beta_1
  \end{pmatrix} = 
\begin{pmatrix}
  \text{Var}(\hat\beta_0) & \text{Cov}(\hat\beta_0,\hat\beta_1) \\
\text{Cov}(\hat\beta_0,\hat\beta_1) & \text{Var}(\hat\beta_1)
\end{pmatrix},
\end{equation}

entonces <tt>vcovHC()</tt> da $\widehat{\text{Var}}(\hat\beta_0)$, $\widehat{\text{Var}}(\hat\beta_1)$ y $\widehat{\text{Cov}}(\hat\beta_0,\hat\beta_1)$, pero la mayoría de las veces se está interesado en los elementos diagonales de la matriz estimada.

```

```{r, 364}
# calcular la raíz cuadrada de los elementos diagonales en vcov
robust_se <- sqrt(diag(vcov))
robust_se
```

Ahora suponga que se quiere generar un resumen de coeficientes como lo proporciona **summary()**, pero con errores estándar *robustos* de los estimadores de coeficientes, estadísticos robustas de $t$ y valores de $p$ correspondientes para el modelo de regresión **linear_model**. Esto se puede hacer usando **coeftest()** del paquete **lmtest**, ver **?coeftest**. Más adelante se especifica en el argumento **vcov.** que debe usarse **vcov**, la estimación de Eicker-Huber-White de la matriz de varianza que se ha calculado antes.

```{r, 365}
# se invoca la función `coeftest()` en el modelo
coeftest(linear_model, vcov. = vcov)
```

Se puede ver que los valores reportados en la columna **Std. Error** son iguales a los de **sqrt(diag(vcov))**.

¿Qué tan severas son las implicaciones de usar errores estándar de solo homocedasticidad en presencia de heterocedasticidad? La respuesta es, depende. Como se mencionó anteriormente, se corre el riesgo de sacar conclusiones erróneas al realizar pruebas de significancia. 

Resulta importante ilustrar este punto generando otro ejemplo de un conjunto de datos heterocedásticos, usándolo para estimar un modelo de regresión simple. En este caso se toma:

$$ Y_i = \beta_1 \cdot X_i + u_i \ \ , \ \ u_i \overset{i.i.d.}{\sim} \mathcal{N}(0,0.36 \cdot X_i^2)  $$ 

con $\beta_1=1$ como proceso de generación de datos. Claramente, aquí se viola el supuesto de homocedasticidad, dado que la varianza de los errores es una función creciente no lineal de $X_i$, pero los errores tienen media cero y son i.i.d. de manera que no se violen las suposiciones hechas en el Concepto clave 4.3. Como antes, se está interesado en estimar $\beta_1$.

```{r, 366}
set.seed(905)

# generar datos heterocedásticos
X <- 1:500
Y <- rnorm(n = 500, mean = X, sd = 0.6 * X)

# estimar un modelo de regresión simple
reg <- lm(Y ~ X)
```

Se grafican los datos y se agrega la línea de regresión.

```{r, 367, fig.align='center'}
# graficar los datos
plot(x = X, y = Y, 
     pch = 19, 
     col = "steelblue", 
     cex = 0.8)

# agregar la línea de regresión al gráfico
abline(reg, 
       col = "darkred", 
       lwd = 1.5)
```

La gráfica muestra que los datos son heterocedásticos a medida que la varianza de $Y$ crece con $X$. A continuación, se realiza una prueba de significancia de la hipótesis nula (verdadera) $H_0: \beta_1 = 1$ dos veces, una vez usando la fórmula de error estándar de homocedasticidad solamente y una vez con la versión robusta (<a href="#mjx-eqn-5.6">5.6</a>). Una manera fácil de hacer esto en **R** es la función **linearHypothesis()** del paquete **car**, ver **?LinearHypothesis**. Permite probar hipótesis lineales sobre parámetros en modelos lineales de manera similar a como se hace con un estadístico $t$ y ofrecer varios estimadores de matrices de covarianza robustas. Se prueba comparando los valores $p$ de las pruebas con el nivel de significancia de $5\%$.

```{block2, linearhypothesis, type='rmdknit'}
<tt>linearHypothesis()</tt> calcula una estadística de prueba que sigue una distribución $F$ bajo la hipótesis nula. No es momento de centrarse en los detalles de la teoría subyacente. En general, la idea de la prueba $F$ es comparar el ajuste de diferentes modelos. Cuando se prueba una hipótesis sobre un coeficiente *único* usando una prueba $F$, se puede mostrar que la estadística de prueba es simplemente el cuadrado del estadístico $t$ correspondiente:

$$ F = t ^ 2 = \ left (\ frac {\ hat \ beta_i - \ beta_ {i, 0}} {SE (\ hat \ beta_i)} \ right) ^ 2 \ sim F_ {1, nk-1 } $$

En <tt>linearHypothesis()</tt>, existen diferentes formas de especificar la hipótesis que se va a probar; por ejemplo, utilizando un vector del tipo <tt>character</tt> (como se hace en el siguiente fragmento de código), ver <tt>?linearHypothesis</tt> para alternativas. La función devuelve un objeto de clase <tt>anova</tt> que contiene más información sobre la prueba a la que se puede acceder utilizando el operador <tt>$</tt>.
```

```{r, 368}
# probar la hipótesis utilizando la fórmula de error estándar predeterminada
linearHypothesis(reg, hypothesis.matrix = "X = 1")$'Pr(>F)'[2] < 0.05

# probar hipótesis usando la fórmula robusta de error estándar
linearHypothesis(reg, hypothesis.matrix = "X = 1", white.adjust = "hc1")$'Pr(>F)'[2] < 0.05
```

Este es un buen ejemplo de lo que puede salir mal si se ignora la heterocedasticidad: Para el conjunto de datos en cuestión, el método predeterminado rechaza la hipótesis nula $\beta_1 = 1$ aunque es cierta. Cuando se utiliza la fórmula robusta de error estándar, la prueba no rechaza la hipótesis nula. Por supuesto, se podría pensar que esto es solo una coincidencia y ambas pruebas funcionan igualmente bien para mantener la tasa de error de tipo I de $5\%$. Esto se puede investigar más a fondo calculando estimaciones de *Monte Carlo* de las frecuencias de rechazo de ambas pruebas sobre la base de un gran número de muestras aleatorias. Se procede de la siguiente manera:

- Inicializar los vectores **t** y **t.rob**.
- Usar un bucle **for()** para generar $10000$ muestras aleatorias heterocedásticas de tamaño $1000$, se estima el modelo de regresión y se verifica si las pruebas rechazan falsamente la hipótesis nula al nivel de $5\%$ usando operadores de comparación. Los resultados se almacenan en los vectores respectivos **t** y **t.rob**.
- Después de la simulación, se calcula la fracción de falsos rechazos para ambas pruebas.

```{r, 369, cache=T, echo=-1}
set.seed(905)

# inicializar los vectores t y t.rob
t <- c()
t.rob <- c()

# muestreo y estimación de bucles
for (i in 1:10000) {
  
  # datos de muestra
  X <- 1:1000
  Y <- rnorm(n = 1000, mean = X, sd = 0.6 * X)

  # estimar modelo de regresión
  reg <- lm(Y ~ X)

  # prueba de significación solo de homocedasticidad
  t[i] <- linearHypothesis(reg, "X = 1")$'Pr(>F)'[2] < 0.05

  # prueba de significancia robusta
  t.rob[i] <- linearHypothesis(reg, "X = 1", white.adjust = "hc1")$'Pr(>F)'[2] < 0.05

}

# calcular la fracción de falsos rechazos
round(cbind(t = mean(t), t.rob = mean(t.rob)), 3)
```

Estos resultados revelan un mayor riesgo de rechazar falsamente la hipótesis nula utilizando el error estándar de solo homocedasticidad para el problema de prueba en cuestión: Con el error estándar común, $`r round(sum(t)/length(t)*100,3)`\%$  de todas las pruebas rechazan falsamente la hipótesis nula. Por el contrario, con la estadística de prueba robusta se esta más cerca del nivel nominal de $5\%$.
 
## El teorema de Gauss-Markov

Al estimar modelos de regresión, se sabe que los resultados del procedimiento de estimación son aleatorios. Sin embargo, al utilizar estimadores insesgados, al menos en promedio, se estima el parámetro verdadero. Por lo tanto, al comparar diferentes estimadores insesgados, es interesante saber cuál tiene la mayor precisión: Siendo conscientes de que la probabilidad de estimar el valor *exacto* del parámetro de interés es $0$ en una aplicación empírica, se quiere asegurar que la probabilidad de obtener una estimación muy cercana al valor real es lo más alta posible. Esto implica que se quiere utilizar el estimador con la varianza más baja de todos los estimadores insesgados, siempre que se preocupe por el insesgado. El teorema de Gauss-Markov establece que, en la clase de estimadores lineales condicionalmente insesgados, el estimador MCO tiene esta propiedad bajo ciertas condiciones.

```{r, 370, eval = my_output == "html", results='asis', echo=F, purl=F}
cat('
<div class = "keyconcept" id="KC5.5">
<h3 class = "right"> Concepto clave 5.5 </h3>          
<h3 class = "left"> El teorema de Gauss-Markov para $\\hat{\\beta}_1$ </h3>
Suponga que las *suposiciones hechas en el Concepto clave 4.3 son válidas* y *que los errores son homocedásticos*. El estimador MCO es el mejor estimador lineal condicionalmente insesgado (AZUL) (en el sentido de la varianza más pequeña) en este entorno.

Echando un vistazo más de cerca a lo que esto significa:

- Los estimadores de $\\beta_1$ que son funciones lineales de $Y_1, \\dots, Y_n$ y que son insesgados condicionalmente en el regresor $X_1, \\dots, X_n$ se pueden escribir como 
$$\\overset{\\sim}{\\beta}_1 = \\sum_{i = 1}^n a_i Y_i$$ 

donde $a_i$ son pesos que pueden depender de $X_i$ pero *no* de $Y_i$.

- Ya se sabe que $\\overset{\\sim}{\\beta}_1$  tiene una distribución de muestreo: $\\overset{\\sim}{\\beta}_1$ es una función lineal de $Y_i$ que son variables aleatorias. Si ahora

$$ E(\\overset{\\sim}{\\beta}_1 | X_1, \\dots, X_n) = \\beta_1,$$

$\\overset{\\sim}{\\beta}_1$ es un estimador lineal imparcial de $\\beta_1$, condicionalmente en $X_1, \\dots, X_n$.

- Se puede preguntar si $\\overset{\\sim}{\\beta}_1$ es también el *mejor* estimador de esta clase; es decir, el más eficiente de todos los estimadores lineales condicionalmente insesgados donde "más eficiente" significa varianza más pequeña. Los pesos $a_i$ juegan un papel importante aquí y resulta que MCO usa los pesos correctos para tener la propiedad AZUL.
</div>
')
```

```{r, 371, eval = my_output == "latex", results='asis', echo=F, purl=F}
cat('\\begin{keyconcepts}[El teorema de Gauss-Markov para $\\hat{\\beta}_1$]{5.5}
Suponga que las \\textit{suposiciones hechas en el Concepto clave 4.3 son válidas} y \\textit{que los errores son homocedásticos}. El estimador MCO es el mejor estimador lineal condicionalmente insesgado (AZUL) (en el sentido de la varianza más pequeña) en este entorno.\\newline

Echando un vistazo más de cerca a lo que esto significa:\\newline

\\begin{itemize}
\\item Los estimadores de $\\beta_1$ que son funciones lineales de $Y_1, \\dots, Y_n$ y que son insesgados condicionalmente en el regresor $X_1, \\dots, X_n$ se pueden escribir como 

$$\\overset{\\sim}{\\beta}_1 = \\sum_{i = 1}^n a_i Y_i $$ 

donde $a_i$ son pesos que pueden depender de $X_i$ pero \\textit{no} de $Y_i$.

\\item Ya se sabe que $\\overset{\\sim}{\\beta}_1$  tiene una distribución de muestreo: $\\overset{\\sim}{\\beta}_1$ es una función lineal de $Y_i$ que son variables aleatorias. Si ahora 

$$ E(\\overset{\\sim}{\\beta}_1 | X_1, \\dots, X_n) = \\beta_1, $$

$\\overset{\\sim}{\\beta}_1$ es un estimador lineal imparcial de $\\beta_1$, condicionalmente en $X_1, \\dots, X_n$.

\\item Se puede preguntar si $\\overset{\\sim}{\\beta}_1$ es también el \\textit{mejor} estimador de esta clase; es decir, el más eficiente de todos los estimadores lineales condicionalmente insesgados donde "más eficiente" significa varianza más pequeña. Los pesos $a_i$ juegan un papel importante aquí y resulta que MCO usa los pesos correctos para tener la propiedad AZUL.

\\end{itemize}
\\end{keyconcepts}
')
```

### Estudio de simulación: Estimador AZUL {-}

Considere el caso de una regresión de $Y_i,\dots,Y_n$ solo en una constante. Aquí, se supone que $Y_i$ es una muestra aleatoria de una población con media $\mu$ y varianza $\sigma^2$. El estimador de MCO en este modelo es simplemente la media de la muestra, consulte el Capítulo \@ref(PMM).

\begin{equation}
\hat{\beta}_1 = \sum_{i=1}^n \underbrace{\frac{1}{n}}_{=a_i} Y_i (\#eq:bluemean)
\end{equation}

Claramente, cada observación está ponderada por 

$$a_i = \frac{1}{n}.$$

y también se sabe que $\text{Var}(\hat{\beta}_1)=\frac{\sigma^2}{n}$.

Ahora se usa **R** para realizar un estudio de simulación que demuestra lo que sucede con la varianza de \@ref(eq:bluemean) si diferentes pesos $w_i = \frac{1 \pm \epsilon}{n}$ se asignan a la mitad de la muestra $Y_1, \dots, Y_n$ en lugar de usar $\frac{1}{n}$, los pesos de MCO.

```{r, 372, fig.align='center', cache=T}
# establecer el tamaño de la muestra y el número de repeticiones
n <- 100      
reps <- 1e5

# elegir épsilon y crear un vector de pesos como se define arriba
epsilon <- 0.8
w <- c(rep((1 + epsilon) / n, n / 2), 
       rep((1 - epsilon) / n, n / 2) )

# extraer una muestra aleatoria y_1, ..., y_n de la distribución normal estándar,
# usar ambos estimadores 1e5 veces y almacene el resultado en los vectores 'ols' y
# 'weightedestimator'

ols <- rep(NA, reps)
weightedestimator <- rep(NA, reps)

for (i in 1:reps) {
  
  y <- rnorm(n)
  ols[i] <- mean(y)
  weightedestimator[i] <- crossprod(w, y)
  
}

# graficar estimaciones de densidad de kernel de las distribuciones de los estimadores:
# MCO
plot(density(ols), 
     col = "purple", 
     lwd = 3, 
     main = "Densidad de MCO y estimador ponderado",
     xlab = "Estimados")

# ponderado
lines(density(weightedestimator), 
      col = "steelblue", 
      lwd = 3) 

# agregar una línea discontinua en 0 y agregar una leyenda al gráfico
abline(v = 0, lty = 2)

legend('topright', 
       c("MCO", "ponderado"), 
       col = c("purple", "steelblue"), 
       lwd = 3)
```

¿Qué conclusión se puede obtener del resultado?

- Ambos estimadores parecen no sesgados: Las medias de sus distribuciones estimadas son cero.
- El estimador que usa ponderaciones que se desvían de las implícitas en los MCO es menos eficiente que el estimador MCO: Existe una mayor dispersión cuando las ponderaciones son $w_i = \frac{1 \pm 0.8}{100}$ en lugar de $w_i=\frac{1}{100}$ según lo requiera la solución MCO.

Por tanto, los resultados de la simulación apoyan el teorema de Gauss-Markov.

## Uso del estadístico t en regresión cuando el tamaño de la muestra es pequeño

Los tres supuestos de MCO discutidos en el Capítulo \@ref(RLR) (ver Concepto clave 4.3) son la base de los resultados de la distribución muestral grande de los estimadores de MCO en el modelo de regresión simple. ¿Qué se puede decir acerca de la distribución de los estimadores y sus estadísticos $t$ cuando el tamaño de la muestra es pequeño y se desconoce la distribución poblacional de los datos? Siempre que se cumplan los tres supuestos de mínimos cuadrados y los errores estén distribuidos normalmente y sean homocedásticos (se hace referecnia a estas condiciones como supuestos de regresión normal homocedásticos), se tienen estimadores distribuidos normalmente y estadísticos de prueba distribuidos en $t$ en muestras pequeñas.

Recuerde la [definición](#thetdist) de una variable distribuida $t$

$$\frac{Z}{\sqrt{W/M}}\sim t_M$$

donde $Z$ es una variable aleatoria normal estándar, $W$ es $\chi^2$ distribuida con $M$ grados de libertad y $Z$ y $W$ son independientes. Ejemplo de la distribución muestral pequeña del estadístico $t$ en los métodos de regresión.

Simulando la distribución de los estadísticos de regresión $t$ basados en un gran número de pequeñas muestras aleatorias, suponiendo $n = 20$, y comparando las distribuciones simuladas con las distribuciones teóricas que deberían ser $t_{18}$, la distribución $t$ con $18$ grados de libertad (recuerde que $\text{DF} = nk-1$).

```{r, 373, fig.align="center", cache=T}
# inicializar dos vectores
beta_0 <- c()
beta_1 <- c()

# muestreo de bucle/estimación/estadístico t
for (i in 1:10000) {
  
  X <- runif(20, 0, 20)
  Y <- rnorm(n = 20, mean = X)
  reg <- summary(lm(Y ~ X))
  beta_0[i] <- (reg$coefficients[1, 1] - 0)/(reg$coefficients[1, 2])
  beta_1[i] <- (reg$coefficients[2, 1] - 1)/(reg$coefficients[2, 2])
  
}

# graficar las distribuciones y comparar con la densidad t_18:
# dividir el área del gráfico
par(mfrow = c(1, 2))

# graficar la densidad simulada de beta_0
plot(density(beta_0), 
     lwd = 2 , 
     main = expression(widehat(beta)[0]), 
     xlim = c(-4, 4))

# agregar la densidad t_18 al gráfico
curve(dt(x, df = 18), 
      add = T, 
      col = "red", 
      lwd = 2, 
      lty = 2)

# graficar la densidad simulada de beta_1
plot(density(beta_1), 
     lwd = 2, 
     main = expression(widehat(beta)[1]), xlim = c(-4, 4)
     )

# agregar la densidad t_18 al gráfico
curve(dt(x, df = 18), 
      add = T, 
      col = "red", 
      lwd = 2, 
      lty = 2) 
```

Los resultados son consistentes con las expectativas: Las distribuciones empíricas de ambos estimadores parecen seguir la distribución teórica $t_{18}$ bastante de cerca.

## Ejercicios {#Ejercicios-5}

```{r, 374, echo=F, purl=F, results='asis'}
if (my_output=="html") {
  cat('
<div  class = "DCexercise">

#### 1. Prueba de dos hipótesis nulas por separado {-}

Considere el modelo de regresión estimado

$$ \\widehat{TestScore} = \\underset{(23.96)}{567.43} - \\underset{(0.85)}{7.15} \\times STR, \\, R^2 = 0.8976, \\, SER=15.19 $$

con errores estándar entre paréntesis.

**Instrucciones:**

  + Calcular el valor $p$ para una prueba $t$ de la hipótesis de que la intersección es cero frente a la alternativa de dos lados que no es cero. Guardar el resultado en <tt>p_int</tt>
  + Calcular el valor $p$ para una prueba $t$ de la hipótesis de que el coeficiente de <tt>STR</tt> es cero frente a la alternativa bilateral de que no es cero. Guardar el resultado en <tt>p_STR</tt>

<iframe src="DCL/ex5_1.html" frameborder="0" scrolling="no" style="width:100%;height:340px"></iframe>

**Sugerencia:** 

  + Ambas hipótesis pueden probarse individualmente mediante una prueba de dos caras. Utilizar <tt>pnorm()</tt> para obtener probabilidades acumuladas de resultados estándar distribuidos normalmente.

</div>')
} else {
  cat('\\begin{center}\\textit{Esta parte interactiva del curso solo está disponible en la versión HTML.}\\end{center}')
}
```

```{r, 375, echo=F, purl=F, results='asis'}
if (my_output=="html") {
  cat('
<div  class = "DCexercise">

#### 2. Dos hipótesis nulas que no puedes rechazar, ¿verdad? {-}

Considere nuevamente el modelo de regresión estimado

$$\\widehat{TestScore} = \\underset{(23.96)}{567.43} - \\underset{(0.85)}{7.15} \\times STR, \\, R^2 = 0.8976, \\,SER=15.19$$

¿Puede rechazar las hipótesis nulas discutidas en el ejercicio de código anterior utilizando pruebas $t$ individuales al nivel de significancia de $5\\%$?

Las variables <tt>t_int</tt> y <tt>t_STR</tt> son los estadísticos $t$. Ambos están disponibles en el entorno de trabajo.

**Instrucciones:**

  + Reunir <tt>t_int</tt> y <tt>t_STR</tt> en una <tt>prueba</tt> vectorial y usar operadores lógicos para verificar si se aplica la regla de rechazo correspondiente.

<iframe src="DCL/ex5_2.html" frameborder="0" scrolling="no" style="width:100%;height:340px"></iframe>

**Sugerencias:**

  + Ambas pruebas son pruebas $t$ de dos caras. El Concepto clave 5.2 resume cómo se realiza una prueba $t$ bilateral.
  + Utilizar <tt>qnorm()</tt> para obtener valores críticos normales estándar.

</div>')}
```

```{r, 376, echo=F, purl=F, results='asis'}
if (my_output=="html") {
  cat('    
<div  class = "DCexercise">

#### 3. Intervalos de confianza {-}

<tt>mod</tt>, el objeto de la clase <tt>lm</tt> que contiene el modelo de regresión estimado 

$$\\widehat{TestScore} = \\underset{(23.96)}{567.43} - \\underset{(0.85)}{7.15} \\times STR, \\, R^2 = 0.8976, \\,SER=15.19$$ 

está disponible en el entorno de trabajo.

**Instrucciones:**

  + Calcuar los intervalos de confianza de $90\\% $ para ambos coeficientes.

<iframe src="DCL/ex5_3.html" frameborder="0" scrolling="no" style="width:100%;height:340px"></iframe>

**Sugerencia:**

  + Utilizar la función <tt>confint()</tt>, consultar <tt>?Confint</tt>. El argumento <tt>level</tt> establece el nivel de confianza que se utilizará.

</div>')}
```

```{r, 377, echo=F, purl=F, results='asis'}
if (my_output=="html") {
  cat('
<div  class = "DCexercise">

#### 4. Un intervalo de confianza para la media I {-}

Considere el modelo de regresión $$Y_i = \\beta_1 + u_i$$ donde $Y_i \\sim \\mathcal{N}(\\mu, \\sigma^2)$. Siguiendo la discusión que precede a la ecuación \\@ref(eq:KI), un intervalo de confianza de $95\\%$ para la media de $Y_i$ se puede calcular como

$$CI^{\\mu}_{0.95} = \\left[\\hat\\mu - 1.96 \\times \\frac{\\sigma}{\\sqrt{n}}; \\, \\hat\\mu + 1.96 \\times \\frac{\\sigma}{\\sqrt{n}} \\right].$$

**Instrucciones:**

  + Muestra $n = 100$ observaciones de una distribución normal con varianza $100$ y media $10$.
  + Utilizar la muestra para estimar $\\beta_1$. Guardar la estimación en <tt>mu_hat</tt>.
  + Suponga que se conoce $\\sigma^2 = 100$. Reemplazar las <tt>NA</tt> en el código siguiente para obtener un intervalo de confianza de $95\\%$ para la media de $Y_i$.

<iframe src="DCL/ex5_4.html" frameborder="0" scrolling="no" style="width:100%;height:340px"></iframe>

**Sugerencia:**

  + Utilizar la función <tt>confint()</tt>, consultar <tt>?Confint</tt>. El argumento <tt>level</tt> establece el nivel de confianza.

</div>')}
```

```{r, 378, echo=F, purl=F, results='asis'}
if (my_output=="html") {
  cat('
<div  class = "DCexercise">

#### 5. Un intervalo de confianza para la media II {-}

Por razones históricas, algunas funciones de <tt>R</tt> que se usan para obtener inferencias sobre los parámetros del modelo, entre ellas <tt>confint()</tt> y <tt>summary()</tt>, se basan en la distribución $t$ en lugar de utilizar la aproximación normal de muestra grande. Esta es la razón por la que para tamaños de muestra pequeños (y por lo tanto pequeños grados de libertad), los valores de $p$ y los intervalos de confianza informados por estas funciones se desvían de los calculados utilizando valores críticos o probabilidades acumuladas de la distribución normal estándar.

El intervalo de confianza de $95\\%$ para la media del ejercicio anterior es $[9.13, 13.05]$.

**Instrucciones:**

  + Se han asignado 100 observaciones muestreadas de una distribución normal con $\\mu = 10$ y $\\sigma^2 = 100$ al vector <tt>s</tt> que está disponible en el entorno de trabajo.
  + Configurar un modelo de regresión adecuado para estimar la media de las observaciones en <tt>s</tt>. Luego usar <tt>confint()</tt> para calcular un intervalo de confianza de $95\\%$ para la media.
  + (Comprobar que el resultado sea diferente del intervalo informado anteriormente).

<iframe src="DCL/ex5_5.html" frameborder="0" scrolling="no" style="width:100%;height:340px"></iframe>

</div>')}
```

```{r, 379, echo=F, purl=F, results='asis'}
if (my_output=="html") {
  cat('
<div  class = "DCexercise">

#### 6. Regresión en una variable ficticia I {-}

El capítulo \\@ref(RCXVB) analiza la regresión cuando $X$ es una variable ficticia. Se ha utilizado un ciclo <tt>for()</tt> para generar una variable binaria que indica si un distrito escolar en el conjunto de datos de <tt>CASchools</tt> tiene una proporción alumno-maestro por debajo de $20$. Aunque es instructivo usar un bucle para esto, existen formas alternativas de lograr lo mismo con menos líneas de código.

Un <tt>data.frame</tt> <tt>DF</tt> con $100$ observaciones de una variable <tt>X</tt> está disponible en el entorno de trabajo.

**Instrucciones:**

  + Use <tt>ifelse()</tt> para generar un vector binario <tt>ficticio</tt> que indique si las observaciones en <tt>X</tt> son *positivas*.

  + Agregar <tt>dummy</tt> al <tt>data.frame</tt> <tt>DF</tt>.

<iframe src="DCL/ex5_6.html" frameborder="0" scrolling="no" style="width:100%;height:340px"></iframe>

</div>')}
```

```{r, 380, echo=F, purl=F, results='asis'}
if (my_output=="html") {
  cat('
<div  class = "DCexercise">

#### 7. Regresión en una variable ficticia II {-}

Un <tt>data.frame</tt> <tt>DF</tt> con $100$ observaciones en <tt>Y</tt> y la variable binaria <tt>D</tt> del ejercicio anterior está disponible en el entorno de trabajo.

**Instrucciones:**

  + Calcular las medias muestrales específicas del grupo de las observaciones en <tt>Y</tt>: Guardar la media de las observaciones en <tt>Y</tt> donde <tt>dummy == 1</tt> a <tt>mu_Y_D1</tt> y asignar la media de esas observaciones con <tt>D == 0</tt> a <tt>mu_Y_D0</tt>.

  + Usar <tt>lm()</tt> para retroceder <tt>Y</tt> en <tt>D</tt>; es decir, estimar los coeficientes en el modelo $$Y_i = \\beta_0 + \\beta_1 \\times D_i + u_i.$$

  + Verificar que las estimaciones de los coeficientes $\\beta_0$ y $\\beta_1$ reflejen medias de muestra específicas. ¿Puede decir cuál (no es necesario enviar el código)?

<iframe src="DCL/ex5_7.html" frameborder="0" scrolling="no" style="width:100%;height:340px"></iframe>

</div>')}
```

```{r, 381, echo=F, purl=F, results='asis'}
if (my_output=="html") {
  cat('
<div  class = "DCexercise">

#### 8. Regresión en una variable ficticia III {-}

En este ejercicio, se deben visualizar algunos de los resultados del modelo de regresión ficticia $$\\widehat{Y}_i = -0.66 + 1.43 \\times D_i$$ estimado en el ejercicio anterior.

Un <tt>data.frame</tt> <tt>DF</tt> con 100 observaciones en <tt>X</tt> y la variable binaria <tt>dummy</tt> así como el objeto modelo <tt>dummy_mod</tt> del ejercicio anterior están disponibles en el entorno de trabajo.

**Instrucciones:**

  + Dibujar un diagrama visualmente atractivo de las observaciones en $Y$ y $D$ basado en el fragmento de código proporcionado en <tt>Script.R</tt>. Reemplazar <tt>???</tt> por las expresiones correctas.

  + Agregar la línea de regresión al gráfico.

<iframe src="DCL/ex5_8.html" frameborder="0" scrolling="no" style="width:100%;height:340px"></iframe>

</div>')}
```

```{r, 382, echo=F, purl=F, results='asis'}
if (my_output=="html") {
  cat('
<div  class = "DCexercise">

#### 9. Brecha salarial de género I {-}

El conjunto de datos de corte transversal <tt>CPS1985</tt> es una submuestra de la *Encuesta de población actual* de mayo de 1985 realizada por la *Oficina del Censo de EE.UU.* Que contiene observaciones sobre, entre otras cosas, el salario y el género de los empleados.

<tt>CPS1985</tt> es parte del paquete <tt>AER</tt>.

**Instrucciones:**

  + Adjuntar el paquete <tt>AER</tt> y cargar el conjunto de datos <tt>CPS1985</tt>.

  + Estimar el modelo de regresión ficticia $$wage_i = \\beta_0 + \\beta_1 \\cdot female_i + u_i$$ donde

    \\begin{align*}
      female_i = 
      \\begin{cases}
        1, & \\text{si empleada} \\, i \\, \\text{es female,} \\\\
        0, & \\text{si empleado} \\, i \\,  \\text{es male.}
      \\end{cases}
    \\end{align*}
  
  + Guardar el resultado en <tt>wage_mod</tt>.

<iframe src="DCL/ex5_9.html" frameborder="0" scrolling="no" style="width:100%;height:340px"></iframe>

</div>')}
```

```{r, 383, echo=F, purl=F, results='asis'}
if (my_output=="html") {
  cat('
<div  class = "DCexercise">

#### 10. Brecha salarial de género II {-}

La regresión salarial del ejercicio anterior arroja $$\\widehat{wage}_i = 9.995 - 2.116 \\cdot female_i.$$

El objeto modelo <tt>dummy_mod</tt> está disponible en el entorno de trabajo.

**Instrucciones:**

  + Probar la hipótesis de que el coeficiente de $female_i$ es cero frente a la alternativa de que no es cero. La hipótesis nula implica que no existe brecha salarial de género. Utilizar el estimador robusto de heterocedasticidad propuesto por @white1980.

<iframe src="DCL/ex5_10.html" frameborder="0" scrolling="no" style="width:100%;height:340px"></iframe>

**Sugerencias:**

  + <tt>vcovHC()</tt> calcula estimaciones robustas a la heterocedasticidad de la matriz de covarianza de los estimadores de coeficientes para el modelo proporcionado. El estimador propuesto por @white1980 se calcula si establece <tt>type = "HC0"</tt>.

  + La función <tt>coeftest()</tt> realiza pruebas de significancia para los coeficientes en los objetos del modelo. Se puede proporcionar una matriz de covarianza usando el argumento <tt>vcov.</tt>.

</div>')}
```

```{r, 384, echo=F, purl=F, results='asis'}
if (my_output=="html") {
  cat('
<div  class = "DCexercise">

#### 11. Cálculo de errores estándar robustos de heterocedasticidad {-}

En el modelo de regresión simple, la matriz de covarianza de los estimadores de coeficientes se denota

\\begin{equation}
\\text{Var}
  \\begin{pmatrix}
    \\hat\\beta_0 \\
    \\hat\\beta_1
  \\end{pmatrix} = 
\\begin{pmatrix}
  \\text{Var}(\\hat\\beta_0) & \\text{Cov}(\\hat\\beta_0,\\hat\\beta_1) \\\\
\\text{Cov}(\\hat\\beta_0,\\hat\\beta_1) & \\text{Var}(\\hat\\beta_1)
\\end{pmatrix}
\\end{equation}

La función <tt>vcovHC</tt> se puede utilizar para obtener estimaciones de esta matriz para un objeto modelo de interés.

<tt>dummy_mod</tt>, un objeto modelo que contiene la regresión salarial tratada en los Ejercicios 9 y 10 está disponible en el entorno de trabajo.

**Instrucciones:**

  + Calcular errores estándar robustos del tipo <tt>HC1</tt> para los estimadores de coeficientes en el objeto modelo <tt>dummy_mod</tt>. Almacenar los errores estándar en un vector llamado <tt>rob_SEs</tt>.

<iframe src="DCL/ex5_11.html" frameborder="0" scrolling="no" style="width:100%;height:340px"></iframe>

**Sugerencias**

  + Los errores estándar que se buscan se pueden obtener tomando la raíz cuadrada de los elementos diagonales de la matriz de covarianza estimada.
  + <tt>diag(A)</tt> devuelve los elementos diagonales de la matriz <tt>A</tt>.

</div>')}
```

```{r, 385, echo=F, purl=F, results='asis'}
if (my_output=="html") {
  cat('
<div  class = "DCexercise">

#### 12. Intervalos de confianza robustos {-}

La función <tt>confint()</tt> calcula los intervalos de confianza para los modelos de regresión que utilizan errores estándar de solo homocedasticidad, por lo que esta función no es una opción cuando hay heterocedasticidad.

La función <tt>Rob_CI()</tt> en <tt>script.R</tt> está destinada a calcular e informar intervalos de confianza robustos a la heterocedasticidad para ambos coeficientes del modelo en un modelo de regresión simple.

<tt>gender_mod</tt>, un objeto modelo que contiene la regresión salarial tratada en los ejercicios anteriores está disponible en el entorno de trabajo.

**Instrucciones:**

  + Completar el código de <tt>Rob_CI()</tt> dado en <tt>Script.R</tt> de modo que se devuelvan los límites superior e inferior de los intervalos de confianza robustos de $95\\%$. Utilizar errores estándar del tipo <tt>HC1</tt>.
  + Utilizar la función <tt>Rob_CI()</tt> para obtener intervalos de confianza de $95\\%$ para los coeficientes del modelo en <tt>dummy_mod</tt>.

<iframe src="DCL/ex5_12.html" frameborder="0" scrolling="no" style="width:100%;height:340px"></iframe>

</div>')}
```

```{r, 386, echo=F, purl=F, results='asis'}
if (my_output=="html") {
  cat('
<div  class = "DCexercise">

#### 13. Un pequeño estudio de simulación --- I {-}

Considere el proceso de generación de datos (DGP)

\\begin{align}
  X_i \\sim& \\, \\mathcal{U}[2,10], \\notag \\\\
  e_i \\sim& \\, \\mathcal{N}(0, X_i), \\notag \\\\
  Y_i =& \\, \\beta_1 X_i + e_i, (\\#eq:asss)  
\\end{align}

donde $\\mathcal{U}[2,10]$ denota la distribución uniforme en el intervalo $[2,10]$ y $\\beta_1=2$.

Observe que los errores $e_i$ son heterocedásticos, ya que la varianza de $e_i$ es una función de $X_i$.

**Instrucciones:**

  + Escribir una función <tt>DGP_OLS</tt> que genere una muestra $(X_i,Y_i)$, $i=1,…,100$ usando el DGP anterior y devuelva la estimación de MCO de $\\beta_1$ basada en esta muestra.

<iframe src="DCL/ex5_13.html" frameborder="0" scrolling="no" style="width:100%;height:340px"></iframe>

**Sugerencia:**

  + <tt>runif()</tt> se puede utilizar para obtener muestras aleatorias de una distribución uniforme, consultar <tt>?runif</tt>.

</div>')}
```

```{r, 387, echo=F, purl=F, results='asis'}
if (my_output=="html") {
  cat('
<div  class = "DCexercise">

#### 14. Un pequeño estudio de simulación --- II {-}

La función <tt>DGP_OLS()</tt> del ejercicio anterior está disponible en el entorno de trabajo.

**Instrucciones:**

  + Utilizar <tt>replicate()</tt> para generar una muestra de $1000$ estimaciones de MCO $\\widehat{\\beta}_1$ usando la función <tt>DGP_OLS</tt>. Almacenar las estimaciones en un vector llamado <tt>estimates</tt>.
  + Luego, estimar la varianza de $\\widehat{\\beta}_1$ en \\@ref(eq:asss): Calcular la varianza muestral de las estimaciones de $1000$ MCO en <tt>estimates</tt>. Almacenar el resultado en <tt>est_var_OLS</tt>.

<iframe src="DCL/ex5_14.html" frameborder="0" scrolling="no" style="width:100%;height:340px"></iframe>

</div>')}
```

```{r, 388, echo=F, purl=F, results='asis'}
if (my_output=="html") {
  cat('
<div  class = "DCexercise">

#### 15. Un pequeño estudio de simulación --- III {-}

Según el teorema de Gauss-Markov, el estimador MCO en modelos de regresión lineal ya no es el estimador más eficiente entre los estimadores lineales condicionalmente insesgados cuando hay heterocedasticidad. En otras palabras, el estimador MCO pierde la propiedad AZUL cuando se viola el supuesto de homocedasticidad.

Resulta que MCO aplicado a las observaciones ponderadas $(w_i X_i, w_i Y_i)$ donde $w_i=\\frac{1}{\\sigma_i}$ es el estimador AZUL bajo heterocedasticidad. Este estimador se denomina estimador de *Mínimos Cuadrados Ponderados* (MCP). Por tanto, cuando hay heterocedasticidad, el estimador MCP tiene una varianza menor que MCO.

La función <tt>DGP_OLS()</tt> y la varianza estimada <tt>est_var_OLS</tt> de los ejercicios anteriores están disponibles en el entorno de trabajo.

**Instrucciones:**

  + Escribir una función <tt>DGP_WLS()</tt> que genere muestras de $100$ usando el DGP presentado en el ejercicio 13 y devuelva la estimación de MCP de $\\beta_1$. Tratar $\\sigma_i$ como conocido; es decir, establecer $w_i=\\frac{1}{\\sqrt{X_i}}$.
  + Repetir el ejercicio 14 usando <tt>DGP_WLS()</tt>. Almacenar la estimación de la varianza en <tt>est_var_GLS</tt>.
  + Comparar las varianzas estimadas <tt>est_var_OLS</tt> y <tt>est_var_GLS</tt> utilizando operadores lógicos (<tt> < </tt> o <tt> > </tt>).

<iframe src="DCL/ex5_15.html" frameborder="0" scrolling="no" style="width:100%;height:340px"></iframe>

**Sugerencias:**

  + <tt>DGP_WLS()</tt> se puede obtener usando un código modificado de <tt>DGP_OLS()</tt>.
  + Recordar que las funciones son objetos y se puede imprimir el código de una función en la consola.

</div>')}
```

<!--chapter:end:Capitulo_06.Rmd-->

# Modelos de regresión con varios regresores {#MRVR}

```{r, echo = F}
options(knitr.duplicate.label = "allow")
```

```{r, 389, child="_setup.Rmd"}
```

```{r, 390, eval=knitr::opts_knit$get("rmarkdown.pandoc.to") == "html", results='asis', echo=FALSE}
cat('<hr style="background-color:#03193b;height:2px">')
```

A continuación, se presentan modelos de regresión lineal que utilizan más de una variable explicativa y se analizan conceptos clave importantes en la regresión múltiple. A medida que se amplia el alcance más allá de la relación de solo dos variables (la variable dependiente y un regresor único), surgen algunos problemas potenciales nuevos como *multicolinealidad* y *sesgo de variable omitida* (SVO). En particular, este capítulo trata de las variables omitidas y su implicación para la interpretación causal de los coeficientes estimados por MCO.

Naturalmente, se discutrá la estimación de modelos de regresión múltiple usando **R**. También se ilustrará la importancia del uso cuidadoso de modelos de regresión múltiple a través de estudios de simulación que demuestran las consecuencias del uso de regresores altamente correlacionados o modelos mal especificados.

Los paquetes **AER** [@R-AER] y **MASS** [@R-MASS] son necesarios para reproducir el código presentado en este capítulo. Asegúrese de que el siguiente fragmento de código se ejecute sin errores.

```{r, 391, warning=FALSE, message=FALSE, eval=FALSE}
library(AER)
library(MASS)
```

## Sesgo variable omitido

El análisis anterior de la relación entre la puntuación de la prueba y el tamaño de la clase discutido en los Capítulos \@ref(RLR) y \@ref(PHICMRLS) tiene un defecto importante: Se ignoran otros determinantes de la variable dependiente (puntuación de la prueba) que se correlacionan con el regresor (tamaño de la clase). Se debe recordar que las influencias sobre la variable dependiente que no son capturadas por el modelo se recogen en el término de error, que hasta ahora se ha supuesto que no está correlacionado con el regresor. Sin embargo, este supuesto se viola si se excluyen los determinantes de la variable dependiente que varían con el regresor. Esto podría inducir un sesgo de estimación; es decir, la media de la distribución muestral del estimador de MCO ya no es igual a la media verdadera. En el ejemplo, por lo tanto, se estima erróneamente el efecto causal en los puntajes de las pruebas de un cambio de unidad en la proporción alumno-maestro, en promedio. Este problema se denomina *sesgo de variable omitida* (SVO) y se resume en el Concepto clave 6.1.

```{r, 392, eval = my_output == "html", results='asis', echo=F, purl=F}
cat('
<div class = "keyconcept" id="KC6.1">
<h3 class = "right"> Concepto clave 6.1 </h3>          
<h3 class = "left"> Sesgo de variable omitida en regresión con un regresor único </h3>

El sesgo de variable omitida es el sesgo en el estimador de MCO que surge cuando el regresor, $X$, se *correlaciona* con una variable omitida. Para que ocurra el sesgo de la variable omitida, se deben cumplir dos condiciones:

1. $X$ está correlacionado con la variable omitida.
2. La variable omitida es un determinante de la variable dependiente $Y$.

Juntos, 1. y 2. dan como resultado una violación del primer supuesto de MCO $E(u_i\\vert X_i) = 0$. Formalmente, el sesgo resultante se puede expresar como

$$ \\hat\\beta_1 \\xrightarrow[]{p} \\beta_1 + \\rho_{Xu} \\frac{\\sigma_u}{\\sigma_X}. \\tag{6.1} $$

El SVO es un problema que no se puede resolver aumentando el número de observaciones utilizadas para estimar $\\beta_1$, como $\\hat\\beta_1$ es inconsistente (<a href="#mjx-eqn-6.1">6.1</a>) : SVO evita que el estimador converja en probabilidad con el valor verdadero del parámetro. La fuerza y la dirección del sesgo están determinadas por $\\rho_{Xu}$, la correlación entre el término de error y el regresor.

</div>
')
```

```{r, 393, eval = my_output == "latex", results='asis', echo=F, purl=F}
cat('\\begin{keyconcepts}[Sesgo de variable omitida en regresión con un regresor único]{6.1}
El sesgo de variable omitida es el sesgo en el estimador de MCO que surge cuando el regresor, $X$, se \\textit{correlaciona} con una variable omitida. Para que ocurra el sesgo de la variable omitida, se deben cumplir dos condiciones:\\newline

\\begin{enumerate}
\\item $X$ está correlacionado con la variable omitida. 
\\item La variable omitida es un determinante de la variable dependiente $Y$.
\\end{enumerate}\\vspace{0.5cm}

Juntos, 1. y 2. dan como resultado una violación del primer supuesto de MCO $E(u_i\\vert X_i) = 0$. Formalmente, el sesgo resultante se puede expresar como

\\begin{align}
\\hat\\beta_1 \\xrightarrow[]{p} \\beta_1 + \\rho_{Xu} \\frac{\\sigma_u}{\\sigma_X}.
\\end{align}

El SVO es un problema que no se puede resolver aumentando el número de observaciones utilizadas para estimar $\\beta_1$, como $\\hat\\beta_1$ es inconsistente (<a href="#mjx-eqn-6.1">6.1</a>) : SVO evita que el estimador converja en probabilidad con el valor verdadero del parámetro. La fuerza y la dirección del sesgo están determinadas por $\\rho_{Xu}$, la correlación entre el término de error y el regresor.

\\end{keyconcepts}
')
```

En el ejemplo del puntaje de la prueba y el tamaño de la clase, es fácil encontrar variables que puedan causar tal sesgo, si se omiten del modelo. Una variable muy relevante podría ser el porcentaje de estudiantes de inglés en el distrito escolar: es plausible que la capacidad de hablar, leer y escribir en inglés sea un factor importante para un aprendizaje exitoso. Por lo tanto, es probable que los estudiantes que todavía están aprendiendo inglés obtengan peores resultados en las pruebas que los hablantes nativos. Además, es concebible que la proporción de estudiantes que aprenden inglés sea mayor en los distritos escolares donde el tamaño de las clases es relativamente grande: piense en los distritos urbanos pobres donde viven muchos inmigrantes. 
<br>

Si se piensa en un posible sesgo inducido al omitir la proporción de estudiantes que aprenden inglés ($PctEL$) en vista de (6.1). Cuando el modelo de regresión estimado no incluye $PctEL$ como regresor aunque el verdadero proceso de generación de datos (DGP) es

$$ TestScore = \beta_0 + \beta_1 \times STR + \beta_2 \times PctEL \tag{6.2}$$ 

donde $STR$ y $PctEL$ están correlacionados, se tiene

$$\rho_{STR,PctEL}\neq0.$$

Se puede investigar esto usando **R**. Después de definir las variables, se puede calcular la correlación entre $STR$ y $PctEL$, así como la correlación entre $STR$ y $TestScore$.

```{r, 394, message=F, warnings=F}
# cargar el paquete AER
library(AER)

# cargar el conjunto de datos
data(CASchools)   

# definir variables
CASchools$STR <- CASchools$students/CASchools$teachers       
CASchools$score <- (CASchools$read + CASchools$math)/2

# calcular correlaciones
cor(CASchools$STR, CASchools$score)
cor(CASchools$STR, CASchools$english)
```

El hecho de que $\widehat{\rho}_{STR, Testscore} = -0.2264$ es motivo de preocupación de que omitir $PctEL$ conduce a una estimación con sesgo negativo $\hat\beta_1$, ya que esto indica que $\rho_{Xu} < 0$. Como consecuencia, se espera que $\hat\beta_1$, el coeficiente de $STR$, sea demasiado grande en valor absoluto. Dicho de otra manera, la estimación de MCO de $\hat\beta_1$ sugiere que las clases pequeñas mejoran los puntajes de las pruebas, pero que el efecto de las clases pequeñas se sobrestima, ya que captura el efecto de tener menos estudiantes de inglés también.

¿Qué pasa con la magnitud de $\hat\beta_1$ si se suma la variable $PctEL$ a la regresión? En otras palabras, si se estima el siguiente modelo

$$ TestScore = \beta_0 + \beta_1 \times STR + \beta_2 \times PctEL + u $$

¿Qué se espera del signo de $\hat\beta_2$, el coeficiente estimado en $PctEL$? Siguiendo el razonamiento anterior, se debría terminar con una estimación de coeficiente negativa pero mayor a $\hat\beta_1$ que antes y una estimación negativa $\hat\beta_2$.

Se necesita estimar ambos modelos de regresión y compararlos. Reealizar una regresión múltiple en **R** es sencillo. Uno puede simplemente agregar variables adicionales al lado derecho del argumento **formula** de la función **lm()** usando sus nombres y el operador **+**.

```{r, 395}
# estimar ambos modelos de regresión
mod <- lm(score ~ STR, data = CASchools) 
mult.mod <- lm(score ~ STR + english, data = CASchools)

# imprimir los resultados en la consola
mod
mult.mod
```

Se encontró que los resultados son consistentes con las expectativas.

La siguiente sección analiza algunas teorías sobre modelos de regresión múltiple.

## El modelo de regresión múltiple {#MRM}

El modelo de regresión múltiple extiende el concepto básico del modelo de regresión simple discutido en los Capítulos \@ref(RLR) y \@ref(PHICMRLS). Un modelo de regresión múltiple permite estimar el efecto en $Y_i$ de cambiar un regresor $X_{1i}$ si los regresores restantes $X_{2i},X_{3i}\dots,X_{ki}$ *no varían*. De hecho, ya se ha realizado la estimación del modelo de regresión múltiple (<a href="#mjx-eqn-6.2">6.2</a>) usando **R** en la sección anterior. La interpretación del coeficiente de la proporción de estudiantes por maestro es el efecto en los puntajes de las pruebas de un cambio de una unidad de la proporción de estudiantes por maestro si el porcentaje de estudiantes de inglés se mantiene constante.

Al igual que en el modelo de regresión simple, se asume que la verdadera relación entre $Y$ y $X_{1i},X_{2i}\dots\dots,X_{ki}$ es lineal. En promedio, esta relación viene dada por la función de regresión poblacional

$$ E(Y_i\vert X_{1i}=x_1, X_{2i}=x_2,  X_{3i}=x_3,\dots, X_{ki}=x_k) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \dots + \beta_k x_k. \tag{6.3} $$

Como en el modelo de regresión simple, la relación 

$$Y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + \beta_3 X_{3i} + \dots + \beta_k X_{ki}$$ 

no se mantienen exactamente, ya que existen influencias perturbadoras en la variable dependiente $Y$ que no se puede observar como variables explicativas. Por lo tanto, se agrega un término de error $u$ que representa las desviaciones de las observaciones de la línea de regresión de la población a (<a href="#mjx-eqn-6.3">6.3</a>). Esto produce el modelo de regresión múltiple de población 

$$ Y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + \beta_3 X_{3i} + \dots + \beta_k X_{ki} + u_i, \ i=1,\dots,n. \tag{6.4} $$ 

El Concepto clave 6.2 resume los conceptos centrales del modelo de regresión múltiple.

```{r, 396, eval = my_output == "html", results='asis', echo=F, purl=F}
cat('
<div class = "keyconcept" id="KC6.2">
<h3 class = "right"> Concepto clave 6.2 </h3>          
<h3 class = "left"> El modelo de regresión múltiple </h3>

El modelo de regresión múltiple es

$$ Y_i = \\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\beta_3 X_{3i} + \\dots + \\beta_k X_{ki} + u_i \\ \\ , \\ \\ i=1,\\dots,n.  $$ 

Las designaciones son similares a las del modelo de regresión simple:

- $Y_i$ es la observación $i^{th}$ en la variable dependiente. Las observaciones sobre los regresores $k$ se indican mediante $X_{1i},X_{2i},\\dots,X_{ki}$ y $u_i$ es el término de error.
- La relación promedio entre $Y$ y los regresores está dada por la línea de regresión poblacional

$$ E(Y_i\\vert X_{1i}=x_1, X_{2i}=x_2,  X_{3i}=x_3,\\dots, X_{ki}=x_k) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\dots + \\beta_k x_k. $$

- $\\beta_0$ es la intersección; es el valor esperado de $Y$ cuando todos los $X$ son iguales a $0$. $\\beta_j \\ , \\ j=1,\\dots,k$ son los coeficientes en $X_j \\ , \\ j=1,\\dots,k$. $\\beta_1$ mide el cambio esperado en $Y_i$ que resulta de un cambio de una unidad en $X_{1i}$ mientras se mantienen constantes todos los demás regresores.
</div>
')
```

```{r, 397, eval = my_output == "latex", results='asis', echo=F, purl=F}
cat('\\begin{keyconcepts}[El modelo de regresión múltiple]{6.2}

El modelo de regresión múltiple es

$$ Y_i = \\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\beta_3 X_{3i} + \\dots + \\beta_k X_{ki} + u_i \\ \\ , \\ \\ i=1,\\dots,n.  $$ 

Las designaciones son similares a las del modelo de regresión simple:

\\begin{itemize}
\\item $Y_i$ es la observación $i^{th}$ en la variable dependiente. Las observaciones sobre los regresores $k$ se indican mediante $X_{1i},X_{2i},\\dots,X_{ki}$ y $u_i$ es el término de error.
\\item La relación promedio entre $Y$ y los regresores está dada por la línea de regresión poblacional

$$ E(Y_i\\vert X_{1i}=x_1, X_{2i}=x_2,  X_{3i}=x_3,\\dots, X_{ki}=x_k) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\dots + \\beta_k x_k. $$

\\item $\\beta_0$ es la intersección; es el valor esperado de $Y$ cuando todos los $X$ son iguales a $0$. $\\beta_j \\ , \\ j=1,\\dots,k$ son los coeficientes en $X_j \\ , \\ j=1,\\dots,k$. $\\beta_1$ mide el cambio esperado en $Y_i$ que resulta de un cambio de una unidad en $X_{1i}$ mientras se mantienen constantes todos los demás regresores.
\\end{itemize}
\\end{keyconcepts}
')
```

¿Cómo se pueden estimar los coeficientes del modelo de regresión múltiple (<a href="#mjx-eqn-6.4">6.4</a>)? No se entrará demasiado en detalles sobre este tema, ya que el enfoque está en el uso de **R**. Sin embargo, cabe señalar que, al igual que en el modelo de regresión simple, los coeficientes del modelo de regresión múltiple se pueden estimar mediante MCO. Como en el modelo simple, se busca minimizar la suma de errores al cuadrado eligiendo estimaciones $b_0,b_1,\dots,b_k$ para los coeficientes $\beta_0,\beta_1,\dots,\beta_k$ tales que

$$\sum_{i=1}^n (Y_i - b_0 - b_1 X_{1i} - b_2 X_{2i} - \dots -  b_k X_{ki})^2 \tag{6.5}$$

se minimiza. Se debe tener en cuenta que (<a href="#mjx-eqn-6.5">6.5</a>) es simplemente una extensión de $SSR$ en el caso de un solo regresor y una constante. Por tanto, los estimadores que minimizan (<a href="#mjx-eqn-6.5">6.5</a>) se denominan $\hat\beta_0,\hat\beta_1,\dots,\hat\beta_k$ y, como en el modelo de regresión simple, se llaman estimadores de mínimos cuadrados ordinarios de $\beta_0,\beta_1,\dots,\beta_k$. Para el valor predicho de $Y_i$ dados los regresores y las estimaciones $\hat\beta_0,\hat\beta_1,\dots,\hat\beta_k$ se tiene:

$$ \hat{Y}_i = \hat\beta_0 + \hat\beta_1 X_{1i} + \dots +\hat\beta_k X_{ki}. $$

La diferencia entre $Y_i$ y su valor predicho $\hat{Y}_i$ se denomina MCO residual de la observación $i$: $\hat{u} = Y_i - \hat{Y}_i$.

Para obtener más información sobre la teoría detrás de la regresión múltiple, se presenta una derivación del estimador MCO en el modelo de regresión múltiple utilizando notación matricial.

Volviendo al ejemplo de los resultados de las pruebas y el tamaño de las clases. El objeto de modelo estimado es **mult.mod**. En cuanto a los modelos de regresión simple, se puede usar **summary()** para obtener información sobre los coeficientes estimados y las estadísticas del modelo.

```{r, 398}
summary(mult.mod)$coef
```

Entonces, el modelo de regresión múltiple estimado es

$$ \widehat{TestScore} = 686.03 - 1.10 \times STR - 0.65 \times PctEL \tag{6.6}.  $$

A diferencia del modelo de regresión simple donde los datos se pueden representar por puntos en el sistema de coordenadas bidimensionales, ahora se tienen tres dimensiones. Por tanto, las observaciones se pueden representar mediante puntos en el espacio tridimensional. Por lo tanto (<a href="#mjx-eqn-6.6">6.6</a>) ahora ya no es más una línea de regresión sino un *plano de regresión*. Esta idea se extiende a dimensiones superiores cuando se amplian aún más el número de regresores $k$. Luego, se dice que el modelo de regresión se puede representar mediante un hiperplano en el espacio dimensional $k+1$. Ya es difícil imaginar un espacio así con $k = 3$ y lo mejor es seguir con la idea general de que, en el modelo de regresión múltiple, la variable dependiente se explica por una *combinación lineal de regresores*. Sin embargo, en el presente caso se puede visualizar la situación. La siguiente figura es una visualización 3D interactiva de los datos y el plano de regresión estimado (<a href="#mjx-eqn-6.6">6.6</a>).

<center>
```{r plotlyfig, 399, echo=F, fig.height=5, fig.width=9, warning=F, message=F}
if(knitr::opts_knit$get("rmarkdown.pandoc.to") == "html"){
library(plotly)

y <- CASchools$score
x1 <- CASchools$STR
x2 <- CASchools$english
df <- data.frame(y, x1, x2)

reg <- lm(y ~ x1 + x2)
cf.mod <- coef(reg)

x1.seq <- seq(min(x1),max(x1),length.out=25)
x2.seq <- seq(min(x2),max(x2),length.out=25)
z <- t(outer(x1.seq, x2.seq, function(x,y) cf.mod[1] + cf.mod[2]*x + cf.mod[3]*y))


rbPal <- colorRampPalette(c('red','blue'))
cols <- rbPal(10)[as.numeric(cut(abs(y-reg$fitted.values), breaks = 10))]

m <- list(
  t = 5
)

p <- plot_ly(x=x1.seq, 
             y=x2.seq, 
             z=z,
             colors = "red",
             opacity = 0.9,
             name = "Reg.Plane",
             type="surface"
     ) %>%
  add_trace(data=df, name='CASchools', x=x1, y=x2, z=y, mode="markers", type="scatter3d",
            marker = list(color=cols, opacity=0.85, symbol=105, size=4)
  ) %>%
  hide_colorbar() %>%
  layout(
    margin = m,
    showlegend = FALSE,
    scene = list(
      aspectmode = "manual", aspectratio = list(x=1, y=1.3, z=1),
      xaxis = list(title = "STR"),
      yaxis = list(title = "PctEL"),
      zaxis = list(title = "TestScore"),
      camera = list(eye = list(x = -2,y = -0.1, z=0.05),
                    center = list(x = 0,
                                  y = 0,
                                  z = 0
                                  )
               )
    )
    
  )

p <- p %>% config(showLink = F, displayModeBar = F);p
}
```
</center>

```{r, 400, echo=F, purl=F, eval=my_output=="latex", results='asis'}
cat('\\begin{center}\\textit{This interactive part of the book is only available in the HTML version.}\\end{center}')
```

Se puede observar que el plano de regresión estimado se ajusta razonablemente bien a los datos, al menos respecto a la forma y posición espacial de los puntos. El color de los marcadores es un indicador de la desviación absoluta del plano de regresión predicho. Las observaciones que tienen un color más rojizo se encuentran cerca del plano de regresión, mientras que el color cambia a azul con la distancia creciente. Una anomalía que se puede ver en el gráfico es que podría haber heterocedasticidad: Se puede ver que la dispersión de los errores de regresión cometidos; es decir, la distancia de las observaciones al plano de regresión tiende a disminuir a medida que aumenta la proporción de estudiantes que aprenden inglés.

## Medidas de ajuste en regresión múltiple {#MARM}

En la regresión múltiple, las estadísticas de resumen comunes son $SER$, $R^2$ y el $R^2$ ajustado.

Tomando el código de la Sección \@ref(MRM), simplemente se debe usar **summary(mult.mod)** para obtener $SER$, $R^2$ y ajustado $R^2$. Para modelos de regresión múltiple, $SER$ se calcula como

$$ SER = s_{\hat u} = \sqrt{s_{\hat u}^2} $$

donde modificar el denominador del factor premultiplicado en $s_{\hat u}^2$ para acomodar regresores adicionales. Por lo tanto,

$$ s_{\hat u}^2 = \frac{1}{n-k-1} \, SSR $$

donde $k$ denota el número de regresores *excluyendo* la intersección.

Si bien **summary()** calcula $R^2$ como en el caso de un solo regresor, no es una medida confiable para modelos de regresión múltiple. Esto se debe a que $R^2$ aumenta cada vez que se agrega un regresor adicional al modelo. Agregar un regresor disminuye el $SSR$ --- lo reduce a menos que el coeficiente estimado respectivo sea exactamente cero, lo que prácticamente nunca sucede. El $R^2$ ajustado toma esto en consideración al "castigar" la adición de regresores usando un factor de corrección. Entonces, el $R^2$ ajustado, o simplemente $\bar{R}^2$, es una versión modificada de $R^2$. Se define como

$$ \bar{R}^2 = 1-\frac{n-1}{n-k-1} \, \frac{SSR}{TSS}. $$

Como ya se habrá sospechado, **summary()** ajusta la fórmula para $SER$ y calcular $\bar{R}^2$ y, por supuesto, $R^2$ por defecto, dejando así la decisión de qué medida confiar en el usuario.

Puede encontrar ambas medidas en la parte inferior de la salida generada llamando a **summary(mult.mod)**.

```{r, 401}
summary(mult.mod)
```

También se pueden calcular las medidas a mano usando las fórmulas anteriores. Comprobando que los resultados coinciden con los valores proporcionados por **summary()**.

```{r, 402}
# definir los componentes
n <- nrow(CASchools)                            # número de observaciones (filas)
k <- 2                                          # número de regresores

y_mean <- mean(CASchools$score)                 # medida de la media de los resultados de las pruebas

SSR <- sum(residuals(mult.mod)^2)               # suma de residuos cuadrados
TSS <- sum((CASchools$score - y_mean )^2)       # suma total de cuadrados
ESS <- sum((fitted(mult.mod) - y_mean)^2)       # suma explicada de cuadrados

# calcular las medidas

SER <- sqrt(1/(n-k-1) * SSR)                    # error estándar de la regresión
Rsq <- 1 - (SSR / TSS)                          # R^2
adj_Rsq <- 1 - (n-1)/(n-k-1) * SSR/TSS          # R^2 ajustada

# imprimir las medidas en la consola
c("SER" = SER, "R2" = Rsq, "Adj.R2" = adj_Rsq)
```

Ahora, ¿qué se puede decir sobre el ajuste del modelo de regresión múltiple para los puntajes de las pruebas con el porcentaje de estudiantes de inglés como regresor adicional? ¿Mejora el modelo simple que incluye solo una intersección y una medida del tamaño de la clase? La respuesta es sí: Compare $\bar{R}^2$ con el obtenido para el modelo de regresión simple **mod**.

Incluir $PctEL$ como regresor mejora $\bar{R}^2$, que se considera más confiable en vista de la discusión anterior. Se puede observar que la diferencia entre $R^2$ y $\bar{R}^2$ es pequeña ya que $k = 2$ y $n$ es grande. En resumen, el ajuste de (<a href="#mjx-eqn-6.6">6.6</a>) mejora enormemente el ajuste del modelo de regresión simple con $STR$ como único regresor. 
<br>

Al comparar los errores de regresión, se encuentra que la precisión del modelo de regresión múltiple (<a href="#mjx-eqn-6.6">6.6</a>) mejora el modelo simple, ya que agregar $PctEL$ reduce el $SER$ de $18.6$ a $14.5$ unidades de puntaje de prueba.

Como ya se mencionó, $\bar{R}^2$ puede usarse para cuantificar qué tan bien un modelo se ajusta a los datos. Sin embargo, rara vez es una buena idea maximizar estas medidas llenando el modelo con regresores. No encontrará ningún estudio serio que lo haga. En cambio, es más útil incluir regresores que mejoren la estimación del efecto causal de interés que *no se evalúa* mediante los $R^2$ del modelo. El tema de la selección de variables se trata en el Capítulo \@ref(FRNL).

## Supuestos de MCO en regresión múltiple

En el modelo de regresión múltiple se amplian los tres supuestos de mínimos cuadrados del modelo de regresión simple (ver Capítulo \@ref(RLR)) y se agrega un cuarto supuesto. Estos supuestos se presentan en el Concepto clave 6.4. No se entrará en detalles sobre los supuestos 1-3, ya que sus ideas se generalizan fácilmente al caso de regresores múltiples. La atención se centrará en el cuarto supuesto. Este supuesto descarta la correlación perfecta entre regresores.

```{r, 403, eval = my_output == "html", results='asis', echo=F, purl=F}
cat('
<div class = "keyconcept" id="KC6.4">
<h3 class = "right"> Concepto clave 6.4 </h3>          
<h3 class = "left"> Los supuestos de mínimos cuadrados en el modelo de regresión múltiple </h3>

El modelo de regresión múltiple viene dado por

$$ Y_i = \\beta_0 + \\beta_1 X_{1i} + \\beta_1 X_{2i} + \\dots + \\beta_k X_{ki} + u_i \\ , \\ i=1,\\dots,n. $$

Los supuestos de MCO en el modelo de regresión múltiple son una extensión de los realizados para el modelo de regresión simple:

1. Los regresores $(X_{1i}, X_{2i}, \\dots, X_{ki}, Y_i) \\ , \\ i=1,\\dots,n$, se constuye de manera que la suposición i.i.d. se mantiene.
2. $u_i$ es un término de error con media condicional cero dados los regresores; es decir,

$$ E(u_i\\vert X_{1i}, X_{2i}, \\dots, X_{ki}) = 0. $$

3. Los valores atípicos grandes son poco probables, formalmente $X_{1i},\\dots,X_{ki}$ y $Y_i$ tienen cuartos momentos finitos.
4. No existe multicolinealidad perfecta.

</div>
')
```

```{r, 404, eval = my_output == "latex", results='asis', echo=F, purl=F}
cat('\\begin{keyconcepts}[The Least Squares Assumptions in the Multiple Regression Model]{6.4}

El modelo de regresión múltiple viene dado por

$$ Y_i = \\beta_0 + \\beta_1 X_{1i} + \\beta_1 X_{2i} + \\dots + \\beta_k X_{ki} + u_i \\ , \\ i=1,\\dots,n. $$

Los supuestos de MCO en el modelo de regresión múltiple son una extensión de los realizados para el modelo de regresión simple:

\\begin{enumerate}
\\item Los regresores $(X_{1i}, X_{2i}, \\dots, X_{ki}, Y_i) \\ , \\ i=1,\\dots,n$, se constuye de manera que la suposición i.i.d. se mantiene.
\\item $u_i$ es un término de error con media condicional cero dados los regresores; es decir,
$$ E(u_i\\vert X_{1i}, X_{2i}, \\dots, X_{ki}) = 0. $$
\\item Los valores atípicos grandes son poco probables, formalmente $X_{1i},\\dots,X_{ki}$ y $Y_i$ tienen cuartos momentos finitos.
\\item No existe multicolinealidad perfecta.
\\end{enumerate}
\\end{keyconcepts}
')
```

### Multicolinealidad {-}

*Multicolinealidad* significa que dos o más regresores en un modelo de regresión múltiple están *fuertemente* correlacionados. Si la correlación entre dos o más regresores es perfecta; es decir, un regresor puede escribirse como una combinación lineal del otro(s), se tiene *multicolinealidad perfecta*. Si bien la multicolinealidad fuerte en general es desagradable, ya que hace que la varianza del estimador MCO sea grande (se discutirá esto a profundidad más adelante), la presencia de multicolinealidad perfecta hace que sea imposible resolver el estimador MCO; es decir, el modelo no puede estimarse en primer lugar.

La siguiente sección presenta algunos ejemplos de multicolinealidad perfecta y demuestra cómo **lm()** los trata.

#### Ejemplos de multicolinealidad perfecta {-}

¿Cómo reacciona **R** si se intenta estimar un modelo con regresores perfectamente correlacionados?

La función **lm** produce una advertencia en la primera línea de la sección del coeficiente de la salida (**1 no definido debido a singularidades**) e ignorará el regresor que se supone que es una combinación lineal del otro(s). Considere el siguiente ejemplo donde se agrega otra variable **FracEL**, la fracción de estudiantes de inglés, a **CASchools** donde las observaciones son valores escalados de las observaciones para **inglés** y lo se usa como regresor junto con **STR** e **inglés** en un modelo de regresión múltiple. En este ejemplo, **inglés** y **FracEL** son perfectamente colineales. El código **R** es el siguiente.

```{r, 405}
# definir la fracción de estudiantes de inglés        
CASchools$FracEL <- CASchools$english / 100

# estimar el modelo
mult.mod <- lm(score ~ STR + english + FracEL, data = CASchools) 

# obtener un resumen del modelo
summary(mult.mod)                                                 
```

La fila **FracEL** en la sección de coeficientes del resultado consta de entradas **NA**, ya que **FracEL** se excluyó del modelo.

Si se tuviera que calcular el MCO a mano, se encontraría con el mismo problema, ¡pero nadie estaría ayudando! El cálculo simplemente falla. ¿Por qué es esto? Tome el siguiente ejemplo:

Suponga que desea estimar un modelo de regresión lineal simple con una constante y un regresor único $X$. Como se mencionó anteriormente, para que exista una multicolinealidad perfecta, $X$ tiene que ser una combinación lineal de los otros regresores. Dado que el único otro regresor es una constante (piense en el lado derecho de la ecuación del modelo como $\beta_0 \times 1 + \beta_1 X_i + u_i$ de modo que $\beta_1$ siempre se multiplica por $1$ para cada observación), $X$ también tiene que ser constante. Por $\hat\beta_1$ se tiene

$\hat\beta_1 =  \frac{\sum_{i = 1}^n (X_i - \bar{X})(Y_i - \bar{Y})} { \sum_{i=1}^n (X_i - \bar{X})^2} = \frac{\widehat{Cov}(X,Y)}{\widehat{Var}(X)}. \tag{6.7}$

La varianza del regresor $X$ está en el denominador. Dado que la varianza de una constante es cero, no se puede calcular esta fracción y $\hat{\beta}_1$ no está definida.

**Nota:** En este caso especial, el denominador en (<a href="#mjx-eqn-6.7">6.7</a>) también es igual a cero. ¿Se puede mostrar eso?

Se pueden considerar dos ejemplos más en los que la selección de regresores induce una multicolinealidad perfecta. Primero, se debe suponer que se tiene la intención de analizar el efecto del tamaño de la clase en el puntaje de la prueba usando una variable ficticia que identifica las clases que no son pequeñas ($NS$). Se define que una escuela tiene el atributo $NS$ cuando la proporción promedio de estudiantes por maestro de la escuela es de al menos $12$,

$$ NS = \begin{cases} 0, \ \ \ \text{si STR < 12} \\ 1 \ \ \ \text{de lo contrario.} \end{cases} $$

Se agrega la columna correspondiente a **CASchools** y se estima un modelo de regresión múltiple con covariables **computadora** e **inglés**.

```{r, 406}
# si STR menor a 12, NS = 0, en caso contrario NS = 1
CASchools$NS <- ifelse(CASchools$STR < 12, 0, 1)

# estimar el modelo
mult.mod <- lm(score ~ computer + english + NS, data = CASchools)

# obtener un resumen del modelo
summary(mult.mod)                                                  
```

Nuevamente, el resultado de **summary(mult.mod)** indica que la inclusión de **NS** en la regresión haría inviable la estimación. ¿Que pasó aquí? Este es un ejemplo en el que se cometió un error lógico al definir el regresor **NS**: Al observar más de cerca $NS$, la medida redefinida para el tamaño de la clase, revela que no existe una sola escuela con $STR < 12$ por tanto, $NS$ es igual a uno para todas las observaciones. Se puede verificar esto imprimiendo el contenido de **CASchools\\$NS** o usando la función **table()**, vea **?Table**.

```{r, 407}
table(CASchools$NS)
```

**Cascools$NS** es un vector de $420$, mientras que el conjunto de datos analizado incluye $420$ observaciones. Esto obviamente viola la suposición 4 del Concepto clave 6.4: Las observaciones para el intercepto son siempre $1$,

\begin{align*}
  intercept = \, & \lambda \cdot NS
\end{align*}

\begin{align*}
\begin{pmatrix} 1 
  \\ \vdots \\ 1
  \end{pmatrix} = \, & \lambda \cdot 
  \begin{pmatrix} 1 \\ 
  \vdots \\ 1
  \end{pmatrix} \\   
  \Leftrightarrow \, & \lambda = 1.
\end{align*}

Dado que los regresores se pueden escribir como una combinación lineal entre sí, se enfrenta a multicolinenalidad perfecta y **R** excluye **NS** del modelo. Por lo tanto, el mensaje de advertencia es: ¡Piense cuidadosamente sobre cómo se relacionan los regresores en sus modelos!

Otro ejemplo de multicolinealidad perfecta se conoce como la *trampa variable ficticia*. Esto puede ocurrir cuando se utilizan múltiples variables ficticias como regresores. Un caso común para esto es cuando se utilizan variables ficticias para ordenar los datos en categorías mutuamente exclusivas. Por ejemplo, suponiendo que se tiene información espacial que indica si una escuela está ubicada en el norte, al oeste, al sur o al este de los EE. UU. Esto permite crear las variables ficticias

\begin{align*}
  North_i =& 
  \begin{cases}
    1 \ \ \text{Si se encuentra en el norte} \\
    0 \ \ \text{de lo contrario}
  \end{cases} \\
    West_i =& 
  \begin{cases}
    1 \ \ \text{Si se encuentra en el oeste} \\
    0 \ \ \text{de lo contrario}
  \end{cases} \\
    South_i =& 
  \begin{cases}
    1 \ \ \text{Si se encuentra en el sur} \\
    0 \ \ \text{de lo contrario}
  \end{cases} \\
    East_i =& 
  \begin{cases}
    1 \ \ \text{Si se encuentra en el este} \\
    0 \ \ \text{de lo contrario}.
  \end{cases}
\end{align*}

Dado que las regiones son mutuamente excluyentes, por cada escuela $i=1,\dots,n$ se tiene 

$$ North_I + West_i + South_I + East_I = 1. $$

Se enfrenta con problemas al tratar de estimar un modelo que incluye una constante y *las cuatro* variables ficticias en el modelo; por ejemplo, 

$$ TestScore = \beta_0 + \beta_1 \times STR + \beta_2 \times english + \beta_3 \times North_i + \beta_4 \times West_i + \beta_5 \times South_i + \beta_6 \times East_i + u_i \tag{6.8}$$

desde entonces, para todas las observaciones $i=1,\dots,n$ el término constante es una combinación lineal de las variables ficticias:

\begin{align}
  intercept = \, & \lambda_1 \cdot (North + West + South + East) \\
  \begin{pmatrix} 1 \\ \vdots \\ 1\end{pmatrix} = \, & \lambda_1 \cdot \begin{pmatrix} 1 \\ \vdots \\ 1\end{pmatrix} \\   \Leftrightarrow \, & \lambda_1 = 1
\end{align}

y se tiene una perfecta multicolinealidad. Por lo tanto, la "trampa de la variable ficticia" significa no prestar atención e *incluir falsamente dummies exhaustivas* y *una constante* en un modelo de regresión.

¿Cómo la función **lm()** utiliza una regresión como (<a href="#mjx-eqn-6.8">6.8</a>)? Primero se deben generar algunos datos categóricos artificiales y agregar una nueva columna llamada **directions** a **CASchools** y ver cómo **lm()** se comporta cuando se le pide que estime el modelo.

```{r, 408}
# sembrar la semilla para la reproducibilidad
set.seed(1)

# generar datos artificiales en la ubicación
CASchools$direction <- sample(c("West", "North", "South", "East"), 
                              420, 
                              replace = T)

# estimar el modelo
mult.mod <- lm(score ~ STR + english + direction, data = CASchools)

# obtener un resumen del modelo
summary(mult.mod)                                                 
```

Observe que **R** resuelve el problema por sí solo al generar e incluir las variables ficticias **directionNorth**, **directionSouth** y **directionWest** pero omitiendo **directionEast**. Por supuesto, la omisión de todas las demás ficticias lograría lo mismo. Otra solución sería excluir la constante e incluir todas las variables ficticias.

¿Significa esto que se pierde la información sobre las escuelas ubicadas en el Este? Afortunadamente, este no es el caso: la exclusión de **directEast** simplemente altera la interpretación de las estimaciones de coeficientes en las variables ficticias restantes de absoluto a relativo. Por ejemplo, la estimación del coeficiente en **directionNorth** establece que, en promedio, los puntajes de las pruebas en el norte son aproximadamente $1.61$ puntos más altos que en el este.

Un último ejemplo considera el caso en el que una relación lineal perfecta surge de regresores redundantes. Suponiendo que se tiene un regresor $PctES$, el porcentaje de hablantes de inglés en la escuela donde

$$ PctES = 100 -  PctEL$$

y tanto $PctES$ como $PctEL$ se incluyen en un modelo de regresión. Un regresor es redundante ya que el otro transmite la misma información. Dado que obviamente este es un caso en el que los regresores se pueden escribir como una combinación lineal, se termina con una multicolinealidad perfecta, nuevamente.

Haciendo esto en **R**.

```{r, 409}
# porcentaje de hablantes de inglés
CASchools$PctES <- 100 - CASchools$english

# estimar el modelo
mult.mod <- lm(score ~ STR + english + PctES, data = CASchools)

# obtener un resumen del modelo
summary(mult.mod)                                                 
```

Una vez más, **lm()** se niega a estimar el modelo completo usando MCO y excluye **PctES**.

Lo anterior se debe a la multicolinealidad perfecta, sus consecuencias para el estimador MCO en modelos generales de regresión múltiple se pueden demostrar utilizando notación matricial.

#### Multicolinealidad imperfecta {-}

A diferencia de la multicolinealidad perfecta, la multicolinealidad imperfecta es, hasta cierto punto, un problema menor. De hecho, la multicolinealidad imperfecta es la razón por la que se está interesado en estimar modelos de regresión múltiple en primer lugar: El estimador MCO permite *aislar* las influencias de los regresores *correlacionados* en la variable dependiente. Si no fuera por estas dependencias, no habría razón para recurrir a un enfoque de regresión múltiple y simplemente se podría trabajar con un modelo de regresor único. Sin embargo, este rara vez es el caso en las aplicaciones. Ya se sabe que ignorar las dependencias entre regresores que influyen en la variable de resultado tiene un efecto adverso sobre los resultados de la estimación.

Entonces, ¿cuándo y por qué es un problema la multicolinealidad imperfecta? Suponga que tiene el modelo de regresión

$$ Y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + u_i \tag{6.9} $$

y está interesado en estimar $\beta_1$, el efecto en $Y_i$ de un cambio de una unidad en $X_{1i}$, mientras se mantiene constante $X_{2i}$. No sabe que el verdadero modelo incluye $X_2$. Sigue un razonamiento y agrega $X_2$ como una covariable al modelo para abordar un posible sesgo de variable omitida. Está seguro de que $E(u_i\vert X_{1i}, X_{2i})=0$ y que no existe razón alguna para sospechar una violación de los supuestos 2 y 3 del Concepto clave 6.4. Si $X_1$ y $X_2$ están altamente correlacionadas, el MCO tendrá dificultades para estimar con precisión $\beta_1$. Eso significa que aunque $\hat\beta_1$ es un estimador consistente e imparcial para $\beta_1$, tiene una gran variación debido a que $X_2$ está incluido en el modelo. Si los errores son homocedásticos, este problema se puede comprender mejor con la fórmula de la varianza de $\hat\beta_1$ en el modelo (<a href="#mjx-eqn-6.9">6.9</a>):

$$ \sigma^2_{\hat\beta_1} = \frac{1}{n} \left( \frac{1}{1-\rho^2_{X_1,X_2}} \right) \frac{\sigma^2_u}{\sigma^2_{X_1}}. \tag{6.10} $$

Primero, si $\rho_{X_1,X_2}=0$, es decir, si no existe correlación entre ambos regresores, incluir $X_2$ en el modelo no influye en la varianza de $\hat\beta_1$. 

En segundo lugar, si $X_1$ y $X_2$ están correlacionados, $\sigma^2_{\hat\beta_1}$ es inversamente proporcional a $1-\rho^2_{X_1,X_2}$ así que cuanto más fuerte sea la correlación entre $X_1$ y $X_2$, el menor es $1-\rho^2_{X_1,X_2}$ y, por lo tanto, mayor es la varianza de $\hat\beta_1$. 

En tercer lugar, aumentar el tamaño de la muestra ayuda a reducir la varianza de $\hat\beta_1$. Por supuesto, esto no se limita al caso de dos regresores: En regresiones múltiples, la multicolinealidad imperfecta infla la varianza de uno o más estimadores de coeficientes. Es una cuestión empírica qué estimaciones de coeficientes se ven gravemente afectadas por esto y cuáles no. Cuando el tamaño de la muestra es pequeño, a menudo uno se enfrenta a la decisión de aceptar la consecuencia de agregar un gran número de covariables (mayor varianza) o utilizar un modelo con pocos regresores (posible sesgo de variable omitida). Esto se llama *compensación de sesgo-varianza*.

En resumen, las consecuencias indeseables de la multicolinealidad imperfecta generalmente no son el resultado de un error lógico cometido por el investigador (como suele ser el caso de la multicolinealidad perfecta) sino más bien un problema que está vinculado a los datos utilizados, el modelo que se va a estimar y la pregunta de investigación en cuestión.

### Estudio de simulación: Multicolinealidad imperfecta {-}

Resulta importante realizar un estudio de simulación para ilustrar los problemas esbozados anteriormente.

1. Usando (<a href="#mjx-eqn-6.8">6.9</a>) como proceso de generación de datos y al elegir $\beta_0 = 5$, $\beta_1 = 2.5$ y $\beta_2 = 3$, así como $u_i$ (que es un término de error distribuido como $\mathcal{N}(0,5)$). En un primer paso, se toma una muestra de los datos del regresor de una distribución normal bivariada: 

$$ X_i = (X_{1i}, X_{2i}) \overset{i.i.d.}{\sim} \mathcal{N} \left[\begin{pmatrix} 0 \\ 0  \end{pmatrix}, \begin{pmatrix} 10 & 2.5 \\ 2.5 & 10 \end{pmatrix} \right] $$ 

Es sencillo ver que la correlación entre $X_1$ y $X_2$ en la población es bastante baja:

$$ \rho_{X_1,X_2} = \frac{Cov(X_1,X_2)}{\sqrt{Var(X_1)}\sqrt{Var{(X_2)}}} = \frac{2.5}{10} = 0.25 $$

2. A continuación, se estima el modelo (<a href="#mjx-eqn-6.9">6.9</a>) y se guardan las estimaciones para $\beta_1$ y $\beta_2$. Esto se repite $10000$ veces con un bucle **for**, por lo que termina con una gran cantidad de estimaciones que permiten describir las distribuciones de $\hat\beta_1$ y $\hat\beta_2$.

3. Se repiten los pasos 1 y 2, pero aumentando la covarianza entre $X_1$ y $X_2$ de $2.5$ a $8.5$ de manera que la correlación entre los regresores sea alta: 

$$ \rho_{X_1,X_2} = \frac{Cov(X_1,X_2)}{\sqrt{Var(X_1)}\sqrt{Var{(X_2)}}} = \frac{8.5}{10} = 0.85 $$

4. Para evaluar el efecto sobre la precisión de los estimadores de aumentar la colinealidad entre $X_1$ y $X_2$, se estiiman y comparan las varianzas de $\hat\beta_1$ y $\hat\beta_2$ .

```{r, 410, message=F, warning=F, fig.align='center', cache=T}
# cargar paquetes
library(MASS)
library(mvtnorm)

# establecer el número de observaciones
n <- 50

# inicializar vectores de coeficientes
coefs1 <- cbind("hat_beta_1" = numeric(10000), "hat_beta_2" = numeric(10000))
coefs2 <- coefs1

# sembrar semilla
set.seed(1)

# muestreo y estimación de bucles
for (i in 1:10000) {
  
  # para cov(X_1, X_2) = 0.25
  X <- rmvnorm(n, c(50, 100), sigma = cbind(c(10, 2.5), c(2.5, 10)))
  u <- rnorm(n, sd = 5)
  Y <- 5 + 2.5 * X[, 1] + 3 * X[, 2] + u
  coefs1[i, ] <- lm(Y ~ X[, 1] + X[, 2])$coefficients[-1]
  
  # para cov(X_1, X_2) = 0.85
  X <- rmvnorm(n, c(50, 100), sigma = cbind(c(10, 8.5), c(8.5, 10)))
  Y <- 5 + 2.5 * X[, 1] + 3 * X[, 2] + u
  coefs2[i, ] <- lm(Y ~ X[, 1] + X[, 2])$coefficients[-1]
  
}

# obtener estimaciones de varianza
diag(var(coefs1))
diag(var(coefs2))
```

Se está interesado en las variaciones que son los elementos diagonales. Se puede ver que debido a la alta colinealidad, las varianzas de $\hat\beta_1$ y $\hat\beta_2$ se han más que triplicado, lo que significa que es más difícil estimar con precisión los coeficientes verdaderos.

## La distribución de los estimadores de MCO en regresión múltiple

Como en la regresión lineal simple, diferentes muestras producirán diferentes valores de los estimadores de MCO en el modelo de regresión múltiple. Una vez más, esta variación conduce a la incertidumbre de los estimadores que se busca describir utilizando su(s) distribución(es) muestral(es). En resumen, si se cumple la suposición hecha en el Concepto clave 6.4, la distribución muestral grande de $\hat\beta_0,\hat\beta_1,\dots,\hat\beta_k$ es multivariante normal, de modo que los estimadores individuales también se distribuyen normalmente. El Concepto clave 6.5 resume las declaraciones correspondientes.

```{r, 411, eval = my_output == "html", results='asis', echo=F, purl=F}
cat('
<div class = "keyconcept" id="KC6.5">
<h3 class = "right"> Concepto clave 6.5 </h3>          
<h3 class = "left"> Distribución de muestra grande de $\\hat\\beta_0,\\hat\\beta_1,\\dots,\\hat\\beta_k$ </h3>

Si se cumplen los supuestos de mínimos cuadrados en el modelo de regresión múltiple (ver Concepto clave 6.4), entonces, en muestras grandes, los estimadores de MCO $\\hat\\beta_0,\\hat\\beta_1,\\dots,\\hat\\beta_k$ se distribuyen normalmente de forma conjunta. También se dice que su distribución conjunta es *multivariante* normal. Además, cada $\\hat\\beta_j$ se distribuye como $\\mathcal{N}(\\beta_j,\\sigma_{\\beta_j}^2)$.

</div>
')
```

```{r, 412, eval = my_output == "latex", results='asis', echo=F, purl=F}
cat('\\begin{keyconcepts}[Distribución de muestra grande de $\\hat\\beta_0,\\hat\\beta_1,\\dots,\\hat\\beta_k$]{6.5}

Si se cumplen los supuestos de mínimos cuadrados en el modelo de regresión múltiple (ver Concepto clave 6.4), entonces, en muestras grandes, los estimadores de MCO $\\hat\\beta_0,\\hat\\beta_1,\\dots,\\hat\\beta_k$ se distribuyen normalmente de forma conjunta. También se dice que su distribución conjunta es \\textit{multivariante} normal. Además, cada $\\hat\\beta_j$ se distribuye como $\\mathcal{N}(\\beta_j,\\sigma_{\\beta_j}^2)$.

\\end{keyconcepts}
')
```

Esencialmente, el Concepto clave 6.5 establece que, si el tamaño de la muestra es grande, se pueden aproximar las distribuciones muestrales individuales de los estimadores de coeficientes mediante distribuciones normales específicas y su distribución muestral conjunta mediante una distribución normal multivariante.

¿Cómo se puede usar **R** para tener una idea de cómo se ve la FDP conjunta de los estimadores de coeficientes en el modelo de regresión múltiple? Al estimar un modelo sobre algunos datos, se termina con un conjunto de estimaciones puntuales que no revelan mucha información sobre la densidad *conjunta* de los estimadores. Sin embargo, con una gran cantidad de estimaciones que utilizan datos muestreados aleatoriamente repetidamente de la misma población, se puede generar un gran conjunto de estimaciones puntuales que permiten trazar una *estimación* de la función de densidad conjunta.

El enfoque que se usará para hacer esto en **R** es el siguiente:

- Generar $10000$ muestras aleatorias de tamaño $50$ usando el DGP $$ Y_i = 5 + 2.5 \cdot X_{1i} + 3 \cdot X_{2i} + u_i  $$ donde los regresores $X_{1i}$ y $X_{2i}$ se muestrean para cada observación como  $$ X_i = (X_{1i}, X_{2i}) \sim \mathcal{N} \left[\begin{pmatrix} 0 \\ 0  \end{pmatrix}, \begin{pmatrix} 10 & 2.5 \\ 2.5 & 10 \end{pmatrix} \right] $$  y  $$ u_i \sim \mathcal{N}(0,5) $$  es el término de error.

- Para cada uno de los conjuntos de datos de muestra simulados de $10000$, se estima el modelo $$ Y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + u_i $$  y se guardan las estimaciones de coeficientes $\hat\beta_1$ y $\hat\beta_2$.

- Se calcula una estimación de densidad de la distribución conjunta de $\hat\beta_1$ y $\hat\beta_2$ en el modelo anterior usando la función **kde2d()** del paquete **MASS**, ver `?MASS`. Esta estimación luego se grafica usando la función **persp()**.

```{r, 413, echo=T, message=F, warning=F, fig.align='center'}
# cargar paquetes
library(MASS)
library(mvtnorm)

# establecer tamaño de muestra
n <- 50

# inicializar vector de coeficientes
coefs <- cbind("hat_beta_1" = numeric(10000), "hat_beta_2" = numeric(10000))

# sembrar la semilla para la reproducibilidad
set.seed(1)

# muestreo y estimación de bucles
for (i in 1:10000) {
  
  X <- rmvnorm(n, c(50, 100), sigma = cbind(c(10, 2.5), c(2.5, 10)))
  u <- rnorm(n, sd = 5)
  Y <- 5 + 2.5 * X[, 1] + 3 * X[, 2] + u
  coefs[i,] <- lm(Y ~ X[, 1] + X[, 2])$coefficients[-1]
  
}

# calcular la estimación de la densidad
kde <- kde2d(coefs[, 1], coefs[, 2])

# graficar densidad de estimación
persp(kde, 
      theta = 310, 
      phi = 30, 
      xlab = "beta_1", 
      ylab = "beta_2", 
      zlab = "Densidad de estimación")
```

En el gráfico anterior, se puede ver que la estimación de densidad tiene cierta similitud con una distribución normal bivariada (consulte el Capítulo \@ref(TP)), aunque no es muy bonita y probablemente un poco aproximada. Además, existe una correlación entre las estimaciones tal que $\rho\neq0$ (<a href="#mjx-eqn-2.1">2.1</a>). Además, la forma de la distribución se desvía de la forma de campana simétrica de la distribución normal estándar bivariada y, en cambio, tiene un área de superficie elíptica.

```{r, 414}
# estimar la correlación entre estimadores
cor(coefs[, 1], coefs[, 2])
```

¿De dónde proviene esta correlación? Observe que, debido a la forma en que se generan los datos, existe una correlación entre los regresores $X_1$ y $X_2$. La correlación entre los regresores en un modelo de regresión múltiple siempre se traduce en una correlación entre los estimadores. En este caso, la correlación positiva entre $X_1$ y $X_2$ se traduce en una correlación negativa entre $\hat\beta_1$ y $\hat\beta_2$. Para tener una mejor idea de la distribución, puede variar el punto de vista en el subsecuente suave gráfico 3D interactivo de la misma estimación de densidad utilizada para el trazado con **persp()**. Aquí se puede ver que la forma de la distribución está algo estirada debido a $\rho=-0.20$ y también es evidente que ambos estimadores son insesgados ya que su densidad conjunta parece estar centrada cerca del vector de parámetro verdadero $(\beta_1,\beta_2) = (2.5,3)$.

<center>
```{r, 415, echo=F, message=F, warning=F}
if(knitr::opts_knit$get("rmarkdown.pandoc.to") == "html") {
library(plotly)

kde <- kde2d(coefs[, 1], coefs[, 2], n = 100)

p <- plot_ly(x = kde$x, y = kde$y, z = kde$z, 
             type = "surface", showscale = FALSE)

p %>% layout(scene = list(zaxis = list(title = "Est. Density"
                                       ),
                          xaxis = list(title = "hat_beta_1"
                                       ),
                          yaxis = list(title = "hat_beta_2"
                                       )
                          )
             ) %>% 
  config(showLink = F, displayModeBar = F)
}
```
</center>

```{r, 416, echo=F, purl=F, eval=my_output=="latex", results='asis'}
cat('\\begin{center}\\textit{This interactive part of the book is only available in the HTML version.}\\end{center}')
```

## Ejercicios {#Ejercicios-6}

```{r, 417, echo=F, purl=F, results='asis'}
if (my_output=="html") {
  cat('
<div  class = "DCexercise">

#### 1. Conjunto de datos de vivienda de Boston {-}

En el transcurso de esta sección, trabajará con <tt>Boston</tt>, el conjunto de datos de Boston Housing que contiene 506 observaciones sobre el valor de las viviendas en los suburbios de Boston. <tt>Boston</tt> viene con el paquete <tt>MASS</tt> que ya está instalado para los ejercicios interactivos <tt>R</tt> a continuación.

**Instrucciones:**

  + Cargar tanto el paquete como el conjunto de datos.

  + Obtener una descripción general de los datos utilizando funciones conocidas de los capítulos anteriores.

  + Estimar un modelo de regresión lineal simple que explique el valor medio de la vivienda de los distritos (<tt>medv</tt>) por el porcentaje de hogares con un nivel socioeconómico bajo, <tt>lstat</tt>, y una constante. Guardar el modelo en <tt>bh_mod</tt>.

  + Imprimir un resumen de coeficientes en la consola que informe de errores estándar robustos.

<iframe src="DCL/ex6_1.html" frameborder="0" scrolling="no" style="width:100%;height:340px"></iframe>

**Sugerencia:**

  + Aquí solo se necesitan funciones <tt>R</tt> básicas: <tt>library()</tt>, <tt>data()</tt>, <tt>lm()</tt> y <tt>coeftest()</tt>.

</div>') } else {
  cat('\\begin{center}\\textit{Esta parte interactiva del curso solo está disponible en la versión HTML.}\\end{center}')
}
```

```{r, 418, echo=F, purl=F, results='asis', eval=my_output == "html"}
cat('
<div  class = "DCexercise">

#### 2. Un modelo de regresión múltiple de los precios de la vivienda I {-}

Ahora, se amplía el enfoque del ejercicio anterior agregando regresores adicionales al modelo y estimándolo nuevamente.

Como se discutió en el Capítulo \\@ref(MARM), agregar regresores al modelo mejora el ajuste, por lo que $SER$ disminuye y $R^2$ aumenta.

Se han cargado los paquetes <tt>AER</tt> y <tt>MASS</tt>. El objeto modelo <tt>bh_mod</tt> está disponible en el entorno.

**Instrucciones:**

  + Hacer una regresión del valor medio de la vivienda en un distrito, <tt>medv</tt>, sobre la edad promedio de los edificios, <tt>age</tt>, la tasa de delincuencia per cápita, <tt>crim</tt>, el porcentaje de personas con un nivel socioeconómico bajo, <tt>lstat</tt>, y una constante. Dicho de otra manera, estime el modelo $$medv_i = \\beta_0 + \\beta_1 lstat_i + \\beta_2 age_i + \\beta_3 crim_i + u_i.$$

  + Imprimir un resumen de coeficientes en la consola que informe de errores estándar robustos para el modelo aumentado.

  + El $R^2$ del modelo de regresión simple se almacena en <tt>R2_res</tt>. Guardar los modelos de regresión múltiple $R^2$ en <tt>R2_unres</tt> y comprobar si el modelo aumentado produce un $R^2$ más alto. Utilizar <tt> < </tt> o <tt> > </tt> para la comparación.

<iframe src="DCL/ex6_2.html" frameborder="0" scrolling="no" style="width:100%;height:340px"></iframe>

</div>')
```

```{r, 419, echo=F, purl=F, results='asis', eval=my_output == "html"}
cat('
<div  class = "DCexercise">

#### 3. Un modelo de regresión múltiple de precios de la vivienda II {-}

La siguiente ecuación describe el modelo estimado del Ejercicio 2 (errores estándar robustos a la heterocedasticidad entre paréntesis).

$$ \\widehat{medv}_i = \\underset{(0.74)}{32.828} \\underset{(0.08)}{-0.994} \\times lstat_i \\underset{(0.03)}{-0.083} \\times crim_i + \\underset{(0.02)}{0.038} \\times age_i$$

Este modelo se guarda en <tt>bh_mult_mod</tt> que está disponible en el entorno de trabajo.

**Instrucciones:**

Como se enfatizó en el Capítulo \\@ref(MARM), no tiene sentido usar $R^2$ cuando se comparan modelos de regresión con un número diferente de regresores. En su lugar, se debe usar $\\bar{R}^2$. $\\bar{R}^2$ se ajusta a la circunstancia de que $SSR$ se reduce cuando se agrega un regresor al modelo.

  + Utilizar el objeto modelo para calcular el factor de corrección $CF = \\frac{n-1}{n-k-1}$ donde $n$ es el número de observaciones y $k$ es el número de regresores, excluyendo la intersección. Guardarlo en <tt>CF</tt>.

  + Utilizar <tt>summary()</tt> para obtener $R^2$ y $\\bar{R}^2$ para <tt>bh_mult_mod</tt>. Es suficiente si se imprimen ambos valores en la consola.

  + Comprobar que $$\\bar{R}^2 = 1 - (1-R^2) \\cdot CF.$$ Usar el operador <tt>==</tt>.

<iframe src="DCL/ex6_3.html" frameborder="0" scrolling="no" style="width:100%;height:340px"></iframe>

</div>')
```

```{r, 420, echo=F, purl=F, results='asis', eval=my_output == "html"}
cat('
<div  class = "DCexercise">

#### 4. ¿Un modelo completo para los valores de la vivienda? {-}

Echar un vistazo a la <a href="https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/Boston.html"> descripción </a> de las variables contenidas en el conjunto de datos de <tt>Boston</tt>.
¿Qué variable esperaría tener el valor $p$ más alto en un modelo de regresión múltiple que usa *todas* las variables restantes como regresores para explicar <tt>medv</tt>?

**Instrucciones:**

  + Hacer una regresión de <tt>medv</tt> en todas las variables restantes que se encuentran en el conjunto de datos de Boston.

  + Obtener un resumen robusto de heterocedasticidad de los coeficientes.

  + La $\\bar{R}^2 para el modelo del ejercicio 3 es $0.5533$. ¿Qué puedes decir acerca de $\\bar{R}^2$ del modelo de regresión grande? ¿Este modelo mejora el anterior (no es necesario enviar el código)?

Los paquetes <tt>AER</tt> y <tt>MASS</tt> así como el conjunto de datos <tt>Boston</tt> se cargan en el entorno de trabajo.

<iframe src="DCL/ex6_4.html" frameborder="0" scrolling="no" style="width:100%;height:340px"></iframe>

**Sugerencias:**

  + Para abreviar, usar la fórmula de regresión <tt>medv ~.</tt> en su llamada de <tt>lm()</tt>. Este es un atajo que especifica una regresión de <tt>medv</tt> en todas las variables restantes en el conjunto de datos proporcionado al argumento <tt>data</tt>.

  + Usar <tt>summary</tt> en ambos modelos para comparar ambos $\\bar{R}^2$s.

</div>')
```

```{r, 421, echo=F, purl=F, results='asis', eval=my_output == "html"}
cat('
<div  class = "DCexercise">

#### 5. Selección de modelo {-}

¿Quizás se pueda mejorar el modelo eliminando una variable?

En este ejercicio, se deben estimar varios modelos, descartando cada vez una de las variables explicativas utilizadas en el modelo de regresión grande del Ejercicio 4 y comparar $\\bar{R}^2$.

El modelo de regresión completo del ejercicio anterior, <tt>full_mod</tt>, está disponible en el entorno.

**Instrucciones:**

  + Eres completamente libre para resolver este ejercicio. Se recomienda el siguiente enfoque:

    1. Empezar por estimar un modelo <tt>mod_new</tt>, donde, por ejemplo, <tt>lstat</tt> se excluye de las variables explicativas. A continuación, acceder a la barra $\\bar{R}^2$ de este modelo.

    2. Comparar la $\\bar{R}^2$ de este modelo con la $\\bar{R}^2$ del modelo completo (esto fue aproximadamente $0.7338$).

    3. Repetir los pasos 1 y 2 para todas las variables explicativas utilizadas en el modelo de regresión completo. Guardar el modelo con la mayor mejora en $\\bar{R}^2$ en <tt>better_mod</tt>.

<iframe src="DCL/ex6_5.html" frameborder="0" scrolling="no" style="width:100%;height:340px"></iframe>

</div>')
```

<!--chapter:end:Capitulo_07.Rmd-->

# Pruebas de hipótesis e intervalos de confianza en regresiones múltiples {#PHICRM}

```{r, echo = F}
options(knitr.duplicate.label = "allow")
```

```{r, 422, child="_setup.Rmd"}
```

```{r, 423, eval=knitr::opts_knit$get("rmarkdown.pandoc.to") == "html", results='asis', echo=FALSE}
cat('<hr style="background-color:#03193b;height:2px">')
```

En este capítulo se analizan los métodos que permiten cuantificar la incertidumbre muestral en el estimador MCO de los coeficientes en modelos de regresión múltiple. La base para esto son las pruebas de hipótesis y los intervalos de confianza que, al igual que para el modelo de regresión lineal simple, pueden calcularse utilizando funciones básicas **R**. También se abordará la cuestión de probar hipótesis conjuntas sobre dichos coeficientes.

Asegúrese de que los paquetes **AER** [@R-AER] y **stargazer** [@R-stargazer] estén instalados antes de continuar y reproducir los ejemplos. La forma más segura de hacerlo es verificando si el siguiente fragmento de código se ejecuta sin problemas.

```{r, 424, warning=FALSE, message=FALSE, eval=FALSE}
library(AER)
library(stargazer)
```

## Pruebas de hipótesis e intervalos de confianza para un coeficiente único

Primero se discute cómo calcular errores estándar, cómo probar hipótesis y cómo construir intervalos de confianza para un coeficiente de regresión único $\beta_j$ en un modelo de regresión múltiple. La idea básica se resume en el Concepto clave 7.1.

```{r, 425, eval = my_output == "html", results='asis', echo=F, purl=F}
cat('
<div class = "keyconcept" id="KC7.1">
<h3 class = "right"> Concepto clave 7.1 </h3>          
<h3 class = "left"> Probando la hipótesis $\\beta_j = \\beta_{j,0}$ <br>
                    Contra la alternativa $\\beta_j \\neq \\beta_{j,0}$ </h3>
<p>
1. Calcular el error estándar de $\\hat{\\beta_j}$
2. Calcular el estadístico $t$:
$$t^{act} = \\frac{\\hat{\\beta}_j - \\beta_{j,0}} {SE(\\hat{\\beta_j})}$$
3. Calcular el valor de $p$:
$$p\\text{-value} = 2 \\Phi(-|t^{act}|)$$

donde $t^{act}$ es el valor del estadístico $t$ realmente calculada. Rechazar la hipótesis en el nivel de significancia de $5\\%$ si el valor de $p$ es menor que $0.05$ o, de manera equivalente, si $|t^{act}| > 1.96$. 

El error estándar y (típicamente) el estadístico $t$ y el valor $p$ correspondiente para probar $\\beta_j = 0$ se calculan automáticamente mediante las adecuadas funciones en <tt>R</tt>; por ejemplo, <tt>summary</tt>.
</p>
</div>
')
```

```{r, 426, eval = my_output == "latex", results='asis', echo=F, purl=F}
cat('\\begin{keyconcepts}[Probando la hipótesis $\\beta_j = \\beta_{j,0}$
                    Contra la alternativa $\\beta_j \\neq \\beta_{j,0}$]{7.1}
\\begin{enumerate}
\\item Calcular el error estándar de $\\hat{\\beta_j}$
\\item Calcular el estadístico $t$:
$$t^{act} = \\frac{\\hat{\\beta}_j - \\beta_{j,0}} {SE(\\hat{\\beta_j})}$$
\\item Calcular el valor de $p$:
$$p\\text{-value} = 2 \\Phi(-|t^{act}|)$$

donde $t^{act}$ es el valor del estadístico $t$ realmente calculada. Rechazar la hipótesis en el nivel de significancia de $5\\%$ si el valor de $p$ es menor que $0.05$ o, de manera equivalente, si $|t^{act}| > 1.96$. \\end{enumerate}\\vspace{0.5cm}

El error estándar y (típicamente) el estadístico $t$ y el valor $p$ correspondiente para probar $\\beta_j = 0$ se calculan automáticamente mediante las adecuadas funciones en \\texttt{R}; por ejemplo, \\texttt{summary()}.
\\end{keyconcepts}
')
```

La prueba de una sola hipótesis sobre la importancia de un coeficiente en el modelo de regresión múltiple procede como en el modelo de regresión simple.

Puede ver esto fácilmente inspeccionando el resumen de coeficientes del modelo de regresión.

$$ TestScore = \beta_0 + \beta_1 \times size  \beta_2 \times english + u $$

ya discutido en el Capítulo \@ref(MRVR). Repasando esto:

```{r, 427, echo=5:7, warning=F, message=F}
library(AER)
data(CASchools)
CASchools$size <- CASchools$students/CASchools$teachers
CASchools$score <- (CASchools$read + CASchools$math)/2

model <- lm(score ~ size + english, data = CASchools)
coeftest(model, vcov. = vcovHC, type = "HC1")
```

Se puede comprobar que estas cantidades se calculan como en el modelo de regresión simple calculando los estadísticos de $t$ o los valores de $p$ a mano utilizando la salida anterior y **R** como calculadora.

Por ejemplo, utilizando la definición del valor $p$ para una prueba de dos lados como se da en el Concepto clave 7.1, se puede confirmar el valor $p$ para una prueba sobre la hipótesis de que el coeficiente $\beta_1$, el coeficiente de **size**, es aproximadamente cero.

```{r, 428, warning=F, message=F}
# calcular el valor p de dos colas
2 * (1 - pt(abs(coeftest(model, vcov. = vcovHC, type = "HC1")[2, 3]),
            df = model$df.residual))
```

```{r, 429, eval = my_output == "html", results='asis', echo=F, purl=F}
cat('
<div class = "keyconcept" id="KC7.2">
<h3 class = "right"> Concepto clave 7.2 </h3>          
<h3 class = "left"> Intervalos de confianza para un coeficiente único en regresión múltiple </h3>
<p>
Un intervalo de confianza bilateral de $95\\%$ para el coeficiente $\\beta_j$ es un intervalo que contiene el valor verdadero de $\\beta_j$ con una probabilidad de $95\\%$; es decir, contiene el valor real de $\\beta_j$ en $95\\%$ de todas las muestras repetidas. De manera equivalente, es el conjunto de valores de $\\beta_j$ que no puede ser rechazado por una prueba de hipótesis de dos caras de $5\\%$. Cuando el tamaño de la muestra es grande, el intervalo de confianza de $95\\%$ para $\\beta_j$ es

$$\\left[\\hat{\\beta_j}- 1.96 \\times SE(\\hat{\\beta}_j), \\hat{\\beta_j} + 1.96 \\times SE(\\hat{\\beta_j})\\right].$$
</p>
</div>
')
```

```{r, 430, eval = my_output == "latex", results='asis', echo=F, purl=F}
cat('\\begin{keyconcepts}[Intervalos de confianza para un coeficiente único en regresión múltiple]{7.2}
Un intervalo de confianza bilateral de $95\\%$ para el coeficiente $\\beta_j$ es un intervalo que contiene el valor verdadero de $\\beta_j$ con una probabilidad de $95\\%$; es decir, contiene el valor real de $\\beta_j$ en $95\\%$ de todas las muestras repetidas. De manera equivalente, es el conjunto de valores de $\\beta_j$ que no puede ser rechazado por una prueba de hipótesis de dos caras de $5\\%$. Cuando el tamaño de la muestra es grande, el intervalo de confianza de $95\\%$ para $\\beta_j$ es

$$\\left[\\hat{\\beta_j}- 1.96 \\times SE(\\hat{\\beta}_j), \\hat{\\beta_j} + 1.96 \\times SE(\\hat{\\beta_j})\\right].$$
\\end{keyconcepts}
')
```

## Una aplicación para evaluar los puntajes y la proporción de alumnos por maestro

Echando un vistazo a la regresión de la Sección \@ref(MARM) nuevamente.

El cálculo de intervalos de confianza para coeficientes individuales en el modelo de regresión múltiple procede como en el modelo de regresión simple usando la función **confint()**.

```{r, 431, warning=F, message=F}
model <- lm(score ~ size + english, data = CASchools)
confint(model)
```

Para obtener intervalos de confianza en otro nivel, suponiendo $90\%$, simplemente se debe configurar el argumento **level** en la llamada de **confint()**; en consecuencia:

```{r, 432, warning=F, message=F}
confint(model, level = 0.9)
```

La salida ahora informa los intervalos de confianza de $90\%$ deseados para todos los coeficientes.

Una desventaja de **confint()** es que no usa errores estándar robustos para calcular el intervalo de confianza. Para intervalos de confianza de muestras grandes, esto se hace rápidamente de forma manual de la siguiente manera:

```{r, 433, warning=F, message=F}
# calcular errores estándar robustos
rob_se <- diag(vcovHC(model, type = "HC1"))^0.5

# calcular intervalos de confianza robustos del 95%
rbind("lower" = coef(model) - qnorm(0.975) * rob_se,
      "upper" = coef(model) + qnorm(0.975) * rob_se)

# calcular intervalos de confianza robustos del 90%
rbind("lower" = coef(model) - qnorm(0.95) * rob_se,
      "upper" = coef(model) + qnorm(0.95) * rob_se)
```

Sabiendo cómo usar **R** para hacer inferencias sobre los coeficientes en modelos de regresión múltiple, ahora se puede responder la siguiente pregunta:

¿Puede la hipótesis nula de que un cambio en la proporción de estudiantes por maestro, **size**, no tiene una influencia significativa en los puntajes de las pruebas, **scores**, --- si se controla el porcentaje de estudiantes que aprenden inglés en el distrito, **inglés**, --- ser rechazada en el nivel de significancia de $10\%$ y $5\%$?
 
El resultado anterior muestra que cero no es un elemento del intervalo de confianza para el coeficiente de **size** de modo que se puede rechazar la hipótesis nula en niveles de significancia de $5\%$ y $10\%$. Se puede llegar a la misma conclusión a través del valor $p$ para **size**: $0.00398 < 0.05 = \alpha$.

Se debe tener en cuenta que el rechazo en el nivel de $5\%$ implica un rechazo en el nivel de $10\%$ (¿por qué?).

Recordando el Capítulo \@ref(ICCR) el intervalo de confianza de $95\%$ calculado anteriormente *no indica que una disminución de una unidad en la proporción alumno-maestro tenga un efecto en los puntajes de las pruebas* que se encuentran en el intervalo con un límite menor de $-1.9497$ y un límite superior de $-0.2529$. Una vez que se ha calculado un intervalo de confianza, una declaración probabilística como esta es incorrecta: El intervalo contiene el parámetro verdadero o no lo contiene. No se sabe cuál es la verdad.

### Otro aumento del modelo {-}

¿Cuál es el efecto promedio en los puntajes de las pruebas de reducir la proporción de alumnos por maestro cuando los gastos por alumno y el porcentaje de alumnos que aprenden inglés se mantienen constantes?

Aumentnado el modelo con un regresor adicional, que representa una medida del gasto por alumno. Usando **?CASchools** se puede encontrar que **CASchools** contiene la variable **expenditure**, que proporciona el gasto por estudiante.

El modelo ahora es 

$$ TestScore = \beta_0 + \beta_1 \times size + \beta_2 \times english + \beta_3 \times expenditure + u $$

con $expenditure$ la cantidad total de gastos por alumno en el distrito (miles de dólares).

Calculando ahora el modelo:

```{r, 434, echo=6:11, warning=F, message=F}
library(AER)
data(CASchools)
CASchools$size <- CASchools$students/CASchools$teachers
CASchools$score <- (CASchools$read + CASchools$math)/2

# escalar el gasto a miles de dólares
CASchools$expenditure <- CASchools$expenditure/1000

# estimar el modelo
model <- lm(score ~ size + english + expenditure, data = CASchools)
coeftest(model, vcov. = vcovHC, type = "HC1")
```

El efecto estimado de un cambio de una unidad en la proporción de estudiantes por maestro en los puntajes de las pruebas con el gasto y la proporción de alumnos que aprenden inglés manteniéndose constantes es de $-0.29$, que es bastante pequeño. Es más, el coeficiente de $size$ ya no es significativamente diferente de cero incluso a $10\%$ desde $p\text{-value}=0.55$. ¿Se le ocurre una interpretación de estos hallazgos? La insignificancia de $\hat\beta_1$ podría deberse a un error estándar mayor de $\hat{\beta}_1$ resultante de agregar $expenditure$ al modelo de modo que se estima el coeficiente de $size$ con menos precisión. Esto ilustra el problema de los regresores fuertemente correlacionados (multicolinealidad imperfecta). La correlación entre $size$  y $expenditure$  se puede calcular usando **cor()**.

```{r, 435}
# calcular la correlación de la muestra entre tamaño y gasto: 'size' y 'expenditure'
cor(CASchools$size, CASchools$expenditure)
```

En conjunto, se llega a la conclusión de que el nuevo modelo no proporciona evidencia de que cambiar la proporción de estudiantes por maestro; por ejemplo, al contratar nuevos maestros, tenga algún efecto en los puntajes de las pruebas mientras se mantienen constantes los gastos por estudiante y la proporción de estudiantes de inglés.

## Prueba de hipótesis conjunta utilizando el estadístico F

El modelo estimado es

$$ \widehat{TestScore} = \underset{(15.21)}{649.58} -\underset{(0.48)}{0.29} \times size - \underset{(0.04)}{0.66} \times english + \underset{(1.41)}{3.87} \times expenditure. $$

Ahora bien, ¿se puede rechazar la hipótesis de que *el coeficiente de $size$ y el coeficiente de $expenditure$* son cero? Para responder a esto, se tiene que recurrir a pruebas de hipótesis conjuntas. Una hipótesis conjunta impone restricciones a los coeficientes de regresión múltiple. Esto es diferente de realizar pruebas de $t$ individuales en las que se impone una restricción sobre un solo coeficiente. 

El estadístico $F$ de homocedasticidad está dado por

$$ F = \frac{(SSR_{\text{restricted}} - SSR_{\text{unrestricted}})/q}{SSR_{\text{unrestricted}} / (n-k-1)} $$

siendo $SSR_{restricted}$ la suma de los residuos al cuadrado de la regresión restringida; es decir, la regresión en la que se impone la restricción. $SSR_{unrestricted}$ es la suma de los residuos al cuadrado del modelo completo, $q$ es el número de restricciones bajo el valor nulo y $k$ es el número de regresores en la regresión sin restricciones.

Es bastante fácil realizar pruebas $F$ en **R**. Se puede usar la función **linearHypothesis()** contenida en el paquete **car**.

```{r, 436, warning=F, message=F}
# estimar el modelo de regresión múltiple
model <- lm(score ~ size + english + expenditure, data = CASchools)

# ejecutar la función en el objeto modelo y proporcionar ambas restricciones lineales
# para ser probadas como cadenas
linearHypothesis(model, c("size=0", "expenditure=0"))
```

El resultado revela que el estadístico $F$ para esta prueba de hipótesis conjunta es de aproximadamente $8.01$ y el valor correspondiente $p$ es $0.0004$. Por tanto, se puede rechazar la hipótesis nula de que ambos coeficientes son cero en cualquier nivel de significación comúnmente utilizado en la práctica.

Una versión robusta a la heterocedasticidad de esta prueba $F$ (que lleva a la misma conclusión) se puede realizar de la siguiente manera.

```{r, 437, warning=F, message=F}
# prueba F robusta de la heterocedasticidad
linearHypothesis(model, c("size=0", "expenditure=0"), white.adjust = "hc1")
```

La salida estándar del resumen de un modelo también informa un estadístico $F$ y el valor de $p$ correspondiente. La hipótesis nula que pertenece a esta prueba $F$ es que *todos* los coeficientes de población en el modelo, excepto la intersección, son cero, por lo que las hipótesis son 

$$H_0: \beta_1=0, \ \beta_2 =0, \ \beta_3 =0 \quad \text{vs.} \quad H_1: \beta_j \neq 0 \ \text{for at least one} \ j=1,2,3.$$

Esto también se denomina *estadístico $F$ de la regresión general* y la hipótesis nula es obviamente diferente de probar si solo $\beta_1$ y $\beta_3$ son cero.

Ahora se verifica si el estadístico $F$ perteneciente al valor $p$ listado en el resumen del modelo coincide con el resultado reportado por **linearHypothesis()**.

```{r, 438, warning=F, message=F}
# ejecutar la función en el objeto modelo y proporcionar las restricciones
# para ser probado como vector de caracteres
linearHypothesis(model, c("size=0", "english=0", "expenditure=0"))

# acceda a la estadística F general desde el resumen del modelo
summary(model)$fstatistic
```

La entrada **value** es el estadístico $F$ general y es igual al resultado de **linearHypothesis()**. La prueba $F$ rechaza la hipótesis nula de que el modelo no tiene poder para explicar los puntajes de las pruebas. Es importante saber que el estadístico $F$ informado por **summary** *no es robusto a la heterocedasticidad*.

## Conjuntos de confianza para múltiples coeficientes

Con base en el estadístico $F$ que se ha encontrado anteriormente, se pueden especificar conjuntos de confianza. Los conjuntos de confianza son análogos a los intervalos de confianza para coeficientes únicos. Como tal, los conjuntos de confianza consisten en *combinaciones* de coeficientes que contienen la combinación verdadera de coeficientes en, digamos, $95\%$ de todos los casos si se pudieran extraer repetidamente muestras aleatorias, como en el caso univariante. Dicho de otra manera, un conjunto de confianza es el conjunto de todas las combinaciones de coeficientes para las que no se puede rechazar la correspondiente hipótesis nula conjunta probada mediante una prueba $F$.

El conjunto de confianza para dos coeficientes es una elipse que se centra alrededor del punto definido por ambas estimaciones de coeficientes. Nuevamente, existe una forma muy conveniente de graficar el conjunto de confianza para dos coeficientes de objetos del modelo, a saber, la función **trustEllipse()** del paquete **car**.

Ahora se grafica la elipse de confianza de $95\%$ para los coeficientes de **size** y **expenditure** de la regresión realizada anteriormente. Al especificar el argumento adicional **fill**, se colorea el conjunto de confianza.

```{r, 439, fig.align = 'center', warning=F, message=F}
model <- lm(score ~ size + english + expenditure, data = CASchools)

# dibujar el conjunto de confianza del 95% para los coeficientes de size y expenditure
confidenceEllipse(model, 
                  fill = T,
                  lwd = 0,
                  which.coef = c("size", "expenditure"),
                  main = "95% Confidence Set")
```

Se puede ver que la elipse se centra alrededor de $(-0.29, 3.87)$, el par de coeficientes se estima en $size$ y $expenditure$. Además, $(0,0)$ no es un elemento del conjunto de confianza $95\%$, por lo que se puede rechazar $H_0: \beta_1 = 0, \ \beta_3 = 0$.

De forma predeterminada, **trustEllipse()** usa errores estándar de homocedasticidad solamente. El siguiente fragmento de código muestra cómo calcular una elipse de confianza robusta y cómo superponerla con la gráfica anterior.

```{r, 440, fig.align = 'center', warning=F, message=F}
# graficar el conjunto robusto de confianza del 95% para los coeficientes de size y expenditure.
confidenceEllipse(model, 
                  fill = T,
                  lwd = 0,
                  which.coef = c("size", "expenditure"),
                  main = "95% Confidence Sets",
                  vcov. = vcovHC(model, type = "HC1"),
                  col = "red")
                  
# graficar el conjunto de confianza del 95% para los coeficientes de size y expenditure.
confidenceEllipse(model, 
                  fill = T,
                  lwd = 0,
                  which.coef = c("size", "expenditure"),
                  add = T)
```

Como los errores estándar robustos son ligeramente mayores que los válidos bajo homocedasticidad, en este caso, solo el conjunto de confianza robusto es ligeramente mayor. Esto es análogo a los intervalos de confianza para los coeficientes individuales.

## Especificación de modelo para regresión múltiple

Elegir una especificación de regresión; es decir, seleccionar las variables que se incluirán en un modelo de regresión, es una tarea difícil. Sin embargo, existen algunas pautas sobre cómo proceder. El objetivo es claro: Obtener una estimación imparcial y precisa del efecto causal de interés. 

Como punto de partida, piense en las variables omitidas; es decir, para evitar posibles sesgos mediante el uso de variables de control adecuadas. El sesgo de las variables omitidas en el contexto de la regresión múltiple se explica en el Concepto clave 7.3. 

Un segundo paso podría ser comparar diferentes especificaciones por medidas de ajuste. Sin embargo, como se puede observar, uno no debería depender únicamente de $\bar{R}^2$.

```{r, 441, eval = my_output == "html", results='asis', echo=F, purl=F}
cat('
<div class = "keyconcept" id="KC7.3">
<h3 class = "right"> Concepto clave 7.3 </h3>          
<h3 class = "left"> Sesgo variable omitido en regresión múltiple </h3>
<p>
El sesgo de variable omitida es el sesgo en el estimador de MCO que surge cuando los regresores se correlacionan con una variable omitida. Para que surja un sesgo de variable omitida, dos cosas deben ser ciertas:

1. Al menos uno de los regresores incluidos debe estar correlacionado con la variable omitida.
2. La variable omitida debe ser un determinante de la variable dependiente, $Y$.
</p>
</div>
')
```

```{r, 442, eval = my_output == "latex", results='asis', echo=F, purl=F}
cat('\\begin{keyconcepts}[Sesgo variable omitido en regresión múltiple]{7.3}
El sesgo de variable omitida es el sesgo en el estimador de MCO que surge cuando los regresores se correlacionan con una variable omitida. Para que surja un sesgo de variable omitida, dos cosas deben ser ciertas:

\\begin{enumerate}
\\item Al menos uno de los regresores incluidos debe estar correlacionado con la variable omitida.
\\item La variable omitida debe ser un determinante de la variable dependiente, $Y$.
\\end{enumerate}

\\end{keyconcepts}
')
```

Ahora se discute un ejemplo en el que se enfrenta un posible sesgo de variable omitida en un modelo de regresión múltiple:

Considere nuevamente la ecuación de regresión estimada:

$$ \widehat{TestScore} = \underset{(8.7)}{686.0} - \underset{(0.43)}{1.10} \times size - \underset{(0.031)}{0.650} \times english. $$

Se está interesado en estimar el efecto causal del tamaño de la clase en la puntuación de la prueba. Podría haber un sesgo debido a la omisión de "oportunidades de aprendizaje externas" en la regresión, ya que tal medida podría ser un determinante de los puntajes de las pruebas de los estudiantes y también podría correlacionarse con ambos regresores ya incluidos en el modelo (de modo que ambas condiciones de del Concepto clave 7.3 se cumplen). Las "oportunidades de aprendizaje externo" son un concepto complicado que es difícil de cuantificar. Un sustituto que se puede considerar, en cambio, es el entorno económico de los estudiantes que probablemente esté fuertemente relacionado con las oportunidades de aprendizaje externas: Piense en padres adinerados que pueden proporcionar tiempo y/o dinero para la matrícula privada de sus hijos. Por lo tanto, se aumenta el modelo con la variable **lunch**, el porcentaje de estudiantes que califican para un almuerzo gratis o subsidiado en la escuela debido a ingresos familiares por debajo de cierto umbral, y reestimar el modelo.

```{r, 443, echo=6:8, warning=F, message=F}
library(AER)
data(CASchools)
CASchools$size <- CASchools$students/CASchools$teachers
CASchools$score <- (CASchools$read + CASchools$math)/2

# estimar el modelo e imprimir el resumen en la consola
model <- lm(score ~ size + english + lunch, data = CASchools)
coeftest(model, vcov. = vcovHC, type = "HC1")
```

Por tanto, la recta de regresión estimada es

$$ \widehat{TestScore} = \underset{(5.56)}{700.15} - \underset{(0.27)}{1.00} \times size - \underset{(0.03)}{0.12} \times english - \underset{(0.02)}{0.55} \times lunch. $$

No se observan cambios sustanciales en la conclusión sobre el efecto de $size$ en $TestScore$: El coeficiente de $size$ cambia solo $0.1$ y conserva su importancia.

Aunque la diferencia en los coeficientes estimados no es grande, en este caso, es útil mantener **lunch** para hacer más creíble el supuesto de independencia media condicional.

### Especificación del modelo en teoría y en la práctica {-}

El Concepto clave 7.4 enumera algunos errores comunes al usar $R^2$ y $\bar{R}^2$ para evaluar la capacidad predictiva de los modelos de regresión.

```{r, 444, eval = my_output == "html", results='asis', echo=F, purl=F}
cat('
<div class = "keyconcept" id="KC7.4">
<h3 class = "right"> Concepto clave 7.4 </h3>          
<h3 class = "left"> $R^2$ y $\\bar{R}^2$: lo que te dicen --- y lo que no </h3>

<p>

$R^2$ y $\\bar{R}^2$ indican si los regresores son buenos para explicar la variación de la variable independiente en la muestra. Si $R^2$ (o $\\bar{R}^2$) es casi $1$, entonces los regresores producen una buena predicción de la variable dependiente en esa muestra, en el sentido de que la varianza de los residuales de MCO es pequeña en comparación con la varianza de la variable dependiente. Si $R^2$ (o $\\bar{R}^2$) es casi $0$, lo contrario es cierto.

La $R^2$ y $\\bar{R}^2$ *no* indican si:

1. Una variable incluida es estadísticamente significativa.
2. Los regresores son la verdadera causa de los movimientos en la variable dependiente (causalidad).
3. Existe sesgo de variable omitida.
4. Se ha elegido el conjunto de regresores más apropiado.
</p>
  
</div>
')
```

```{r, 445, eval = my_output == "latex", results='asis', echo=F, purl=F}
cat('\\begin{keyconcepts}[$R^2$ y $\\bar{R}^2$: lo que te dicen --- y lo que no]{7.4}

$R^2$ y $\\bar{R}^2$ indican si los regresores son buenos para explicar la variación de la variable independiente en la muestra. Si $R^2$ (o $\\bar{R}^2$) es casi $1$, entonces los regresores producen una buena predicción de la variable dependiente en esa muestra, en el sentido de que la varianza de los residuales de MCO es pequeña en comparación con la varianza de la variable dependiente. Si $R^2$ (o $\\bar{R}^2$) es casi $0$, lo contrario es cierto.\\newline

La $R^2$ y $\\bar{R}^2$ \\textit{no} indican si:\\newline

\\begin{enumerate}
\\item Una variable incluida es estadísticamente significativa.
\\item Los regresores son la verdadera causa de los movimientos en la variable dependiente (causalidad).
\\item Existe sesgo de variable omitida.
\\item Se ha elegido el conjunto de regresores más apropiado.
\\end{enumerate}
\\end{keyconcepts}
')
```

Por ejemplo, piense en hacer una regresión de $TestScore$ en $PLS$, que mide el espacio de estacionamiento disponible en miles de pies cuadrados. Es probable que observe un coeficiente significativo de magnitud razonable y valores de moderados a altos para $R^2$ y $\bar{R}^2$. La razón de esto es que el espacio en el estacionamiento se correlaciona con muchos factores determinantes de la puntuación de la prueba, como la ubicación, el tamaño de la clase, la dotación financiera, entre otrs. Aunque no se tienen observaciones sobre $PLS$, se puede usar **R** para generar algunos datos relativamente realistas.

```{r, 446}
# sembrar la semilla para la reproducibilidad
set.seed(1)

# generar observaciones para el espacio de estacionamiento
CASchools$PLS <- c(22 * CASchools$income 
                   - 15 * CASchools$size 
                   + 0.2 * CASchools$expenditure
                   + rnorm(nrow(CASchools), sd = 80) + 3000)
```

```{r, 447, fig.align='center',}
# geficar el espacio del estacionamiento contra el puntaje de la prueba
plot(CASchools$PLS, 
     CASchools$score,
     xlab = "Espacio de estacionamiento",
     ylab = "Resultado de la prueba",
     pch = 20,
     col = "steelblue")

# puntuación de la prueba de regresión en PLS
summary(lm(score ~ PLS, data = CASchools))
```

$PLS$ se genera como una función lineal de $expenditure$, $income$, $size$ y una perturbación aleatoria. Por lo tanto, los datos sugieren que existe una relación positiva entre el espacio de estacionamiento y la puntuación de la prueba. De hecho, al estimar el modelo

\begin{align}
TestScore = \beta_0 + \beta_1 \times PLS + u (\#eq:plsmod) 
\end{align}

usando **lm()** se encuentra que el coeficiente en $PLS$ es positivo y significativamente diferente de cero. Además, $R^2$ y $\bar{R}^2$ son aproximadamente $0.3$, que es mucho más que los aproximadamente $0.05$ observados al hacer una regresión de los puntajes de las pruebas solo en el tamaño de las clases. Esto sugiere que aumentar el espacio de estacionamiento aumenta las calificaciones de las pruebas de la escuela y que el modelo \@ref(eq:plsmod) explica mejor la heterogeneidad en la variable dependiente que un modelo con $size$ como único regresor. Teniendo en cuenta cómo se construye $PLS$, esto no es ninguna sorpresa. Es evidente que el alto $R^2$ <it>no</it> puede utilizarse para concluir que la relación estimada entre el espacio de estacionamiento y los puntajes de las pruebas es causal: El (relativamente) alto $R^2$ se debe a la correlación entre $PLS$ y otros determinantes y/o variables de control. ¡Aumentar el espacio de estacionamiento *no* es una medida apropiada para generar más éxito en el aprendizaje!

## Análisis del conjunto de datos de puntuación de la prueba

El capítulo \@ref(MRVR) y algunas de las secciones anteriores han enfatizado que es importante incluir variables de control en los modelos de regresión si es plausible que haya factores omitidos. En el ejemplo de puntajes de pruebas, se quiere estimar el efecto causal de un cambio en la proporción alumno-maestro en los puntajes de las pruebas. Ahora se proporciona un ejemplo de cómo usar la regresión múltiple para aliviar el sesgo de las variables omitidas y demostrar cómo informar los resultados usando **R**.

Hasta ahora se han considerado dos variables que controlan las características no observables de los estudiantes, que se correlacionan con la proporción de estudiantes por maestro *y se supone que* tienen un impacto en los puntajes de las pruebas:

  + $English$, el porcentaje de estudiantes que aprenden inglés
  
  + $lunch$, la proporción de estudiantes que califican para un almuerzo subsidiado o incluso gratis en la escuela.
  
Otra nueva variable proporcionada con **CASchools** es **calworks**, el porcentaje de estudiantes que califican para el programa de asistencia de ingresos *CalWorks*. Los estudiantes elegibles para *CalWorks* viven en familias con un ingreso total por debajo del umbral del programa de almuerzo subsidiado, por lo que ambas variables son indicadores de la proporción de niños en desventaja económica. Ambos indicadores están altamente correlacionados:

```{r, 448} 
# estimar la correlación entre 'calworks' y 'lunch'
cor(CASchools$calworks, CASchools$lunch)
```

No existe una manera inequívoca de proceder al decidir qué variable usar. En cualquier caso, puede que no sea una buena idea utilizar ambas variables como regresores en vista de la colinealidad. Por lo tanto, también se consideran especificaciones de modelos alternativos.

Para empezar, se grafican las características de los estudiantes con los puntajes de las pruebas.

```{r, 449}
# configurar la disposición de las gráficas
m <- rbind(c(1, 2), c(3, 0))
graphics::layout(mat = m)

# gráfico de dispersión
plot(score ~ english, 
     data = CASchools, 
     col = "steelblue", 
     pch = 20, 
     xlim = c(0, 100),
     cex.main = 0.9,
     main = "Porcentaje de estudiantes de inglés")

plot(score ~ lunch, 
     data = CASchools, 
     col = "steelblue", 
     pch = 20,
     cex.main = 0.9,
     main = "Porcentaje que califica para el almuerzo a precio reducido")

plot(score ~ calworks, 
     data = CASchools, 
     col = "steelblue", 
     pch = 20, 
     xlim = c(0, 100),
     cex.main = 0.9,
     main = "Porcentaje que califica para asistencia económica")
```

Se divide el área de graficado usando **layout()**. La matriz **m** especifica la ubicación de las gráficas, consultar `?layout`.

Se puede ver que todas las relaciones son negativas. Aquí están los coeficientes de correlación:

```{r, 450}
# estimar la correlación entre las características de los estudiantes y los puntajes de las pruebas
cor(CASchools$score, CASchools$english)
cor(CASchools$score, CASchools$lunch)
cor(CASchools$score, CASchools$calworks)
```

Se consideran cinco ecuaciones modelo diferentes:

\begin{align*}
  (I) \quad TestScore=& \, \beta_0 + \beta_1 \times size + u, \\
  (II) \quad TestScore=& \, \beta_0 + \beta_1 \times size + \beta_2 \times english + u, \\
  (III) \quad TestScore=& \, \beta_0 + \beta_1 \times size + \beta_2 \times english + \beta_3 \times lunch + u, \\
  (IV) \quad TestScore=& \, \beta_0 + \beta_1 \times size + \beta_2 \times english + \beta_4 \times calworks + u, \\
  (V) \quad TestScore=& \, \beta_0 + \beta_1 \times size + \beta_2 \times english + \beta_3 \times lunch + \beta_4 \times calworks + u
\end{align*}

La mejor forma de comunicar los resultados de la regresión es en una tabla. El paquete **stargazer** es muy conveniente para este propósito. Proporciona una función que genera tablas HTML y LaTeX de aspecto profesional que satisfacen los estándares científicos. Uno simplemente tiene que proporcionar uno o varios objetos de clase **lm**. El resto lo hace la función **stargazer()**.

```{r, 451, eval=F}
# cargar la biblioteca stargazer
library(stargazer)

# estimar diferentes especificaciones de modelo
spec1 <- lm(score ~ size, data = CASchools)
spec2 <- lm(score ~ size + english, data = CASchools)
spec3 <- lm(score ~ size + english + lunch, data = CASchools)
spec4 <- lm(score ~ size + english + calworks, data = CASchools)
spec5 <- lm(score ~ size + english + lunch + calworks, data = CASchools)

# recopilar errores estándar robustos en una lista
rob_se <- list(sqrt(diag(vcovHC(spec1, type = "HC1"))),
               sqrt(diag(vcovHC(spec2, type = "HC1"))),
               sqrt(diag(vcovHC(spec3, type = "HC1"))),
               sqrt(diag(vcovHC(spec4, type = "HC1"))),
               sqrt(diag(vcovHC(spec5, type = "HC1"))))

# generar una tabla LaTeX usando stargazer
stargazer(spec1, spec2, spec3, spec4, spec5,
          se = rob_se,
          digits = 3,
          header = F,
          column.labels = c("(I)", "(II)", "(III)", "(IV)", "(V)"))
```

<!--html_preserve-->

```{r, 452, results='asis', echo=F, cache=T, message=FALSE, warning=FALSE, eval=my_output == "html"}
library(stargazer)
spec1 <- lm(score ~ size, data = CASchools)
spec2 <- lm(score ~ size + english, data = CASchools)
spec3 <- lm(score ~ size + english + lunch, data = CASchools)
spec4 <- lm(score ~ size + english + calworks, data = CASchools)
spec5 <- lm(score ~ size + english + lunch + calworks, data = CASchools)

# recopilar errores estándar robustos en una lista
rob_se <- list(
  sqrt(diag(vcovHC(spec1, type = "HC1"))),
  sqrt(diag(vcovHC(spec2, type = "HC1"))),
  sqrt(diag(vcovHC(spec3, type = "HC1"))),
  sqrt(diag(vcovHC(spec4, type = "HC1"))),
  sqrt(diag(vcovHC(spec5, type = "HC1")))
)

stargazer(spec1, spec2, spec3, spec4, spec5, 
          type = "html",
          se = rob_se,
          header = F,
          digits = 3,
          float.env = "sidewaystable",
          object.names = TRUE,
          dep.var.caption = "Variable dependiente: Puntaje de la prueba",
          model.numbers = FALSE,
          column.labels = c("(I)", "(II)", "(III)", "(IV)", "(V)")
          )

stargazer_html_title("Regresiones de los puntajes de las pruebas en la relación alumno-maestro y las variables de control", "rotsostracv")
```

<!--/html_preserve-->

```{r, 453, results='asis', echo=F, cache=T, message=FALSE, warning=FALSE, eval=my_output == "latex"}
library(stargazer)
spec1 <- lm(score ~ size, data = CASchools)
spec2 <- lm(score ~ size + english, data = CASchools)
spec3 <- lm(score ~ size + english + lunch, data = CASchools)
spec4 <- lm(score ~ size + english + calworks, data = CASchools)
spec5 <- lm(score ~ size + english + lunch + calworks, data = CASchools)

# gather robust standard errors in a list
rob_se <- list(
  sqrt(diag(vcovHC(spec1, type = "HC1"))),
  sqrt(diag(vcovHC(spec2, type = "HC1"))),
  sqrt(diag(vcovHC(spec3, type = "HC1"))),
  sqrt(diag(vcovHC(spec4, type = "HC1"))),
  sqrt(diag(vcovHC(spec5, type = "HC1")))
)

stargazer(spec1, spec2, spec3, spec4, spec5, 
          type = "latex",
          title = "\\label{tab:rotsostracv} Regresiones de los puntajes de las pruebas en la relación alumno-maestro y las variables de control",
          float.env = "sidewaystable",
          column.sep.width = "-10pt",
          se = rob_se,
          header = F,
          digits = 3,
          dep.var.caption = "Variable dependiente: Puntaje de la prueba",
          object.names = TRUE,
          model.numbers = FALSE,
          column.labels = c("(I)", "(II)", "(III)", "(IV)", "(V)")
          )
```

La tabla \@ref(tab:rotsostracv) establece que $score$ es la variable dependiente y que se consideran cinco modelos. Se puede ver que las columnas de la Tabla \@ref(tab:rotsostracv) contienen la mayor parte de la información proporcionada por **coeftest()** y **summary()** para los modelos de regresión en consideración: Los coeficientes estimados equipados con códigos de significancia (los asteriscos) y errores estándar entre paréntesis. Aunque no existen estadísticos de $t$, es sencillo para el lector calcularlas simplemente dividiendo una estimación de coeficiente por el error estándar correspondiente. La parte inferior de la tabla informa de estadísticas resumidas para cada modelo y una leyenda.

¿Qué se puede concluir de la comparación del modelo?

1. Se puede ver que la adición de variables de control reduce aproximadamente a la mitad el coeficiente de **size**. Además, la estimación no es sensible al conjunto de variables de control utilizadas. La conclusión es que la disminución de la proporción alumno-maestro ceteris paribus en una unidad conduce a un aumento promedio estimado en los puntajes de las pruebas de alrededor de $1$ punto.

2. Agregar características de los estudiantes como controles aumenta $R^2$ y $\bar{R}^2$ desde $0.049$ (**spec1**) hasta $0.773$ (**spec3** y **spec5**) , por lo que se pueden considerar estas variables como predictores adecuados para los resultados de las pruebas. Además, los coeficientes estimados en todas las variables de control son consistentes.

3. Se puede ver que las variables de control no son estadísticamente significativas en todos los modelos. Por ejemplo, en **spec5**, el coeficiente de $calworks$ no es significativamente diferente de cero a $5\%$ ya que $\lvert-0.048/0.059\rvert=0.81 < 1.64$. También se puede observar que el efecto sobre la estimación (y su error estándar) del coeficiente sobre $size$ de agregar $calworks$ a la especificación base **spec3** es insignificante. Por lo tanto, se puede considerar **calworks** como una variable de control superflua, dada la inclusión de **lunch** en este modelo.

## Ejercicios {#Ejercicios-7}

```{r, 454, echo=F, purl=F, results='asis'}
if (my_output=="html") {
  cat('
<div  class = "DCexercise">

#### 1. Prueba de hipótesis en un modelo de regresión múltiple --- estadísticos $t$ y valores $p$ {-}

Reconsidere el conjunto de datos de <tt>Boston</tt> y el siguiente modelo estimado (errores estándar de solo homocedasticidad entre paréntesis) del capítulo anterior:

$$\\widehat{medv}_i = \\underset{(0.75)}{32.828} -\\underset{(0.05)}{0.994} \\times lstat_i -\\underset{(0.04)}{0.083} \\times crim_i + \\underset{(0.01)}{0.038} \\times age_i.$$

Al igual que en el marco de regresión lineal simple, se pueden realizar pruebas de hipótesis sobre los coeficientes en modelos de regresión múltiples. La hipótesis más común es $H_0:\\beta_j=0$ contra la alternativa $H_1:\\beta_j\\ne 0$ para unos $j$ en $0,1,\\dots,k$.

Se han cargado los paquetes <tt>AER</tt> y <tt>MASS</tt>. Las estimaciones de los coeficientes, así como los errores estándar correspondientes, están disponibles en <tt>coefs</tt> y <tt>SEs</tt>, respectivamente.

**Instrucciones:**

Utilizar aritmética vectorial para resolver las siguientes tareas:

  + Calcular los estadísticos $t$ para cada coeficiente utilizando los objetos predefinidos <tt>coefs</tt> y <tt>SEs</tt>. Asignar el resultado a <tt>tstats</tt>.

  + Calcular los valores de $p$ para cada coeficiente y asignar el resultado a <tt>pval</tt>.

  + Verificar, con la ayuda de operadores lógicos, si las hipótesis se rechazan en el nivel de significancia de $1\\%$.

<iframe src="DCL/ex7_1.html" frameborder="0" scrolling="no" style="width:100%;height:340px"></iframe>

**Sugerencias:**

  + El estadístico $t$ para cada coeficiente se define como $t=\\frac{\\widehat{\\beta}_j-\\beta_{j,0}}{SE(\\widehat{\\beta}_j)}$.

  + El valor $p$ para una prueba de los dos lados usados se calcula como $2\\cdot\\Phi(-|t^{act}|)$ donde $t^{act}$ denota el estadístico $t$ calculado.

</div>') } else {
  cat("\\begin{center}\\textit{Esta parte interactiva del curso solo está disponible en la versión HTML.}\\end{center}")
}
```

```{r, 455, echo=F, purl=F, results='asis', eval=my_output == "html"}
cat('
<div  class = "DCexercise">

#### 2. Prueba de hipótesis en un modelo de regresión múltiple: Intervalos de confianza {-}

Considerar nuevamente el modelo estimado

$$\\widehat{medv}_i = \\underset{(0.75)}{32.828} -\\underset{(0.05)}{0.994} \\times lstat_i -\\underset{(0.04)}{0.083} \\times crim_i + \\underset{(0.01)}{0.038} \\times age_i.$$

que está disponible como objeto <tt>mod</tt> en el entorno de trabajo. Se han cargado los paquetes <tt>AER</tt> y <tt>MASS</tt>.

**Instrucciones:**

  + Construir intervalos de confianza de $99\\%$ para todos los coeficientes del modelo. Utilizar los intervalos para decidir si las hipótesis nulas individuales $H_0:\\beta_j=0$, $j=0,1,2,3,4$ se rechazan en el nivel de $1\\%$.

<iframe src="DCL/ex7_2.html" frameborder="0" scrolling="no" style="width:100%;height:340px"></iframe>

**Sugerencia:**

  + Se puede usar <tt>confint()</tt> para construir intervalos de confianza. El nivel de confianza se puede establecer mediante el argumento <tt>level</tt>.

</div>')
```

```{r, 456, echo=F, purl=F, results='asis', eval=my_output == "html"}
cat('
<div  class = "DCexercise">

#### 3. Prueba de hipótesis sólida en varios modelos de regresión {-}

De <tt>lm</tt> el objeto <tt>mod</tt> de los ejercicios anteriores está disponible en el entorno de trabajo. Se han cargado los paquetes <tt>AER</tt> y <tt>MASS</tt>.

**Instrucciones:**

  + Imprimir un resumen de coeficientes que informen sobre los errores estándar robustos a la heterocedasticidad.

  + Acceder a las entradas de la matriz generada por <tt>coeftest()</tt> para comprobar si las hipótesis se rechazan a un nivel de significancia del 1%. Utilizar operadores lógicos <tt><</tt> y <tt>></tt>.

<iframe src="DCL/ex7_3.html" frameborder="0" scrolling="no" style="width:100%;height:340px"></iframe>

**Sugerencias:**

  + El uso del argumento <tt>vcov.</tt> en <tt>coeftest()</tt> fuerza a la función a utilizar errores estándar robustos.

  + Los valores de $p$ están contenidos en la cuarta columna de la salida generada por <tt>coeftest()</tt>. Utilizar corchetes para crear subconjuntos de la matriz en consecuencia.

</div>')
```

```{r, 457, echo=F, purl=F, results='asis', eval=my_output == "html"}
cat('
<div  class = "DCexercise">

#### 4. Prueba de hipótesis conjunta --- Prueba $F$ I {-}

A veces interesa probar hipótesis conjuntas que imponen restricciones sobre coeficientes de regresión *múltiples*. Por ejemplo, en el modelo

$$medv_i = \\beta_0 + \\beta_1\\times lstat_i + \\beta_2\\times crim_i + \\beta_3\\times age_i + u_i$$

se puede probar la hipótesis nula $H_0: \\beta_2=\\beta_3$ frente a la alternativa $H_1: \\beta_2\\ne\\beta_3$ (que es una hipótesis conjunta, ya que se impone una restricción en *dos* coeficientes de regresión ).

La idea básica detrás de probar una hipótesis de este tipo es realizar dos regresiones y comparar los resultados: Para una de las regresiones, se imponen las restricciones formalizadas por la hipótesis nula (lo se llama modelo de regresión restringida), mientras que para la otra regresión la restricción se deja fuera (a esto lo se le llama el modelo irrestricto). A partir de este punto de partida, se constuye un estadístico de prueba que, bajo el valor nulo, sigue una distribución bien conocida, una distribución $F$ (ver el siguiente ejercicio).

Sin embargo, en este ejercicio se comienza con los cálculos iniciales necesarios para construir el estadístico de prueba.

Se han cargado los paquetes <tt>AER</tt> y <tt>MASS</tt>.

**Instrucciones:**

  + Estimar el modelo restringido; es decir, el modelo donde se asume que la restricción formalizada por $H_0: \\beta_2=\\beta_3$ es verdadera. Guardar el modelo en <tt>model_res</tt>.

  + Calcular el $SSR$ del modelo restringido y asignar el resultado a <tt>RSSR</tt>.

  + Estimar el modelo sin restricciones; es decir, el modelo donde se supone que la restricción es falsa. Guardar en <tt>model_unres</tt>.

  + Calcular el $SSR$ del modelo no restringido y asignar el resultado a <tt>USSR</tt>.

<iframe src="DCL/ex7_4.html" frameborder="0" scrolling="no" style="width:100%;height:340px"></iframe>

**Sugerencias:**

  + El modelo restringido se puede escribir como $$medv_i = \\beta_0 + \\beta_1\\times lstat_i + \\beta_2\\times crim_i + \\beta_2\\times age_i + u_i$$ que, después de reorganizar, puede ser expresado como $$medv_i = \\beta_0 + \\beta_1\\times lstat_i + \\beta_2\\times(crim_i+age_i) + u_i.$$

  + El $SSR$ se define como la suma de los residuos al cuadrado.

  + Se debe tener en cuenta que los residuos de un modelo de regresión están disponibles como <tt>residuals</tt> en el objeto <tt>lm</tt> correspondiente. Por lo tanto, se puede acceder a ellos como de costumbre a través del operador <tt>$</tt>.

</div>')
```

```{r, 458, echo=F, purl=F, results='asis', eval=my_output == "html"}
cat('
<div  class = "DCexercise">

#### 5. Prueba de hipótesis conjunta --- Prueba F II {-}

Después de estimar los modelos y calcular los $SSR$, ahora se tiene que calcular la estadística de prueba y realizar la prueba $F$. Como se mencionó en el último ejercicio, la estadística de prueba sigue una distribución de $F$. Más precisamente, se trata con la distribución $F_{q,n-k-1}$ donde $q$ denota el número de restricciones bajo la hipótesis nula y $k$ es el de regresores en el modelo no restringido, excluyendo la intersección.

Se han cargado los paquetes <tt>AER</tt> y <tt>MASS</tt>. Ambos modelos (<tt>model_res</tt> y <tt>model_unres</tt>) así como su SSR (<tt>RSSR</tt> y <tt>USSR</tt>) están disponibles en el entorno de trabajo.

**Instrucciones:**

  + Calcular el estadístico $F$ y asignar el resultado a <tt>Fstat</tt>.

  + Calcular el valor $p$ y asignar el resultado a <tt>pval</tt>.

  + Comprobar si la hipótesis nula se rechaza en el nivel del $1\\%$ utilizando operadores lógicos.

  + Verificar el resultado usando <tt>linearHypothesis()</tt> e imprimir los resultados.

<iframe src="DCL/ex7_5.html" frameborder="0" scrolling="no" style="width:100%;height:340px"></iframe>

**Sugerencias:**

  + El estadístico $F$ se define como $\\frac{RSSR-USSR/q}{USSR/(n-k-1)}$.

  + El valor $p$ se puede calcular como $1-F_{q,n-k-1}(F^{act})$ donde $F_{q,n-k-1}$ denota el CDF de la distribución $F$ (<tt>pf()</tt>) con grados de libertad $q$ y $nk-1$ y $F^{act}$ del estadístico $F$ calculado.

  + <tt>linearHypothesis()</tt> espera el modelo sin restricciones así como la hipótesis nula como argumentos.

</div>')
```

```{r, 459, echo=F, purl=F, results='asis', eval=my_output == "html"}
cat('
<div  class = "DCexercise">

#### 6. Prueba de hipótesis conjunta: Conjunto de confianza {-}

Como sabrá por los capítulos anteriores, la construcción de un conjunto de confianza para un único coeficiente de regresión da como resultado un intervalo de confianza simple en la línea real. Sin embargo, si se consideran los coeficientes de regresión de $n$ conjuntamente (como lo se hace en un entorno de prueba de hipótesis conjunta), se mueve de $\\mathbb{R}$ a $\\mathbb{R}^n$, lo que da como resultado un conjunto de confianza de n-dimensiones. En aras de la ilustración, a menudo se elige $n = 2$, de modo que se termina con un plano bidimensional representable.

Recuerde el modelo estimado

$$\\widehat{medv}_i = \\underset{(0.75)}{32.828} -\\underset{(0.05)}{0.994} \\times lstat_i -\\underset{(0.04)}{0.083} \\times crim_i + \\underset{(0.01)}{0.038} \\times age_i.$$

que está disponible como <tt>mod</tt> en el entorno de trabajo. Suponga que desea probar la hipótesis nula $H_0: \\beta_2=\\beta_3=0$ frente a $H_1: \\beta_2\\ne 0$ o $\\beta_3\\ne 0$.

Se han cargado los paquetes <tt>AER</tt> y <tt>MASS</tt>.

**Instrucciones:**

  + Construir un conjunto de confianza de $99\\%$ para los coeficientes de <tt>crim</tt> y <tt>lstat</tt>, que es un conjunto de confianza bidimensional. ¿Se puede rechazar la hipótesis nula mencionada anteriormente?

  + Verificar su inspección visual realizando una prueba $F$ correspondiente.

<iframe src="DCL/ex7_6.html" frameborder="0" scrolling="no" style="width:100%;height:340px"></iframe>

**Sugerencias:**

  + Utilizar <tt>trustEllipse()</tt> para construir un conjunto de confianza bidimensional. Además de los coeficientes para los que se construirá el conjunto de confianza (<tt>which.coef</tt>), se debe especificar el nivel de confianza (<tt>levels</tt>).

  + Como de costumbre, se puede usar <tt>linearHypothesis()</tt> para realizar la prueba $F$. Se debe tener en cuenta que ahora existen dos restricciones; por lo tanto, se debe pasar un vector que contenga ambas restricciones.

</div>')
```

<!--chapter:end:Capitulo_08.Rmd-->

# Funciones de regresión no lineal {#FRNL}

```{r, echo = F}
options(knitr.duplicate.label = "allow")
```

```{r, 460, child="_setup.Rmd"}
```

```{r, 461, eval=knitr::opts_knit$get("rmarkdown.pandoc.to") == "html", results='asis', echo=FALSE}
cat('<hr style="background-color:#03193b;height:2px">')
```

Hasta ahora se asume que la función de regresión era lineal; es decir, se ha tratado el parámetro de pendiente de la función de regresión como una constante. Lo cual implica que el efecto en $Y$ de un cambio de una unidad en $X$ no depende del nivel de $X$. Sin embargo, si el efecto de un cambio en $X$ sobre $Y$ depende del valor de $X$, se debería usar una función de regresión no lineal.

Al igual que en el capítulo anterior, los paquetes **AER** [@R-AER] y **stargazer** [@R-stargazer] son necesarios para la reproducción del código presentado en este capítulo. Compruebe si el fragmento de código a continuación se ejecuta sin ningún mensaje de error.

```{r, 462, warning=FALSE, message=FALSE, eval=FALSE}
library(AER)
library(stargazer)
```

## Una estrategia general para modelar funciones de regresión no lineal

Es momento de hechar un vistazo a un ejemplo en que el uso de una función de regresión no lineal es más adecuado para estimar la relación de la población entre el regresor, $X$, y el regresante, $Y$: La relación entre los ingresos de los distritos escolares y sus puntajes de prueba.

```{r dataset, 463, results='hide', echo=TRUE, message=FALSE}
# preparar los datos
library(AER)                                                     
data(CASchools)
CASchools$size <- CASchools$students/CASchools$teachers
CASchools$score <- (CASchools$read + CASchools$math) / 2       
```

Se comienza el análisis calculando la correlación entre ambas variables.

```{r, 464}
cor(CASchools$income, CASchools$score)
```

Aquí, los ingresos y los puntajes de las pruebas están relacionados positivamente: Los distritos escolares con ingresos por encima del promedio tienden a obtener puntajes por encima del promedio. ¿Una función de regresión lineal modela los datos de manera adecuada? Por tanto, se deben graficar los datos y agregar una línea de regresión lineal.

```{r, 465, fig.align = 'center'}
# ajustar un modelo lineal simple
linear_model<- lm(score ~ income, data = CASchools)

# trazar las observaciones
plot(CASchools$income, CASchools$score,
     col = "steelblue",
     pch = 20,
     xlab = "Ingresos del distrito (miles de dólares)", 
     ylab = "Resultado de la prueba",
     cex.main = 0.9,
     main = "Puntuación de la prueba frente a los ingresos del distrito y una función de regresión lineal MCO")

# agregar la línea de regresión al gráfico
abline(linear_model, 
       col = "red", 
       lwd = 2)
```

La línea de regresión lineal parece sobrestimar la verdadera relación cuando los ingresos son muy altos o muy bajos y la subestima para el grupo de ingresos medios.

Afortunadamente, MCO no solo maneja funciones lineales de los regresores. Por ejemplo, se pueden modelar las puntuaciones de las pruebas en función de los ingresos y el cuadrado de los ingresos. El modelo de regresión correspondiente es

$$TestScore_i = \beta_0 + \beta_1 \times income_i + \beta_2 \times income_i^2 + u_i,$$ llamado *modelo de regresión cuadrática*; esto es, $income^2$ se trata como una variable explicativa adicional. Por tanto, el modelo cuadrático es un caso especial de un modelo de regresión multivariante. Al ajustar el modelo con **lm()** se tiene que usar el operador **^** junto con la función **I()** para agregar el término cuadrático como un regresor adicional al argumento **formula**. Esto se debe a que la fórmula de regresión que se pasa por la función **formula** se convierte en un objeto de la clase **formula**. Para objetos de esta clase, los operadores **+**, **-**, \textbf{*} y **^** tienen una interpretación no aritmética. **I()** garantiza que se utilicen como operadores aritméticos, consultar **?I**,

```{r, 466}
# ajustar el modelo cuadrático
quadratic_model <- lm(score ~ income + I(income^2), data = CASchools)

# obtener el resumen del modelo
coeftest(quadratic_model, vcov. = vcovHC, type = "HC1")
```

La salida indica que la función de regresión estimada es

$$\widehat{TestScore}_i = \underset{(2.90)}{607.3} + \underset{(0.27)}{3.85} \times income_i - \underset{(0.0048)}{0.0423} \times income_i^2.$$

Este modelo permite probar la hipótesis de que la relación entre los puntajes de las pruebas y los ingresos del distrito es lineal frente a la alternativa de que es cuadrática. Esto corresponde a la prueba

$$H_0: \beta_2 = 0 \ \ \text{vs.} \ \  H_1: \beta_2\neq0,$$

ya que $\beta_2=0$ corresponde a una ecuación lineal simple y $\beta_2\neq0$ implica una relación cuadrática. Se encuentra que $t=(\hat\beta_2 - 0)/SE(\hat\beta_2) = -0.0423/0.0048 = -8.81$, por lo que el valor de la hipótesis nula se rechaza en cualquier nivel común de significancia y se concluye que la relación no es lineal. Esto es consistente con la impresión obtenida de la gráfica.

Ahora se dibuja el mismo diagrama de dispersión que para el modelo lineal y se agrega la línea de regresión para el modelo cuadrático. Debido a que **abline()** solo puede dibujar líneas rectas, no se puede usar aquí. **lines()** es una función que permite dibujar líneas no rectas, ver `?lines`. La llamada más básica de **lines()** es **lines(x_values, y_values)** donde **x_values** y **y_values** son vectores de misma longitud que proporcionan las coordenadas de los puntos que se conectarán *secuencialmente* por una línea. Esto hace que sea necesario ordenar los pares de coordenadas de acuerdo con los valores X. Aquí se usa la función **order()** para ordenar los valores ajustados de **score** de acuerdo con las observaciones de **income**.

```{r, 467, fig.align="center"}
# graficar un diagrama de dispersión de las observaciones para los ingresos y la puntuación de la prueba
plot(CASchools$income, CASchools$score,
     col  = "steelblue",
     pch = 20,
     xlab = "Ingresos del distrito (miles de dólares)",
     ylab = "Resultado de la prueba",
     main = "Funciones de regresión lineal y cuadrática estimadas")

# agregar una función lineal a la gráfica
abline(linear_model, col = "black", lwd = 2)

# agregar función cuadrática a la gráfica
order_id <- order(CASchools$income)

lines(x = CASchools$income[order_id], 
      y = fitted(quadratic_model)[order_id],
      col = "red", 
      lwd = 2) 
```

Se puede ver que la función cuadrática se ajusta a los datos mucho mejor que la función lineal.

## Funciones no lineales de una única variable independiente {#FNLUVI}

### Polinomios {-}

El enfoque utilizado para obtener un modelo cuadrático se puede generalizar a modelos polinomiales de grado arbitrario $r$,

$$Y_i = \beta_0 + \beta_1 X_i + \beta_2 X_i^2 + \cdots + \beta_r X_i^r + u_i.$$

Un modelo cúbico, por ejemplo, se puede estimar de la misma forma que el modelo cuadrático; solo se tiene que usar un polinomio de grado $r = 3$ en **income**. Esto se hace convenientemente usando la función **poly()**.

```{r cubic, 468}
# estimar un modelo cúbico
cubic_model <- lm(score ~ poly(income, degree = 3, raw = TRUE), data = CASchools)
```

**poly()** genera polinomios ortogonales que son ortogonales a la constante por defecto. Aquí, se establece **raw = TRUE** de modo que los polinomios sin procesar se evalúen, consultar `?Poly`.

En la práctica, surgirá la cuestión de qué orden polinómico debería elegirse. Primero, de manera similar a $r = 2$, se puede probar la hipótesis nula de que la relación verdadera es lineal contra la hipótesis alternativa de que la relación es un polinomio de grado $r$:

$$ H_0: \beta_2=0, \ \beta_3=0,\dots,\beta_r=0 \ \ \ \text{vs.} \ \ \ H_1: \text{at least one} \ \beta_j\neq0, \ j=2,\dots,r $$

Esta es una hipótesis nula conjunta con restricciones de $r-1$, por lo que puede probarse utilizando la prueba $F$ presentada en el Capítulo \@ref(PHICRM). **linearHypothesis()** se puede utilizar para realizar tales pruebas. Por ejemplo, se puede probar la hipótesis nula de un modelo lineal contra la alternativa de un polinomio de grado máximo $r = 3$ como sigue.

```{r, 469, warning=F, message=F}
# probar la hipótesis de un modelo lineal contra alternativas cuadráticas o polinominales

# configurar la matriz de hipótesis
R <- rbind(c(0, 0, 1, 0),
            c(0, 0, 0, 1))

# hacer la prueba
linearHypothesis(cubic_model,
                 hypothesis.matrix = R,
                 white.adj = "hc1")
```

Se proporciona una matriz de hipótesis como argumento **hypothesis.matrix**. Esto es útil cuando los coeficientes tienen nombres largos, como es el presente caso debido al uso de **poly()**, o cuando las restricciones incluyen múltiples coeficientes. La forma en que **linearHypothesis()** interpreta la matriz de hipótesis $\mathbf{R}$ se ve mejor usando álgebra matricial:

Para las dos restricciones lineales anteriores, se tiene:
\begin{align*}
  \mathbf{R}\boldsymbol{\beta} =& \mathbf{s} \\
  \begin{pmatrix}
    0 & 0 & 1 & 0 \\
    0 & 0 & 0 & 1
  \end{pmatrix}
  \begin{pmatrix}
    \beta_0 \\
    \beta_1 \\
    \beta_2 \\
    \beta_3 \\
  \end{pmatrix} =&
  \begin{pmatrix}
   0 \\
   0
  \end{pmatrix} \\
  \begin{pmatrix}
    \beta_2 \\
    \beta_3
  \end{pmatrix}= &
  \begin{pmatrix}
    0 \\
    0
  \end{pmatrix}.
\end{align*}

**linearHypothesis()** usa el vector cero para $\mathbf{s}$ por defecto, ver `?linearHypothesis`.

El valor de $p$ para es muy pequeño, por lo que se rechaza la hipótesis nula. Sin embargo, esto no indica *qué* $r$ elegir. En la práctica, un enfoque para determinar el grado del polinomio es utilizar *pruebas secuenciales*:

1. Estimar un modelo polinomial para un valor máximo $r$.
2. Utilizar una prueba $t$ para probar $\beta_r = 0$. *El rechazo* de la hipótesis nula significa que $X^r$ pertenece a la ecuación de regresión.
3. *La aceptación* de la hipótesis nula en el paso 2 implica que $X^r$ se puede eliminar del modelo. Continuar repitiendo el paso 1 con el orden $r-1$ y probar si $\beta_{r-1}=0$. Si la prueba es rechazada, usar un modelo polinomial de orden $r-1$.
4. Si las pruebas del paso 3 son rechazadas, continuar con el procedimiento hasta que el coeficiente de la potencia más alta sea estadísticamente significativo.

No existe una pauta inequívoca sobre cómo elegir $r$ en el paso uno. Sin embargo, como se señala en @stock2015, los datos económicos a menudo son fluidos, por lo que es apropiado elegir órdenes pequeños como $2$, $3$ o $4$.

Se demostrará cómo aplicar pruebas secuenciales con el ejemplo del modelo cúbico.

```{r, 470}
summary(cubic_model)
```

El modelo cúbico estimado almacenado en **cubic_model** es

$$ \widehat{TestScore}_i = \underset{(5.83)}{600.1} + \underset{(0.86)}{5.02} \times income -\underset{(0.03)}{0.96} \times income^2 - \underset{(0.00047)}{0.00069} \times income^3. $$

La estadística $t$ sobre $income^3$ es $1.42$, por lo que el valor nulo de que la relación es cuadrática no puede rechazarse, incluso en el nivel de $10\% $. Esto es contrario a la estrategia de usar errores estándar robustos en todo momento, por lo que también se usará una estimación robusta de varianza-covarianza para reproducir estos resultados.

```{r, 471}
# probar la hipótesis utilizando errores estándar robustos
coeftest(cubic_model, vcov. = vcovHC, type = "HC1")
```

Los errores estándar informados han cambiado. Además, el coeficiente de **income^3** ahora es significativo en el nivel de $5\%$. Esto significa que se rechaza la hipótesis de que la función de regresión es cuadrática frente a la alternativa de que es cúbica. Además, también se puede probar si los coeficientes de **income^2** e **income^3** son conjuntamente significativos utilizando una versión robusta de la prueba $F$.

```{r f-test, 472}
# realizar una prueba F robusta
linearHypothesis(cubic_model, 
                 hypothesis.matrix = R,
                 vcov. = vcovHC, type = "HC1")
```

Con un valor de $p$ de $9.043e^{-16}$, es decir, mucho menos de $0.05$, la hipótesis nula de linealidad se rechaza a favor de la alternativa de que la relación es *cuadrática* o *cúbica*.

#### Interpretación de coeficientes en modelos de regresión no lineal {-}

Los coeficientes de la regresión polinomial no tienen una interpretación simple. ¿Por qué? Piense en un modelo cuadrático: No es útil pensar en el coeficiente de $X$ como el cambio esperado en $Y$ asociado con un cambio en $X$ manteniendo constantes los otros regresores porque $X^2$ cambia conforme $X$ varía. Este es también el caso de otras desviaciones de la linealidad, por ejemplo, en modelos en los que los regresores y/o la variable dependiente se transforman logarítmicamente. Una forma de abordar esto es calcular el efecto estimado en $Y$ asociado con un cambio en $X$ para uno o más valores de $X$. Esta idea se resume en el Concepto clave 8.1.

```{r, 473, eval = my_output == "html", results='asis', echo=F, purl=F}
cat('
<div class = "keyconcept" id="KC8.1">
<h3 class = "right"> Concepto clave 8.1 </h3>          
<h3 class = "left"> El efecto esperado en $Y$ de un cambio en $X_1$ en un modelo de regresión no lineal </h3>

Considere el modelo de regresión poblacional no lineal

$$ Y_i = f(X_{1i}, X_{2i}, \\dots, X_{ki}) + u_i \\ , \\ i=1,\\dots,n,$$

donde $f(X_{1i}, X_{2i}, \\dots, X_{ki})$ es la función de regresión poblacional y $u_i$ es el término de error.

Denote por $\\Delta Y$ el cambio esperado en $Y$ asociado con $\\Delta X_1$, el cambio en $X_1$ mientras se mantiene constante $X_2, \\cdots , X_k$. En otras palabras, el cambio esperado en $Y$ es la diferencia

$$\\Delta Y = f(X_1 + \\Delta X_1, X_2, \\cdots, X_k) - f(X_1, X_2, \\cdots, X_k).$$

El estimador de esta diferencia poblacional desconocida es la diferencia entre los valores predichos para estos dos casos. Sea $\\hat{f}(X_1, X_2, \\cdots, X_k)$ el valor predicho de $Y$ basado en el estimador $\\hat{f}$ de la función de regresión poblacional. Entonces el cambio predicho en $Y$ es

$$\\Delta \\widehat{Y} = \\hat{f}(X_1 + \\Delta X_1, X_2, \\cdots, X_k) - \\hat{f}(X_1, X_2, \\cdots, X_k).$$
</p>
</div>
')
```

```{r, 474, eval = my_output == "latex", results='asis', echo=F, purl=F}
cat('\\begin{keyconcepts}[El efecto esperado en $Y$ de un cambio en $X_1$ en un modelo de regresión no lineal]{8.1}

Considere el modelo de regresión poblacional no lineal

$$ Y_i = f(X_{1i}, X_{2i}, \\dots, X_{ki}) + u_i \\ , \\ i=1,\\dots,n,$$

donde $f(X_{1i}, X_{2i}, \\dots, X_{ki})$ es la función de regresión poblacional y $u_i$ es el término de error.

Denote por $\\Delta Y$ el cambio esperado en $Y$ asociado con $\\Delta X_1$, el cambio en $X_1$ mientras se mantiene constante $X_2, \\cdots , X_k$. En otras palabras, el cambio esperado en $Y$ es la diferencia

$$\\Delta Y = f(X_1 + \\Delta X_1, X_2, \\cdots, X_k) - f(X_1, X_2, \\cdots, X_k).$$

El estimador de esta diferencia poblacional desconocida es la diferencia entre los valores predichos para estos dos casos. Sea $\\hat{f}(X_1, X_2, \\cdots, X_k)$ el valor predicho de $Y$ basado en el estimador $\\hat{f}$ de la función de regresión poblacional. Entonces el cambio predicho en $Y$ es

$$\\Delta \\widehat{Y} = \\hat{f}(X_1 + \\Delta X_1, X_2, \\cdots, X_k) - \\hat{f}(X_1, X_2, \\cdots, X_k).$$
\\end{keyconcepts}
')
```

Por ejemplo, se puede preguntar lo siguiente: ¿Cuál es el cambio predicho en los puntajes de las pruebas asociado con un cambio de una unidad (es decir, $\$1000$) en los ingresos, según la función de regresión cuadrática estimada?

$$\widehat{TestScore} = 607.3 + 3.85 \times income - 0.0423 \times income^2\ ?$$

Dado que la función de regresión es cuadrática, este efecto depende del ingreso *inicial* del distrito. Por tanto, se consideran dos casos:

1. Un aumento en los ingresos del distrito de $10$ a $11$ (es decir, de $\$10000$ per cápita a $\$11000$).

2. Un aumento en los ingresos del distrito de $40$ a $41$ (es decir, de $\$40000$ per cápita a $\$41000$).

Para obtener el $\Delta \widehat{Y}$ asociado con un cambio en el ingreso de $10$ a $11$, se usa la siguiente fórmula:

$$\Delta \widehat{Y} = \left(\hat{\beta}_0 + \hat{\beta}_1 \times 11 + \hat{\beta}_2 \times 11^2\right) - \left(\hat{\beta}_0 + \hat{\beta}_1 \times 10 + \hat{\beta}_2 \times 10^2\right) $$

Para calcular $\widehat{Y}$ usando **R**, se puede usar **predict()**.

```{r quadratic, 475}
# calcular y asignar el modelo cuadrático
quadriatic_model <- lm(score ~ income + I(income^2), data = CASchools)

# configurar datos para la predicción
new_data <- data.frame(income = c(10, 11))

# hacer la predicción
Y_hat <- predict(quadriatic_model, newdata = new_data)

# calcular la diferencia
diff(Y_hat)
```

De manera análoga, se puede calcular el efecto de un cambio en los ingresos del distrito de $40$ a $41$:

```{r, 476}
# configurar datos para la predicción
new_data <- data.frame(income = c(40, 41))

# hacer la predicción
Y_hat <- predict(quadriatic_model, newdata = new_data)

# calcula la diferencia
diff(Y_hat)
```

Entonces, para el modelo cuadrático, el cambio esperado en $TestScore$ inducido por un aumento en $income$ de $10$ a $11$ es aproximadamente $2.96$ puntos, pero un aumento en $income$ de $40$ a $41$ aumenta la puntuación prevista en solo $0.42$. Por lo tanto, la pendiente de la función de regresión cuadrática estimada es *más pronunciada* en los niveles de ingresos bajos que en los niveles más altos.

### Logaritmos {-}

Otra forma de especificar una función de regresión no lineal es usar el logaritmo natural de $Y$ y/o $X$.

Los logaritmos convierten los cambios en las variables en cambios porcentuales. Esto es conveniente, ya que muchas relaciones se expresan naturalmente en términos de porcentajes.

Existen tres casos diferentes en los que se pueden utilizar logaritmos.

1. Transformar $X$ con su logaritmo, pero no $Y$.

2. De manera análoga, se podría transformar $Y$ a su logaritmo, pero dejar $X$ en su nivel original.

3. Tanto $Y$ como $X$ se transforman a sus logaritmos.

La interpretación de los coeficientes de regresión es diferente en cada caso.

#### Caso I: $X$ está en logaritmo, $Y$ no. {-}

El modelo de regresión entonces es $$Y_i = \beta_0 + \beta_1 \times \ln(X_i) + u_i \text{, } i=1,...,n. $$, similar a la regresión polinomial, no se tiene que crear una nueva variable antes de usar **lm()**. Simplemente se puede ajusta el argumento **lm()** de la función **formula** para decirle a **R** que se debe usar la transformación logarítmica de una variable.

```{r, 477}
# estimar un modelo lineal-log (logarítmico de nivel)
LinearLog_model <- lm(score ~ log(income), data = CASchools)

# calcular resumen robusto
coeftest(LinearLog_model, 
         vcov = vcovHC, type = "HC1")
```

Por tanto, la función de regresión estimada es

$$\widehat{TestScore} = 557.8 + 36.42 \times \ln(income).$$

Resulta importante dibujar una gráfica de esta función.

```{r, 478, fig.align="center"}
# dibujar un diagrama de dispersión
plot(score ~ income, 
     col = "steelblue",
     pch = 20,
     data = CASchools,
     main = "Línea de regresión logarítmica lineal")

# agregar la línea de regresión logarítmica lineal
order_id  <- order(CASchools$income)

lines(CASchools$income[order_id],
      fitted(LinearLog_model)[order_id], 
      col = "red", 
      lwd = 2)
```

Se puede interpretar $\hat{\beta}_1$ de la siguiente manera: Un aumento de $1\%$ en los ingresos está asociado con un aumento en los puntajes de las pruebas de $0.01 \times 36.42 = 0.36$ puntos. Para obtener el efecto estimado de un cambio de una unidad en el ingreso (es decir, un cambio en las unidades originales, miles de dólares) en los puntajes de las pruebas, se puede utilizar el método presentado en el Concepto clave 8.1.

```{r, 479}
# configurar nuevos datos
new_data <- data.frame(income = c(10, 11, 40, 41))

# predecir los resultados
Y_hat <- predict(LinearLog_model, newdata = new_data)

# calcula la diferencia esperada
Y_hat_matrix <- matrix(Y_hat, nrow = 2, byrow = TRUE)
Y_hat_matrix[, 2] - Y_hat_matrix[, 1]
```

Al establecer **nrow = 2** y **byrow = TRUE** en **matrix()** se asegura que **Y_hat_matrix** es una matriz de $2\times2$ rellenada por filas con las entradas de **Y_hat**.

El modelo estimado establece que para un aumento de ingresos de $\$10000$ a $\$11000$, los puntajes de las pruebas aumentan en una cantidad esperada de $3.47$ puntos. Cuando los ingresos aumentan de $\$40000$ a $\$41000$, el aumento esperado en los puntajes de las pruebas es de solo $0.90$ puntos.

#### Caso II: $Y$ está en logaritmo, $X$ no {-}

Existen casos en los que es útil hacer una regresión $\ln(Y)$.

El modelo de regresión correspondiente entonces es

$$ \ln(Y_i) = \beta_0 + \beta_1 \times X_i + u_i , \ \ i=1,...,n. $$

```{r, 480}
# estimar un modelo log-lineal
LogLinear_model <- lm(log(score) ~ income, data = CASchools)

# obtener un resumen robusto de coeficientes
coeftest(LogLinear_model, 
         vcov = vcovHC, type = "HC1")
```

La función de regresión estimada es $$\widehat{\ln(TestScore)} = 6.439 + 0.00284 \times income.$$ Se espera que un aumento en los ingresos del distrito en $\$1000$ aumente los puntajes de las pruebas en $100\times 0.00284 \% = 0.284\%$. 

Cuando la variable dependiente está en logaritmo, uno no puede simplemente usar $e^{\log(\cdot)}$ para transformar las predicciones de nuevo a la escala original.

#### Caso III: $Y$ y $X$ están en logaritmos {-}

El modelo de regresión log-log es 

$$\ln(Y_i) = \beta_0 + \beta_1 \times \ln(X_i) + u_i, \ \ i=1,...,n.$$

```{r log log, 481}
# estimate the log-log model
LogLog_model <- lm(log(score) ~ log(income), data = CASchools)

# imprimir un resumen robusto de coeficientes en la consola
coeftest(LogLog_model, 
         vcov = vcovHC, type = "HC1")
```

Por tanto, la función de regresión estimada es $$\widehat{\ln(TestScore)} = 6.336 + 0.0554 \times \ln(income).$$ En un modelo log-log, se asocia un cambio de $1\%$ en $X$ con un cambio estimado de $\hat\beta_1 \%$ en $Y$.

```{r, 482, fig.align="center"}
# generar un diagrama de dispersión
plot(log(score) ~ income, 
     col = "steelblue", 
     pch = 20, 
     data = CASchools,
     main = "Función de regresión lineal-logarítmica")

# agregar la línea de regresión log-lineal
order_id  <- order(CASchools$income)

lines(CASchools$income[order_id], 
      fitted(LogLinear_model)[order_id], 
      col = "red", 
      lwd = 2)

# agregar la línea de regresión log-log
lines(sort(CASchools$income), 
      fitted(LogLog_model)[order(CASchools$income)], 
      col = "green", 
      lwd = 2)

# agregar una leyenda
legend("bottomright",
       legend = c("modelo log-lineal "," modelo log-log"),
       lwd = 2,
       col = c("red", "green"))
```

El Concepto clave 8.2 resume los tres modelos de regresión logarítmica.

```{r, 483, eval = my_output == "html", results='asis', echo=F, purl=F}
cat('
<div class = "keyconcept" id="KC8.2">
<h3 class = "right"> Concepto clave 8.2 </h3>          
<h3 class = "left"> Logaritmos en regresión: Tres casos </h3>

<p> Los logaritmos se pueden usar para transformar la variable dependiente $Y$ o la variable independiente $X$, o ambas (la variable que se transforma debe ser positiva). 

La siguiente tabla resume estos tres casos y la interpretación del coeficiente de regresión $\\beta_1$. En cada caso, $\\beta_1$, se puede estimar aplicando MCO después de tomar el o los logaritmos de la variable dependiente y/o independiente. </p>

<table>
<thead>
<tr class="header">
<th align="left">Caso</th>
<th align="left">Especificación del modelo</th>
<th align="left">Interpretación de $\\beta_1$</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">$(I)$</td>
<td align="left">$Y_i = \\beta_0 + \\beta_1 \\ln(X_i) + u_i$</td>
<td align="left">Un cambio de $1\\%$ en $X$ está asociado con un cambio en $Y$ de $0.01 \\times \\beta_1$.</td>
</tr>
<tr class="even">
<td align="left">$(II)$</td>
<td align="left">$\\ln(Y_i) = \\beta_0 + \\beta_1 X_i + u_i$</td>
<td align="left">Un cambio en $X$ por una unidad ($\\Delta X = 1$) está asociado con un cambio de $100 \\times \\beta_1 \\%$ en $Y$.</td>
</tr>
<tr class="odd">
<td align="left">$(III)$</td>
<td align="left">$\\ln(Y_i) = \\beta_0 + \\beta_1 \\ln(X_i) + u_i$</td>
<td align="left">Un cambio de $1\\%$ en $X$ está asociado con un cambio $\\beta_1\\%$ en $Y$, por lo que $\\beta_1$ es la elasticidad de $Y$ respecto a $X$.</td>
</tr>
</tbody>
</table>

</div>
')
```

```{r, 484, eval = my_output == "latex", results='asis', echo=F, purl=F}
cat('\\begin{keyconcepts}[Logaritmos en regresión: Tres casos]{8.2}
    \\begin{tabularx}{\\textwidth}{llX}
    \\textbf{Caso}  & \\textbf{Especificación del modelo} & \\textbf{Interpretación de $\\beta_1$} \\\\
    $(I)$ & $Y_i = \\beta_0 + \\beta_1 \\ln(X_i) + u_i$ & Un cambio de $1\\%$ en $X$ está asociado con un cambio en $Y$ de \\newline $0.01 \\times \\beta_1$. \\\\
 $(II)$  & $\\ln(Y_i) = \\beta_0 + \\beta_1 X_i + u_i$ & Un cambio en $X$ por una unidad ($\\Delta X = 1$) está asociado con un cambio de \\newline $100 \\times \\beta_1 \\%$ en $Y$. \\\\
    $(III)$ & $\\ln(Y_i) = \\beta_0 + \\beta_1 \\ln(X_i) + u_i$ & Un cambio de $1\\%$ en $X$ está asociado con un cambio $\\beta_1\\%$ en $Y$, por lo que \\newline $\\beta_1$ es la elasticidad de $Y$ respecto a $X$. \\\\
    \\end{tabularx}
\\end{keyconcepts}
')
```

Por supuesto, también se puede estimar un modelo *polylog* como

$$ TestScore_i = \beta_0 + \beta_1 \times \ln(income_i) + \beta_2 \times \ln(income_i)^2 + \beta_3 \times \ln(income_i)^3 + u_i $$

que modela la variable dependiente $TestScore$ mediante un polinomio de tercer grado del regresor $income$ transformado logarítmicamente.

```{r poly log, 485}
# estimar el modelo polylog
polyLog_model <- lm(score ~ log(income) + I(log(income)^2) + I(log(income)^3), 
                    data = CASchools)

# imprimir un resumen robusto en la consola
coeftest(polyLog_model, 
         vcov = vcovHC, type = "HC1")
```

Comparando por $\bar{R}^2$ se puede encontrar que, dejando fuera el modelo log-lineal, todos los modelos tienen un ajuste similar. En la clase de modelos polinomiales, la especificación cúbica tiene la $\bar{R}^2$ más alta, mientras que la especificación de registro lineal es la mejor de los modelos de registro.

```{r, 486}
# calcular el R^2 ajustado para los modelos no lineales
adj_R2 <-rbind("quadratic" = summary(quadratic_model)$adj.r.squared,
               "cubic" = summary(cubic_model)$adj.r.squared,
               "LinearLog" = summary(LinearLog_model)$adj.r.squared,
               "LogLinear" = summary(LogLinear_model)$adj.r.squared,
               "LogLog" = summary(LogLog_model)$adj.r.squared,
               "polyLog" = summary(polyLog_model)$adj.r.squared)

# asignar nombres de columna
colnames(adj_R2) <- "adj_R2"

adj_R2
```

Comparar ahora el modelo cúbico y el modelo logarítmico lineal trazando las correspondientes funciones de regresión estimadas.

```{r, 487, fig.align='center'}
# generar un diagrama de dispersión
plot(score ~ income, 
     data = CASchools,
     col = "steelblue", 
     pch = 20,
     main = "Funciones de regresión cúbica y logarítmica lineal")

# agregar la línea de regresión logarítmica lineal
order_id  <- order(CASchools$income)

lines(CASchools$income[order_id],
      fitted(LinearLog_model)[order_id], 
      col = "darkgreen", 
      lwd = 2)

# agregar la línea de regresión cúbica
lines(x = CASchools$income[order_id], 
      y = fitted(cubic_model)[order_id],
      col = "darkred", 
      lwd = 2) 
```

Ambas líneas de regresión parecen casi idénticas. En conjunto, el modelo logarítmico lineal puede ser preferible, ya que es más parsimonioso en términos de regresores: No incluye polinomios de mayor grado.

## Interacciones entre variables independientes

Existen preguntas de investigación en las que es interesante saber cómo el efecto sobre $Y$ de un cambio en una variable independiente, depende del valor de otra variable independiente. Por ejemplo, se puede preguntar si los distritos con muchos estudiantes de inglés se benefician de manera diferente de una disminución en el tamaño de las clases que aquellos con pocos estudiantes de inglés. Para evaluar esto usando un modelo de regresión múltiple, se incluye un término de interacción. Se consideran tres casos:

1. Interacciones entre dos variables binarias.

2. Interacciones entre una variable binaria y una continua.

3. Interacciones entre dos variables continuas.

Las siguientes subsecciones discuten estos casos brevemente y demuestran cómo realizar tales regresiones en **R**.

#### Interacciones entre dos variables binarias {-}

Tomando dos variables binarias $D_1$ y $D_2$, así como el modelo de regresión de población

$$ Y_i = \beta_0 + \beta_1 \times D_{1i} + \beta_2 \times D_{2i} + u_i. $$

Se asume que:

\begin{align*}
  Y_i=& \, \ln(Ganancias_i),\\
  D_{1i} =& \,
   \begin{cases}
      1 & \text{si $i^{ésimo}$ la persona tiene un título universitario,} \\
      0 & \text{de otro modo}.
    \end{cases} \\
  D_{2i} =& \, 
    \begin{cases}
      1 & \text{si $i^{ésimo}$ la persona es mujer,} \\
      0 & \text{si $i^{ésimo}$ la persona es hombre}.
    \end{cases}
\end{align*}

Se sabe que $\beta_1$ mide la diferencia promedio en $\ln(Ganancias)$ entre personas con y sin título universitario y $\beta_2$ es la diferencia de género en $\ln(Ganancias)$, ceteris paribus. Este modelo *no permite determinar si existe un efecto específico del género al tener un título universitario* y, de ser así, *qué tan fuerte es este efecto*. Es fácil llegar a una especificación de modelo que permita investigar esto:

$$ Y_i = \beta_0 + \beta_1 \times D_{1i} + \beta_2 \times D_{2i} + \beta_3 \times (D_{1i} \times D_{2i}) + u_i $$

$(D_{1i} \times D_{2i})$ se llama un término de interacción y $\beta_3$ mide la diferencia en el efecto de tener un título universitario para mujeres versus hombres.

```{r, 488, eval = my_output == "html", results='asis', echo=F, purl=F}
cat('
<div class = "keyconcept" id="KC8.3">
<h3 class = "right"> Concepto clave 8.3 </h3>          
<h3 class = "left"> Un método para interpretar coeficientes en regresión con variables binarias </h3>

Calcular los valores esperados de $Y$ para cada conjunto posible descrito por el conjunto de variables binarias. Comparar los valores esperados.

Los coeficientes se pueden expresar como valores esperados o como la diferencia entre al menos dos valores esperados.

</div>
')
```

```{r, 489, eval = my_output == "latex", results='asis', echo=F, purl=F}
cat('\\begin{keyconcepts}[Un método para interpretar coeficientes en regresión con variables binarias]{8.3}

Calcular los valores esperados de $Y$ para cada conjunto posible descrito por el conjunto de variables binarias. Comparar los valores esperados.

Los coeficientes se pueden expresar como valores esperados o como la diferencia entre al menos dos valores esperados.

\\end{keyconcepts}
')
```

Siguiendo el Concepto clave 8.3 se tiene que

\begin{align*}
  E(Y_i\vert D_{1i}=0, D_{2i} = d_2) =& \, \beta_0 + \beta_1 \times 0 + \beta_2 \times d_2 + \beta_3 \times (0 \times d_2) \\
  =& \, \beta_0 + \beta_2 \times d_2.
\end{align*}

Si $D_ {1i}$ cambia de $0$ a $1$ se tiene

\begin{align*}
  E(Y_i\vert D_{1i}=1, D_{2i} = d_2) =& \, \beta_0 + \beta_1 \times 1 + \beta_2 \times d_2 + \beta_3 \times (1 \times d_2) \\
  =& \, \beta_0 + \beta_1 + \beta_2 \times d_2 + \beta_3 \times d_2.
\end{align*}

Por lo tanto, el efecto general es

$$ E(Y_i\vert D_{1i}=1, D_{2i} = d_2) - E(Y_i\vert D_{1i}=0, D_{2i} = d_2) = \beta_1 + \beta_3 \times d_2 $$

por lo que el efecto es una diferencia de los valores esperados.

#### Aplicación a la proporción de estudiantes por maestro y el porcentaje de estudiantes de inglés {-}

Ahora sea

\begin{align*}
  HiSTR =& \, 
    \begin{cases}
      1, & \text{si $STR \geq 20$} \\
      0, & \text{de otro modo}.
    \end{cases} \\
  \\
  HiEL =& \,
    \begin{cases}
      1, & \text{si $PctEL \geq 10$} \\
      0, & \text{de otro modo}.
    \end{cases}
\end{align*}

Se puede usar **R** para construir las variables anteriores de la siguiente manera:

```{r, 490}
# adjuntar HiSTR a CASchools
CASchools$HiSTR <- as.numeric(CASchools$size >= 20)

# adjuntar HiEL a CASchools
CASchools$HiEL <- as.numeric(CASchools$english >= 10)
```

Se procede a estimar el modelo

\begin{align}
TestScore = \beta_0 + \beta_1 \times HiSTR + \beta_2 \times HiEL + \beta_3 \times (HiSTR \times HiEL) + u_i. (\#eq:im)
\end{align}

Existen varias formas de agregar el término de interacción al argumento **formula** cuando se usa **lm()**, pero la forma más intuitiva es usar \textbf{HiEL*HiSTR}.^[Anexando \textbf{HiEL*HiSTR} a la fórmula agregará **HiEL**, **HiSTR** y **su interacción** como regresores, mientras que **HiEL: HiSTR** solo agrega el término de interacción.]

```{r, 491}
# estimar el modelo con un término de interacción binaria
bi_model <- lm(score ~ HiSTR * HiEL, data = CASchools)

# imprimir un resumen sólido de los coeficientes
coeftest(bi_model, vcov. = vcovHC, type = "HC1")
```

El modelo de regresión estimado es 

$$\widehat{TestScore} = \underset{(1.39)}{664.1} - \underset{(1.93)}{1.9} \times HiSTR - \underset{(2.33)}{18.3} \times HiEL - \underset{(3.12)}{3.3} \times (HiSTR \times HiEL)$$ 

y predice que el efecto de pasar de un distrito escolar con una proporción baja de alumnos por maestro a un distrito con una alta proporción de alumnos por maestro, dependiendo del porcentaje alto o bajo de estudiantes de inglés, es de $-1.9-3.3\times HiEL$. Entonces, para los distritos con una baja proporción de estudiantes de inglés ($HiEL = 0$), el efecto estimado es una disminución de $1.9$ puntos en los puntajes de las pruebas, mientras que para los distritos con una gran fracción de estudiantes de inglés ($HiEL = 1$), la disminución prevista en los puntajes de las pruebas asciende a $1.9 + 3.3 = 5.2$ puntos.

También se puede utilizar el modelo para estimar la puntuación media de la prueba para cada combinación posible de las variables binarias incluidas.

```{r, 492}
# medias estimadas para todas las combinaciones de HiSTR y HiEL

# 1.
predict(bi_model, newdata = data.frame("HiSTR" = 0, "HiEL" = 0))

# 2.
predict(bi_model, newdata = data.frame("HiSTR" = 0, "HiEL" = 1))

# 3.
predict(bi_model, newdata = data.frame("HiSTR" = 1, "HiEL" = 0))

# 4.
predict(bi_model, newdata = data.frame("HiSTR" = 1, "HiEL" = 1))
```

Ahora se verifica que estas predicciones son diferencias en las estimaciones de coeficientes presentadas en la ecuación \@ref(eq:im):

\begin{align*}
\widehat{TestScore} = \hat\beta_0 = 664.1 \quad &\Leftrightarrow \quad HiSTR = 0, \, HIEL = 0\\
\widehat{TestScore} = \hat\beta_0 + \hat\beta_2 = 664.1 - 18.3 = 645.8 \quad &\Leftrightarrow \quad HiSTR = 0, \, HIEL = 1\\
\widehat{TestScore} = \hat\beta_0 + \hat\beta_1 = 664.1 - 1.9 = 662.2 \quad &\Leftrightarrow \quad HiSTR = 1, \, HIEL = 0\\
\widehat{TestScore} = \hat\beta_0 + \hat\beta_1 + \hat\beta_2 + \hat\beta_3  = 664.1 - 1.9 - 18.3 - 3.3 = 640.6 \quad &\Leftrightarrow \quad HiSTR = 1, \, HIEL = 1
\end{align*}

#### Interacciones entre una variable continua y una binaria {-}

Ahora suponga que $X_i$ denota los años de experiencia laboral de la persona $i$, que es una variable continua. Se tiene que:

\begin{align*}
  Y_i =& \, \ln(Ganancias_i), \\
  \\
  X_i =& \, \text{experiencia laboral de la persona }i, \\
  \\
  D_i =& \,  
    \begin{cases}
      1, & \text{si $i^{ésimo}$ persona tiene un título universitario} \\
      0, & \text{de otro modo}.
    \end{cases}
\end{align*}

Por tanto, el modelo de línea base es

$$ Y_i = \beta_0 + \beta_1 X_i + \beta_2 D_i + u_i, $$

un modelo de regresión múltiple que permite estimar el beneficio promedio de tener un título universitario manteniendo constante la experiencia laboral, así como el efecto promedio sobre los ingresos de un cambio en la experiencia laboral manteniendo constante el título universitario.

Al agregar el término de interacción $X_i \times D_i$ permite que el efecto de un año adicional de experiencia laboral difiera entre individuos con y sin título universitario,

$$ Y_i = \beta_0 + \beta_1 X_i + \beta_2 D_i + \beta_3  (X_i \times D_i) + u_i. $$

Aquí, $\beta_3$ es la diferencia esperada en el efecto de un año adicional de experiencia laboral para graduados universitarios versus no graduados. Otra posible especificación es:

$$ Y_i = \beta_0 + \beta_1 X_i + \beta_2 (X_i \times D_i) + u_i. $$

Este modelo establece que el impacto esperado de un año adicional de experiencia laboral en los ingresos difiere para los graduados universitarios y los no graduados, pero que graduarse por sí solo no aumenta los ingresos.

Las tres funciones de regresión se pueden visualizar mediante líneas rectas. El Concepto clave 8.4 resume las diferencias.

```{r, 493, eval = my_output == "html", results='asis', echo=F, purl=F}
cat('
<div class = "keyconcept" id="KC8.4">
<h3 class = "right"> Concepto clave 8.4 </h3>          
<h3 class = "left"> Interacciones entre variables binarias y continuas </h3>

Un término de interacción como $X_i \\times D_i$ (donde $X_i$ es continuo y $D_i$ es binario) permite que la pendiente dependa de la variable binaria $D_i$. Existen tres posibilidades:

1. Diferente intersección y misma pendiente:
$$ Y_i = \\beta_0 + \\beta_1 X_i + \\beta_2 D_i + u_i $$

2. Diferente intersección y diferente pendiente:
$$ Y_i = \\beta_0 + \\beta_1 X_i + \\beta_2 D_i + \\beta_3 \\times (X_i \\times D_i) + u_i $$

3. Misma intersección y pendiente diferente:
$$ Y_i = \\beta_0 + \\beta_1 X_i + \\beta_2 (X_i \\times D_i) + u_i $$
</div>
')
```

```{r, 494, eval = my_output == "latex", results='asis', echo=F, purl=F}
cat('\\begin{keyconcepts}[Interacciones entre variables binarias y continuas]{8.4}

Un término de interacción como $X_i \\times D_i$ (donde $X_i$ es continuo y $D_i$ es binario) permite que la pendiente dependa de la variable binaria $D_i$. Existen tres posibilidades:\\newline

\\begin{enumerate}
\\item Diferente intersección y misma pendiente:
$$ Y_i = \\beta_0 + \\beta_1 X_i + \\beta_2 D_i + u_i $$
\\item Diferente intersección y diferente pendiente:
$$ Y_i = \\beta_0 + \\beta_1 X_i + \\beta_2 D_i + \\beta_3 \\times (X_i \\times D_i) + u_i $$
\\item Misma intersección y pendiente diferente:
$$ Y_i = \\beta_0 + \\beta_1 X_i + \\beta_2 (X_i \\times D_i) + u_i $$
\\end{enumerate}

\\end{keyconcepts}
')
```

El siguiente fragmento de código demuestra cómo replicar los resultados utilizando datos artificiales.

```{r, 495, fig.align='center'}
# generar datos artificiales
set.seed(1)

X <- runif(200,0, 15)
D <- sample(0:1, 200, replace = T)
Y <- 450 +  150 * X + 500 * D + 50 * (X * D) + rnorm(200, sd = 300)

# dividir el área de graficado de manera congruente
m <- rbind(c(1, 2), c(3, 0))
graphics::layout(m)

# estimar los modelos y graficar las líneas de regresión

# 1. (modelo de línea base)
plot(X, log(Y),
     pch = 20,
     col = "steelblue",
     main = "Diferentes intercepciones, Misma pendiente")

mod1_coef <- lm(log(Y) ~ X + D)$coefficients

abline(coef = c(mod1_coef[1], mod1_coef[2]), 
       col = "red",
       lwd = 1.5)

abline(coef = c(mod1_coef[1] + mod1_coef[3], mod1_coef[2]), 
       col = "green",
       lwd = 1.5)
       
# 2. (modelo de línea base + término de interacción)
plot(X, log(Y),
     pch = 20,
     col = "steelblue",
     main = "Diferentes interceptos, Diferentes pendientes")

mod2_coef <- lm(log(Y) ~ X + D + X:D)$coefficients

abline(coef = c(mod2_coef[1], mod2_coef[2]), 
       col = "red",
       lwd = 1.5)

abline(coef = c(mod2_coef[1] + mod2_coef[3], mod2_coef[2] + mod2_coef[4]), 
       col = "green",
       lwd = 1.5)

# 3. (omisión de D como regresor + término de interacción)
plot(X, log(Y),
     pch = 20,
     col = "steelblue",
     main = "Mismo intercepto, Diferentes pendientes")

mod3_coef <- lm(log(Y) ~ X + X:D)$coefficients

abline(coef = c(mod3_coef[1], mod3_coef[2]), 
       col = "red",
       lwd = 1.5)

abline(coef = c(mod3_coef[1], mod3_coef[2] + mod3_coef[3]), 
       col = "green",
       lwd = 1.5)
```

#### Aplicación a la proporción de estudiantes por maestro y el porcentaje de estudiantes de inglés {-}

Usando una especificación de modelo como la segunda discutida en el Concepto clave 8.3 (pendiente diferente, intersección diferente) se puede responder a la pregunta de si el efecto en los puntajes de las pruebas de disminuir la proporción de estudiantes por maestro depende de si hay muchos o pocos estudiantes de inglés. Estimando el modelo de regresión:

$$ \widehat{TestScore_i} = \beta_0 + \beta_1 \times size_i + \beta_2 \times HiEL_i + \beta_2 (size_i \times HiEL_i) + u_i. $$

```{r, 496}
# estimar el modelo
bci_model <- lm(score ~ size + HiEL + size * HiEL, data = CASchools)

# imprimir un resumen sólido de coeficientes en la consola
coeftest(bci_model, vcov. = vcovHC, type = "HC1")
```

El modelo de regresión estimado es 

$$ \widehat{TestScore} = \underset{(11.87)}{682.2} - \underset{(0.59)}{0.97} \times size + \underset{(19.51)}{5.6} \times HiEL - \underset{(0.97)}{1.28} \times (size \times HiEL). $$  

La línea de regresión estimada para distritos con una fracción baja de estudiantes de inglés ($HiEL_i=0$) es 

$$ \widehat{TestScore} = 682.2 - 0.97\times size_i. $$

Para los distritos con una alta fracción de estudiantes de inglés, se tiene:

\begin{align*} 
  \widehat{TestScore} =& \, 682.2 + 5.6 - 0.97\times size_i - 1.28 \times size_i \\
   =& \, 687.8 - 2.25 \times size_i.
\end{align*}

El aumento previsto en los puntajes de las pruebas después de una reducción de la proporción de estudiantes por maestro en $1$ unidad es de aproximadamente $0.97$ puntos en distritos donde la fracción de estudiantes de inglés es baja, pero $2.25$ en distritos con una alta proporción de estudiantes de inglés. A partir del coeficiente del término de interacción $size \times HiEL$, se puede ver que la diferencia entre ambos efectos es de $1.28$ puntos.

El siguiente fragmento de código grafica ambas líneas que pertenecen al modelo. Para hacer observaciones con $HiEL = 0$ distinguibles de aquellas con $HiEL = 1$, usando diferentes colores.

```{r, 497, fig.align = 'center'}
# identificar observaciones con PctEL> = 10
id <- CASchools$english >= 10

# graficar observaciones con HiEL = 0 como puntos rojos
plot(CASchools$size[!id], CASchools$score[!id],
     xlim = c(0, 27),
     ylim = c(600, 720),
     pch = 20,
     col = "red",
     main = "",
     xlab = "Tamaño de la clase",
     ylab = "Resultado de la prueba")

# graficar observaciones con HiEL = 1 como puntos verdes
points(CASchools$size[id], CASchools$score[id],
     pch = 20,
     col = "green")

# leer los coeficientes estimados de bci_model
coefs <- bci_model$coefficients

# dgraficar la línea de regresión estimada para HiEL = 0
abline(coef = c(coefs[1], coefs[2]),
       col = "red",
       lwd = 1.5)

# graficar la línea de regresión estimada para HiEL = 1
abline(coef = c(coefs[1] + coefs[3], coefs[2] + coefs[4]),
       col = "green", 
       lwd = 1.5 )

# agregar una leyenda a la trama
legend("topright", 
       pch = c(20, 20), 
       col = c("red", "green"), 
       legend = c("HiEL = 0", "HiEL = 1"))
```

#### Interacciones entre dos variables continuas {-}

Considere un modelo de regresión con $Y$ las ganancias logarítmicas y dos regresores continuos $X_1$, los años de experiencia laboral, y $X_2$, los años de escolaridad. Se quiere estimar el efecto sobre los salarios de un año adicional de experiencia laboral en función de un nivel de escolaridad determinado. Este efecto se puede evaluar al incluir el término de interacción $(X_{1i} \times X_{2i})$ en el modelo:

$$ \Delta Y_i = \beta_0 + \beta_1 \times X_{1i} + \beta_2 \times X_{2i} + \beta_3 \times (X_{1i} \times X_{2i}) + u_i $$

Siguiendo el Concepto clave 8.1 se puede encontrar que el efecto en $Y$ de un cambio en $X_1$ dado $X_2$ es:

$$ \frac{\Delta Y}{\Delta X_1} = \beta_1 + \beta_3 X_2. $$

En el ejemplo de ingresos, un $\beta_3$ positivo implica que el efecto sobre los ingresos logarítmicos de un año adicional de experiencia laboral crece linealmente con los años de escolaridad. Viceversa, se tiene $$ \frac{\Delta Y}{\Delta X_2} = \beta_2 + \beta_3 X_1 $$ como el efecto sobre los ingresos logarítmicos de un año adicional de escolaridad manteniendo constante la experiencia laboral.

En total, se puede encontrar que $\beta_3$ mide el efecto de un aumento unitario en $X_1$ y $X_2$ *más allá de* los efectos de aumentar $X_1$ y $X_2$ solo en una unidad. El cambio general en $Y$ es por lo tanto

\begin{align}
Y_i = (\beta_1 + \beta_3 X_2) \Delta X_1 + (\beta_2 + \beta_3 X_1) \Delta X_2 + \beta_3\Delta X_1 \Delta X_2. (\#eq:generalinteraction)
\end{align}

El Concepto clave 8.5 resume las interacciones entre dos regresores en regresión múltiple.

```{r, 498, eval = my_output == "html", results='asis', echo=F, purl=F}
cat('
<div class = "keyconcept" id="KC8.5">
<h3 class = "right"> Concepto clave 8.5 </h3>          
<h3 class = "left"> Interacciones en regresión múltiple </h3>

El término de interacción entre los dos regresores $X_1$ y $X_2$ viene dado por su producto $X_1 \\times X_2$. 

Agregar dicho término de interacción como regresor al modelo $$ Y_i = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + u_i $$ permite que el efecto en $Y$ de un cambio en $X_2$ dependa del valor de $X_1$ y viceversa. Así, el coeficiente $\\beta_3$ en el modelo $$ Y_i = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 (X_1 \\times X_2) + u_i $$ mide el efecto de un aumento unitario en $X_1$ <it>y</it> $X_2$ por encima y más allá de la suma de ambos efectos individuales. Esto es válido para regresores *continuos* y *binarios*.

</div>
')
```

```{r, 499, eval = my_output == "latex", results='asis', echo=F, purl=F}
cat('\\begin{keyconcepts}[Interacciones en regresión múltiple]{8.5}

El término de interacción entre los dos regresores $X_1$ y $X_2$ viene dado por su producto $X_1 \\times X_2$. 

Agregar dicho término de interacción como regresor al modelo $$ Y_i = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + u_i $$ permite que el efecto en $Y$ de un cambio en $X_2$ dependa del valor de $X_1$ y viceversa. Así, el coeficiente $\\beta_3$ en el modelo $$ Y_i = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 (X_1 \\times X_2) + u_i $$ mide el efecto de un aumento unitario en $X_1$ \\textit{y} $X_2$ por encima y más allá de la suma de ambos efectos individuales. Esto es válido para regresores \\textit{continuos} y \\textit{binarios}.

\\end{keyconcepts}
')
```

#### Aplicación a la proporción de estudiantes por maestro y el porcentaje de estudiantes de inglés

Ahora se necesita examinar la interacción entre las variables continuas de la proporción de estudiantes por maestro y el porcentaje de estudiantes de inglés.

```{r, 500}
# estimar el modelo de regresión, incluida la interacción entre 'PctEL' y 'size'
cci_model <- lm(score ~ size + english + english * size, data = CASchools) 

# imprimir un resumen en la consola
coeftest(cci_model, vcov. = vcovHC, type = "HC1")
```

La ecuación del modelo estimada es 

$$ \widehat{TestScore} = \underset{(11.76)}{686.3} - \underset{(0.59)}{1.12} \times STR - \underset{(0.37)}{0.67} \times PctEL + \underset{(0.02)}{0.0012} \times (STR\times PctEL). $$

Para la interpretación, se consideran los cuartiles de $PctEL$.

```{r, 501}
summary(CASchools$english)
```

De acuerdo con \@ref(eq:generalinteraction), si $PctEL$ se encuentra en su valor mediano de $8.78$, se predice que la pendiente de la función de regresión que relaciona los puntajes de las pruebas y la proporción alumno-maestro será $-1.12 + 0.0012 \times 8.78 = -1.11$. Esto implica que el aumento de la proporción de estudiantes por maestro en una unidad deteriora los puntajes de las pruebas en $1.11$ puntos. Para el cuantil $75\%$, el cambio estimado en $TestScore$ de un aumento de una unidad en $STR$ se estima en $-1.12 + 0.0012 \times 23.0 = -1.09$, por lo que la pendiente es algo menor. La interpretación es que para un distrito escolar con una participación del $23\%$ de estudiantes de inglés, se espera que una reducción de la proporción de estudiantes por maestro en una unidad aumente los puntajes de las pruebas en solo $1.09$ puntos.

Sin embargo, el resultado de **summary()** indica que la diferencia del efecto para la mediana y el cuantil $75\%$ no es estadísticamente significativa. $H_0: \beta_3 = 0$ no puede rechazarse en el nivel de significancia de $5\%$ (el valor $p$ es $0.95$).

#### Ejemplo: La demanda de revistas económicas {-}

En esta sección se replica el ejemplo empírico \textit{La demanda de revistas económicas}. La pregunta central es: ¿Qué tan elástica es la demanda por parte de las bibliotecas de revistas económicas? La idea aquí es analizar la relación entre el número de suscripciones a una revista en las bibliotecas de EE. UU. y el precio de suscripción de la revista. El estudio utiliza el conjunto de datos **Journals** que se proporciona con el paquete **AER** y contiene observaciones para revistas económicas de $180$ para el año 2000. Puede usar la función de ayuda (`?Journals`) para obtener más información sobre los datos después de cargar el paquete.

```{r, 502}
# cargar el paquete y conjunto de datos
library(AER)
data("Journals")
```

Se mide el precio como "precio por cita" y se calcula la antigüedad de la revista y el número de caracteres manualmente. Para mantener la coherencia, también se cambia el nombre de las variables.

```{r, 503}
# definir y renombrar variables
Journals$PricePerCitation <- Journals$price/Journals$citations
Journals$Age <- 2000 - Journals$foundingyear
Journals$Characters <- Journals$charpp * Journals$pages/10^6
Journals$Subscriptions <- Journals$subs
```

El rango de "precio por cita" es bastante amplio:

```{r, 504}
# calcular estadísticas resumidas para el precio por cita
summary(Journals$PricePerCitation)
```

El precio más bajo observado es de solo $0.5$¢ por cita, mientras que el precio más alto es de más de $20$¢ por cita.

Ahora se estiman cuatro especificaciones de modelo diferentes. Todos los modelos son modelos log-log. Esto es útil porque permite interpretar directamente los coeficientes como elasticidades, ver Concepto clave 8.2. $(I)$ sobre un modelo lineal. Para aliviar un posible sesgo de variable omitida, $(II)$ aumenta $(I)$ por las covariables $\ln(Age)$ y $\ln(Characters)$. El modelo más grande $(III)$ intenta capturar las no linealidades en la relación de $\ln(Subscriptions)$ y $\ln(PricePerCitation)$ usando una función de regresión cúbica de $\ln(PricePerCitation)$ y también agrega el término de interacción $(PricePerCitation \times Age)$ mientras que la especificación $(IV)$ no incluye el término cúbico.

\begin{align*}
  (I)\quad \ln(Subscriptions_i) =& \, \beta_0 + \beta_1 \ln(PricePerCitation_i) + u_i \\
  \\
  (II)\quad \ln(Subscriptions_i) =& \, \beta_0 + \beta_1 \ln(PricePerCitation_i) + \beta_4 \ln(Age_i) + \beta_6 \ln(Characters_i) + u_i \\
  \\
  (III)\quad \ln(Subscriptions_i) =& \, \beta_0 + \beta_1 \ln(PricePerCitation_i) + \beta_2 \ln(PricePerCitation_i)^2 \\
  +& \, \beta_3 \ln(PricePerCitation_i)^3 + \beta_4 \ln(Age_i) + \beta_5 \left[\ln(Age_i) \times \ln(PricePerCitation_i)\right] \\ +& \, \beta_6 \ln(Characters_i) + u_i \\
  \\
  (IV)\quad \ln(Subscriptions_i) =& \, \beta_0 + \beta_1 \ln(PricePerCitation_i) + \beta_4 \ln(Age_i) + \beta_6 \ln(Characters_i) + u_i
\end{align*}

```{r, 505}
# Estimar modelos (I) - (IV)
Journals_mod1 <- lm(log(Subscriptions) ~ log(PricePerCitation), 
                    data = Journals)

Journals_mod2 <- lm(log(Subscriptions) ~ log(PricePerCitation) 
                    + log(Age) + log(Characters), 
                    data = Journals)

Journals_mod3 <- lm(log(Subscriptions) ~ 
                    log(PricePerCitation) + I(log(PricePerCitation)^2) 
                    + I(log(PricePerCitation)^3) + log(Age) 
                    + log(Age):log(PricePerCitation) + log(Characters), 
                    data = Journals)

Journals_mod4 <- lm(log(Subscriptions) ~ 
                    log(PricePerCitation) + log(Age) 
                    + log(Age):log(PricePerCitation) + 
                    log(Characters), 
                    data = Journals)
```

Usando **summary()**, se obtienen los siguientes modelos estimados:

\begin{align*}
  (I)\quad \widehat{\ln(Subscriptions_i)} =& \, 4.77 - 0.53 \ln(PricePerCitation_i) \\
  \\
  (II)\quad \widehat{\ln(Subscriptions_i)} =& \, 3.21 - 0.41 \ln(PricePerCitation_i) + 0.42 \ln(Age_i) + 0.21 \ln(Characters_i) \\
  \\
  (III)\quad \widehat{\ln(Subscriptions_i)} =& \, 3.41 - 0.96 \ln(PricePerCitation_i) + 0.02 \ln(PricePerCitation_i)^2 \\
  &+ 0.004 \ln(PricePerCitation_i)^3 + 0.37 \ln(Age_i) \\
  &+ 0.16 \left[\ln(Age_i) \times \ln(PricePerCitation_i)\right] \\ &+ 0.23 \ln(Characters_i) \\
  \\
  (IV)\quad \widehat{\ln(Subscriptions_i)} =& \, 3.43 - 0.90 \ln(PricePerCitation_i) + 0.37 \ln(Age_i) \\ 
  &+ 0.14 \left[\ln(Age_i) \times \ln(PricePerCitation_i)\right] + 0.23 \ln(Characters_i)
\end{align*}

Se usa una prueba $F$ para probar si las transformaciones de $\ln(PricePerCitation)$ en el modelo $(III)$ son estadísticamente significativas.

```{r, 506}
# Prueba F de significancia de términos cúbicos
linearHypothesis(Journals_mod3, 
                 c("I(log(PricePerCitation)^2)=0", "I(log(PricePerCitation)^3)=0"),
                 vcov. = vcovHC, type = "HC1")
```

Claramente, no se puede rechazar la hipótesis nula $H_0: \beta_3=\beta_4=0$ en el modelo $(III)$.

Ahora se muestra cómo se puede usar la función **stargazer()** para generar una representación tabular de los cuatro modelos.

```{r, 507, eval=FALSE}
# load the stargazer package
library(stargazer)

# recopilar errores estándar robustos en una lista
rob_se <- list(sqrt(diag(vcovHC(Journals_mod1, type = "HC1"))),
               sqrt(diag(vcovHC(Journals_mod2, type = "HC1"))),
               sqrt(diag(vcovHC(Journals_mod3, type = "HC1"))),
               sqrt(diag(vcovHC(Journals_mod4, type = "HC1"))))

# generar una tabla LaTeX usando stargazer
stargazer(Journals_mod1, Journals_mod2, Journals_mod3, Journals_mod4,
          se = rob_se,
          digits = 3,
          column.labels = c("(I)", "(II)", "(III)", "(IV)"))
```

<!--html_preserve-->

```{r, 508, results='asis', echo=F, purl=F, message=FALSE, warning=FALSE, eval=my_output == "html"}
# carga el paquete stargazer
library(stargazer)

# recopilar errores estándar robustos en una lista
rob_se <- list(
  sqrt(diag(vcovHC(Journals_mod1, type = "HC1"))),
  sqrt(diag(vcovHC(Journals_mod2, type = "HC1"))),
  sqrt(diag(vcovHC(Journals_mod3, type = "HC1"))),
  sqrt(diag(vcovHC(Journals_mod4, type = "HC1")))
)

# generar una tabla LaTeX usando Stargazer
stargazer(Journals_mod1, Journals_mod2, Journals_mod3, Journals_mod4,
          type = "html", 
          model.numbers = FALSE,
          header = FALSE,
          dep.var.caption = "Variable dependiente: Logaritmo de suscripciones",
          se = rob_se,
          digits = 3,
          column.labels = c("(I)", "(II)", "(III)", "(IV)")
          )

stargazer_html_title("Modelos de regresión no lineal de suscripciones a revistas", "nrmojs")
```

<!--/html_preserve-->

```{r, 509, results='asis', echo=F, purl=F, message=FALSE, warning=FALSE, eval=my_output == "latex"}
library(stargazer)

rob_se <- list(
  sqrt(diag(vcovHC(Journals_mod1, type = "HC1"))),
  sqrt(diag(vcovHC(Journals_mod2, type = "HC1"))),
  sqrt(diag(vcovHC(Journals_mod3, type = "HC1"))),
  sqrt(diag(vcovHC(Journals_mod4, type = "HC1")))
)

stargazer(Journals_mod1, Journals_mod2, Journals_mod3, Journals_mod4,
          type = "latex", 
          float.env = "sidewaystable",
          dep.var.caption = "Variable dependiente: Logaritmo de suscripciones",
          title = "\\label{tab:nrmojs} Modelos de regresión no lineal de suscripciones a revistas",
          model.numbers = FALSE,
          header=FALSE,
          se = rob_se,
          digits = 3,
          column.labels = c("(I)", "(II)", "(III)", "(IV)")
          )
```

El siguiente fragmento de código reproduce diversos gráficos.

```{r, 510}
# dividir el área de graficado
m <- rbind(c(1, 2), c(3, 0))
graphics::layout(m)

# gráfico de dispersión
plot(Journals$PricePerCitation, 
     Journals$Subscriptions, 
     pch = 20, 
     col = "steelblue",
     ylab = "Suscripciones",
     xlab = "ln(Precio por cita)",
     main = "(a)")

# diagrama de dispersión log-log y línea de regresión estimada (I)
plot(log(Journals$PricePerCitation), 
     log(Journals$Subscriptions), 
     pch = 20, 
     col = "steelblue",
     ylab = "ln(Suscripciones)",
     xlab = "ln(Precio por cita)",
     main = "(b)")

abline(Journals_mod1,
       lwd = 1.5)

# diagrama de dispersión log-log y líneas de regresión (IV) para Age = 5 y Age = 80
plot(log(Journals$PricePerCitation), 
     log(Journals$Subscriptions), 
     pch = 20, 
     col = "steelblue",
     ylab = "ln(Suscripciones)",
     xlab = "ln(Precio por cita)",
     main = "(c)")

JM4C <-Journals_mod4$coefficients

# Age = 80
abline(coef = c(JM4C[1] + JM4C[3] * log(80), 
                JM4C[2] + JM4C[5] * log(80)),
       col = "darkred",
       lwd = 1.5)

# Age = 5
abline(coef = c(JM4C[1] + JM4C[3] * log(5), 
                JM4C[2] + JM4C[5] * log(5)),
       col = "darkgreen",
       lwd = 1.5)
```

Como puede verse en los gráficos (a) y (b), la relación entre las suscripciones y el precio de la cita es adversa y no lineal. La transformación logarítmica de ambas variables la hace aproximadamente lineal. El gráfico (c) muestra que la elasticidad del precio de las suscripciones a revistas depende de la edad de la revista: La línea roja muestra la relación estimada para $Age = 80$ mientras que la línea verde representa la predicción del modelo $(IV)$ para $Age = 5$.

¿Qué conclusión se pueden obtener?

1. Se concluye que la demanda de revistas es más elástica para revistas jóvenes que para revistas antiguas.

2. Para el modelo $(III)$ no se puede rechazar la hipótesis nula de que los coeficientes en $\ln(PricePerCitation)^2$ y $\ln(PricePerCitation)^3$ son ambos cero usando una prueba $F$. Esta es una evidencia compatible con una relación lineal entre las suscripciones logarítmicas y el precio logarítmico.

3. La demanda es mayor para las revistas con más caracteres, manteniendo constante el precio y la edad.

En conjunto, las estimaciones sugieren que la demanda es muy inelástica; es decir, la demanda de revistas económicas de las bibliotecas es bastante insensible al precio: Utilizando el modelo $(IV)$, incluso para una revista joven ($Age = 5$) la estimación de la elasticidad del precio es $-0.899+0.374\times\ln(5)+0.141\times\left[\ln(1)\times\ln(5)\right] \approx -0.3$ , por lo que un aumento del precio en $1\%$ Se predice que reducirá la demanda en solo $0.3\%$.

Este hallazgo no sorprende, ya que proporcionar las publicaciones más recientes es una necesidad para las bibliotecas.

## Efectos no lineales en los puntajes de las pruebas de la proporción alumno-maestro

En esta sección se discutirán tres preguntas específicas sobre la relación entre los puntajes de las pruebas y la proporción alumno-maestro:

1. ¿El efecto en las calificaciones de las pruebas de disminuir la proporción de estudiantes por maestro depende de la fracción de estudiantes de inglés cuando se controlan las idiosincrasias económicas de los diferentes distritos?

2. ¿Depende este efecto de la proporción de alumnos por maestro?

3. *¿Qué tan fuerte* es el efecto de disminuir la proporción de estudiantes por maestro (en dos estudiantes por maestro) si se toman en cuenta las características económicas y las no linealidades?

Para responder a estas preguntas, se consideran un total de siete modelos, algunos de los cuales son especificaciones de regresión no lineal de los tipos que se han discutido anteriormente. Como medidas de los antecedentes económicos de los estudiantes, se consideran adicionalmente los regresores $lunch$ y $\ln(income)$. Se usa el logaritmo de $income$ porque el análisis del Capítulo \@ref(FNLUVI) mostró que la relación no lineal entre $income$ y $TestScores$ es aproximadamente logarítmica. No se incluye el gasto por alumno ($expenditure$) porque hacerlo implicaría que el gasto varía con la proporción alumno-maestro.

#### Modelos de regresión no lineal de puntajes de prueba {-}

Las especificaciones del modelo consideradas son:

\begin{align}
 TestScore_i =& \beta_0 + \beta_1 size_i + \beta_4 english_i + \beta_9 lunch_i + u_i \\
 TestScore_i =& \beta_0 + \beta_1 size_i + \beta_4 english_i + \beta_9 lunch_i + \beta_{10} \ln(income_i) + u_i \\
  TestScore_i =& \beta_0 + \beta_1 size_i + \beta_5 HiEL_i + \beta_6 (HiEL_i\times size_i) + u_i \\
  TestScore_i =& \beta_0 + \beta_1 size_i + \beta_5 HiEL_i + \beta_6 (HiEL_i\times size_i) + \beta_9 lunch_i + \beta_{10} \ln(income_i) + u_i \\
  TestScore_i =& \beta_0 + \beta_1 size_i + \beta_2 size_i^2 + \beta_5 HiEL_i + \beta_9 lunch_i + \beta_{10} \ln(income_i) + u_i \\
  TestScore_i =& \beta_0 + \beta_1 size_i + \beta_2 size_i^2 + \beta_3 size_i^3 + \beta_5 HiEL_i + \beta_6 (HiEL\times size) \\  &+ \beta_7 (HiEL_i\times size_i^2) + \beta_8 (HiEL_i\times size_i^3) + \beta_9 lunch_i + \beta_{10} \ln(income_i) + u_i \\
  TestScore_i =& \beta_0 + \beta_1 size_i + \beta_2 size_i^2 + \beta_3 size_i^3 + \beta_4 english + \beta_9 lunch_i + \beta_{10} \ln(income_i) + u_i
\end{align}

```{r, 511, tidy=TRUE}
# estimar todos los modelos
TestScore_mod1 <- lm(score ~ size + english + lunch, data = CASchools)

TestScore_mod2 <- lm(score ~ size + english + lunch + log(income), data = CASchools)

TestScore_mod3 <- lm(score ~ size + HiEL + HiEL:size, data = CASchools)

TestScore_mod4 <- lm(score ~ size + HiEL + HiEL:size + lunch + log(income), data = CASchools)

TestScore_mod5 <- lm(score ~ size + I(size^2) + I(size^3) + HiEL + lunch + log(income), data = CASchools)

TestScore_mod6 <- lm(score ~ size + I(size^2) + I(size^3) + HiEL + HiEL:size + HiEL:I(size^2) + HiEL:I(size^3) + lunch + log(income), data = CASchools)

TestScore_mod7 <- lm(score ~ size + I(size^2) + I(size^3) + english + lunch + log(income), data = CASchools)
```

Se puede usar **summary()** para evaluar el ajuste de los modelos. Usando **stargazer()** también se puede obtener una representación tabular de todas las salidas de regresión y que es más conveniente para la comparación de los modelos.

```{r, 512, eval=FALSE, message=FALSE, warning=FALSE}
# recopilar errores estándar robustos en una lista
rob_se <- list(sqrt(diag(vcovHC(TestScore_mod1, type = "HC1"))),
               sqrt(diag(vcovHC(TestScore_mod2, type = "HC1"))),
               sqrt(diag(vcovHC(TestScore_mod3, type = "HC1"))),
               sqrt(diag(vcovHC(TestScore_mod4, type = "HC1"))),
               sqrt(diag(vcovHC(TestScore_mod5, type = "HC1"))),
               sqrt(diag(vcovHC(TestScore_mod6, type = "HC1"))),
               sqrt(diag(vcovHC(TestScore_mod7, type = "HC1"))))

# generar una tabla LaTeX de salidas de regresión
stargazer(TestScore_mod1, 
          TestScore_mod2, 
          TestScore_mod3, 
          TestScore_mod4, 
          TestScore_mod5, 
          TestScore_mod6, 
          TestScore_mod7,
          digits = 3,
          dep.var.caption = "Variable dependiente: Puntaje de la prueba",
          se = rob_se,
          column.labels = c("(1)", "(2)", "(3)", "(4)", "(5)", "(6)", "(7)"))
```

<!--html_preserve-->

```{r, 513, echo=F, results='asis', warning=FALSE, message=FALSE, eval = my_output == "html"}
rob_se <- list(
  sqrt(diag(vcovHC(TestScore_mod1, type = "HC1"))),
  sqrt(diag(vcovHC(TestScore_mod2, type = "HC1"))),
  sqrt(diag(vcovHC(TestScore_mod3, type = "HC1"))),
  sqrt(diag(vcovHC(TestScore_mod4, type = "HC1"))),
  sqrt(diag(vcovHC(TestScore_mod5, type = "HC1"))),
  sqrt(diag(vcovHC(TestScore_mod6, type = "HC1"))),
  sqrt(diag(vcovHC(TestScore_mod7, type = "HC1")))
)

stargazer(TestScore_mod1, 
          TestScore_mod2, 
          TestScore_mod3, 
          TestScore_mod4, 
          TestScore_mod5, 
          TestScore_mod6, 
          TestScore_mod7,
          digits = 3,
          dep.var.caption = "Variable dependiente: Puntaje de la prueba",
          se = rob_se,
          type = "html", 
          model.numbers = FALSE,
          header=FALSE,
          column.labels = c("(1)", "(2)", "(3)", "(4)", "(5)", "(6)", "(7)")
          )

stargazer_html_title("Modelos no lineales de puntajes de prueba", "nmots")
```

<!--/html_preserve-->

```{r, 514, echo=F, results='asis', warning= FALSE, message=FALSE, eval = my_output == "latex"}
rob_se <- list(
  sqrt(diag(vcovHC(TestScore_mod1, type = "HC1"))),
  sqrt(diag(vcovHC(TestScore_mod2, type = "HC1"))),
  sqrt(diag(vcovHC(TestScore_mod3, type = "HC1"))),
  sqrt(diag(vcovHC(TestScore_mod4, type = "HC1"))),
  sqrt(diag(vcovHC(TestScore_mod5, type = "HC1"))),
  sqrt(diag(vcovHC(TestScore_mod6, type = "HC1"))),
  sqrt(diag(vcovHC(TestScore_mod7, type = "HC1")))
)

stargazer(TestScore_mod1, 
          TestScore_mod2, 
          TestScore_mod3, 
          TestScore_mod4, 
          TestScore_mod5, 
          TestScore_mod6, 
          TestScore_mod7,
          dep.var.caption = "Variable dependiente: Puntaje de la prueba",
          title = "\\label{tab:nmots} Modelos no lineales de puntajes de prueba",
          digits = 3,
          se = rob_se,
          type = "latex", 
          float.env = "sidewaystable",
          model.numbers = FALSE,
          omit.stat = c("f","ser"),
          header=FALSE,
          column.labels = c("(1)", "(2)", "(3)", "(4)", "(5)", "(6)", "(7)")
          )
```

Es momento de resumir lo que se puede concluir a partir de los resultados presentados en la Tabla \@ref(tab:nmots).

En primer lugar, el coeficiente de $size$ es estadísticamente significativo en los siete modelos. Sumando $\ln(income)$ al modelo (1) se encuentra que el coeficiente correspondiente es estadísticamente significativo a $1\%$ mientras que todos los demás coeficientes permanecen en su nivel de significancia. Además, la estimación del coeficiente de $size$ es aproximadamente $0.27$ puntos más grande, lo que puede ser un signo de sesgo atenuado de variable omitida. Considerando que esta es una razón para incluir $\ln(income)$ como regresor también en otros modelos.

Las regresiones (3) y (4) tienen como objetivo evaluar el efecto de permitir una interacción entre $size$ y $HiEL$, sin y con variables de control económico. En ambos modelos, tanto el coeficiente del término de interacción como el coeficiente de la variable ficticia no son estadísticamente significativos. Por lo tanto, incluso con controles económicos no se puede rechazar la hipótesis nula de que el efecto de la proporción de estudiantes por maestro en los puntajes de las pruebas es el mismo para los distritos con una proporción alta y los distritos con una proporción baja de estudiantes que aprenden inglés.

La regresión (5) incluye un término cúbico para la relación alumno-maestro y omite la interacción entre $size$ y $HiEl$. Los resultados indican que existe un efecto no lineal de la proporción alumno-maestro en los puntajes de las pruebas (¿Puede verificar esto usando una prueba $F$ de $H_0: \beta_2=\beta_3=0$?)

En consecuencia, la regresión (6) explora más a fondo si la fracción de estudiantes de inglés afecta la proporción de estudiantes por maestro al usar $HiEL \times size$ y las interacciones $HiEL \times size^2$ y $HiEL \times size^3$. Todas las pruebas de $t$ individuales indican que existen efectos significativos. Se puede verificar esto usando una prueba $F$ robusta de $H_0: \beta_6=\beta_7=\beta_8=0$. 

```{r, 515}
# comprobar la importancia conjunta de los términos de interacción
linearHypothesis(TestScore_mod6, 
                 c("size:HiEL=0", "I(size^2):HiEL=0", "I(size^3):HiEL=0"),
                 vcov. = vcovHC, type = "HC1")
```

Se puede encontrar que el valor nulo se puede rechazar al nivel de $5\%$ y se concluye que la función de regresión difiere para los distritos con un porcentaje alto y bajo de estudiantes de inglés.

La especificación (7) usa una medida continua para la proporción de estudiantes de inglés en lugar de una variable ficticia (y por lo tanto no incluye términos de interacción). Se pueden observar solo pequeños cambios en las estimaciones de coeficientes en los otros regresores y, por lo tanto, se concluye que los resultados observados para la especificación (5) no son sensibles a la forma en que se mide el porcentaje de estudiantes de inglés.

Se continua produciendo una gráfica para la interpretación de las especificaciones no lineales (2), (5) y (7).

```{r, 516, fig.align='center'}
# gráfico de dispersión
plot(CASchools$size, 
     CASchools$score, 
     xlim = c(12, 28),
     ylim = c(600, 740),
     pch = 20, 
     col = "gray", 
     xlab = "Proporción alumno-maestro", 
     ylab = "Resultado de la prueba")

# agregar una leyenda
legend("top", 
       legend = c("Regresión lineal (2)", 
                  "Regresión cúbica (5)", 
                  "Regresión cúbica (7)"),
       cex = 0.8,
       ncol = 3,
       lty = c(1, 1, 2),
       col = c("blue", "red", "black"))

# datos para usar con predict()
new_data <- data.frame("size" = seq(16, 24, 0.05), 
                       "english" = mean(CASchools$english),
                       "lunch" = mean(CASchools$lunch),
                       "income" = mean(CASchools$income),
                       "HiEL" = mean(CASchools$HiEL))

# agregar función de regresión estimada para el modelo (2)
fitted <- predict(TestScore_mod2, newdata = new_data)

lines(new_data$size, 
      fitted,
      lwd = 1.5,
      col = "blue")

# agregar función de regresión estimada para el modelo (5)
fitted <- predict(TestScore_mod5, newdata = new_data)

lines(new_data$size, 
      fitted, 
      lwd = 1.5,
      col = "red")

# agregar función de regresión estimada para el modelo (7)
fitted <- predict(TestScore_mod7, newdata = new_data)

lines(new_data$size, 
      fitted, 
      col = "black",
      lwd = 1.5,
      lty = 2)
```

Para la figura anterior, todos los regresores excepto $size$ se establecen en sus promedios muestrales. Se puede ver que las regresiones cúbicas (5) y (7) son casi idénticas. Indican que la relación entre los puntajes de las pruebas y la proporción alumno-maestro solo tiene una pequeña cantidad de no linealidad ya que no se desvían mucho de la función de regresión de (2).

El siguiente fragmento de código reproduce otra gráfica. Se usa **plot()** y **points()** para colorear las observaciones dependiendo de $HiEL$. Una vez más, las líneas de regresión se dibujan en función de las predicciones que utilizan promedios muestrales promedio de todos los regresores, excepto $size$.

```{r, 517, fig.align='center'}
# graficar un diagrama de dispersión

# observaciones con HiEL = 0
plot(CASchools$size[CASchools$HiEL == 0], 
     CASchools$score[CASchools$HiEL == 0], 
     xlim = c(12, 28),
     ylim = c(600, 730),
     pch = 20, 
     col = "gray", 
     xlab = "Proporción alumno-maestro", 
     ylab = "Resultado de la prueba")

# observaciones con HiEL = 1
points(CASchools$size[CASchools$HiEL == 1], 
       CASchools$score[CASchools$HiEL == 1],
       col = "steelblue",
       pch = 20)

# agrega una leyenda
legend("top", 
       legend = c("Regresión (6) con HiEL=0", "Regresión (6) con HiEL=1"),
       cex = 0.7,
       ncol = 2,
       lty = c(1, 1),
       col = c("green", "red"))

# datos para usar con 'predict()'
new_data <- data.frame("size" = seq(12, 28, 0.05), 
                       "english" = mean(CASchools$english),
                       "lunch" = mean(CASchools$lunch),
                       "income" = mean(CASchools$income),
                       "HiEL" = 0)

# agregar la función de regresión estimada para el modelo (6) con HiEL = 0
fitted <- predict(TestScore_mod6, newdata = new_data)

lines(new_data$size, 
      fitted, 
      lwd = 1.5,
      col = "green")

# agregar la función de regresión estimada para el modelo (6) con HiEL = 1
new_data$HiEL <- 1

fitted <- predict(TestScore_mod6, newdata = new_data)

lines(new_data$size, 
      fitted, 
      lwd = 1.5,
      col = "red")
```

El resultado de la regresión muestra que el modelo (6) encuentra coeficientes estadísticamente significativos en los términos de interacción $HiEL:size$, $HiEL:size^2$ y $HiEL:size^3$; es decir, existe evidencia de que la prueba de conexión de la relación no lineal de los puntajes y la proporción de estudiantes por maestro depende de la fracción de estudiantes que aprenden inglés en el distrito. Sin embargo, la figura anterior muestra que esta diferencia no es de importancia práctica y es un buen ejemplo de por qué se debe tener cuidado al interpretar modelos no lineales: Aunque las dos funciones de regresión se ven diferentes, se puede ver que la pendiente de ambas funciones es casi idéntica para la proporción alumno-maestro entre $17$ y $23$. Dado que este rango incluye casi el $90\%$ de todas las observaciones, se puede estar seguro de que se pueden descuidar las interacciones no lineales entre la fracción de estudiantes de inglés y la proporción de estudiantes por maestro.

Uno podría tener la tentación de objetar ya que ambas funciones muestran pendientes opuestas para proporciones alumno-maestro por debajo de $15$ y más allá de $24$. Existen al menos posibles objeciones:

1. Existen pocas observaciones con valores bajos y altos de la relación alumno-maestro, por lo que hay poca información para explotar al estimar el modelo. Esto significa que la función estimada es menos precisa en las colas del conjunto de datos.

2. El comportamiento descrito anteriormente de la función de regresión es una advertencia típica cuando se usan funciones cúbicas, ya que generalmente muestran un comportamiento extremo para valores de regresores extremos. Piense en la gráfica de $f(x) = x^3$.

Por lo tanto, no se encuentra evidencia clara de una relación entre el tamaño de la clase y los puntajes de las pruebas en el porcentaje de estudiantes de inglés en el distrito.

#### Resumen {-}

Ahora se pueden responder a las tres preguntas planteadas al comienzo de esta sección.

1. En los modelos lineales, el porcentaje de estudiantes de inglés tiene poca influencia en el efecto en los puntajes de las pruebas al cambiar la proporción de estudiantes por maestro. Este resultado sigue siendo válido si se controlan los antecedentes económicos de los estudiantes. Si bien la especificación cúbica (6) proporciona evidencia de que el efecto de la proporción alumno-maestro en la puntuación de la prueba depende de la proporción de estudiantes de inglés, la fuerza de este efecto es insignificante.

2. Al controlar por los antecedentes económicos de los estudiantes, se encuentra evidencia de no linealidades en la relación entre la proporción de estudiantes por maestro y los puntajes de las pruebas.

3. La especificación lineal (2) predice que una reducción de la proporción de estudiantes por maestro en dos estudiantes por maestro conduce a una mejora en los puntajes de las pruebas de aproximadamente $-0.73 \times (-2) = 1.46$ puntos. Dado que el modelo es lineal, este efecto es independiente del tamaño de la clase. Suponga que la proporción de estudiantes por maestro es de $20$. Por ejemplo, el modelo no lineal (5) predice que la reducción aumenta los puntajes de las pruebas en $$64.33\cdot18+18^2\cdot(-3.42)+18^3\cdot(0.059) - (64.33\cdot20+20^2\cdot(-3.42)+20^3\cdot(0.059)) \approx 3.3$$ puntos. Si la proporción es de $22$, una reducción a $20$ conduce a una mejora prevista en las puntuaciones de las pruebas de $$64.33\cdot20+20^2\cdot(-3.42)+20^3\cdot(0.059) - (64.33\cdot22+22^2\cdot(-3.42)+22^3\cdot(0.059)) \approx 2.4$$ puntos. Esto sugiere que el efecto es más fuerte en clases más pequeñas.

## Ejercicios {#Ejercicios-8}

```{r, 518, echo=F, purl=F, results='asis'}
if (my_output=="html"){
  cat('
<div  class = "DCexercise">

#### 1. Correlación y (no) linealidad I {-}
 
Considere el modelo de regresión lineal simple estimado $$\\widehat{medv_i} = 34.554 - 0.95\\times lstat_i,$$

donde <tt>medv</tt> (el valor medio de la vivienda en el suburbio) y <tt>lstat</tt> (el porcentaje de hogares con un nivel socioeconómico bajo en el suburbio) son variables del ya conocido conjunto de datos <tt>Boston</tt>.

El objeto <tt>lm()</tt> para el modelo anterior está disponible como <tt>mod</tt> en el entorno de trabajo. Se ha cargado el paquete <tt>MASS</tt>.

**Instrucciones:**

  + Calcular el coeficiente de correlación entre <tt>medv</tt> y <tt>lstat</tt> y guardarlo en <tt>corr</tt>.
  
  + Graficar <tt>medv</tt> contra <tt>lstat</tt> y agregar la línea de regresión usando el objeto modelo <tt>mod</tt>. ¿Que puede notar?

<iframe src="DCL/ex8_1.html" frameborder="0" scrolling="no" style="width:100%;height:340px"></iframe>
        
**Sugerencias:**
        
  + Puede usar <tt>cor()</tt> para calcular la correlación entre variables.
      
  + Puede usar <tt>plot()</tt> y <tt>abline()</tt> para visualizar los resultados de la regresión.
      
</div>')
} else {
  cat('\\begin{center}\\textit{Esta parte interactiva del curso solo está disponible en la versión HTML.}\\end{center}')
}
```

```{r, 519, echo=F, purl=F, results='asis'}
if (my_output=="html") {
  cat('
<div  class = "DCexercise">

#### 2. Correlación y (no) linealidad II {-}

En el ejercicio anterior se vio un ejemplo donde la correlación entre la variable dependiente <tt>medv</tt> y el regresor <tt>medv</tt> no es útil para elegir la forma funcional de la regresión, ya que la correlación captura solo la relación lineal.

Como alternativa, considere la especificación no lineal

$$medv_i = \\beta_0 + \\beta_1\\times\\log(lstat_i) + u_i.$$

Se ha cargado el paquete <tt>MASS</tt>.
      
**Instrucciones:**
        
  + Realizar la regresión desde arriba y asigne el resultado a <tt>log_mod</tt>.
      
  + Visualizar sus resultados usando un diagrama de dispersión y agregar la línea de regresión. En comparación con el ejercicio anterior, ¿qué nota ahora?

<iframe src="DCL/ex8_2.html" frameborder="0" scrolling="no" style="width:100%;height:340px"></iframe>

**Sugerencias:**
  
  + Utilizar <tt>lm()</tt> para realizar la regresión.

  + Utilizar <tt>plot()</tt> y <tt>abline()</tt> para visualizar los resultados de la regresión.
  
</div>')
}
```

```{r, 520, echo=F, purl=F, results='asis'}
if (my_output=="html") {
  cat('
<div  class = "DCexercise">

#### 3. El orden polinómico óptimo --- Prueba secuencial {-}

Recuerde el siguiente modelo del ejercicio anterior $$medv_i = \\beta_0 + \\beta_1\\times\\log(lstat_i) + u_i.$$

Se vio que esta especificación de modelo parece ser una opción razonable. Sin embargo, un polinomio de orden superior en $\\log(lstat_i)$ puede ser más adecuado para explicar $medv$.

Se han cargado los paquetes <tt>AER</tt> y <tt>MASS</tt>.
      
**Instrucciones:**
        
  + Determinar el orden óptimo de un modelo polylog usando pruebas secuenciales. Utilizar un orden polinomial máximo de $r = 4$ y el nivel de significancia $\\alpha=0.05$. Se recomienda el uso de un bucle <tt>for()</tt> y se le recomienda el siguiente enfoque:

    1. Calcular un modelo, suponiendo <tt>mod</tt>, que comienza con el orden polinomial más alto.
    2. Guardar el valor $p$ (usar errores estándar robustos) del parámetro relevante y compararlo con el nivel de significancia $\\alpha$
    3. Si no puede rechazar la hipótesis nula, repetir los pasos 1 y 2 para el siguiente orden polinomial más bajo; de lo contrario, detener el ciclo e imprimir el orden polinomial.

  + Calcular $R^2$ del modelo seleccionado y asignarlo a <tt>R2</tt>.

<iframe src="DCL/ex8_3.html" frameborder="0" scrolling="no" style="width:100%;height:340px"></iframe>

**Sugerencias:**

  + El índice del bucle <tt>for()</tt> debe comenzar en 4 y terminar en 1.

  + El uso de <tt>poly()</tt> en el argumento <tt>formula</tt> de <tt>lm()</tt> es una forma genérica de incorporar órdenes superiores de una determinada variable en el modelo. Además de la variable, se debe especificar el grado del polinomio mediante el argumento <tt>degree</tt> y establecer <tt>raw = TRUE</tt>.

  + Usar <tt>coeftest()</tt> junto con el argumento <tt>vcov.</tt> para obtener valores $p$ (¡usar errores estándar robustos!). Utilizar la estructura del objeto resultante para extraer el valor $p$ relevante.

  + Una instrucción <tt>if()</tt> puede ser útil para verificar si se cumple la condición para la aceptación del nulo en el paso 3.

  + Un bucle <tt>for()</tt> se detiene usando <tt>break</tt>.

  + Usar <tt>summary()</tt> para obtener $R^2$. Puede extraerlo agregando <tt>$r.squared</tt> a la llamada de la función.

</div>') 
}

```

```{r, 521, echo=F, purl=F, results='asis'}
if (my_output=="html") {
  cat('
<div  class = "DCexercise">

#### 4. El efecto estimado de un cambio de unidad {-}

Reconsidere el modelo polylog del ejercicio anterior que fue seleccionado por el enfoque de prueba secuencial. Como este modelo es logarítmico y de forma cuadrática, no se puede simplemente leer el efecto estimado de un cambio de unidad (es decir, uno por ciento) en <tt>lstat</tt> del resumen de coeficientes porque este efecto depende del nivel de <tt>lstat</tt>. Se puede calcular esto manualmente.

El modelo polylog <tt>mod_pl</tt> seleccionado está disponible en el entorno de trabajo. Se ha cargado el paquete <tt>MASS</tt>.
      
**Instrucciones:**

Suponga que se está interesado en el efecto en <tt>medv</tt> de un aumento en <tt>lstat</tt> de $10\\%$ a $11\\%$.

  + Configurar un <tt>data.frame</tt> con las observaciones relevantes de <tt>lstat</tt>.

  + Utilizar las nuevas observaciones para predecir los valores correspondientes de <tt>medv</tt>.

  + Calcular el efecto esperado con la ayuda de <tt>diff()</tt>.

<iframe src="DCL/ex8_4.html" frameborder="0" scrolling="no" style="width:100%;height:340px"></iframe>

**Sugerencias:**

- Se puede usar <tt>predict()</tt> junto con los nuevos datos para obtener los valores predichos de <tt>medv</tt>. Se debe tener en cuenta que los nombres de las columnas del <tt>data.frame</tt> deben coincidir con los nombres de los regresores cuando se usa <tt>predict()</tt>.

- <tt>diff()</tt> espera un vector. Calcula las diferencias entre todas las entradas de este vector.

</div>') 
}

```

```{r, 522, echo=F, purl=F, results='asis'}
if (my_output=="html") {
  cat('
<div  class = "DCexercise">

#### 5. Interacciones entre variables independientes I {-}

Considere el siguiente modelo de regresión

$$medv_i=\\beta_0+\\beta_1\\times chas_i+\\beta_2\\times old_i+\\beta_3\\times (chas_i\\cdot old_i)+u_i$$

donde $chas_i$ y $old_i$ son variables ficticias. El primero toma el valor $1$, si el río Charles (un río corto en las proximidades de Boston) pasa por el suburbio $i$ y es $0$ en caso contrario. Este último indica una alta proporción de edificios antiguos y está construido como

\\begin{align}
old_i = & \\,
    \\begin{cases}
      1 & \\text{si $age_i\\geq 95$},\\\\
      0 & \\text{de otro modo}.
    \\end{cases}
\\end{align}

siendo $age_i$ la proporción de unidades ocupadas por sus propietarios construidas antes de 1940 en el suburbio $i$.

Se han cargado los paquetes <tt>MASS</tt> y <tt>AER</tt>.
      
**Instrucciones:**
        
  + Generar y agregar la variable binaria <tt>old</tt> al conjunto de datos <tt>Boston</tt>.

  + Realizar la regresión indicada anteriormente y asignar el resultado a <tt>mod_bb</tt>.

  + Obtener un resumen robusto de coeficientes del modelo. ¿Cómo interpreta los resultados?

<iframe src="DCL/ex8_5.html" frameborder="0" scrolling="no" style="width:100%;height:340px"></iframe>

**Sugerencias:**

  + El operador <tt>>=</tt> puede usarse para generar un vector lógico. Transformar un vector lógico al tipo numérico a través de <tt>as.numeric()</tt>.

  + En <tt>lm()</tt> existen dos formas de incluir términos de interacción usando el argumento <tt>formula</tt>:

      1. <tt>Var1*Var2</tt> para agregar <tt>Var1</tt>, <tt>Var2</tt> y el término de interacción correspondiente a la vez

      2. <tt>Var1:Var2</tt> para agregar manualmente el término de interacción (lo que, por supuesto, requiere que se agreguen los términos restantes manualmente también)

</div>')}
```

```{r, 523, echo=F, purl=F, results='asis'}
if (my_output=="html") {
  cat('
<div  class = "DCexercise">

#### 6. Interacciones entre variables independientes II {-}

Ahora considere el modelo de regresión

$$medv_i=\\beta_0+\\beta_1\\times indus_i+\\beta_2\\times old_i+\\beta_3\\times (indus_i\\cdot old_i)+u_i$$

con $old_i$ definido como en el ejercicio anterior y $indus_i$ siendo la proporción de acres comerciales no minoristas en el suburbio $i$.

El vector <tt>old</tt> del ejercicio anterior se ha agregado al conjunto de datos. Se ha cargado el paquete <tt>MASS</tt>.

**Instrucciones:**
        
  + Estimar el modelo de regresión anterior y asignar el resultado a <tt>mod_bc</tt>.

  + Extraer los coeficientes estimados del modelo y asígnarlos a <tt>params</tt>.

  + Graficar <tt>medv</tt> contra <tt>indus</tt> y agregar las líneas de regresión para ambos estados de la variable binaria $old$.

<iframe src="DCL/ex8_6.html" frameborder="0" scrolling="no" style="width:100%;height:340px"></iframe>

**Sugerencias:**

  + Utilizar la estructura de <tt>mod_bc</tt> la salida generada por <tt>coef()</tt> para extraer los coeficientes estimados.

  + Además de pasar un objeto <tt>lm()</tt> a <tt>abline()</tt> también se puede especificar la intercepción y la pendiente manualmente usando los argumentos <tt>a</tt> y <tt>b</tt>, respectivamente.

</div>') 
}
```

<!--chapter:end:Capitulo_09.Rmd-->

# Evaluación de estudios basados en regresión múltiple {#EEBRM}

```{r, echo = F}
options(knitr.duplicate.label = "allow")
```

```{r, 524, child="_setup.Rmd"}
```

```{r, 525, eval=knitr::opts_knit$get("rmarkdown.pandoc.to") == "html", results='asis', echo=FALSE}
cat('<hr style="background-color:#03193b;height:2px">')
```

```{r, 526, echo=F, purl=F, message=FALSE}
# cargar el paquete AER
library(AER)   

# cargar el conjunto de datos en el espacio de trabajo
data(CASchools) 

# calcular STR y agregarlo a CASchools
CASchools$STR <- CASchools$students/CASchools$teachers 

# calcular TestScore y agregarlo a CASchools
CASchools$score <- (CASchools$read + CASchools$math)/2  

# Agregar HiSTR a CASchools
CASchools$HiSTR <- as.numeric(CASchools$STR >= 20)

# Agregar HiEL a CASchools
CASchools$HiEL <- as.numeric(CASchools$english >= 10)

# modelo (2) para California
TestScore_mod2 <- lm(score ~ STR + english + lunch + log(income), data = CASchools)
```

La mayor parte del capítulo 9 es de naturaleza teórica. Por lo tanto, esta sección revisa brevemente los conceptos de validez interna y externa, en general, y analiza ejemplos de amenazas a la validez interna y externa de modelos de regresión múltiple, en particular. De igual forma, se discuten las consecuencias de:

- Especificación incorrecta de la forma funcional de la función de regresión.
- Errores de medición.
- Datos faltantes y selección de muestras
- Causalidad simultánea

Asimismo, se abordan las fuentes de inconsistencia de los errores estándar de MCO. También se revisan las preocupaciones respecto a la validez interna y externa en el contexto de la predicción mediante modelos de regresión.

El capítulo cierra con una aplicación en **R** donde se evalúa si los resultados encontrados por regresión múltiple usando los datos de **CASchools** se pueden generalizar a los distritos escolares de otro estado federal de los Estados Unidos.

Para un tratamiento más detallado de estos temas, se recomienda que lea el siguiente **[artículo](https://core.ac.uk/download/pdf/224733178.pdf)**.

Los siguientes paquetes y sus dependencias son necesarios para la reproducción de los fragmentos de código presentados a lo largo de este capítulo:

+ **AER**
+ **mvtnorm** 
+ **stargazer**

```{r, 527, warning=FALSE, message=FALSE, eval=FALSE}
library(AER)
library(mvtnorm)
library(stargazer)
```

## Validez interna y externa

```{r, 528, eval = my_output == "html", results='asis', echo=F, purl=F}
cat("
<div class = 'keyconcept' id='KC9.1'>
<h3 class = 'right'> Concepto clave 9.1 </h3>
<h3 class = 'left'> Validez interna y externa </h3>

Un análisis estadístico tiene validez *interna* si la inferencia estadística realizada sobre los efectos causales es válida para la población considerada.

Se dice que un análisis tiene validez *externa* si las inferencias y la conclusión son válidas para la población de los estudios y pueden generalizarse a otras poblaciones y entornos.

</div>
")
```

```{r, 529, eval = my_output == "latex", results='asis', echo=F, purl=F}
cat("\\begin{keyconcepts}[Validez interna y externa]{9.1}

Un análisis estadístico tiene validez \\textit{interna} si la inferencia estadística realizada sobre los efectos causales es válida para la población considerada.\\newline

Se dice que un análisis tiene validez \\textit{externa} si las inferencias y la conclusión son válidas para la población de los estudios y pueden generalizarse a otras poblaciones y entornos.

\\end{keyconcepts}
")
```

#### Amenazas a la validez interna {-}

Existen dos condiciones para que se cumpla con la validez interna:

1. El estimador del efecto causal, en el que se miden los coeficientes de interés, debe ser insesgado y consistente.

2. La inferencia estadística es válida; es decir, las pruebas de hipótesis deben tener el tamaño deseado y los intervalos de confianza deben tener la probabilidad de cobertura deseada.

En regresión múltiple, se estiman los coeficientes del modelo usando MCO. Por lo tanto, para que se cumpla la condición 1, se necesita que el estimador MCO sea insesgado y consistente. Para que la segunda condición sea válida, los errores estándar deben ser válidos de manera que la prueba de hipótesis y el cálculo de los intervalos de confianza produzcan resultados confiables. Recuerde que una condición suficiente para que se cumplan las condiciones 1. y 2. es que se cumplan los supuestos del Concepto clave 6.4.

#### Amenazas a la validez externa {-}

La validez externa puede no ser válida:

1. Si existen diferencias entre la población estudiada y la población de interés.

2. Si existen diferencias en los *entornos* de las poblaciones consideradas; por ejemplo, el marco legal o el momento histórico de la investigación.

## Amenazas a la validez interna del análisis de regresión múltiple {#AVIARM}

Esta sección trata cinco fuentes que causan que el estimador de MCO en modelos de regresión (múltiple) sea sesgado e inconsistente para el efecto causal de interés y discute posibles soluciones. Las cinco fuentes implican una violación del primer supuesto de mínimos cuadrados presentado en el Concepto clave 6.4.

Esta sección trata:

- Sesgo de variable omitida.

- Especificación incorrecta de la forma funcional.

- Errores de medición.

- Datos faltantes y selección de muestras.

- Sesgo de causalidad simultánea.

Además de estas amenazas para la cosistencia del estimador, también se discuten brevemente las causas de la estimación inconsistente de los errores estándar de MCO.

#### Sesgo de variable omitida {-}

```{r, 530, eval = my_output == "html", results='asis', echo=F, purl=F}
cat('
<div class = "keyconcept" id="KC9.2">
<h3 class = "right"> Concepto clave 9.2 </h3>
<h3 class = "left"> Sesgo de variable omitida: ¿Debería incluir más variables en mi regresión? </h3>

La inclusión de variables adicionales reduce el riesgo de sesgo de variable omitida, pero puede aumentar la varianza del estimador del coeficiente de interés.

Se presentan algunas pautas que ayudan a decidir si incluir una variable adicional:

1. Especificar el o los coeficientes de interés.
2. Identificar las fuentes potenciales más importantes de sesgo de variables omitidas utilizando el conocimiento disponible *antes* de estimar el modelo. Debería terminar con una especificación de referencia y un conjunto de regresores que son cuestionables.
3. Utilizar diferentes especificaciones de modelo para probar si los regresores cuestionables tienen coeficientes diferentes de cero.
4. Utilizar tablas para proporcionar una divulgación completa de sus resultados; es decir, presentar diferentes especificaciones de modelos que apoyen su argumento y permitan al lector ver el efecto de incluir regresores cuestionables.

</div>
')
```

```{r, 531, eval = my_output == "latex", results='asis', echo=F, purl=F}
cat('\\begin{keyconcepts}[Sesgo de variable omitida: ¿Debería incluir más variables en mi regresión?]{9.2}

La inclusión de variables adicionales reduce el riesgo de sesgo de variable omitida, pero puede aumentar la varianza del estimador del coeficiente de interés.\\newline

Se presentan algunas pautas que ayudan a decidir si incluir una variable adicional:\\newline

\\begin{itemize}
\\item Especificar el o los coeficientes de interés.
\\item Identificar las fuentes potenciales más importantes de sesgo de variables omitidas utilizando el conocimiento disponible \\textit{antes} de estimar el modelo. Debería terminar con una especificación de referencia y un conjunto de regresores que son cuestionables.
\\item Utilizar diferentes especificaciones de modelo para probar si los regresores cuestionables tienen coeficientes diferentes de cero.
\\item Utilizar tablas para proporcionar una divulgación completa de sus resultados; es decir, presentar diferentes especificaciones de modelos que apoyen su argumento y permitan al lector ver el efecto de incluir regresores cuestionables.
\\end{itemize}

\\end{keyconcepts}
')
```

A estas alturas, debe conocer el sesgo de variable omitida y sus consecuencias. El Concepto clave 9.2 da algunas pautas sobre cómo proceder si hay variables de control que posiblemente permitan reducir el sesgo de las variables omitidas. Si incluir variables adicionales para mitigar el sesgo no es una opción porque no hay controles adecuados, existen diferentes enfoques para resolver el problema:

+ Usar métodos de datos de panel (discutido en el Capítulo \@ref(RDP))

+ Usar regresiones de variables instrumentales (discutido en el Capítulo \@ref(RVI))

+ Usar experimentos de control aleatorio (discutido en el Capítulo \@ref(EC))

#### Especificación incorrecta de la forma funcional de la función de regresión {-}

Si la función de regresión de la población no es lineal pero la función de regresión es lineal, la forma funcional del modelo de regresión está mal especificada. Esto conduce a un sesgo del estimador MCO.

```{r, 532, eval = my_output == "html", results='asis', echo=F, purl=F}
cat('
<div class = "keyconcept" id="KC9.3">
<h3 class = "right"> Concepto clave 9.3 </h3>
<h3 class = "left"> Especificación incorrecta de la forma funcional </h3>

Se dice que una regresión adolece de una especificación incorrecta de la forma funcional cuando la forma funcional del modelo de regresión estimado difiere de la forma funcional de la función de regresión poblacional. La especificación incorrecta de la forma funcional conduce a estimadores de coeficientes sesgados e inconsistentes. Una forma de detectar la especificación incorrecta de la forma funcional es trazar la función de regresión estimada y los datos. Esto también puede resultar útil para elegir la forma funcional correcta.

</div>
')
```

```{r, 533, eval = my_output == "latex", results='asis', echo=F, purl=F}
cat('\\begin{keyconcepts}[Especificación incorrecta de la forma funcional]{9.3}

Se dice que una regresión adolece de una especificación incorrecta de la forma funcional cuando la forma funcional del modelo de regresión estimado difiere de la forma funcional de la función de regresión poblacional. La especificación incorrecta de la forma funcional conduce a estimadores de coeficientes sesgados e inconsistentes. Una forma de detectar la especificación incorrecta de la forma funcional es trazar la función de regresión estimada y los datos. Esto también puede resultar útil para elegir la forma funcional correcta.

\\end{keyconcepts}
')
```

Es fácil encontrar ejemplos de especificación incorrecta de la forma funcional: Considere el caso en el que la función de regresión de la población es $$Y_i = X_i^2$$, pero el modelo utilizado es $$Y_i = \beta_0 + \beta_1 X_i + u_i.$$ Claramente, la función de regresión está mal especificada aquí. Ahora, simulando y visualizándo los datos.

```{r, 534, fig.align='center'}
# sembrar la semilla para la reproducibilidad
set.seed(3)

# simular conjunto de datos
X <- runif(100, -5, 5)
Y <- X^2 + rnorm(100)

# estimar la función de regresión
ms_mod <- lm(Y ~ X)
ms_mod
```

```{r, 535, fig.align='center'}
# graficar los datos
plot(X, Y, 
     main = "Especificación incorrecta de la forma funcional",
     pch = 20,
     col = "steelblue")

# graficar la línea de regresión lineal
abline(ms_mod, 
       col = "darkred",
       lwd = 2)
```

Es evidente que los errores de regresión son relativamente pequeños para observaciones cercanas a $X = -3$ y $X = 3$, pero que los errores aumentan para valores de $X$ más cercanos a cero e incluso más para valores superiores a $-4$ y $4$. Las consecuencias son drásticas: La intersección se estima en $8.1$ y para el parámetro de pendiente se obtiene una estimación obviamente muy cercana a cero. Este problema no desaparece a medida que aumenta el número de observaciones porque la MCO está *sesgada* y es *inconsistente* debido a la especificación incorrecta de la función de regresión.

#### Error de medición y sesgo de errores en variables {-}

```{r, 536, eval = my_output == "html", results='asis', echo=F, purl=F}
cat('
<div class = "keyconcept" id="KC9.4">
<h3 class = "right"> Concepto clave 9.4 </h3>
<h3 class = "left"> Errores en sesgo variable </h3>

Cuando las variables independientes se miden de manera imprecisa, se habla de sesgo de errores en las variables. Este sesgo no desaparece si el tamaño de la muestra es grande. Si el error de medición tiene media cero y es independiente de la variable afectada, el estimador MCO del coeficiente respectivo está sesgado hacia cero.

</div>
')
```

```{r, 537, eval = my_output == "latex", results='asis', echo=F, purl=F}
cat('\\begin{keyconcepts}[Errores en sesgo variable]{9.4}

Cuando las variables independientes se miden de manera imprecisa, se habla de sesgo de errores en las variables. Este sesgo no desaparece si el tamaño de la muestra es grande. Si el error de medición tiene media cero y es independiente de la variable afectada, el estimador MCO del coeficiente respectivo está sesgado hacia cero.

\\end{keyconcepts}
')
```

Suponga que está midiendo incorrectamente el regresor único $X_i$, de modo que existe un error de medición y observa $\overset{\sim}{X}_i$, en lugar de $X_i$. Entonces, en lugar de estimar la población, el modelo de regresión $$ Y_i = \beta_0 + \beta_1 X_i + u_i $$ terminas estimando

\begin{align*}
  Y_i =& \, \beta_0 + \beta_1 \overset{\sim}{X}_i + \underbrace{\beta_1 (X_i - \overset{\sim}{X}_i) + u_i}_{=v_i} \\
  Y_i =& \, \beta_0 + \beta_1 \overset{\sim}{X}_i + v_i
\end{align*}

donde $\overset{\sim}{X}_i$ y el término de error $v_i$ están correlacionados. Por lo tanto, MCO estaría sesgado e inconsistente para el verdadero $\beta_1$ en este ejemplo. Se puede demostrar que la dirección y la fuerza del sesgo dependen de la correlación entre el regresor observado, $\overset{\sim}{X}_i$, y el error de medición, $w_i =X_i - \overset{\sim}{X}_i$. Esta correlación, a su vez, depende del tipo de error de medición cometido.

El modelo de error de medición clásico supone que el error de medición, $w_i$, tiene media cero y que no está correlacionado con la variable, $X_i$, y el término de error del modelo de regresión poblacional, $u_i$:

\begin{equation}
  \overset{\sim}{X}_i = X_i + w_i, \ \ \rho_{w_i,u_i}=0, \ \ \rho_{w_i,X_i}=0 
\end{equation}

Entonces sostiene que:

\begin{equation}
  \widehat{\beta}_1 \xrightarrow{p}{\frac{\sigma_{X}^2}{\sigma_{X}^2 + \sigma_{w}^2}} \beta_1 (\#eq:cmembias)
\end{equation}

lo que implica una inconsistencia como $\sigma_{X}^2, \sigma_{w}^2 > 0$, tal que la fracción en \@ref(eq:cmembias) es menor que $1$. Se debe tener en cuenta que se deben reconocer dos casos extremos:

1. Si no existe ningún error de medición, $\sigma_{w}^2=0$ tal que $\widehat{\beta}_1 \xrightarrow{p}{\beta_1}$. 

2. Si $\sigma_{w}^2 \gg \sigma_{X}^2$, se tiene $\widehat{\beta}_1 \xrightarrow{p}{0}$. Este es el caso si el error de medición es tan grande que esencialmente no hay información sobre $X$ en los datos que pueda usarse para estimar $\beta_1$.

La forma más obvia de lidiar con el sesgo de errores en las variables es usar un $X$ medido con precisión. Si esto no es posible, la regresión de variables instrumentales es una opción. También se podría abordar el problema mediante el uso de un modelo matemático del error de medición y ajustar las estimaciones de manera apropiada: Si es plausible que se aplique el modelo clásico de error de medición y si existe información que se pueda utilizar para estimar la razón en la ecuación \@ref(eq:cmembias), se podría calcular una estimación que corrija el sesgo a la baja.

Por ejemplo, considere dos variables aleatorias bivariadas distribuidas normalmente $X, Y$. Es un [resultado bien conocido](https://en.wikipedia.org/wiki/Multivariate_normal_distribution#Bivariate_case) que la función de expectativa condicional de $Y$ dado $X$ tiene la forma:

\begin{align}
  E(Y\vert X) = E(Y) + \rho_{X,Y} \frac{\sigma_{Y}}{\sigma_{X}}\left[X-E(X)\right]. (\#eq:bnormexpfn) 
\end{align} Thus for 
\begin{align}
  (X, Y) \sim \mathcal{N}\left[\begin{pmatrix}50\\ 100\end{pmatrix},\begin{pmatrix}10 & 5 \\ 5 & 10 \end{pmatrix}\right] (\#eq:bvnormd)
\end{align} de acuerdo con \@ref(eq:bnormexpfn), la función de regresión poblacional es
\begin{align*}
  Y_i =& \, 100 + 0.5 (X_i - 50) \\
      =& \, 75 + 0.5 X_i. (\#eq:bnormregfun)
\end{align*}

Ahora suponga que recopila datos sobre $X$ y $Y$, pero que solo puede medir $\overset{\sim}{X_i} = X_i + w_i$ con $w_i \overset{i.i.d.}{\sim} \mathcal{N}(0,10)$. Dado que $w_i$ son independientes de $X_i$, no hay correlación entre $X_i$ y $w_i$, por lo que se tiene un caso del modelo clásico de error de medición. Ahora se ilustra este ejemplo en **R** usando el paquete **mvtnorm** [@R-mvtnorm].

```{r, 538, eval = T}
# sembrar semilla
set.seed(1)

# cargar el paquete 'mvtnorm' y simular datos normales bivariados
library(mvtnorm)
dat <- data.frame(
  rmvnorm(1000, c(50, 100), 
          sigma = cbind(c(10, 5), c(5, 10))))

# establecer nombres de columnas
colnames(dat) <- c("X", "Y")
```

Ahora se estima una regresión lineal simple de $Y$ en $X$ usando estos datos de muestra y se vuelve a ejecutar la misma regresión, pero esta vez se agregan i.i.d. $\mathcal{N}(0,10)$ errores agregados a $X$.

```{r, 539, eval = T}
# estimar el modelo (sin error de medición)
noerror_mod <- lm(Y ~ X, data = dat)

# estimar el modelo (con error de medición en X)
dat$X <- dat$X + rnorm(n = 1000, sd = sqrt(10))
error_mod <- lm(Y ~ X, data = dat)

# imprimir los coeficientes estimados en la consola
noerror_mod$coefficients
error_mod$coefficients
```

A continuación, se visualizan los resultados y se comparan con la función de regresión poblacional.

```{r, 540, fig.align='center'}
# graficar datos de muestra
plot(dat$X, dat$Y, 
     pch = 20, 
     col = "steelblue",
     xlab = "X",
     ylab = "Y")

# agregar función de regresión poblacional
abline(coef = c(75, 0.5), 
       col = "darkgreen",
       lwd  = 1.5)

# agregar funciones de regresión estimadas
abline(noerror_mod, 
       col = "purple",
       lwd  = 1.5)

abline(error_mod, 
       col = "darkred",
       lwd  = 1.5)

# agregar leyenda
legend("topleft",
       bg = "transparent",
       cex = 0.8,
       lty = 1,
       col = c("darkgreen", "purple", "darkred"), 
       legend = c("Población","Sin errores","Errores"))
```

En la situación sin error de medición, la función de regresión estimada está cerca de la función de regresión de la población. Las cosas son diferentes cuando se usa el regresor mal medido $X$: Tanto la estimación de la intersección como la estimación del coeficiente en $X$ difieren considerablemente de los resultados obtenidos usando los datos "limpios" en $X$. En particular $\widehat{\beta}_1 = 0.255$, por lo que existe un sesgo a la baja. Se está en una situación cómoda para conocer $\sigma_X^2$ y $\sigma^2_w$. Lo anterior permite corregir el sesgo usando \@ref(eq:cmembias). Con esta información se obtiene la estimación con corrección sesgada $$\frac{\sigma_X^2 + \sigma_w^2}{\sigma_X^2} \cdot \widehat{\beta}_1 = \frac{10+10}{10} \cdot 0.255 = 0.51$$ que está bastante cerca de $\beta_1=0.5$, el verdadero coeficiente de la función de regresión poblacional.

Se debe tener en cuenta que el análisis anterior utiliza una sola muestra. Por tanto, se puede argumentar que los resultados son solo una coincidencia. ¿Se puede demostrar lo contrario utilizando un estudio de simulación?

#### Selección de muestra y datos faltantes {-}

```{r, 541, eval = my_output == "html", results='asis', echo=F, purl=F}
cat('
<div class = "keyconcept" id="KC9.5">
<h3 class = "right"> Concepto clave 9.5 </h3>
<h3 class = "left"> Sesgo de selección de muestra </h3>

Cuando el proceso de muestreo influye en la disponibilidad de datos y cuando existe una relación de este proceso de muestreo con la variable dependiente que va más allá de la dependencia de los regresores, se dice que existe un sesgo de selección de la muestra. Este sesgo se debe a la correlación entre uno o más regresores y el término de error. La selección de la muestra implica tanto sesgo como inconsistencia del estimador MCO.

</div>
')
```

```{r, 542, eval = my_output == "latex", results='asis', echo=F, purl=F}
cat('\\begin{keyconcepts}[Sesgo de selección de muestra]{9.5}

Cuando el proceso de muestreo influye en la disponibilidad de datos y cuando existe una relación de este proceso de muestreo con la variable dependiente que va más allá de la dependencia de los regresores, se dice que existe un sesgo de selección de la muestra. Este sesgo se debe a la correlación entre uno o más regresores y el término de error. La selección de la muestra implica tanto sesgo como inconsistencia del estimador MCO.

\\end{keyconcepts}
')
```

Existen tres casos de selección de muestras. Solo uno de ellos representa una amenaza para la validez interna de un estudio de regresión. Los tres casos son:

1. Faltan datos al azar.

2. Faltan datos basados en el valor de un regresor.

3. Faltan datos debido a un proceso de selección relacionado con la variable dependiente.

Volviendo al ejemplo de las variables $X$ y $Y$ distribuidas como se indica en la ecuación \@ref(eq:bvnormd) y se ilustran los tres casos usando **R**.

Si faltan datos al azar, esto no es más que perder observaciones. Por ejemplo, perder $50\%$ de la muestra sería lo mismo que no haber visto nunca la mitad (elegida al azar) de la muestra observada. Por lo tanto, los datos faltantes no introducen un sesgo de estimación y "solo" conducen a estimadores menos eficientes.

```{r, 543, fig.align='center'}
# sembrar semilla
set.seed(1)

# simular datos
dat <- data.frame(
  rmvnorm(1000, c(50, 100), 
          sigma = cbind(c(10, 5), c(5, 10))))

colnames(dat) <- c("X", "Y")

# marcar 500 observaciones seleccionadas al azar
id <- sample(1:1000, size = 500)

plot(dat$X[-id], 
     dat$Y[-id], 
     col = "steelblue", 
     pch = 20,
     cex = 0.8,
     xlab = "X",
     ylab = "Y")

points(dat$X[id], 
       dat$Y[id],
       cex = 0.8,
       col = "gray", 
       pch = 20)

# agregar la función de regresión poblacional
abline(coef = c(75, 0.5), 
       col = "darkgreen",
       lwd  = 1.5)

# agregar la función de regresión estimada para la muestra completa
abline(noerror_mod)

# estimar el caso del modelo 1 y agregar la línea de regresión
dat <- dat[-id, ]

c1_mod <- lm(dat$Y ~ dat$X, data = dat)
abline(c1_mod, col = "purple")

# agrega una leyenda
legend("topleft",
       lty = 1,
       bg = "transparent",
       cex = 0.8,
       col = c("darkgreen", "black", "purple"), 
       legend = c("Población","Muestra completa","500 observaciones seleccionado aleatoriamente"))
```

Los puntos grises representan las observaciones descartadas de $500$. Cuando se utilizan las observaciones restantes, los resultados de la estimación se desvían solo marginalmente de los resultados obtenidos con la muestra completa.

La selección de datos aleatoriamente basada en el valor de un regresor también tiene el efecto de reducir el tamaño de la muestra y no introduce sesgo de estimación. Ahora se descartan todas las observaciones con $X > 45$, se estimará el modelo nuevamente y se comparará.

```{r, 544, fig.align='center'}
# establecer semilla aleatoria
set.seed(1)

# simular datos
dat <- data.frame(
  rmvnorm(1000, c(50, 100), 
          sigma = cbind(c(10, 5), c(5, 10))))

colnames(dat) <- c("X", "Y")

# marcar observaciones
id <- dat$X >= 45

plot(dat$X[-id], 
     dat$Y[-id], 
     col = "steelblue",
     cex = 0.8,
     pch = 20,
     xlab = "X",
     ylab = "Y")

points(dat$X[id], 
       dat$Y[id], 
       col = "gray",
       cex = 0.8,
       pch = 20)

# agregar función de regresión poblacional
abline(coef = c(75, 0.5), 
       col = "darkgreen",
       lwd  = 1.5)

# agregar la función de regresión estimada para la muestra completa
abline(noerror_mod)

# estimar el caso del modelo 1, agregar la línea de regresión
dat <- dat[-id, ]

c2_mod <- lm(dat$Y ~ dat$X, data = dat)
abline(c2_mod, col = "purple")

# agregar leyenda
legend("topleft",
       lty = 1,
       bg = "transparent",
       cex = 0.8,
       col = c("darkgreen", "black", "purple"), 
       legend = c("Población","Muestra completa","Observaciones con X <= 45"))
```

Tenga en cuenta que, aunque se redujeron todas las observaciones en más del $90\%$, la función de regresión estimada está muy cerca de la línea estimada basada en la muestra completa.

En el tercer caso, se enfrenta a un sesgo de selección de la muestra. Se puede ilustrar esto usando solo observaciones con $X_i < 55$ y $Y_i > 100$. Estas observaciones se identifican fácilmente usando la función **which()** y los operadores lógicos: `which (dat $ X <55 & dat $ Y> 100)`

```{r, 545, fig.align='center'}
# establecer semilla aleatoria
set.seed(1)

# simular datos
dat <- data.frame(
  rmvnorm(1000, c(50,100), 
          sigma = cbind(c(10,5), c(5,10))))

colnames(dat) <- c("X","Y")

# marcar observaciones
id <- which(dat$X <= 55 & dat$Y >= 100)

plot(dat$X[-id], 
       dat$Y[-id], 
       col = "gray",
       cex = 0.8,
       pch = 20,
       xlab = "X",
       ylab = "Y")

points(dat$X[id], 
     dat$Y[id], 
     col = "steelblue",
     cex = 0.8,
     pch = 20)

# agregar función de regresión poblacional
abline(coef = c(75, 0.5), 
       col = "darkgreen",
       lwd  = 1.5)

# agregar la función de regresión estimada para la muestra completa
abline(noerror_mod)

# estimar el caso del modelo 1, agregar la línea de regresión
dat <- dat[id, ]

c3_mod <- lm(dat$Y ~ dat$X, data = dat)
abline(c3_mod, col = "purple")

# agregar leyenda
legend("topleft",
       lty = 1,
       bg = "transparent",
       cex = 0.8,
       col = c("darkgreen", "black", "purple"), 
       legend = c("Población", "Muestra completa", "X <= 55 & Y >= 100"))
```

Se puede ver que el proceso de selección conduce a resultados de estimación sesgados.

Existen métodos que permiten corregir el sesgo de selección de la muestra. Sin embargo, estos métodos están más allá del alcance del presente curso y, por lo tanto, no se consideran aquí. El concepto de sesgo de selección de la muestra se resume en el Concepto clave 9.5.

#### Causalidad simultánea {-}

```{r, 546, eval = my_output == "html", results='asis', echo=F, purl=F}
cat('
<div class = "keyconcept" id="KC9.6">
<h3 class = "right"> Concepto clave 9.6 </h3>
<h3 class = "left"> Sesgo de causalidad simultáneo </h3>

Hasta ahora se ha asumido que los cambios en la variable independiente $X$ son responsables de los cambios en la variable dependiente $Y$. Cuando lo contrario también es cierto, se dice que existe *causalidad simultánea* entre $X$ y $Y$. Esta causalidad inversa conduce a una correlación entre $X$ y el error en la regresión poblacional de interés, de modo que el coeficiente de $X$ se estima con sesgo.

</div>
')
```

```{r, 547, eval = my_output == "latex", results='asis', echo=F, purl=F}
cat('\\begin{keyconcepts}[Simultaneous Causality Bias]{9.6}

Hasta ahora se ha asumido que los cambios en la variable independiente $X$ son responsables de los cambios en la variable dependiente $Y$. Cuando lo contrario también es cierto, se dice que existe \\textit{causalidad simultánea} entre $X$ y $Y$. Esta causalidad inversa conduce a una correlación entre $X$ y el error en la regresión poblacional de interés, de modo que el coeficiente de $X$ se estima con sesgo.

\\end{keyconcepts}
')
```

Suponga que se está interesado en estimar el efecto de un aumento de $20\%$ en los precios de los cigarrillos sobre el consumo de cigarrillos en los Estados Unidos utilizando un modelo de regresión múltiple. Esto se puede investigar utilizando el conjunto de datos **CigarettesSW** que es parte del paquete **AER**. **CigarettesSW** es un conjunto de datos de panel sobre el consumo de cigarrillos para los 48 estados federales continentales de EE. UU. desde 1985-1995 y proporciona datos sobre indicadores económicos y precios locales promedio, impuestos y consumo de paquetes per cápita.

Después de cargar el conjunto de datos, se seleccionan observaciones para el año 1995 y se trazan los logaritmos del precio por paquete, **price**, contra el consumo del paquete, **packs**, y se estima un modelo de regresión lineal simple.

```{r, 548}
# cargar el conjunto de datos
library(AER)
data("CigarettesSW")
c1995 <- subset(CigarettesSW, year == "1995")

# estimar el modelo
cigcon_mod <- lm(log(packs) ~ log(price), data = c1995)
cigcon_mod
```

```{r, 549}
# trazar la línea de regresión estimada y los datos
plot(log(c1995$price), log(c1995$packs),
     xlab = "ln(Precio)",
     ylab = "ln(Consumo)",
     main = "Demanda de cigarrillos",
     pch = 20,
     col = "steelblue")

abline(cigcon_mod, 
       col = "darkred", 
       lwd = 1.5)
```

Recuerde del Capítulo \@ref(FRNL) que, debido a la especificación log-log, en la regresión poblacional el coeficiente del logaritmo del precio se interpreta como la elasticidad precio del consumo. El coeficiente estimado sugiere que un aumento de $1\%$ en los precios de los cigarrillos reduce el consumo de cigarrillos en aproximadamente $1.2\%$, en promedio. ¿Se ha estimado una curva de demanda? La respuesta es no: Este es un ejemplo clásico de causalidad simultánea, ver Concepto clave 9.6. Las observaciones son equilibrios de mercado que están determinados tanto por cambios en la oferta como por cambios en la demanda. Por lo tanto, el precio está correlacionado con el término de error y el estimador de MCO está sesgado. No se puede estimar una curva de oferta ni de demanda de manera consistente utilizando este enfoque.

Se volverá a este tema en el Capítulo \@ref(RVI) que trata la regresión de variables instrumentales, un enfoque que permite una estimación consistente cuando hay causalidad simultánea.

#### Fuentes de inconsistencia de errores estándar de MCO {-}

Existen dos amenazas centrales para el cálculo de errores estándar consistentes de MCO:

1. Heteroscedasticidad: Las implicaciones de la heterocedasticidad se han discutido en el Capítulo \@ref(PHICMRLS). Los errores estándar robustos a la heterocedasticidad calculados por la función **vcovHC()** del paquete **sandwich** producen errores estándar válidos bajo heterocedasticidad.

2. Correlación en serie: Si el error de regresión de la población está correlacionado entre las observaciones, se tiene una correlación en serie. Esto sucede a menudo en aplicaciones en las que se utilizan observaciones repetidas; por ejemplo, en estudios de datos de panel. En cuanto a la heterocedasticidad, **vcovHC()** se puede utilizar para obtener errores estándar válidos cuando existe correlación serial.

Los errores estándar inconsistentes producirán pruebas de hipótesis no válidas e intervalos de confianza incorrectos. Por ejemplo, cuando se prueba el valor nulo de que algún coeficiente del modelo es cero, ya no se puede confiar en el resultado porque la prueba puede no tener un tamaño de $5\%$ debido al error estándar calculado incorrectamente.

El Concepto clave 9.7 resume todas las amenazas a la validez interna discutidas anteriormente.

```{r, 550, eval = my_output == "html", results='asis', echo=F, purl=F}
cat('
<div class = "keyconcept" id="KC9.7">
<h3 class = "right"> Concepto clave 9.7 </h3>
<h3 class = "left"> Amenazas a la validez interna de un estudio de regresión </h3>

Las cinco amenazas principales a la validez interna de un estudio de regresión múltiple son:

1. Variables omitidas.
2. Especificación incorrecta de la forma funcional.
3. Errores en las variables (errores de medición en los regresores).
4. Selección de muestras.
5. Causalidad simultánea.

Todas estas amenazas conducen al fracaso del primer supuesto de mínimos cuadrados $$E(u_i\\vert X_{1i},\\dots ,X_{ki}) \\neq 0$$ de modo que el estimador de MCO está *sesgado* y es *inconsistente*.<br>

Además, si uno no ajusta por heterocedasticidad *y*/*o* correlación serial, los errores estándar incorrectos pueden ser una amenaza para la validez interna del estudio.

</div>
')
```

```{r, 551, eval = my_output == "latex", results='asis', echo=F, purl=F}
cat('\\begin{keyconcepts}[Amenazas a la validez interna de un estudio de regresión]{9.7}

Las cinco amenazas principales a la validez interna de un estudio de regresión múltiple son:\\newline

\\begin{enumerate}
\\item Variables omitidas.
\\item Especificación incorrecta de la forma funcional.
\\item Errores en las variables (errores de medición en los regresores).
\\item Selección de muestras.
\\item Causalidad simultánea.
\\end{enumerate}\\vspace{0.5cm}

Todas estas amenazas conducen al fracaso del primer supuesto de mínimos cuadrados $$E(u_i\\vert X_{1i},\\dots ,X_{ki}) \\neq 0$$ de modo que el estimador de MCO está \\textit{sesgado} y es \\textit{inconsistente}.\\newline

Además, si uno no ajusta por heterocedasticidad \\textit{y}/\\textit{o} correlación serial, los errores estándar incorrectos pueden ser una amenaza para la validez interna del estudio.

\\end{keyconcepts}
')
```

## Validez interna y externa cuando la regresión se usa para pronosticar

Recuerde la regresión de los puntajes de las pruebas en la proporción alumno-maestro ($STR$) realizada en el Capítulo \@ref(RLR):

```{r, 552}
linear_model <- lm(score ~ STR, data = CASchools)
linear_model
```

La función de regresión estimada fue

$$ \widehat{TestScore} = 698.9 - 2.28 \times STR.$$

Se analiza el ejemplo de un padre que se muda a un área metropolitana y planea elegir dónde vivir en función de la calidad de las escuelas locales: El puntaje promedio de las pruebas de un distrito escolar es una medida adecuada de la calidad. Sin embargo, el padre solo tiene información sobre la proporción de alumnos por maestro, de modo que es necesario predecir los puntajes de las pruebas. Aunque se ha establecido que existe un sesgo de variable omitida en este modelo debido a la omisión de variables como las oportunidades de aprendizaje de los estudiantes fuera de la escuela, la proporción de estudiantes de inglés, entre otros, **modelo_lineal** puede, de hecho, ser útil para los padres:

Al padre no le importa si el coeficiente de $STR$ tiene una interpretación causal, quiere que $STR$ explique la mayor variación posible en los puntajes de las pruebas. Por lo tanto, a pesar del hecho de que **linear_model** no se puede usar para estimar el *efecto causal* de un cambio en $STR$ en los puntajes de las pruebas, se puede considerar un *predictor confiable* de los puntajes de las pruebas en general.

Por lo tanto, las amenazas a la validez interna resumidas en el Concepto clave 9.7 son insignificantes para el padre. Esto es diferente para un superintendente al que se le ha encomendado tomar medidas que aumenten los puntajes de las pruebas: Necesita un modelo más confiable que no sufra las amenazas enumeradas en el Concepto clave 9.7.

## Ejemplo: Puntajes de exámenes y tamaño de la clase {#EPETC}

Esta sección analiza la validez interna y externa de los resultados obtenidos del análisis de los datos de las calificaciones de las pruebas de California utilizando modelos de regresión múltiple.

#### Validez externa del estudio {-}

La validez externa del análisis de puntaje de la prueba de California significa que sus resultados se pueden generalizar. Que esto sea posible depende de la población y el entorno. Se realiza el mismo análisis utilizando datos para estudiantes de cuarto grado en distritos escolares públicos de $220$ en Massachusetts en 1998. Al igual que **CASchools**, el conjunto de datos **MASchools** es parte del paquete **AER** [@R-AER]. Se puede utilizar la función de ayuda (`?MASchools`) para obtener información sobre las definiciones de todas las variables contenidas.

Se debe comenzar cargando el conjunto de datos y procediendo a calcular algunas estadísticas resumidas.

```{r, 553, warning=FALSE, message=FALSE}
library("AER")

# adjuntar el conjunto de datos 'MASchools'
data("MASchools")
summary(MASchools)
```

Es bastante fácil replicar los componentes clave en una tabla usando **R**. Para ser coherentes con los nombres de las variables utilizadas en el conjunto de datos **CASchools**, se aplican algunos formatos de antemano.

```{r, 554}
# variables personalizadas en MASchools
MASchools$score <- MASchools$score4 
MASchools$STR <- MASchools$stratio

# crear la tabla
vars <- c("score", "STR", "english", "lunch", "income")

cbind(CA_mean = sapply(CASchools[, vars], mean),
      CA_sd   = sapply(CASchools[, vars], sd),
      MA_mean = sapply(MASchools[, vars], mean),
      MA_sd   = sapply(MASchools[, vars], sd))
```

Las estadísticas resumidas revelan que el puntaje promedio de las pruebas es más alto para los distritos escolares de Massachusetts. La prueba que se usa en Massachusetts es algo diferente a la que se usa en California (la calificación de la prueba de Massachusetts también incluye los resultados de la materia escolar "Ciencias"), por lo que no es apropiada una comparación directa de las calificaciones de las pruebas. También se puede ver que, en promedio, las clases son más pequeñas en Massachusetts que en California y que el ingreso promedio del distrito, el porcentaje promedio de estudiantes de inglés y el porcentaje promedio de estudiantes que reciben almuerzos subsidiados difieren considerablemente de los promedios calculados para California. También existen diferencias notables en la dispersión observada de las variables.

Examinando la relación entre los ingresos del distrito y los puntajes de las pruebas en Massachusetts como se hizo antes en el Capítulo \@ref(FRNL) para los datos de California y se produce una gráfica.

```{r, 555}
# estimar modelo lineal
Linear_model_MA <- lm(score ~ income, data = MASchools)
Linear_model_MA

# estimar modelo lineal logarítmico
Linearlog_model_MA <- lm(score ~ log(income), data = MASchools) 
Linearlog_model_MA

# estimar modelo cúbico
cubic_model_MA <- lm(score ~ I(income) + I(income^2) + I(income^3), data = MASchools)
cubic_model_MA
```

```{r, 556, fig.align='center'}
# graficar datos
plot(MASchools$income, MASchools$score,
     pch = 20,
     col = "steelblue",
     xlab = "Ingresos del distrito",
     ylab = "Resultado de la prueba",
     xlim = c(0, 50),
     ylim = c(620, 780))

# agregar una línea de regresión estimada para el modelo lineal
abline(Linear_model_MA, lwd = 2)

# agregar función de regresión estimada para el modelo lineal logarítmico
order_id  <- order(MASchools$income)

lines(MASchools$income[order_id],
      fitted(Linearlog_model_MA)[order_id], 
      col = "darkgreen", 
      lwd = 2)

# agregar función de regresión cúbica estimada
lines(x = MASchools$income[order_id], 
      y = fitted(cubic_model_MA)[order_id],
      col = "orange", 
      lwd = 2) 

# agrega una leyenda
legend("topleft",
       legend = c("Lineal","Lineal logarítmico","Cúbico"),
       lty = 1,
       col = c("Black", "darkgreen", "orange"))
```

La gráfica indica que la especificación cúbica se ajusta mejor a los datos. Curiosamente, esto es diferente de los datos de **CASchools**, donde el patrón de no linealidad se describe mejor mediante la especificación del modelo lineal logarítmico.

Se continua estimando la mayoría de las especificaciones del modelo utilizadas para el análisis del conjunto de datos **CASchools** en el Capítulo \@ref(FRNL) y usando **stargazer()** [@R-stargazer] para generar una representación tabular de los resultados de la regresión.

```{r, 557, eval=TRUE, message=FALSE, warning=FALSE}
# agregar 'HiEL' a 'MASchools'
MASchools$HiEL <- as.numeric(MASchools$english > median(MASchools$english))

# estimar las especificaciones del modelo a partir de la tabla
TestScore_MA_mod1 <- lm(score ~ STR, data = MASchools)

TestScore_MA_mod2 <- lm(score ~ STR + english + lunch + log(income), 
                        data = MASchools)

TestScore_MA_mod3 <- lm(score ~ STR + english + lunch + income + I(income^2) 
                        + I(income^3), data = MASchools)

TestScore_MA_mod4 <- lm(score ~ STR + I(STR^2) + I(STR^3) + english + lunch + income 
                        + I(income^2) + I(income^3), data = MASchools)

TestScore_MA_mod5 <- lm(score ~ STR + I(income^2) + I(income^3) + HiEL:STR + lunch 
                        + income, data = MASchools)

TestScore_MA_mod6 <- lm(score ~ STR + I(income^2) + I(income^3) + HiEL + HiEL:STR + lunch 
                        + income, data = MASchools)

# recopilar errores estándar robustos
rob_se <- list(sqrt(diag(vcovHC(TestScore_MA_mod1, type = "HC1"))),
               sqrt(diag(vcovHC(TestScore_MA_mod2, type = "HC1"))),
               sqrt(diag(vcovHC(TestScore_MA_mod3, type = "HC1"))),
               sqrt(diag(vcovHC(TestScore_MA_mod4, type = "HC1"))),
               sqrt(diag(vcovHC(TestScore_MA_mod5, type = "HC1"))),
               sqrt(diag(vcovHC(TestScore_MA_mod6, type = "HC1"))))
```

```{r, 558, results='asis', echo=T, cache=T, message=FALSE, warning=FALSE, eval=FALSE}
# generar una tabla con 'stargazer()'
library(stargazer)

stargazer(Linear_model_MA, TestScore_MA_mod2, TestScore_MA_mod3, 
          TestScore_MA_mod4, TestScore_MA_mod5, TestScore_MA_mod6,
          title = "Regresiones usando datos de puntajes de las prueba en Massachusetts",
          type = "latex",
          digits = 3,
          header = FALSE,
          se = rob_se,
          object.names = TRUE,
          model.numbers = FALSE,
          column.labels = c("(I)", "(II)", "(III)", "(IV)", "(V)", "(VI)"))
```

<!--html_preserve-->

```{r, 559, results='asis', echo=F, cache=T, message=FALSE, warning=FALSE, eval=my_output == "html"}
library(stargazer)
MASchools$HiEL <- as.numeric(MASchools$english > median(MASchools$english))
TestScore_MA_mod1 <- lm(score ~ STR, data = MASchools)
TestScore_MA_mod2 <- lm(score ~ STR + english + lunch + log(income), 
                        data = MASchools)
TestScore_MA_mod3 <- lm(score ~ STR + english + lunch + income + I(income^2) 
                        + I(income^3), data = MASchools)
TestScore_MA_mod4 <- lm(score ~ STR + I(STR^2) + I(STR^3) + english + lunch + income + I(income^2) 
                        + I(income^3), data = MASchools)
TestScore_MA_mod5 <- lm(score ~ STR + HiEL + HiEL:STR + lunch + income + I(income^2) 
                        + I(income^3), data = MASchools)
TestScore_MA_mod6 <- lm(score ~ STR + lunch + income + I(income^2) 
                        + I(income^3), data = MASchools)

rob_se <- list(
  sqrt(diag(vcovHC(TestScore_MA_mod1, type="HC1"))),
  sqrt(diag(vcovHC(TestScore_MA_mod2, type="HC1"))),
  sqrt(diag(vcovHC(TestScore_MA_mod3, type="HC1"))),
  sqrt(diag(vcovHC(TestScore_MA_mod4, type="HC1"))),
  sqrt(diag(vcovHC(TestScore_MA_mod5, type="HC1"))),
  sqrt(diag(vcovHC(TestScore_MA_mod6, type="HC1")))
)

stargazer(TestScore_MA_mod1, TestScore_MA_mod2, TestScore_MA_mod3, TestScore_MA_mod4, TestScore_MA_mod5, TestScore_MA_mod6, 
          se = rob_se,
          type = "html",
          header = FALSE,
          model.numbers = FALSE,
          dep.var.caption = "Variable dependiente: Puntaje",
          column.sep.width = "1pt",
          column.labels = c("(I)", "(II)", "(III)", "(IV)", "(V)", "(VI)")
          )

stargazer_html_title("Regresiones usando datos de puntajes de las prueba en Massachusetts", "rumtsd")
```

<!--/html_preserve-->

```{r, 560, results='asis', echo=F, cache=T, message=FALSE, warning=FALSE, eval=my_output == "latex"}
library(stargazer)
MASchools$HiEL <- as.numeric(MASchools$english > median(MASchools$english))
TestScore_MA_mod1 <- lm(score ~ STR, data = MASchools)
TestScore_MA_mod2 <- lm(score ~ STR + english + lunch + log(income), 
                        data = MASchools)
TestScore_MA_mod3 <- lm(score ~ STR + english + lunch + income + I(income^2) 
                        + I(income^3), data = MASchools)
TestScore_MA_mod4 <- lm(score ~ STR + I(STR^2) + I(STR^3) + english + lunch + income + I(income^2) 
                        + I(income^3), data = MASchools)
TestScore_MA_mod5 <- lm(score ~ STR + HiEL + HiEL:STR + lunch + income + I(income^2) 
                        + I(income^3), data = MASchools)
TestScore_MA_mod6 <- lm(score ~ STR + lunch + income + I(income^2) 
                        + I(income^3), data = MASchools)

rob_se <- list(
  sqrt(diag(vcovHC(TestScore_MA_mod1, type="HC1"))),
  sqrt(diag(vcovHC(TestScore_MA_mod2, type="HC1"))),
  sqrt(diag(vcovHC(TestScore_MA_mod3, type="HC1"))),
  sqrt(diag(vcovHC(TestScore_MA_mod4, type="HC1"))),
  sqrt(diag(vcovHC(TestScore_MA_mod5, type="HC1"))),
  sqrt(diag(vcovHC(TestScore_MA_mod6, type="HC1")))
)

stargazer(TestScore_MA_mod1, TestScore_MA_mod2, TestScore_MA_mod3, TestScore_MA_mod4, TestScore_MA_mod5, TestScore_MA_mod6,
          digits = 3,
          title = "\\label{tab:rumtsd} Regresiones usando datos de puntajes de las prueba en Massachusetts",
          type = "latex",
          float.env = "sidewaystable",
          column.sep.width = "-7pt",
          se = rob_se,
          omit.stat = "f",
          model.numbers = FALSE,
          column.labels = c("(I)", "(II)", "(III)", "(IV)", "(V)", "(VI)")
          )
```

A continuación, se reproducen los estadísticos $F$ y los valores de $p$ para probar la exclusión de grupos de variables.

```{r, 561, eval=FALSE, message=FALSE, warning=FALSE}
# modelo de prueba F (3)
linearHypothesis(TestScore_MA_mod3, 
                 c("I(income^2)=0", "I(income^3)=0"), 
                 vcov. = vcovHC, type = "HC1")

# modelo de pruebas F (4)
linearHypothesis(TestScore_MA_mod4, 
                 c("STR=0", "I(STR^2)=0", "I(STR^3)=0"), 
                 vcov. = vcovHC, type = "HC1")

linearHypothesis(TestScore_MA_mod4, 
                 c("I(STR^2)=0", "I(STR^3)=0"), 
                 vcov. = vcovHC, type = "HC1")

linearHypothesis(TestScore_MA_mod4, 
                 c("I(income^2)=0", "I(income^3)=0"), 
                 vcov. = vcovHC, type = "HC1")

# modelo de pruebas F (5)
linearHypothesis(TestScore_MA_mod5, 
                 c("STR=0", "STR:HiEL=0"), 
                 vcov. = vcovHC, type = "HC1")

linearHypothesis(TestScore_MA_mod5, 
                 c("I(income^2)=0", "I(income^3)=0"), 
                 vcov. = vcovHC, type = "HC1")

linearHypothesis(TestScore_MA_mod5, 
                 c("HiEL=0", "STR:HiEL=0"), 
                 vcov. = vcovHC, type = "HC1")

# modelo de prueba F (6)
linearHypothesis(TestScore_MA_mod6, 
                 c("I(income^2)=0", "I(income^3)=0"), 
                 vcov. = vcovHC, type = "HC1")
```

Se puede ver que, en términos de $\bar{R}^2$, la especificación (3), que usa un cúbico para modelar la relación entre el ingreso del distrito y los puntajes de las pruebas, funciona mejor que la especificación de registro lineal (2). Usando diferentes pruebas $F$ en los modelos (4) y (5), no se puede rechazar la hipótesis de que no existe una relación no lineal entre la proporción de alumnos por maestro y el puntaje de la prueba y también que la proporción de estudiantes de inglés tiene una influencia en la relación de interés. Además, la regresión (6) muestra que el porcentaje de estudiantes de inglés se puede omitir como regresor. Debido a que las especificaciones del modelo hechas en (4) a (6) no conducen a resultados sustancialmente diferentes a los de la regresión (3), se elige el modelo (3) como la especificación más adecuada.

En comparación con los datos de California, se observan los siguientes resultados:

1. Controlar las características de antecedentes de los estudiantes en la especificación del modelo (2) reduce el coeficiente de interés (proporción estudiante-maestro) en aproximadamente $60\%$. Los coeficientes estimados están próximos entre sí.

2. El coeficiente de la proporción alumno-maestro siempre es significativamente diferente de cero al nivel de $1\%$ para ambos conjuntos de datos. Esto es válido para todas las especificaciones del modelo consideradas en ambos estudios.

3. En ambos estudios, la proporción de estudiantes de inglés en un distrito escolar es de poca importancia para el impacto estimado de un cambio en la proporción de estudiantes por maestro en el puntaje de la prueba.

La mayor diferencia es que, en contraste con los resultados de California, no se encuentra evidencia de una relación no lineal entre los puntajes de las pruebas y la proporción de alumnos por maestro para los datos de Massachusetts, ya que las pruebas correspondientes de $F$ para el modelo (4) no rechazan la hipótesis nula.

Como se señala, los puntajes de las pruebas de California y Massachusetts tienen diferentes unidades porque las pruebas subyacentes son diferentes. Por lo tanto, los coeficientes estimados en la proporción alumno-maestro en ambas regresiones no se pueden comparar antes de estandarizar los puntajes de las pruebas a las mismas unidades que $$\frac{Testscore - \overline{TestScore}}{\sigma_{TestScore}}$$ para todas las observaciones en ambos conjuntos de datos y ejecutando las regresiones de interés utilizando nuevamente los datos estandarizados. Se puede demostrar que el coeficiente de la proporción de alumnos por maestro en la regresión que utiliza las puntuaciones de las pruebas estandarizadas es el coeficiente de la regresión original dividido por la desviación estándar de las puntuaciones de las pruebas.

Para el modelo (3) de los datos de Massachusetts, el coeficiente estimado de la proporción alumno-maestro es de $-0.64$. Se predice que una reducción de la proporción de estudiantes por maestro en dos estudiantes aumentará los puntajes de las pruebas en $-2 \cdot (-0.64) = 1.28$ puntos. Por lo tanto, se puede calcular el efecto de una reducción de la proporción de estudiantes por maestro de dos estudiantes en los puntajes de las pruebas estandarizadas de la siguiente manera:

```{r, 562}
TestScore_MA_mod3$coefficients[2] / sd(MASchools$score) * (-2)
```

Para Massachusetts, el aumento previsto de los puntajes de las pruebas debido a una reducción de la proporción de estudiantes por maestro en dos estudiantes es de $0.085$ desviaciones estándar de la distribución observada de los puntajes de las pruebas.

Utilizando la especificación lineal (2) para California, el coeficiente estimado de la proporción alumno-maestro es $-0.73$, por lo que el aumento previsto de las puntuaciones de las pruebas inducido por una reducción de la proporción alumno-maestro por dos estudiantes es $-0.73 \cdot (-2) = 1.46$. Usando **R** para calcular el cambio previsto en las unidades de desviación estándar:

```{r, 563}
TestScore_mod2$coefficients[2] / sd(CASchools$score) * (-2)
```

Esto muestra que el aumento previsto de los puntajes de las pruebas debido a una reducción de la proporción de estudiantes por maestro en dos estudiantes es de $0.077$ desviación estándar de la distribución observada de los puntajes de las pruebas para los datos de California.

En cuanto a los resultados de las pruebas estandarizadas, el cambio previsto es esencialmente el mismo para los distritos escolares de California y Massachusetts.

En conjunto, los resultados apoyan la validez externa de las inferencias hechas utilizando datos sobre los distritos de escuelas primarias de California, al menos para Massachusetts.

#### Validez interna del estudio {-}

La validez externa del estudio *no asegura su validez interna*. Aunque la especificación del modelo elegido mejora sobre un modelo de regresión lineal simple, la validez interna aún puede violarse debido a algunas de las amenazas enumeradas en el Concepto clave 9.7. Dichas amenazas son:
 
- Sesgo variable omitido.

- Especificación incorrecta de la forma funcional.

- Errores en variables.

- Problemas de selección de muestras.

- Causalidad simultánea.

- Heterocedasticidad.

- Correlación de errores entre observaciones.

#### Resumen {-}

Se ha encontrado que *existe un efecto pequeño pero estadísticamente significativo* de la proporción de alumnos por maestro en los puntajes de las pruebas. Sin embargo, no está claro si de hecho se ha estimado el efecto causal de interés ya que --- a pesar de que el enfoque incluye variables de control, teniendo en cuenta las no linealidades en la función de regresión de la población y la inferencia estadística utilizando errores estándar robustos --- los resultados aún podrían estar sesgados; por ejemplo, si hay factores omitidos que no se han considerado. Por tanto, la *validez interna* del estudio sigue siendo cuestionable. Como se concluyó de la comparación con el análisis del conjunto de datos de Massachusetts, este resultado puede ser *válido externamente*.

Los siguientes capítulos abordan técnicas que pueden ser remedios para todas las amenazas a la validez interna enumeradas en el Concepto clave 9.7 si la regresión múltiple por sí sola es insuficiente. Esto incluye regresión usando datos de panel y enfoques que emplean variables instrumentales.

## Ejercicios {#Ejercicios-9}

```{r, 564, echo=F, purl=F, results='asis'}
if (my_output=="html") {
  cat('
<div  class = "DCexercise">

#### 1. Estudio de simulación: especificación incorrecta de la forma funcional {-}

Como se indicó en el Capítulo \\@ref(AVIARM), la especificación incorrecta de la función de regresión viola el supuesto 1 del Concepto clave 6.3, por lo que el estimador MCO estará sesgado e inconsistente. Se ha ilustrado el sesgo de $\\hat{\\beta}_0$ para el ejemplo de la función de regresión poblacional cuadrática

$$Y_i = X_i^2 $$

y el modelo lineal $$Y_i = \\beta_0 + \\beta_1 X_i + u_i, \\, u_i \\sim \\mathcal{N}(0,1)$$ usando 100 observaciones generadas aleatoriamente. Estrictamente hablando, este hallazgo podría ser solo una coincidencia porque se considera solo una estimación obtenida usando un solo conjunto de datos.

En este ejercicio, se debe generar evidencia de simulación para el sesgo de $\\hat{\\beta}_0$ en el modelo $$Y_i = \\beta_0 + \\beta_1 X_i + u_i$$ si la función de regresión de la población es $$Y_i = X_i^2.$$

**Instrucciones:**

Asegúrese de utilizar las definiciones sugeridas en el código esqueleto en <tt>script.R</tt> para completar las siguientes tareas:

  + Genere 1000 estimaciones de MCO de $\\beta_0$ en el modelo anterior utilizando un bucle <tt>for()</tt> donde $X_i \\sim \\mathcal{U}[-5,5]$, $u_i \\sim \\mathcal{N}(0,1)$ usando muestras de tamaño $100$. Guardar las estimaciones en <tt>beta_hats</tt>.

  + Comparar la media muestral de las estimaciones con el parámetro verdadero utilizando el operador <tt>==</tt>.

<iframe src="DCL/ex9_1.html" frameborder="0" scrolling="no" style="width:100%;height:360px"></iframe>

**Sugerencia:**

  + Puede generar números aleatorios a partir de una distribución uniforme utilizando <tt>runif()</tt>.

</div>') } else {
  cat("\\begin{center}\\textit{Esta parte interactiva del curso solo está disponible en la versión HTML.}\\end{center}")
}
```

```{r, 565, echo=F, purl=F, results='asis'}
if (my_output=="html") {
  cat('
<div  class = "DCexercise">

#### 2. Estudio de simulación: Sesgo de errores en variables {-}

Considere nuevamente la aplicación del modelo clásico de error de medición presentado en el Capítulo \\@ref(AVIARM):

El regresor único $X_i$ se mide con error de modo que en su lugar se observa $\\overset{\\sim}{X}_i$. Por tanto, se estima $\\beta_1$ en

\\begin{align*}
  Y_i =& \\, \\beta_0 + \\beta_1 \\overset{\\sim}{X}_i + \\underbrace{\\beta_1 (X_i -\\overset{\\sim}{X}_i) + u_i}_{=v_i} \\\\
  Y_i =& \\, \\beta_0 + \\beta_1 \\overset{\\sim}{X}_i + v_i
\\end{align*}

en lugar de 

$$Y_i = \\beta_0 + \\beta_1 X_i + u_i,$$

con el error medio cero $w_i$ no correlacionado con $X_i$ y $u_i$. Entonces $\\beta_1$ es estimado de manera inconsistente por MCO:

\\begin{equation}
  \\widehat{\\beta}_1 \\xrightarrow{p}{\\frac{\\sigma_{X}^2}{\\sigma_{X}^2 + \\sigma_{w}^2}} \\beta_1
\\end{equation}

Deje 

$$(X, Y) \\sim \\mathcal{N}\\left[\\begin{pmatrix}50\\\\ 100\\end{pmatrix},\\begin{pmatrix}10 & 5 \\\\ 5 & 10 \\end{pmatrix}\\right].$$ 

Recuerde de \\@ref(eq:bnormexpfn) que $E(Y_i\\vert X_i) = 75 + 0.5 X_i$ en este caso. Además, suponga que $\\overset{\\sim}{X_i} = X_i + w_i$ con $w_i \\overset{i.i.d}{\\sim} \\mathcal{N}(0,10)$.

Como se mencionó en el ejercicio 1, el capítulo \\@ref(AVIARM) analiza las consecuencias del error de medición para el estimador de MCO de $\\beta_1$ en este entorno basado en una *única muestra* y, por lo tanto, *una sola estimación*. Estrictamente hablando, la conclusión obtenida podría ser incorrecta porque el sesgo observado puede deberse a una variación aleatoria. Una simulación de Monto Carlo es más apropiada aquí.

**Instrucciones:**

Muestre que $\\beta_1$ se estima con un sesgo utilizando un estudio de simulación. Asegúrese de utilizar las definiciones sugeridas en el código esqueleto en <tt>script.R</tt> para completar las siguientes tareas:

  + Generar 1000 estimaciones de $\\beta_1$ en el modelo de regresión simple $$Y_i = \\beta_0 + \\beta_1 X_i + u_i.$$ Usar <tt>rmvnorm()</tt> para generar muestras de 100 observaciones aleatorias de la distribución normal bivariada indicada anteriormente.

  + Guardar las estimaciones en <tt>beta_hats</tt>.

  + Calcular la media muestral de las estimaciones.

<iframe src="DCL/ex9_2.html" frameborder="0" scrolling="no" style="width:100%;height:400px"></iframe>

</div>')}
```

<!--chapter:end:Capitulo_10.Rmd-->

# Regresión con datos de panel {#RDP}

```{r, echo = F}
options(knitr.duplicate.label = "allow")
```

```{r, 566, child="_setup.Rmd"}
```

```{r, 567, eval=knitr::opts_knit$get("rmarkdown.pandoc.to") == "html", results='asis', echo=FALSE}
cat('<hr style="background-color:#03193b;height:2px">')
```

La regresión utilizando datos de tipo panel puede mitigar el sesgo de la variable omitida cuando no existe información sobre las variables que se correlacionan tanto con los regresores de interés como con la variable independiente y si estas variables son constantes en la dimensión de tiempo o entre entidades. Siempre que se disponga de datos de panel, los métodos de regresión de panel pueden mejorar los modelos de regresión múltiple que, como se explica en el Capítulo \@ref(EEBRM), producen resultados que no son válidos internamente en dicho entorno.

Este capitulo cubre los siguientes topicos:

- Notación para datos de panel.
- Regresión de efectos fijos usando efectos fijos de tiempo y/o entidad.
- Cálculo de errores estándar en modelos de regresión de efectos fijos.

Siguiendo con el curso, para las aplicaciones se utiliza el conjunto de datos **Fatalities** del paquete **AER** [@R-AER], que es un conjunto de datos de panel que informa sobre observaciones anuales a nivel estatal sobre muertes de tránsito en los EE. UU. para el período de 1982 a 1988. Las aplicaciones analizan si hay efectos de los impuestos sobre el alcohol y las leyes de conducción sobre el estado de ebriedad en las muertes en carretera y, si están presentes, *qué tan fuertes* son estos efectos.

Se intruduce **plm()**, una conveniente función **R** que permite estimar modelos de regresión de panel lineal que viene con el paquete **plm** [@R-plm]. El uso de **plm()** es muy similar al de la función **lm()** que se ha utilizado a lo largo de los capítulos anteriores para la estimación de modelos de regresión simple y múltiple.

Los siguientes paquetes y sus dependencias son necesarios para la reproducción de los fragmentos de código presentados a lo largo de este capítulo en su computadora:

+ **AER**
+ **plm** 
+ **stargazer**

```{r, 568, warning=FALSE, message=FALSE, eval=FALSE}
install.packages("AER")
install.packages("plm")
install.packages("stargazer")
```

Compruebe si el siguiente fragmento de código se ejecuta sin errores.

```{r, 569, warning=FALSE, message=FALSE, eval=FALSE}
library(AER)
library(plm)
library(stargazer)
```

## Datos de panel

```{r, 570, eval = my_output == "html", results='asis', echo=F, purl=F}
cat('
<div class = "keyconcept" id="KC10.1">
<h3 class = "right"> Concepto clave 10.1 </h3>
<h3 class = "left"> Notación para datos de panel </h3>

En contraste con los datos de sección transversal donde se tienen observaciones sobre $n$ sujetos (entidades), los datos de panel tienen observaciones sobre $n$ entidades en períodos de tiempo $T\\geq2$. Esto se denota:

$$(X_{it},Y_{it}), \\ i=1,\\dots,n \\ \\ \\ \\text{and} \\ \\ \\ t=1,\\dots,T $$

donde el índice $i$ se refiere a la entidad mientras que $t$ se refiere al período de tiempo.

</div>
')
```

```{r, 571, eval = my_output == "latex", results='asis', echo=F, purl=F}
cat('\\begin{keyconcepts}[Notación para datos de panel]{10.1}

En contraste con los datos de sección transversal donde se tienen observaciones sobre $n$ sujetos (entidades), los datos de panel tienen observaciones sobre $n$ entidades en períodos de tiempo $T\\geq2$. Esto se denota:

$$(X_{it},Y_{it}), \\ i=1,\\dots,n \\ \\ \\ \\text{and} \\ \\ \\ t=1,\\dots,T $$

donde el índice $i$ se refiere a la entidad mientras que $t$ se refiere al período de tiempo.

\\end{keyconcepts}
')
```

A veces, los datos de panel también se denominan datos longitudinales, ya que agregan una dimensión temporal a los datos transversales. Echando un vistazo al conjunto de datos **Fatalities** comprobando su estructura y enumerando las primeras observaciones.

```{r, 572, warning=FALSE, message=FALSE}
# cargar el paquete y el conjunto de datos
library(AER)
data(Fatalities)
```

```{r, 573, warning=FALSE, message=FALSE}
# obtener la dimensión e inspeccionar la estructura
is.data.frame(Fatalities)
dim(Fatalities)
```

```{r, 574}
str(Fatalities)
```

```{r, 575, warning=FALSE, message=FALSE}
# enumerar las primeras observaciones
head(Fatalities)
```

```{r, 576, warning=FALSE, message=FALSE}
# resumir las variables 'state' y 'year'
summary(Fatalities[, c(1, 2)])
```

Se encuentra que el conjunto de datos consta de 336 observaciones sobre 34 variables. Se puede observar que la variable **state** es una variable de factor con 48 niveles (uno para cada uno de los 48 estados federales contiguos de los EE. UU.).

La variable **year** también es una variable factorial que tiene 7 niveles que identifican el período de tiempo en que se realizó la observación. Esto nos da $7\times48 = 336$ observaciones en total. Dado que todas las variables se observan para todas las entidades y durante todos los períodos de tiempo, el panel está *equilibrado*. Si faltaran datos para al menos una entidad en al menos un período de tiempo, se llamarían datos de panel *desequilibrado*.

#### Ejemplo: Muertes por accidentes de tránsito e impuestos sobre el alcohol {-}

Se debe comenzar con la creación de un gráfico. Para ello, se estiman regresiones simples utilizando datos de los años 1982 y 1988 que modelan la relación entre el impuesto a la cerveza (ajustado por dólares de 1988) y la tasa de fatalidades de tránsito, medida como el número de fatalidades por 10000 habitantes. Luego, se grafican los datos y se agregan las funciones de regresión estimadas correspondientes.

```{r, 577}
# definir la tasa de letalidad
Fatalities$fatal_rate <- Fatalities$fatal / Fatalities$pop * 10000

# subconjunto de los datos
Fatalities1982 <- subset(Fatalities, year == "1982")
Fatalities1988 <- subset(Fatalities, year == "1988")
```

```{r, 578, warning=FALSE, message=FALSE}
# estimar modelos de regresión simple usando datos de 1982 y 1988
fatal1982_mod <- lm(fatal_rate ~ beertax, data = Fatalities1982)
fatal1988_mod <- lm(fatal_rate ~ beertax, data = Fatalities1988)

coeftest(fatal1982_mod, vcov. = vcovHC, type = "HC1")
coeftest(fatal1988_mod, vcov. = vcovHC, type = "HC1")
```

Las funciones de regresión estimadas son:

\begin{align*}
  \widehat{\text{Tasa de fatalidad}} =& \, \underset{(0.15)}{2.01} + \underset{(0.13)}{0.15} \times \text{Impuesto a la cerveza} \quad (\text{Datos de } 1982), \\
  \widehat{\text{Tasa de fatalidad}} =& \, \underset{(0.11)}{1.86} + \underset{(0.13)}{0.44} \times \text{Impuesto a la cerveza} \quad (\text{Datos de } 1988).
\end{align*}

```{r, 579}
# graficar las observaciones y agregar la línea de regresión estimada para los datos de 1982
plot(x = Fatalities1982$beertax, 
     y = Fatalities1982$fatal_rate, 
     xlab = "Impuesto a la cerveza (en dólares de 1988)",
     ylab = "Tasa de mortalidad (muertes por 10000)",
     main = "Tasas de accidentes de tráfico e impuestos a la cerveza en 1982",
     ylim = c(0, 4.5),
     pch = 20, 
     col = "steelblue")

abline(fatal1982_mod, lwd = 1.5)

# graficar las observaciones y agregar la línea de regresión estimada para los datos de 1988
plot(x = Fatalities1988$beertax, 
     y = Fatalities1988$fatal_rate, 
     xlab = "Impuesto a la cerveza (en dólares de 1988)",
     ylab = "Tasa de mortalidad (muertes por 10000)",
     main = "Tasas de accidentes de tráfico e impuestos a la cerveza en 1988",
     ylim = c(0, 4.5),
     pch = 20, 
     col = "steelblue")

abline(fatal1988_mod, lwd = 1.5)
```

En ambas gráficas, cada punto representa las observaciones del impuesto a la cerveza y la tasa de mortalidad para un estado dado en el año respectivo. Los resultados de la regresión indican una relación positiva entre el impuesto a la cerveza y la tasa de letalidad para ambos años. El coeficiente estimado del impuesto a la cerveza para los datos de 1988 es casi tres veces mayor que el del conjunto de datos de 1988. Esto es contrario a las expectativas: Se supone que los impuestos sobre el alcohol *reducen* la tasa de accidentes de tránsito. Como se sabe por el Capítulo \@ref(MRVR), esto posiblemente se deba al sesgo de la variable omitida, ya que ambos modelos no incluyen ninguna covariable; por ejemplo, condiciones económicas. Esto podría corregirse mediante el uso de un enfoque de regresión múltiple. Sin embargo, esto no puede tener en cuenta los factores omitidos *no observables* que difieren de un estado a otro, pero se puede suponer que son constantes durante el período de observación; por ejemplo, la actitud de la población hacia la conducción bajo los efectos del alcohol. Como se muestra en la siguiente sección, los datos de panel nos permiten mantener dichos factores constantes.

## Datos de panel con dos períodos de tiempo: Comparaciones "antes y después" {#DPDPTCAD}

Suponga que solo existen $T = 2$ períodos de tiempo $t = 1982, 1988$. Esto permite analizar las diferencias en los cambios de la tasa de letalidad del año 1982 a 1988. Para empezar se debe considerar el modelo de regresión poblacional $$\text{Tasa de mortalidad}_{it} = \beta_0 + \beta_1 \text{Impuesto a la cerveza}_{it} + \beta_2 Z_{i} + u_{it}$$ donde $Z_i$ son características específicas del estado que difieren entre los estados pero que son *constantes en el tiempo*. Para $t = 1982$ y $t = 1988$ se tiene:

\begin{align*}
  \text{Tasa de mortalidad}_{i1982} =&\, \beta_0 + \beta_1 \text{Impuesto a la cerveza}_{i1982} + \beta_2 Z_i + u_{i1982}, \\
  \text{Tasa de mortalidad}_{i1988} =&\, \beta_0 + \beta_1 \text{Impuesto a la cerveza}_{i1988} + \beta_2 Z_i + u_{i1988}.
\end{align*}

Se puede eliminar el $Z_i$ haciendo una regresión de la diferencia en la tasa de mortalidad entre 1988 y 1982 sobre la diferencia en el impuesto a la cerveza entre esos años:

$$\text{Tasa de mortalidad}_{i1988} - \text{Tasa de mortalidad}_{i1982} = \\\\ \beta_1 (\text{Impuesto a la cerveza}_{i1988} - \text{Impuesto a la cerveza}_{i1982}) + u_{i1988} - u_{i1982}$$

Este modelo de regresión arroja una estimación robusta de $\beta_1$, un posible sesgo debido a la omisión de $Z_i$, ya que estas influencias se eliminan del modelo. A continuación, se usa **R** para estimar una regresión basada en los datos diferenciados y se grafica la función de regresión estimada.

```{r, 580}
# calcular las diferencias
diff_fatal_rate <- Fatalities1988$fatal_rate - Fatalities1982$fatal_rate
diff_beertax <- Fatalities1988$beertax - Fatalities1982$beertax

# estimar una regresión usando datos diferenciados
fatal_diff_mod <- lm(diff_fatal_rate ~ diff_beertax)

coeftest(fatal_diff_mod, vcov = vcovHC, type = "HC1")
```

La inclusión de la intersección permite un cambio en la tasa de mortalidad media en el período comprendido entre 1982 y 1988 en ausencia de un cambio en el impuesto a la cerveza.

Se obtiene la función de regresión estimada de MCO $$\widehat{\text{Tasa de mortalidad}_{i1988} - \text{Tasa de mortalidad}_{i1982}} = \\\\ - \underset{(0.065)}{0.072} - \underset{(0.36)}{1.04} \times (\text{Impuesto a la cerveza}_{i1988} - \text{Impuesto a la cerveza}_{i1982}).$$

```{r, 581, fig.align='center'}
# graficar los datos diferenciados
plot(x = diff_beertax, 
     y = diff_fatal_rate, 
     xlab = "Cambio en el impuesto a la cerveza (en dólares de 1988)",
     ylab = "Cambio en la tasa de mortalidad (muertes por 10000)",
     main = "Cambios en las tasas de mortalidad por accidentes de tráfico y los impuestos a la cerveza en 1982-1988",
     xlim = c(-0.6, 0.6),
     ylim = c(-1.5, 1),
     pch = 20, 
     col = "steelblue")

# agregar la línea de regresión para graficar
abline(fatal_diff_mod, lwd = 1.5)
```

El coeficiente estimado del impuesto a la cerveza es ahora negativo y significativamente diferente de cero a $5\%$. Su interpretación es que aumentar el impuesto a la cerveza en $\$1$ hace que las muertes por accidentes de tránsito disminuyan en $1.04$ por $10000$ personas. Esto es bastante grande ya que la tasa de mortalidad promedio es de aproximadamente $2$ personas por $10000$ personas.

```{r, 582}
# calcular la tasa de mortalidad media en todos los estados para todos los períodos de tiempo
mean(Fatalities$fatal_rate)
```

Una vez más, es probable que este resultado sea una consecuencia de la omisión de factores en la regresión de un año que influyen en la tasa de mortalidad y están *correlacionados con el impuesto a la cerveza* y *cambian a lo largo del tiempo*. El mensaje es que se debe ser más cuidadoso y controlar esos factores antes de sacar conclusiones sobre el efecto de un aumento en los impuestos a la cerveza.

El enfoque presentado en esta sección descarta información para los años $1983$ a $1987$. Un método que permite usar datos de más de $T = 2$ períodos de tiempo y permite agregar variables de control es el enfoque de regresión de efectos fijos.

## Regresión de efectos fijos

Considere el modelo de regresión de panel

$$Y_{it} = \beta_0 + \beta_1 X_{it} + \beta_2 Z_i +  u_{it}$$

donde $Z_i$ son heterogeneidades invariantes en el tiempo no observadas entre las entidades $i=1,\dots,n$. El objetivo es estimar $\beta_1$, el efecto en $Y_i$ de un cambio en $X_i$ manteniendo constante $Z_i$. Dejando $\alpha_i = \beta_0 + \beta_2 Z_i$ se obtiene el siguiente modelo:

\begin{align}
Y_{it} = \alpha_i + \beta_1 X_{it} + u_{it} (\#eq:femodel).
\end{align}

Teniendo intersecciones específicas individuales $\alpha_i$, $i=1,\dots,n$, donde cada una de estas puede entenderse como el efecto fijo de la entidad $i$, este modelo se llama *modelo de efectos fijos*.

La variación en $\alpha_i$, $i=1,\dots,n$  proviene de $Z_i$. \@ref(eq:femodel) se puede reescribir como un modelo de regresión que contiene $n-1$ regresores ficticios y una constante:

\begin{align}
Y_{it} = \beta_0 + \beta_1 X_{it} + \gamma_2 D2_i + \gamma_3 D3_i + \cdots + \gamma_n Dn_i + u_{it} (\#eq:drmodel).
\end{align}

El modelo \@ref(eq:drmodel) tiene $n$ intersecciones diferentes --- una para cada entidad. \@ref(eq:femodel) y \@ref(eq:drmodel) son representaciones equivalentes del modelo de efectos fijos.

El modelo de efectos fijos se puede generalizar para que contenga más de un determinante de $Y$ que está correlacionado con $X$ y cambia con el tiempo. El Concepto clave 10.2 presenta el modelo de regresión de efectos fijos generalizados.

```{r, 583, eval = my_output == "html", results='asis', echo=F, purl=F}
cat('
<div class = "keyconcept" id="KC10.2">
<h3 class = "right"> Concepto clave 10.2 </h3>
<h3 class = "left"> El modelo de regresión de efectos fijos </h3>

El modelo de regresión de efectos fijos es

\\begin{align}
Y_{it} = \\beta_1 X_{1,it} + \\cdots + \\beta_k X_{k,it} + \\alpha_i + u_{it} (\\#eq:gfemodel)
\\end{align}

con $i=1,\\dots,n$ y $t=1,\\dots,T$. Los $\\alpha_i$ son intersecciones específicas de entidades que capturan heterogeneidades entre entidades. Una representación equivalente de este modelo viene dada por

\\begin{align}
Y_{it} = \\beta_0 + \\beta_1 X_{1,it} + \\cdots + \\beta_k X_{k,it} + \\gamma_2 D2_i + \\gamma_3 D3_i + \\cdots + \\gamma_n Dn_i  + u_{it} (\\#eq:gdrmodel)
\\end{align}

donde $D2_i,D3_i,\\dots,Dn_i$ son variables ficticias.

</div>
')
```

```{r, 584, eval = my_output == "latex", results='asis', echo=F, purl=F}
cat('\\begin{keyconcepts}[El modelo de regresión de efectos fijos]{10.2}

El modelo de regresión de efectos fijos es

\\begin{align}
Y_{it} = \\beta_1 X_{1,it} + \\cdots + \\beta_k X_{k,it} + \\alpha_i + u_{it} \\label{eq:gfemodel}
\\end{align}

con $i=1,\\dots,n$ y $t=1,\\dots,T$. Los $\\alpha_i$ son intersecciones específicas de entidades que capturan heterogeneidades entre entidades. Una representación equivalente de este modelo viene dada por

\\begin{align}
Y_{it} = \\beta_0 + \\beta_1 X_{1,it} + \\cdots + \\beta_k X_{k,it} + \\gamma_2 D2_i + \\gamma_3 D3_i + \\cdots + \\gamma_n Dn_i  + u_{it} \\label{eq:gdrmodel}
\\end{align}

donde $D2_i,D3_i,\\dots,Dn_i$ son variables ficticias.

\\end{keyconcepts}
')
```

### Estimación e inferencia {-}

Los paquetes de software utilizan el llamado algoritmo MCO "degradado por entidad" que es computacionalmente más eficiente que estimar modelos de regresión con $k + n$ regresores según sea necesario para los modelos \@ref(eq:gfemodel) y \@ref(eq:gdrmodel).

Tomando promedios en ambos lados de \@ref(eq:femodel) se obtiene:

\begin{align*}
\frac{1}{n} \sum_{i=1}^n Y_{it} =& \, \beta_1 \frac{1}{n} \sum_{i=1}^n X_{it} + \frac{1}{n} \sum_{i=1}^n a_i + \frac{1}{n} \sum_{i=1}^n u_{it} \\
\overline{Y} =& \, \beta_1 \overline{X}_i + \alpha_i + \overline{u}_i. 
\end{align*}

La resta de \@ref(eq:femodel) produce:

\begin{align}
\begin{split}
Y_{it} - \overline{Y}_i =& \, \beta_1(X_{it}-\overline{X}_i) + (u_{it} - \overline{u}_i) \\
\overset{\sim}{Y}_{it} =& \, \beta_1 \overset{\sim}{X}_{it} + \overset{\sim}{u}_{it}. 
\end{split} (\#eq:edols)
\end{align}

En este modelo, la estimación de MCO del parámetro de interés $\beta_1$ es igual a la estimación obtenida usando \@ref(eq:drmodel) --- sin la necesidad de estimar $n-1$ ficticias y una intersección.

Se concluye que existen dos formas de estimar $\beta_1$ en la regresión de efectos fijos:

1. MCO del modelo de regresión ficticia como se muestra en \@ref(eq:drmodel)

2. MCO usando los datos degradados de la entidad como en \@ref(eq:edols)

Siempre que se cumplan los supuestos de regresión de efectos fijos establecidos en el Concepto clave 10.3, la distribución muestral del estimador de MCO en el modelo de regresión de efectos fijos es normal en muestras grandes. La varianza de las estimaciones puede estimarse y se pueden calcular errores estándar, estadísticos $t$ e intervalos de confianza para los coeficientes. En la siguiente sección, se ve cómo estimar un modelo de efectos fijos usando **R** y cómo obtener un resumen del modelo que reporta errores estándar robustos a la heterocedasticidad. Dejando de lado las complicadas fórmulas de los estimadores.

### Aplicación a muertes por accidentes de tráfico {-}

Siguiendo el Concepto clave 10.2, el modelo simple de efectos fijos para la estimación de la relación entre las tasas de accidentes de tránsito y los impuestos a la cerveza es

\begin{align}
\text{Tasa de fatalidad}_{it} = \beta_1 \text{Impuesto a la cerveza}_{it} + \text{Efectos fijos estatales} + u_{it}, (\#eq:fatsemod)
\end{align}

una regresión de la tasa de mortalidad por accidentes de tráfico en el impuesto a la cerveza y 48 regresores binarios, uno para cada estado federal.

Simplemente se puede usar la función **lm()** para obtener una estimación de $\beta_1$.

```{r, 585}
fatal_fe_lm_mod <- lm(fatal_rate ~ beertax + state - 1, data = Fatalities)
fatal_fe_lm_mod
```

Como se discutió en la sección anterior, también es posible estimar $\beta_1$ aplicando MCO a los datos degradados; es decir, para ejecutar la regresión:

$$\overset{\sim}{\text{Tasa de fatalidad}} = \beta_1 \overset{\sim}{\text{Impuesto a la cerveza}}_{it} + u_{it}. $$

```{r, 586, eval=F}
# obtener datos degradados
Fatalities_demeaned <- with(Fatalities,
            data.frame(fatal_rate = fatal_rate - ave(fatal_rate, state),
            beertax = beertax - ave(beertax, state)))

# estimar la regresión
summary(lm(fatal_rate ~ beertax - 1, data = Fatalities_demeaned))
```

La función **ave** es conveniente para calcular promedios de grupo. Se usa para obtener promedios estatales específicos de la tasa de mortalidad y el impuesto a la cerveza.

Alternativamente, se puede usar **plm()** del paquete con el mismo nombre.

```{r, 587, eval=-2, message=F, warning=F}
# instalar y cargar el paquete 'plm'
install.packages("plm")
library(plm)
```

En cuanto a **lm()**, se tiene que especificar la fórmula de regresión y los datos que se utilizarán en la llamada de **plm()**. Además, se requiere pasar un vector con los nombres de cada entidad y variables de identificación de tiempo al argumento **index**. Para **Fatalities**, la variable de identificación de las entidades se denomina **state** y la variable de identificación de tiempo es **year**. Dado que el estimador de efectos fijos también se denomina *estimador interno*, se establece **model = "within"**. Finalmente, la función **coeftest()** permite obtener inferencias basadas en errores estándar robustos.

```{r, 588}
# estimar la regresión de efectos fijos con plm()
fatal_fe_mod <- plm(fatal_rate ~ beertax, 
                    data = Fatalities,
                    index = c("state", "year"), 
                    model = "within")

# imprimir resumen usando errores estándar robustos
coeftest(fatal_fe_mod, vcov. = vcovHC, type = "HC1")
```

El coeficiente estimado es de nuevo $-0.6559$. Se debe tener en cuenta que **plm()** utiliza el algoritmo MCO degradado por entidad y, por lo tanto, no informa coeficientes ficticios. La función de regresión estimada es

\begin{align}
\widehat{\text{Tasa de fatalidad}} = -\underset{(0.29)}{0.66} \times \text{Impuesto a la cerveza} + \text{Efectos fijos estatales}. (\#eq:efemod)
\end{align}

El coeficiente de $\text{Impuesto a la cerveza}$ es negativo y significativo. La interpretación es que la reducción estimada en las muertes por accidentes de tránsito debido a un aumento en el impuesto real a la cerveza en $\$1$ es de $0.66$ por $10000$ personas, lo que sigue siendo bastante alto. Si bien la inclusión de efectos fijos estatales elimina el riesgo de sesgo debido a factores omitidos que varían entre los estados, pero no a lo largo del tiempo, se sospecha que hay otras variables omitidas que varían con el tiempo y, por lo tanto, causan un sesgo.

## Regresión con efectos fijos en el tiempo

El control de las variables que son constantes entre entidades pero que varían con el tiempo se puede realizar al incluir efectos fijos en el tiempo. Si existe *solo efectos fijos de tiempo*, el modelo de regresión de efectos fijos se convierte en $$Y_{it} = \beta_0 + \beta_1 X_{it} + \delta_2 B2_t + \cdots + \delta_T BT_t + u_{it},$$ donde solo se incluyen variables ficticias $T-1$ (se omite $B1$), ya que el modelo incluye una intersección. Este modelo elimina el sesgo de variables omitidas causado por la exclusión de variables no observadas que evolucionan con el tiempo, pero son constantes en todas las entidades.

En algunas aplicaciones, es significativo incluir efectos fijos tanto de entidad como de tiempo. El *modelo de efectos fijos de entidad y tiempo* es $$Y_{it} = \beta_0 + \beta_1 X_{it} + \gamma_2 D2_i + \cdots + \gamma_n DT_i + \delta_2 B2_t + \cdots + \delta_T BT_t + u_{it} .$$ El modelo combinado permite eliminar el sesgo de las variables inobservables que cambian con el tiempo, pero son constantes en las entidades y controla los factores que difieren entre las entidades pero que son constantes en el tiempo. Estos modelos se pueden estimar utilizando el algoritmo MCO que se implementa en **R**.

El siguiente fragmento de código muestra cómo estimar la entidad combinada y el modelo de efectos fijos de tiempo de la relación entre las muertes y el impuesto a la cerveza, $$\text{Tasa de fatalidad}_{it} = \beta_1 \text{Impuesto a la cerveza}_{it} + \text{Efectos de estado} + \text{Efectos fijos de tiempo} + u_{it}$$ usando tanto **lm()** como **plm()**. Es sencillo estimar esta regresión con **lm()**, ya que es solo una extensión de \@ref(eq:fatsemod) por lo que solo se tiene que ajustar el argumento **formula** agregando el regresor adicional **year** para efectos fijos de tiempo. En la llamada de **plm()** se establece otro argumento **effect = "twoways"** para la inclusión de variables *ficticias de entidad* y *tiempo*.

```{r, 589}
# estimar un modelo de regresión combinado de efectos fijos de entidad y tiempo

# a través de lm()
fatal_tefe_lm_mod <- lm(fatal_rate ~ beertax + state + year - 1, data = Fatalities)
fatal_tefe_lm_mod

# a través de plm()
fatal_tefe_mod <- plm(fatal_rate ~ beertax, 
                      data = Fatalities,
                      index = c("state", "year"), 
                      model = "within", 
                      effect = "twoways")

coeftest(fatal_tefe_mod, vcov = vcovHC, type = "HC1")
```

Antes de discutir los resultados, se deben convencer de que **state** y **year** son de la clase **factor**.

```{r, 590}
# marcar la clase de 'state' y 'year'
class(Fatalities$state)
class(Fatalities$year)
```

Las funciones **lm()** convierten los factores en variables ficticias automáticamente. Dado que se excluye la intersección agregando **-1** al lado derecho de la fórmula de regresión, **lm()** estima los coeficientes para las variables binarias $n + (T-1) = 48 + 6 = 54$ (variables ficticias de 6 años y 48 variables ficticias estatales). Una vez más, **plm()** solo informa el coeficiente estimado en $Impuesto a la cerveza$.

La función de regresión estimada es

\begin{align}
\widehat{\text{Tasa de fatalidad}} =  -\underset{(0.35)}{0.64} \times \text{Impuesto a la cerveza} + \text{Efectos de estado} + \text{Efectos fijos de tiempo}. (\#eq:cbnfemod)
\end{align}

El resultado $-0.66$ está cerca del coeficiente estimado para el modelo de regresión que incluye solo los efectos fijos de la entidad. Como era de esperar, el coeficiente se estima con menos precisión, pero es significativamente diferente de cero a $10\%$.

En vista de \@ref(eq:efemod) y \@ref(eq:cbnfemod), se concluye que relación estimada entre las muertes por accidentes de tránsito y el impuesto real a la cerveza no se ve afectada por el sesgo de la variable omitida debido a factores que son constantes en el tiempo.

## Los supuestos de regresión de efectos fijos y los errores estándar para la regresión de efectos fijos {#SREFEEREF}

Esta sección se centra en el modelo de efectos fijos de la entidad y presenta los supuestos del modelo que deben cumplirse para que MCO produzca estimaciones insesgadas que normalmente se distribuyen en muestras grandes. Estos supuestos son una extensión de los supuestos hechos para el modelo de regresión múltiple (ver Concepto clave 6.4) y se dan en el Concepto clave 10.3. También se discuten brevemente los errores estándar en los modelos de efectos fijos que difieren de los errores estándar en la regresión múltiple, ya que el error de regresión puede exhibir una correlación serial en los modelos de panel.

```{r, 591, eval = my_output == "html", results='asis', echo=F, purl=F}
cat('
<div class = "keyconcept" id="KC10.3">
<h3 class = "right"> Concepto clave 10.3 </h3>
<h3 class = "left"> Los supuestos de regresión de efectos fijos </h3>

En el modelo de efectos fijos $$ Y_{it} = \\beta_1 X_{it} + \\alpha_i + u_{it} \\ \\ , \\ \\ i=1,\\dots,n, \\ t=1,\\dots,T, $$ se asume lo siguiente:

1. El término de error $u_{it}$ tiene una media condicional cero; es decir, $E(u_{it}|X_{i1}, X_{i2},\\dots, X_{iT})$.
2. $(X_{i1}, X_{i2}, \\dots, X_{i3}, u_{i1}, \\dots, u_{iT})$, $i=1,\\dots,n$ son i.i.d. extraidas de su distribución conjunta.
3. Los valores atípicos grandes son poco probables; es decir, $(X_{it}, u_{it})$ tienen cuartos momentos finitos distintos de cero.
4. No existe una multicolinealidad perfecta.

Cuando existen varios regresores, $X_{it}$ se reemplaza por $X_{1,it}, X_{2,it}, \\dots, X_{k,it}$.

</div>
')
```

```{r, 592, eval = my_output == "latex", results='asis', echo=F, purl=F}
cat('\\begin{keyconcepts}[Los supuestos de regresión de efectos fijos]{10.3}
En el modelo de efectos fijos $$ Y_{it} = \\beta_1 X_{it} + \\alpha_i + u_{it} \\ \\ , \\ \\ i=1,\\dots,n, \\ t=1,\\dots,T, $$ se asume lo siguiente:\\newline

\\begin{enumerate}
\\item El término de error $u_{it}$ tiene una media condicional cero; es decir, $E(u_{it}|X_{i1}, X_{i2},\\dots, X_{iT})$.
\\item $(X_{i1}, X_{i2}, \\dots, X_{i3}, u_{i1}, \\dots, u_{iT})$, $i=1,\\dots,n$ son i.i.d. extraidas de su distribución conjunta.
\\item Los valores atípicos grandes son poco probables; es decir, $(X_{it}, u_{it})$ tienen cuartos momentos finitos distintos de cero.
\\item No existe una multicolinealidad perfecta.
\\end{enumerate}\\vspace{0.5cm}

Cuando existen varios regresores, $X_{it}$ se reemplaza por $X_{1,it}, X_{2,it}, \\dots, X_{k,it}$.

\\end{keyconcepts}
')
```

La primera suposición es que el error no está correlacionado con *todas* las observaciones de la variable $X$ para la entidad $i$ a lo largo del tiempo. Si se viola este supuesto, se enfrenta al sesgo de las variables omitidas. El segundo supuesto asegura que las variables sean i.i.d. *en* entidades $i=1,\dots,n$. Esto no requiere que las observaciones no estén correlacionadas *dentro* de una entidad. Los $X_{it}$ pueden estar *autocorrelacionados* dentro de las entidades. Ésta es una propiedad común de los datos de series de tiempo. Lo mismo se permite para los errores $u_{it}$. Consulte el Capítulo \@ref(SREFEEREF) para obtener una explicación detallada de por qué la autocorrelación es plausible en aplicaciones de panel. El segundo supuesto se justifica si las entidades se seleccionan mediante muestreo aleatorio simple. Los supuestos tercero y cuarto son análogos a los supuestos de regresión múltiple realizados en el Concepto clave 6.4.

#### Errores estándar para regresión de efectos fijos {-}

Al igual que para la heterocedasticidad, la autocorrelación invalida las fórmulas de error estándar habituales, así como los errores estándar robustos a la heterocedasticidad, ya que estos se derivan bajo el supuesto de que no hay autocorrelación. Cuando existe *heterocedasticidad* y *autocorrelación*, es necesario utilizar los llamados *errores estándar de heterocedasticidad y autocorrelación consistente (HAC)*. *Los errores estándar agrupados* pertenecen a este tipo de errores estándar. Permiten heterocedasticidad y errores autocorrelacionados dentro de una entidad, pero *no correlación entre entidades*.

Como se muestra en los ejemplos a lo largo de este capítulo, es bastante fácil especificar el uso de errores estándar agrupados en resúmenes de regresión producidos por funciones como **coeftest()** junto con **vcovHC()** del paquete **sandwich**. Convenientemente, **vcovHC()** reconoce los objetos del modelo de panel (objetos de la clase **plm**) y calcula los errores estándar agrupados de forma predeterminada.

Las regresiones realizadas en este capítulo son un buen ejemplo de por qué el uso de errores estándar agrupados es crucial en aplicaciones empíricas de modelos de efectos fijos. Por ejemplo, considere el modelo de efectos fijos de entidad y tiempo para fatalidades. Dado que **fatal_tefe_lm_mod** es un objeto de la clase **lm**, **coeftest()** no calcula los errores estándar agrupados, sino que utiliza errores estándar robustos que solo son válidos en ausencia de errores autocorrelacionados.

```{r, 593}
# comprobar la clase del objeto modelo
class(fatal_tefe_lm_mod)

# obtener un resumen basado en errores estándar robustos a heterocedasticidad
# (sin ajuste solo por heterocedasticidad)
coeftest(fatal_tefe_lm_mod, vcov = vcovHC, type = "HC1")[1, ]

# comprobar la clase del objeto modelo (plm)
class(fatal_tefe_mod)

# obtener un resumen basado en los errores estándar de clusterd
# (ajuste por autocorrelación + heterocedasticidad)
coeftest(fatal_tefe_mod, vcov = vcovHC, type = "HC1")
```

Los resultados difieren bastante: Al no imponer autocorrelación, se obtiene un error estándar de $0.25$ que implica una significancia de $\hat\beta_1$, el coeficiente de $\text{Impuesto a la cerveza}$ al nivel de $5\%$. Por el contrario, usar el error estándar agrupado $0.35$ conduce a la aceptación de la hipótesis $H_0: \beta_1 = 0$ en el mismo nivel, ver ecuación \@ref(eq:cbnfemod).

## Leyes de conducción en estado de ebriedad y muertes por accidentes de tráfico

Existen dos fuentes principales de sesgo de variables omitidas que no se tienen en cuenta en todos los modelos de la relación entre accidentes de tránsito e impuestos a la cerveza que se han considerado hasta ahora: Las condiciones económicas y las leyes de conducción. Afortunadamente, **Fatalities** tiene datos sobre la edad legal para beber (**drinkage**), castigo (**jail**, **service**) y varios indicadores económicos como la tasa de desempleo (**unemp**) y la renta per cápita (**income**). Se pueden utilizar estas covariables para ampliar el análisis anterior.

Estas covariables se definen de la siguiente manera:

- **unemp**: Una variable numérica que indica la tasa de desempleo específica del estado.
- **log(income)**: El logaritmo del ingreso real per cápita (en precios de 1988).
- **miles**: El promedio estatal de millas por conductor.
- **drinkage**: El estado especifica la edad mínima legal para beber.
- **drinkagc**: Una versión discretizada de **drinkage** que clasifica los estados en cuatro categorías de edad mínima para beber; $18$, $19$, $20$, $21$ y mayores. **R** denota esto como **[18, 19)**, **[19, 20)**, **[20, 21)** y **[21, 22]**. Estas categorías se incluyen como regresores ficticios donde **[21, 22]** se elige como categoría de referencia.
- **punish**: Una variable ficticia con niveles **yes** y **no** que mide si conducir en estado de ebriedad es severamente castigado con encarcelamiento obligatorio o servicio comunitario obligatorio (primera condena).

Al principio, se definen las variables de acuerdo con los resultados de la regresión presentados en una tabla.

```{r, 594}
# discretizar la edad mínima legal para beber
Fatalities$drinkagec <- cut(Fatalities$drinkage,
                            breaks = 18:22, 
                            include.lowest = TRUE, 
                            right = FALSE)

# establecer la edad mínima para beber [21, 22] como el nivel de referencia
Fatalities$drinkagec <- relevel(Fatalities$drinkagec, "[21,22]")

# ¿cárcel privada o servicio comunitario?
Fatalities$punish <- with(Fatalities, factor(jail == "yes" | service == "yes", 
                                             labels = c("no", "yes")))

# el conjunto de observaciones sobre todas las variables para 1982 y 1988
Fatalities_1982_1988 <- Fatalities[with(Fatalities, year == 1982 | year == 1988), ]
```

A continuación, se estiman los siete modelos utilizando **plm()**.

```{r, 595}
# estimar los siete modelos
fatalities_mod1 <- lm(fatal_rate ~ beertax, data = Fatalities)

fatalities_mod2 <- plm(fatal_rate ~ beertax + state, data = Fatalities)

fatalities_mod3 <- plm(fatal_rate ~ beertax + state + year,
                       index = c("state","year"),
                       model = "within",
                       effect = "twoways", 
                       data = Fatalities)

fatalities_mod4 <- plm(fatal_rate ~ beertax + state + year + drinkagec 
                       + punish + miles + unemp + log(income), 
                       index = c("state", "year"),
                       model = "within",
                       effect = "twoways",
                       data = Fatalities)

fatalities_mod5 <- plm(fatal_rate ~ beertax + state + year + drinkagec 
                       + punish + miles,
                       index = c("state", "year"),
                       model = "within",
                       effect = "twoways",
                       data = Fatalities)

fatalities_mod6 <- plm(fatal_rate ~ beertax + year + drinkage 
                       + punish + miles + unemp + log(income), 
                       index = c("state", "year"),
                       model = "within",
                       effect = "twoways",
                       data = Fatalities)

fatalities_mod7 <- plm(fatal_rate ~ beertax + state + year + drinkagec 
                       + punish + miles + unemp + log(income), 
                       index = c("state", "year"),
                       model = "within",
                       effect = "twoways",
                       data = Fatalities_1982_1988)
```

De nuevo, se usa **stargazer()** [@R-stargazer] para generar una presentación tabular completa de los resultados.

```{r, 596, message=F, warning=F, results='asis', eval=F}
library(stargazer)

# recopilar errores estándar agrupados en una lista
rob_se <- list(sqrt(diag(vcovHC(fatalities_mod1, type = "HC1"))),
               sqrt(diag(vcovHC(fatalities_mod2, type = "HC1"))),
               sqrt(diag(vcovHC(fatalities_mod3, type = "HC1"))),
               sqrt(diag(vcovHC(fatalities_mod4, type = "HC1"))),
               sqrt(diag(vcovHC(fatalities_mod5, type = "HC1"))),
               sqrt(diag(vcovHC(fatalities_mod6, type = "HC1"))),
               sqrt(diag(vcovHC(fatalities_mod7, type = "HC1"))))

# generar la tabla
stargazer(fatalities_mod1, fatalities_mod2, fatalities_mod3, 
          fatalities_mod4, fatalities_mod5, fatalities_mod6, fatalities_mod7, 
          digits = 3,
          header = FALSE,
          type = "latex", 
          se = rob_se,
          title = "Modelos de regresión lineal de panel de accidentes mortales debido a conducción en estado de ebriedad",
          model.numbers = FALSE,
          column.labels = c("(1)", "(2)", "(3)", "(4)", "(5)", "(6)", "(7)"))
```

<!--html_preserve-->

```{r, 597, message=F, warning=F, results='asis', echo=F, eval=my_output == "html"}
library(stargazer)

rob_se <- list(
  sqrt(diag(vcovHC(fatalities_mod1, type="HC1"))),
  sqrt(diag(vcovHC(fatalities_mod2, type="HC1"))),
  sqrt(diag(vcovHC(fatalities_mod3, type="HC1"))),
  sqrt(diag(vcovHC(fatalities_mod4, type="HC1"))),
  sqrt(diag(vcovHC(fatalities_mod5, type="HC1"))),
  sqrt(diag(vcovHC(fatalities_mod6, type="HC1"))),
  sqrt(diag(vcovHC(fatalities_mod7, type="HC1")))
)

stargazer(fatalities_mod1, fatalities_mod2, fatalities_mod3, fatalities_mod4, fatalities_mod5, fatalities_mod6, fatalities_mod7, 
          digits = 3,
          type = "html",
          header = FALSE,
          se = rob_se,
          dep.var.caption = "Variable dependiente: Tasa de mortalidad",
          model.numbers = FALSE,
          column.labels = c("(1)", "(2)", "(3)", "(4)", "(5)", "(6)", "(7)")
          )

stargazer_html_title("Modelos de regresión lineal de panel de accidentes mortales debido a conducción en estado de ebriedad", "lprmotfdtdd")
```

<!--/html_preserve-->

```{r, 598, message=F, warning=F, results='asis', echo=F, eval=my_output == "latex"}
library(stargazer)

rob_se <- list(
  sqrt(diag(vcovHC(fatalities_mod1, type="HC1"))),
  sqrt(diag(vcovHC(fatalities_mod2, type="HC1"))),
  sqrt(diag(vcovHC(fatalities_mod3, type="HC1"))),
  sqrt(diag(vcovHC(fatalities_mod4, type="HC1"))),
  sqrt(diag(vcovHC(fatalities_mod5, type="HC1"))),
  sqrt(diag(vcovHC(fatalities_mod6, type="HC1"))),
  sqrt(diag(vcovHC(fatalities_mod7, type="HC1")))
)

stargazer(fatalities_mod1, fatalities_mod2, fatalities_mod3, fatalities_mod4, fatalities_mod5, fatalities_mod6, fatalities_mod7, 
          digits = 3,
          type = "latex",
          float.env = "sidewaystable",
          column.sep.width = "-5pt",
          se = rob_se,
          header = FALSE,
          model.names = FALSE,
          column.labels = c('MCO','','','Regresión de panel lineal'),
          omit.stat = "f",
          title = "\\label{tab:lprmotfdtdd} Modelos de regresión lineal de panel de accidentes mortales debido a conducción en estado de ebriedad")
```

Mientras que las columnas (2) y (3) recapitulan los resultados \@ref(eq:efemod) y \@ref(eq:cbnfemod), la columna (1) presenta una estimación del coeficiente de interés en la regresión MCO ingenua de la tasa de fatalidad sobre el impuesto a la cerveza sin efectos fijos. Se obtiene una estimación *positiva* para el coeficiente del impuesto a la cerveza que probablemente tenga un sesgo al alza. El ajuste del modelo también es bastante malo ($\bar{R}^2 = 0.091$). El signo de la estimación cambia a medida que se amplía el modelo por efectos fijos tanto de entidad como de tiempo en los modelos (2) y (3). Además, $\bar{R}^2$ aumenta sustancialmente a medida que se incluyen efectos fijos en la ecuación del modelo. No obstante, como se discutió anteriormente, las magnitudes de ambas estimaciones pueden ser demasiado grandes.

Las especificaciones del modelo (4) a (7) incluyen covariables que deben capturar el efecto de las condiciones económicas estatales generales, así como el marco legal. Considerando (4) como la especificación de línea de base, se observan cuatro resultados interesantes:

1. La inclusión de las covariables no conduce a una reducción importante del efecto estimado del impuesto a la cerveza. El coeficiente no es significativamente diferente de cero al nivel de $5\%$, ya que la estimación es bastante imprecisa.

2. La edad mínima legal para beber *no* tiene un efecto sobre las muertes por accidentes de tránsito: Ninguna de las tres variables ficticias es significativamente diferente de cero en ningún nivel común de significancia. Además, una prueba $F$ de la hipótesis conjunta de que los tres coeficientes son cero no rechaza la hipótesis nula. El siguiente fragmento de código muestra cómo probar esta hipótesis.

```{r, 599}
# probar si la edad legal para consumir alcohol no tiene poder explicativo
linearHypothesis(fatalities_mod4,
                 test = "F",
                 c("drinkagec[18,19)=0", "drinkagec[19,20)=0", "drinkagec[20,21)"), 
                 vcov. = vcovHC, type = "HC1")
```

3. No existe evidencia de que el castigo para los primeros infractores tenga un efecto disuasorio sobre la conducción en estado de ebriedad: El coeficiente correspondiente no es significativo al nivel de $10\%$.

4. Las variables económicas explican significativamente las muertes por accidentes de tránsito. Se puede comprobar que la tasa de empleo y el ingreso per cápita son conjuntamente significativos al nivel de $0.1\%$.

```{r, 600}
# probar si los indicadores económicos no tienen poder explicativo
linearHypothesis(fatalities_mod4, 
                 test = "F",
                 c("log(income)", "unemp"), 
                 vcov. = vcovHC, type = "HC1")
```

El modelo (5) omite los factores económicos. El resultado apoya la noción de que los indicadores económicos deben permanecer en el modelo, ya que el coeficiente del impuesto a la cerveza es sensible a la inclusión de este último.

Los resultados del modelo (6) demuestran que la edad legal para beber tiene poco poder explicativo y que el coeficiente de interés no es sensible a los cambios en la forma funcional de la relación entre la edad para beber y las muertes por accidentes de tránsito.

La especificación (7) revela que reducir la cantidad de información disponible (aquí solo se usan 95 observaciones para el período 1982 a 1988) infla los errores estándar pero no conduce a cambios drásticos en las estimaciones de los coeficientes.

#### Resumen {-}

No se ha encontrado evidencia de que los castigos severos y el aumento de la edad mínima para beber reduzcan las muertes por accidentes de tránsito debido a la conducción en estado de ebriedad. No obstante, parece haber un efecto negativo de los impuestos sobre el alcohol en las muertes por accidentes de tránsito que; sin embargo, se estima de manera imprecisa y no puede interpretarse como el efecto causal de interés, ya que aún puede haber un sesgo. El problema es que puede haber variables omitidas que *difieren entre los estados* y *cambian con el tiempo, dicho sesgo permanece a pesar de que se usa un enfoque de panel que controla las variables no observables invariantes en el tiempo y específicos de la entidad.

Un método poderoso que se puede utilizar si los enfoques de regresión de panel comunes fallan es la regresión de variables instrumentales. Se volverá a este concepto en el capítulo \@ref(RVI).

## Ejercicios {#Ejercicios-10}

```{r, 601, echo=F, purl=F, results='asis'}
if (my_output=="html") {
  cat('

En el transcurso de esta sección, trabajará con <tt>Guns</tt>, un panel equilibrado que contiene observaciones sobre variables criminales y demográficas para todos los estados de EE. UU. entre los años 1977-1999. El conjunto de datos viene con el paquete <tt>AER</tt> que ya está instalado para los ejercicios de R interactivos a continuación.

<div  class = "DCexercise">

#### 1. El conjunto de datos de armas {-}

**Instrucciones:**

  + Cargar el paquete <tt>AER</tt> y el conjunto de datos <tt>Guns</tt>. 
  + Obtener una descripción general del conjunto de datos utilizando la función <tt>summary()</tt>. Utilizar <tt>?Guns</tt> para obtener información detallada sobre las variables.
  + Verificar que <tt>Guns</tt> sea un panel equilibrado: Extraer el número de años y estados del conjunto de datos y asignarlos a las variables predefinidas <tt>years</tt> y <tt>states</tt>, respectivamente. Luego usar estas variables para una comparación lógica: Verificar que el panel esté balanceado.

<iframe src="DCL/ex10_1.html" frameborder="0" scrolling="no" style="width:100%;height:340px"></iframe>
        
**Sugerencias:**
  
  + Usar <tt>library()</tt> y <tt>data()</tt> para adjuntar el paquete y cargar el conjunto de datos, respectivamente.
  + Utilizar <tt>summary()</tt> para obtener una descripción general completa del conjunto de datos.
  + Recordar que en un panel balanceado el número de entidades multiplicado por el número de años es igual al número total de observaciones en el conjunto de datos. Las funciones básicas <tt>levels()</tt>, <tt>length()</tt> y <tt>nrow()</tt> pueden ser útiles.

</div>') } else {
  cat("\\begin{center}\\textit{Esta parte interactiva del curso solo está disponible en la versión HTML.}\\end{center}")
}
```

```{r, 602, echo=F, purl=F, results='asis'}
if (my_output=="html") {
  cat('
<div  class = "DCexercise">

#### 2.¿Estricto o suelto? Leyes sobre armas y el efecto sobre el crimen I {-}

Existe un debate controvertido sobre si el derecho a portar un arma influye en el crimen y en qué medida. Los defensores de las leyes de "porte de armas ocultas" (CCW) argumentan que el efecto disuasorio de las armas previene el delito, mientras que los opositores argumentan que la disponibilidad pública de armas aumenta su uso y, por lo tanto, facilita la comisión de delitos. En los siguientes ejercicios investigará empíricamente este tema.

Para empezar, considere el siguiente modelo estimado

$$\\widehat{{\\log(violent_i)}} = 6.135 - 0.443 \\times law_i,$$

con $i=1,\\ldots,51$ donde <tt>violent</tt> es la tasa de delitos violentos (incidentes por cada 100000 residentes) y <tt>law</tt> es una variable binaria que indica la implementación de una Ley de la CCW (1 = sí, 0 = no), respectivamente.

El modelo estimado está disponible como <tt>model</tt> en su entorno de trabajo. Se han cargado los paquetes <tt>AER</tt> y <tt>plm</tt>.
      
**Instrucciones:**
        
  + Ampliar y estimar el modelo incluyendo efectos fijos de estado usando la función <tt>plm()</tt> y asignar el objeto del modelo a la variable predefinida <tt>model_se</tt>. ¿Puede pensar en una variable no observada que sea capturada por esta especificación de modelo?
  + Imprimir un resumen del modelo que informe los errores estándar robustos del clúster.
  + Probar si los efectos de estado fijo son conjuntamente significativos desde cero. Para hacerlo, utilizar la funcióntion <tt>pFtest()</tt>. Usar <tt>?pFtest</tt> para información adicional.

<iframe src="DCL/ex10_2.html" frameborder="0" scrolling="no" style="width:100%;height:340px"></iframe>

**Sugerencias:**
  
  + La función <tt>plm()</tt> le permite realizar regresiones con datos de panel y funciona de manera muy similar a <tt>lm()</tt>. Debe especificar la entidad y los indicadores de tiempo dentro como un vector usando el argumento <tt>index</tt> y especificar el estimador que se usará con el argumento <tt>model</tt> (para el estimador de efectos fijos; esto es, <tt>"dentro de o within"</tt>).
  + Como de costumbre, se puede usar <tt>coeftest()</tt> junto con los argumentos apropiados para obtener una salida resumida con errores estándar robustos.
  + <tt>pFtest()</tt> espera dos objetos modelo. El primer modelo incluye efectos fijos, el segundo no.

</div>')
}
```

```{r, 603, echo=F, purl=F, results='asis'}
if (my_output=="html") {
  cat('
<div  class = "DCexercise">

#### 3. ¿Estricto o suelto? Leyes de armas y el efecto sobre el crimen II{-}

Como se mencionó al final del último ejercicio, es razonable incluir también los efectos de tiempo, por lo que ahora se considera el siguiente modelo:

\\begin{align}\\log(violent_i)  & = \\beta_1\\times law_i + \\alpha_i + \\lambda_t + u_i,\\end{align}

para $i=1,\\ldots,51$ y $t=1977,\\ldots,1999$.

Los modelos <tt>model</tt> y <tt>model_se</tt> de los ejercicios anteriores están disponibles en el entorno de trabajo. Se han adjuntado los paquetes <tt>AER</tt> y <tt>plm</tt>.

**Instrucciones:**
        
  + Estimar el modelo anterior y asignarlo a la variable <tt>model_sete</tt> usando <tt>plm()</tt>.
  + Imprimir un resumen del modelo que informe errores estándar robustos.
  + Probar si los efectos fijos de estado y de tiempo son conjuntamente significativos.

<iframe src="DCL/ex10_3.html" frameborder="0" scrolling="no" style="width:100%;height:340px"></iframe>

**Sugerencias:**
  
  + Para incorporar adicionalmente efectos fijos de tiempo, se puede establecer el argumento <tt>effect = "twoways"</tt> dentro de <tt>plm()</tt>.
  + Se debe tomar en cuenta que se quiere probar si los efectos fijos de *estado* y *tiempo* son conjuntamente significativos.
  
</div>')
}
```

```{r, 604, echo=F, purl=F, results='asis'}
if (my_output=="html") {
  cat('
<div  class = "DCexercise">

#### 4. ¿Estricto o suelto? Leyes de armas y el efecto sobre el crimen III {-}

A pesar de la evidencia de los efectos del estado y del tiempo encontrada en el ejercicio 3, aún podría haber un sesgo debido a las variables omitidas, como las características sociodemográficas. El siguiente modelo da cuenta de este último:

\\begin{align}\\log(violent_i)  & = \\beta_1\\times law_i + \\beta_2\\times density_i + \\beta_3\\times income_i + \\beta_4\\times population_i \\\\&\\quad + \\beta_5\\times afam_i + \\beta_6\\times cauc_i + \\beta_7\\times male_i + \\alpha_i + \\lambda_t + u_i.\\end{align}

Consultar <tt>?Guns</tt> para obtener información detallada sobre las variables adicionales.

Se han cargado los paquetes <tt>AER</tt> y <tt>plm</tt>.

**Instrucciones:**
        
  + Estimar el modelo extendido y asígnarlo a la variable predefinida <tt>model_sete_ext</tt>.
  + Imprimir un resumen sólido del modelo estimado. ¿Qué puede decir sobre el efecto de una ley de CCW?

<iframe src="DCL/ex10_4.html" frameborder="0" scrolling="no" style="width:100%;height:340px"></iframe>

</div>')
}
```

```{r, 605, echo=F, purl=F, results='asis'}
if (my_output=="html") {
  cat('
<div  class = "DCexercise">

#### 5. Regresión de efectos fijos: Dos períodos de tiempo {-}

Recuerde el modelo de efectos fijos del ejercicio 10.2, pero ahora suponga que solo tiene observaciones para los años 1978 y 1984. Considere las dos especificaciones del modelo

\\begin{align}
\\log(violent_{i1984}) - \\log(violent_{i1978}) = \\beta_{BA}\\times(law_{i1984}-law_{i1978}) + (u_{i1984} - u_{i1978})
\\end{align}

y

\\begin{align}
\\log(violent_{it}) = \\beta_{FE}\\times law_{it} + \\alpha_i + u_{it},\\\\
\\end{align}

con $i=1,\\ldots,51$ y $t=1978,1984$.

En este ejercicio, se debe demostrar que $\\widehat{\\beta}_{BA}=\\widehat{\\beta}_{FE}$.

Los subconjuntos de <tt>Guns</tt> para los años 1978 y 1984 ya están disponibles como <tt>Guns78</tt> y <tt>Guns84</tt> en el entorno de trabajo. Se han cargado los paquetes <tt>AER</tt> y <tt>plm</tt>.

**Instrucciones:**

  + Calcular las diferencias necesarias para estimar el primer modelo y asignarlas a las variables <tt>diff_logv</tt> y <tt>diff_law</tt>.
  + Estimar ambos modelos. Utilizar los datos diferenciados para estimar el primer modelo y <tt>plm()</tt> para el segundo.
  + Verificar con una comparación lógica que ambos procedimientos den numéricamente la misma estimación. Utilizar las variables <tt>coef_diff</tt> y <tt>coef_plm</tt> que contienen los coeficientes relevantes redondeados al cuarto decimal.

<iframe src="DCL/ex10_5.html" frameborder="0" scrolling="no" style="width:100%;height:340px"></iframe>

**Sugerencias:**
  
  + Tenga en cuenta que la variable dependiente se transforma logarítmicamente.
  + Puede usar <tt>plm()</tt> como en los ejercicios anteriores. Tenga en cuenta que solo necesita un subconjunto del conjunto de datos <tt>Guns</tt> original. El argumento <tt>subset</tt> permite crear un subconjunto del conjunto de datos pasado al argumento <tt>data</tt>. Alternativamente, puede unir los dos conjuntos de datos <tt>Guns78</tt> y <tt>Guns84</tt> usando; por ejemplo, <tt>rbind()</tt>.
  + Utilizar el operador lógico <tt>==</tt> para comparar ambas estimaciones.

</div>')
}
```

<!--chapter:end:Capitulo_11.Rmd-->

# Regresión con una variable dependiente binaria {#RVDB}

```{r, echo = F}
options(knitr.duplicate.label = "allow")
```

```{r, 606, child="_setup.Rmd"}
```

```{r, 607, eval=knitr::opts_knit$get("rmarkdown.pandoc.to") == "html", results='asis', echo=FALSE}
cat('<hr style="background-color:#03193b;height:2px">')
```

Este capítulo analiza una clase especial de modelos de regresión que tienen como objetivo explicar una variable dependiente limitada. En particular, se consideran modelos donde la variable dependiente es binaria. Se vera que en tales modelos, la función de regresión se puede interpretar como una función de probabilidad condicional de la variable dependiente binaria.

Se repasarán los siguientes conceptos:

- El modelo de probabilidad lineal
- El modelo Probit
- El modelo Logit
- Estimación de máxima verosimilitud de modelos de regresión no lineal

Por supuesto, también se verá cómo estimar los modelos anteriores usando **R** y se discutirá una aplicación en la que se examinara la cuestión de si existe discriminación racial en el mercado hipotecario de EE. UU.

Los siguientes paquetes y sus dependencias son necesarios para la reproducción de los fragmentos de código presentados a lo largo de este capítulo en su computadora:

+ **AER** [@R-AER]
+ **stargazer** [@R-stargazer]

```{r, 608, warning=FALSE, message=FALSE, eval=FALSE}
install.packages("AER")
install.packages("stargazer")
```

Comprobar si el siguiente fragmento de código se ejecuta sin errores.

```{r, 609, warning=FALSE, message=FALSE}
library(AER)
library(stargazer)
```

## Variables dependientes binarias y el modelo de probabilidad lineal

```{r, 610, eval = my_output == "html", results='asis', echo=F, purl=F}
cat('
<div class = "keyconcept" id="KC11.1">
<h3 class = "right"> Concepto clave 11.1 </h3>
<h3 class = "left"> El modelo de probabilidad lineal </h3>

El modelo de regresión lineal

$$Y_i = \\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\dots + \\beta_k X_{ki} + u_i$$

con una variable dependiente binaria $Y_i$ se denomina modelo de probabilidad lineal. En el modelo de probabilidad lineal se tiene $$E(Y\\vert X_1,X_2,\\dots,X_k) = P(Y=1\\vert X_1, X_2,\\dots, X_3)$$, donde $$ P(Y = 1 \\vert X_1, X_2, \\dots, X_k) = \\beta_0 + \\beta_1 + X_{1i} + \\beta_2 X_{2i} + \\dots + \\beta_k X_{ki}.$$

Por tanto, $\\beta_j$ puede interpretarse como el cambio en la probabilidad de que $Y_i = 1$, manteniendo constantes los otros regresores $k-1$. Al igual que en la regresión múltiple común, $\\beta_j$ se puede estimar usando MCO y las fórmulas robustas de error estándar se pueden usar para probar hipótesis y calcular intervalos de confianza.

En la mayoría de los modelos de probabilidad lineal, $R^2$ no tiene una interpretación significativa ya que la línea de regresión nunca puede ajustarse perfectamente a los datos si la variable dependiente es binaria y los regresores son continuos. Esto se puede ver en la aplicación a continuación.

Es *esencial* utilizar errores estándar robustos, ya que los $u_i$, en un modelo de probabilidad lineal, son siempre heterocedásticos.

Los modelos de probabilidad lineal se estiman fácilmente en <tt>R</tt> usando la función <tt>lm()</tt>.

</div>
')
```

```{r, 611, eval = my_output == "latex", results='asis', echo=F, purl=F}
cat('\\begin{keyconcepts}[El modelo de probabilidad lineal]{11.1}

El modelo de regresión lineal

$$Y_i = \\beta_0 + \\beta_1 + X_{1i} + \\beta_2 X_{2i} + \\dots + \\beta_k X_{ki} + u_i$$

con una variable dependiente binaria $Y_i$ se denomina modelo de probabilidad lineal. En el modelo de probabilidad lineal se tiene $$E(Y\\vert X_1,X_2,\\dots,X_k) = P(Y=1\\vert X_1, X_2,\\dots, X_3)$$, donde $$ P(Y = 1 \\vert X_1, X_2, \\dots, X_k) = \\beta_0 + \\beta_1 + X_{1i} + \\beta_2 X_{2i} + \\dots + \\beta_k X_{ki}.$$

Por tanto, $\\beta_j$ puede interpretarse como el cambio en la probabilidad de que $Y_i=1$, manteniendo constantes los otros regresores $k-1$. Al igual que en la regresión múltiple común, $\\beta_j$ se puede estimar usando MCO y las fórmulas robustas de error estándar se pueden usar para probar hipótesis y calcular intervalos de confianza.\\newline 

En la mayoría de los modelos de probabilidad lineal, $R^2$ no tiene una interpretación significativa ya que la línea de regresión nunca puede ajustarse perfectamente a los datos si la variable dependiente es binaria y los regresores son continuos. Esto se puede ver en la aplicación a continuación.\\newline 

Es *esencial* utilizar errores estándar robustos, ya que los $u_i$, en un modelo de probabilidad lineal, son siempre heterocedásticos.\\newline 

Los modelos de probabilidad lineal se estiman fácilmente en \\texttt{R} usando la función \\texttt{lm()}.

\\end{keyconcepts}
')
```

#### Datos hipotecarios {-}

Se debe comenzar cargando el conjunto de datos **HMDA** que proporciona datos relacionados con las solicitudes de hipotecas presentadas en Boston en el año de 1990.

```{r, 612, warning=FALSE, message=FALSE}
# cargar el paquete `AER` y adjuntar los datos de "HMDA"
library(AER)
data(HMDA)
```

Se continuan inspeccionando las primeras observaciones y luego calculando las estadísticas resumidas.

```{r, 613}
# inspeccionar los datos
head(HMDA)
summary(HMDA)
```

La variable que interesa modelar es **deny**, un indicador de si la solicitud de hipoteca de un solicitante ha sido aceptada (**deny = no**) o denegada (**deny = yes**). Un regresor que debería tener poder para explicar si una solicitud de hipoteca ha sido denegada es **pirat**, el tamaño de los pagos mensuales totales anticipados del préstamo en relación con los ingresos del solicitante. Es sencillo traducir esto al modelo de regresión simple

\begin{align}
  deny = \beta_0 + \beta_1 \times P/I\ ratio + u. (\#eq:denymod1)
\end{align}

Se estima este modelo como cualquier otro modelo de regresión lineal utilizando **lm()**. Antes de hacerlo, la variable **deny** debe convertirse en una variable numérica usando **as.numeric()**, ya que **lm()** no acepta que la *variable dependiente* sea de la clase **factor**. Se debe tener en cuenta que `as.numeric(HMDA$deny)` convertirá **deny = no** en **deny = 1** y **deny = yes** en **deny = 2**, por lo que al usar **as.numeric(HMDA$deny)-1** se obtienen los valores **0** y **1**.

```{r, 614}
# convertir 'deny' a numérico
HMDA$deny <- as.numeric(HMDA$deny) - 1

# estimar un modelo de probabilidad lineal simple
denymod1 <- lm(deny ~ pirat, data = HMDA)
denymod1
```

A continuación, se grafican los datos y la línea de regresión.

```{r, 615}
# graficar los datos
plot(x = HMDA$pirat, 
     y = HMDA$deny,
     main = "Diagrama de dispersión denegación de solicitud de hipoteca y relación pago-ingreso",
     xlab = "Relación P/I",
     ylab = "Denegar",
     pch = 20,
     ylim = c(-0.4, 1.4),
     cex.main = 0.8)

# añadir texto y líneas discontinuas horizontales
abline(h = 1, lty = 2, col = "darkred")
abline(h = 0, lty = 2, col = "darkred")
text(2.5, 0.9, cex = 0.8, "Hipoteca denegada")
text(2.5, -0.1, cex= 0.8, "Hipoteca aprobada")

# agregar la línea de regresión estimada
abline(denymod1, 
       lwd = 1.8, 
       col = "steelblue")
```

Según el modelo estimado, una relación pago-ingresos de $1$ se asocia con una probabilidad esperada de denegación de la solicitud de hipoteca de aproximadamente $50\%$. El modelo indica que existe una relación positiva entre la relación pago-ingresos y la probabilidad de una solicitud hipotecaria denegada, por lo que es más probable que las personas con una alta relación entre pagos de préstamos e ingresos sean rechazadas.

Se puede usar **coeftest()** para obtener errores estándar robustos para ambas estimaciones de coeficientes.

```{r, 616}
# imprimir un resumen robusto de coeficientes
coeftest(denymod1, vcov. = vcovHC, type = "HC1")
```

La línea de regresión estimada es:

\begin{align}
\widehat{\text{denegar}} = -\underset{(0.032)}{0.080} + \underset{(0.098)}{0.604} P/I \ ratio. (\#eq:lpm)
\end{align}

El verdadero coeficiente de $P/I \ ratio$ es estadísticamente diferente de $0$ en el nivel de $1\%$. Su estimación se puede interpretar de la siguiente manera: Un aumento de un punto porcentual en $P/I \ ratio$ conduce a un aumento en la probabilidad de denegación de un préstamo en $0.604 \cdot 0.01 = 0.00604 \approx 0.6\%$.

En este sentido, se aumenta el modelo simple \@ref(eq:denymod1) con un regresor adicional $black$ que equivale a $1$ si el solicitante es afroamericano y equivale a $0$ en caso contrario. Dicha especificación es la línea de base para investigar si existe discriminación racial en el mercado hipotecario: Si ser negro tiene una influencia significativa (positiva) en la probabilidad de denegación de un préstamo cuando se controlan los factores que permiten una evaluación objetiva y digna del crédito de un solicitante, este es un indicador de discriminación.

```{r, 617}
# cambiar el nombre de la variable 'afam' por coherencia
colnames(HMDA)[colnames(HMDA) == "afam"] <- "black"

# estimar el modelo
denymod2 <- lm(deny ~ pirat + black, data = HMDA)
coeftest(denymod2, vcov. = vcovHC)
```

La función de regresión estimada es

\begin{align}
  \widehat{\text{denegar}} =& \, -\underset{(0.029)}{0.091} + \underset{(0.089)}{0.559} P/I \ ratio + \underset{(0.025)}{0.177} black. (\#eq:denymod2)
\end{align}

El coeficiente de $black$ es positivo y significativamente diferente de cero en el nivel de $0.01\%$. La interpretación es que, manteniendo constante la relación $P/I \ ratio$, ser negro aumenta la probabilidad de denegación de una solicitud de hipoteca en aproximadamente $17.7\%$. Este hallazgo es compatible con la discriminación racial. Sin embargo, podría estar distorsionado por el sesgo de la variable omitida, por lo que la discriminación podría ser una conclusión prematura.

## Regresión Probit y Logit {#RPL}

El modelo de probabilidad lineal tiene un defecto importante: Supone que la función de probabilidad condicional es lineal. Esto no restringe que $P(Y=1\vert X_1,\dots,X_k)$ se encuentre entre $0$ y $1$. Se puede ver esto fácilmente en la reproducción de: $P/I \ ratio \geq 1.75$, \@ref(eq:lpm) predice que la probabilidad de que la denegación de una solicitud de hipoteca sea mayor que $1$. Para aplicaciones con una relación $P/I \ ratio$ cercana a $0$, la probabilidad de negación predicha es incluso negativa, por lo que el modelo no tiene una interpretación significativa.

Esta circunstancia requiere un enfoque que utilice una función no lineal para modelar la función de probabilidad condicional de una variable dependiente binaria. Los métodos más utilizados son la regresión Probit y Logit.

### Regresión Probit {-}

En la regresión Probit, la función de distribución normal estándar acumulada $\Phi(\cdot)$ se usa para modelar la función de regresión cuando la variable dependiente es binaria; es decir, se asume:

\begin{align}
  E(Y\vert X) = P(Y=1\vert X) = \Phi(\beta_0 + \beta_1 X). (\#eq:probitmodel)
\end{align}

$\beta_0 + \beta_1 X$ en \@ref(eq:probitmodel) desempeña el papel de un cuantil $z$. Recuerde que $$\Phi(z) = P(Z \leq z) \ , \ Z \sim \mathcal{N}(0,1)$$, tal que el coeficiente Probit $\beta_1$ en \@ref(eq:probitmodel) es el cambio en $z$ asociado con un cambio de una unidad en $X$. Aunque el efecto en $z$ de un cambio en $X$ es lineal, el vínculo entre $z$ y la variable dependiente $Y$ no es lineal ya que $\Phi$ es una función no lineal de $X$.

Dado que la variable dependiente es una función no lineal de los regresores, el coeficiente de $X$ no tiene una interpretación simple. De acuerdo con el Concepto clave 8.1, el cambio esperado en la probabilidad de que $Y = 1$ debido a un cambio en $P/I \ ratio$ se puede calcular de la siguiente manera:

1. Calcular la probabilidad predicha de que $Y = 1$ para el valor original de $X$.
2. Calcular la probabilidad predicha de que $Y = 1$ para $X + \Delta X$.
3. Calcular la diferencia entre ambas probabilidades predichas.

Por supuesto, se puede generalizar \@ref(eq:probitmodel) a la regresión Probit con regresores múltiples para mitigar el riesgo de enfrentar sesgos de variables omitidas. Los elementos esenciales de la regresión Probit se resumen en el Concepto clave 11.2.

```{r, 618, eval = my_output == "html", results='asis', echo=F, purl=F}
cat('
<div class = "keyconcept" id="KC11.2">
<h3 class = "right"> Concepto clave 11.2 </h3>
<h3 class = "left"> Modelo Probit, probabilidades pronosticadas y efectos estimados </h3>

Suponga que $Y$ es una variable binaria. El modelo

$$ Y= \\beta_0 + \\beta_1 + X_1 + \\beta_2 X_2 + \\dots + \\beta_k X_k + u $$

con

$$P(Y = 1 \\vert X_1, X_2, \\dots ,X_k) = \\Phi(\\beta_0 + \\beta_1 + X_1 + \\beta_2 X_2 + \\dots + \\beta_k X_k)$$

es el modelo de población Probit con múltiples regresores $X_1, X_2, \\dots, X_k$ y $\\Phi(\\cdot)$ es la función de distribución normal estándar acumulativa.

La probabilidad predicha de que $Y = 1$ dado $X_1, X_2, \\dots, X_k$ se puede calcular en dos pasos:

1. Calcular $z = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_k X_k$

2. Buscar $\\Phi(z)$ llamando <tt>pnorm()</tt>.

$\\beta_j$ es el efecto sobre $z$ de un cambio de una unidad en la regresión $X_j$, manteniendo constantes todos los demás regresores $k-1$.

El efecto sobre la probabilidad predicha de un cambio en un regresor se puede calcular como en el Concepto clave 8.1.

En <tt>R</tt>, los modelos Probit se pueden estimar usando la función <tt>glm()</tt> del paquete <tt>stats</tt>. Usando el argumento <tt>family</tt> especificando que se quiere usar una función de enlace Probit.

</div>
')
```

```{r, 619, eval = my_output == "latex", results='asis', echo=F, purl=F}
cat('\\begin{keyconcepts}[Modelo Probit, probabilidades pronosticadas y efectos estimados]{11.2}

Suponga que $ Y $ es una variable binaria. El modelo

$$ Y= \\beta_0 + \\beta_1 + X_1 + \\beta_2 X_2 + \\dots + \\beta_k X_k + u $$

con

$$P(Y = 1 \\vert X_1, X_2, \\dots ,X_k) = \\Phi(\\beta_0 + \\beta_1 + X_1 + \\beta_2 X_2 + \\dots + \\beta_k X_k)$$

es el modelo de población Probit con múltiples regresores $X_1, X_2, \\dots, X_k$ y $\\Phi(\\cdot)$ es la función de distribución normal estándar acumulativa.\\newline

La probabilidad predicha de que $Y = 1$ dado $X_1, X_2, \\dots, X_k$ se puede calcular en dos pasos:\\newline

\\begin{enumerate}
\\item Calcular $z = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_k X_k$
\\item Buscar $\\Phi(z)$ llamando \\texttt{pnorm()}.
\\end{enumerate}\\vspace{0.5cm}

$\\beta_j$ es el efecto sobre $z$ de un cambio de una unidad en la regresión $X_j$, manteniendo constantes todos los demás regresores $k-1$.\\newline

El efecto sobre la probabilidad predicha de un cambio en un regresor se puede calcular como en el Concepto clave 8.1.\\newline

En \\texttt{R}, los modelos Probit se pueden estimar usando la función \\texttt{glm()} del paquete \\texttt{stats}. Usando el argumento \\texttt{family} especificando que se quiere usar una función de enlace Probit.

\\end{keyconcepts}
')
```

Ahora se estima un modelo Probit simple de la probabilidad de denegación de una hipoteca.

```{r, 620}
# estimar el modelo probit simple
denyprobit <- glm(deny ~ pirat, 
                  family = binomial(link = "probit"), 
                  data = HMDA)

coeftest(denyprobit, vcov. = vcovHC, type = "HC1")
```

El modelo estimado es

\begin{align}
  \widehat{P(deny\vert P/I \ ratio}) = \Phi(-\underset{(0.19)}{2.19} + \underset{(0.54)}{2.97} P/I \ ratio). (\#eq:denyprobit)
\end{align}

Al igual que en el modelo de probabilidad lineal, se encuentra que la relación entre la probabilidad de denegación y la relación pagos-ingresos es positiva y que el coeficiente correspondiente es altamente significativo.

El siguiente fragmento de código reproduce la Figura 11.2 del libro.

```{r, 621, fig.align='center'}
# graficar datos 
plot(x = HMDA$pirat, 
     y = HMDA$deny,
     main = "Modelo probit de probabilidad de negación, dada la relación P/I",
     xlab = "Relación P/I",
     ylab = "Denegar",
     pch = 20,
     ylim = c(-0.4, 1.4),
     cex.main = 0.85)

# añadir texto y líneas discontinuas horizontales
abline(h = 1, lty = 2, col = "darkred")
abline(h = 0, lty = 2, col = "darkred")
text(2.5, 0.9, cex = 0.8, "Hipoteca denegada")
text(2.5, -0.1, cex= 0.8, "Hipoteca aprobada")

# agregar línea de regresión estimada
x <- seq(0, 3, 0.01)
y <- predict(denyprobit, list(pirat = x), type = "response")

lines(x, y, lwd = 1.5, col = "steelblue")
```

La función de regresión estimada tiene una forma de "S" estirada que es típica del FDPA de una variable aleatoria continua con FDP simétrico como el de una variable aleatoria normal. La función es claramente no lineal y se aplana para valores grandes y pequeños de $P/I \ ratio$. Por tanto, la forma funcional también asegura que las probabilidades condicionales predichas de una negación se encuentren entre $0$ y $1$.

Usar **predict()** para calcular el cambio predicho en la probabilidad de negación cuando $P/I \ ratio$ aumenta de $0.3$ a $0.4$.

```{r, 622}
# 1. calcular predicciones para la relación P/I = 0.3, 0.4
predictions <- predict(denyprobit, 
                       newdata = data.frame("pirat" = c(0.3, 0.4)),
                       type = "response")

# 2. Calcular la diferencia de probabilidades
diff(predictions)
```

Se predice que un aumento en la relación pago-ingreso de $0.3$ a $0.4$ aumentará la probabilidad de negación en aproximadamente $6.2\%$.

Continuando utilizando un modelo Probit aumentado para estimar el efecto de la raza en la probabilidad de que se rechace una solicitud de hipoteca.

```{r, 623}
denyprobit2 <- glm(deny ~ pirat + black, 
                   family = binomial(link = "probit"), 
                   data = HMDA)

coeftest(denyprobit2, vcov. = vcovHC, type = "HC1")
```

La ecuación del modelo estimado es

\begin{align}
  \widehat{P(deny\vert P/I \ ratio, black)} = \Phi (-\underset{(0.18)}{2.26} + \underset{(0.50)}{2.74} P/I \ ratio + \underset{(0.08)}{0.71} black). (\#eq:denyprobit2) 
\end{align}

Si bien todos los coeficientes son muy significativos, tanto los coeficientes estimados de la relación pagos-ingresos como el indicador de ascendencia afroamericana son positivos. Nuevamente, los coeficientes son difíciles de interpretar pero indican que, en primer lugar, los afroamericanos tienen una mayor probabilidad de rechazo que los solicitantes blancos, manteniendo constante la proporción de pagos a ingresos y, en segundo lugar, los solicitantes con una alta proporción de pagos a ingresos enfrentan un mayor riesgo de ser rechazado.

¿Qué tan grande es la diferencia estimada en las probabilidades de denegación entre dos solicitantes hipotéticos con la misma proporción de pagos e ingresos? Como antes, se puede usar **predict()** para calcular esta diferencia.

```{r, 624}
# 1. Calcular predicciones para la relación P/I = 0.3
predictions <- predict(denyprobit2, 
                       newdata = data.frame("black" = c("no", "yes"), 
                                            "pirat" = c(0.3, 0.3)),
                       type = "response")

# 2. Calcular la diferencia en probabilidades
diff(predictions)
```

En este caso, la diferencia estimada en las probabilidades de negación es de aproximadamente $15.8\%$.

### Regresión Logit {-}

El Concepto clave 11.3 resume la función de regresión Logit.

```{r, 625, eval = my_output == "html", results='asis', echo=F, purl=F}
cat('
<div class = "keyconcept" id="KC11.3">
<h3 class = "right"> Concepto clave 11.3 </h3>
<h3 class = "left"> Regresión Logit </h3>

La función de regresión Logit poblacional es

\\begin{align*}
  P(Y=1\\vert X_1, X_2, \\dots, X_k) =& \\, F(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_k X_k) \\\\
  =& \\, \\frac{1}{1+e^{-(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_k X_k)}}.
\\end{align*}

La idea es similar a la regresión Probit, excepto que se usa un FDPA diferente: $$F(x) = \\frac{1}{1+e^{-x}}$$ es la FDPA de una variable aleatoria estándar distribuida logísticamente.

</div>
')
```

```{r, 626, eval = my_output == "latex", results='asis', echo=F, purl=F}
cat('\\begin{keyconcepts}[Logit Regression]{11.3}

La función de regresión Logit poblacional es

\\begin{align*}
  P(Y=1\\vert X_1, X_2, \\dots, X_k) =& \\, F(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_k X_k) \\\\
  =& \\, \\frac{1}{1+e^{-(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_k X_k)}}.
\\end{align*}

La idea es similar a la regresión Probit, excepto que se usa un FDPA diferente: $$F(x) = \\frac{1}{1+e^{-x}}$$ es la FDPA de una variable aleatoria estándar distribuida logísticamente.

\\end{keyconcepts}
')
```

En cuanto a la regresión Probit, no existe una interpretación simple de los coeficientes del modelo y es mejor considerar las probabilidades pronosticadas o las diferencias en las probabilidades pronosticadas. Aquí nuevamente, las estadísticas de $t$ y los intervalos de confianza basados en aproximaciones normales de muestras grandes se pueden calcular como de costumbre.

Es bastante fácil estimar un modelo de regresión Logit usando **R**.

```{r, 627}
denylogit <- glm(deny ~ pirat, 
                 family = binomial(link = "logit"), 
                 data = HMDA)

coeftest(denylogit, vcov. = vcovHC, type = "HC1")
```

El siguiente fragmento de código produce la gráfica.

```{r, 628, fig.align='center'}
# graficar datos
plot(x = HMDA$pirat, 
     y = HMDA$deny,
     main = "Modelos Probit y Logit. Modelo de probabilidad de negación, relación P/I dada",
     xlab = "Relación P / I",
     ylab = "Denegar",
     pch = 20,
     ylim = c(-0.4, 1.4),
     cex.main = 0.9)

# añadir texto y líneas discontinuas horizontales
abline(h = 1, lty = 2, col = "darkred")
abline(h = 0, lty = 2, col = "darkred")
text(2.5, 0.9, cex = 0.8, "Hipoteca negada")
text(2.5, -0.1, cex= 0.8, "Hipoteca aprobada")

# agregar una línea de regresión estimada de los modelos Probit y Logit
x <- seq(0, 3, 0.01)
y_probit <- predict(denyprobit, list(pirat = x), type = "response")
y_logit <- predict(denylogit, list(pirat = x), type = "response")

lines(x, y_probit, lwd = 1.5, col = "steelblue")
lines(x, y_logit, lwd = 1.5, col = "black", lty = 2)

# agrega una leyenda
legend("topleft",
       horiz = TRUE,
       legend = c("Probit", "Logit"),
       col = c("steelblue", "black"), 
       lty = c(1, 2))
```

Ambos modelos producen estimaciones muy similares de la probabilidad de que una solicitud de hipoteca sea denegada dependiendo de la relación pago-ingresos del solicitante.

En este contexto, se amplia el modelo Logit simple de denegación de hipoteca con el regresor adicional $black$.

```{r, 629}
# estimar una regresión Logit con múltiples regresores
denylogit2 <- glm(deny ~ pirat + black, 
                  family = binomial(link = "logit"), 
                  data = HMDA)

coeftest(denylogit2, vcov. = vcovHC, type = "HC1")
```

Se obtiene

\begin{align}
  \widehat{P(deny=1 \vert P/I ratio, black)} = F(-\underset{(0.35)}{4.13} + \underset{(0.96)}{5.37} P/I \ ratio + \underset{(0.15)}{1.27} black). (\#eq:denylogit2)
\end{align}

En cuanto al modelo Probit \@ref(eq:denyprobit2) todos los coeficientes del modelo son altamente significativos y se obtienen estimaciones positivas para los coeficientes en $P/I \ ratio$ y $black$. A modo de comparación, se calcula la probabilidad de rechazo prevista para dos solicitantes hipotéticos que difieren en la raza y tienen una relación $P/I \ ratio$ de $0.3$.

```{r, 630}
# 1. Calcular predicciones para la relación P/I = 0.3
predictions <- predict(denylogit2, 
                       newdata = data.frame("black" = c("no", "yes"), 
                                            "pirat" = c(0.3, 0.3)),
                       type = "response")

predictions

# 2. Calcular la diferencia de probabilidades
diff(predictions)
```

Se encuentra que el solicitante blanco enfrenta una probabilidad de denegación de solo $7.5\%$, mientras que el afroamericano es rechazado con una probabilidad de $22.4\%$, una diferencia de $14.9\%$.

#### Comparación de los modelos {-}

El modelo Probit y el modelo Logit ofrecen solo aproximaciones a la función de regresión de población desconocida $E(Y\vert X)$. No es obvio cómo decidir qué modelo utilizar en la práctica. El modelo de probabilidad lineal tiene el claro inconveniente de no poder capturar la naturaleza no lineal de la función de regresión poblacional y puede predecir que las probabilidades se encuentren fuera del intervalo $[0,1]$. Los modelos Probit y Logit son más difíciles de interpretar, pero capturan las no linealidades mejor que el enfoque lineal: Ambos modelos producen predicciones de probabilidades que se encuentran dentro del intervalo $[0,1]$. Las predicciones de los tres modelos suelen estar próximas entre sí. El libro sugiere utilizar el método que sea más fácil de usar en el software estadístico de su elección. Como se ha visto, es igualmente fácil estimar el modelo Probit y Logit usando **R**. Por lo tanto, no se puede dar ninguna recomendación general sobre qué método utilizar.

## Estimación e inferencia en los modelos logit y probit

Hasta ahora no se ha dicho nada sobre *cómo se estiman los modelos Logit y Probit* mediante software estadístico. La razón por la que esto es interesante es que ambos modelos son *no lineales en los parámetros* y, por lo tanto, no pueden estimarse usando MCO. En su lugar, uno se basa en *estimación de máxima verosimilitud* (EMV). Otro enfoque es la estimación por *mínimos cuadrados no lineales* (MCNL).

#### Mínimos cuadrados no lineales {-}

Considere el modelo Probit de regresión múltiple:

\begin{align}
  E(Y_i\vert X_{1i}, \dots, X_{ki}) = P(Y_i=1\vert X_{1i}, \dots, X_{ki}) = \Phi(\beta_0 + \beta_1 X_{1i} + \dots + \beta_k X_{ki}). (\#eq:multprobit)
\end{align}

De manera similar a MCO, MCNL estima los parámetros $\beta_0,\beta_1,\dots,\beta_k$ minimizando la suma de errores al cuadrado

$$\sum_{i=1}^n\left[ Y_i - \Phi(b_0 + b_1 X_{1i} + \dots + b_k X_{ki}) \right]^2.$$

La estimación de MCNL es un enfoque coherente que produce estimaciones que normalmente se distribuyen en muestras grandes. En **R** existen funciones como **NLM()** del paquete **stats** que proporcionan algoritmos para resolver problemas de mínimos cuadrados no lineales.

Sin embargo, MCNL es ineficiente, lo que significa que existen técnicas de estimación que tienen una varianza más pequeña.

#### Estimación de máxima verosimilitud {-}

En la EMV se busca estimar los parámetros desconocidos eligiéndolos de manera que se maximice la probabilidad de extraer la muestra observada. Esta probabilidad se mide mediante la función de verosimilitud, la distribución de probabilidad conjunta de los datos tratados en función de los parámetros desconocidos. Dicho de otra manera, las estimaciones de máxima verosimilitud de los parámetros desconocidos son los valores que dan como resultado un modelo que es más probable que produzca los datos observados. Resulta que EMV es más eficiente que MCNL.

Como las estimaciones de máxima verosimilitud se distribuyen normalmente en muestras grandes, la inferencia estadística de los coeficientes en modelos no lineales como la regresión Logit y Probit se puede realizar utilizando las mismas herramientas que se utilizan para los modelos de regresión lineal: Se puede calcular el estadístico $t$ e intervalos de confianza.

Muchos paquetes de software utilizan un algoritmo EMV para la estimación de modelos no lineales. La función **glm()** utiliza un algoritmo llamado *mínimos cuadrados iterativamente reponderados*.

#### Medidas de ajuste {-}

Es importante tener en cuenta que los valores habituales $R^2$ y $\bar{R}^2$ son *inválidos* para los modelos de regresión no lineal. La razón de esto es simple: Ambas medidas asumen que la relación entre la variable dependiente y explicativa(s) es lineal. Obviamente, esto no es válido para los modelos Probit y Logit. Por tanto, $R^2$ no necesita estar entre $0$ y $1$ y no existe una interpretación significativa. Sin embargo, el software estadístico a veces informa estas medidas de todos modos.

Existen muchas medidas de ajuste para los modelos de regresión no lineal y no existe consenso sobre cuál debe informarse. La situación es aún más complicada porque no existe una medida de ajuste que sea significativa en general. Para modelos con una variable de respuesta binaria como $deny$, uno podría usar la siguiente regla: \newline Si $Y_i = 1$ y $\widehat{P(Y_i|X_{i1}, \dots, X_{ik})} > 0.5$ o si $Y_i = 0$ y $\widehat{P(Y_i|X_{i1}, \dots, X_{ik})} < 0.5$, considere el $Y_i$ como se predijo correctamente. De lo contrario, se dice que $Y_i$ se predice incorrectamente. La medida de ajuste es la proporción de observaciones predichas correctamente. La desventaja de este enfoque es que no refleja la calidad de la predicción: Si $\widehat{P(Y_i = 1|X_{i1}, \dots, X_{ik}) = 0.51}$ o $\widehat{P(Y_i =1|X_{i1}, \dots, X_{ik}) = 0.99}$ no se refleja, solo se predice $Y_i = 1$.^[Esto contrasta con el caso de una variable dependiente numérica en la que se utilizan los errores al cuadrado para evaluar la calidad de la predicción.]

Una alternativa a este último son las llamadas medidas pseudo-$R^2$. Para medir la calidad del ajuste, estas medidas comparan el valor de la probabilidad maximizada (log-) del modelo con todos los regresores (el *modelo completo*) con la probabilidad de un modelo sin regresores (*modelo nulo*, regresión sobre una constante).

Por ejemplo, considere una regresión Probit. El $\text{pseudo-}R^2$ viene dado por $$\text{pseudo-}R^2 = 1 - \frac{\ln(f^{max}_{full})}{\ln(f^{max}_{null})}$$ donde $f^{max}_j \in [0,1]$ denota la probabilidad maximizada para el modelo $j$.

El razonamiento detrás de esto es que la probabilidad maximizada aumenta a medida que se agregan regresores adicionales al modelo, de manera similar a la disminución en $SSR$ cuando se agregan regresores en un modelo de regresión lineal. Si el modelo completo tiene una probabilidad maximizada similar a la del modelo nulo, el modelo completo no mejora realmente sobre un modelo que usa solo la información en la variable dependiente, entonces $\text{pseudo-}R^2 \approx 0$. Si el modelo completo se ajusta muy bien a los datos, la probabilidad maximizada debe estar cerca de $1$, de modo que $\ln(f^{max}_{full}) \approx 0$ y $\text{pseudo-}R^2 \approx 1$. Consultar en internet para obtener más información sobre las medidas EMV y pseudo-$R^2$.

**summary()** no reporta $\text{pseudo-}R^2$ para modelos estimados por **glm()**, pero se pueden usar las entradas *desviación residual* (**desviación**) y *desviación nula* (**desviación nula**) en su lugar. Estos se calculan como

$$\text{deviance} = -2 \times \left[\ln(f^{max}_{saturated}) - \ln(f^{max}_{full}) \right]$$

y

$$\text{null deviance} = -2 \times \left[\ln(f^{max}_{saturated}) - \ln(f^{max}_{null}) \right]$$

donde $f^{max}_{saturated}$ es la probabilidad maximizada para un modelo que asume que cada observación tiene su propio parámetro (existen $n + 1$ parámetros para estimar que conducen a un ajuste perfecto). Para modelos con una variable dependiente binaria, se sostiene que $$\text{pseudo-}R^2 = 1 - \frac{\text{deviance}}{\text{null deviance}} = 1- \frac{\ln(f^{max}_{full})}{\ln(f^{max}_{null})}.$$

Ahora se calcula $\text{pseudo-}R^2$ para el modelo Probit aumentado de denegación de hipoteca.

```{r, 631}
# calcular pseudo-R2 para el modelo probit de denegación de hipoteca
pseudoR2 <- 1 - (denyprobit2$deviance) / (denyprobit2$null.deviance)
pseudoR2
```

Otra forma de obtener $\text{pseudo-}R^2$ es estimar el modelo nulo usando **glm()** y extraer las probabilidades logarítmicas maximizadas tanto para el modelo nulo como para el modelo completo usando la función **logLik()**.

```{r, 632}
# calcular el modelo nulo
denyprobit_null <- glm(formula = deny ~ 1, 
                       family = binomial(link = "probit"), 
                       data = HMDA)

# calcular la pseudo-R2 usando 'logLik'
1 - logLik(denyprobit2)[1]/logLik(denyprobit_null)[1]
```

## Aplicación a los datos de la HMDA de Boston

Los modelos \@ref(eq:denyprobit2) y \@ref(eq:denylogit2) indican que las tasas de denegación son más altas para los solicitantes afroamericanos que mantienen constante la relación pago-ingreso. Ambos resultados podrían estar sujetos a sesgo de variable omitida. Para obtener una estimación más confiable del efecto de ser negro sobre la probabilidad de denegación de una solicitud de hipoteca, se estima un modelo de probabilidad lineal, así como varios modelos Logit y Probit. Por lo tanto, se controlan las variables financieras y las características adicionales del solicitante que probablemente influyan en la probabilidad de denegación y difieran entre los solicitantes blancos y negros.

Los promedios de la muestra se pueden reproducir fácilmente usando las funciones **mean()** (como es habitual para las variables numéricas) y **prop.table()** (para las variables factoriales).

```{r, 633}
# media de la relación P/I
mean(HMDA$pirat)

# relación entre gastos e ingresos totales de la vivienda
mean(HMDA$hirat)

# relación valor del préstamo
mean(HMDA$lvrat)

# puntaje de crédito del consumidor
mean(as.numeric(HMDA$chist))

# puntaje de crédito hipotecario
mean(as.numeric(HMDA$mhist))

# historial de crédito público malo
mean(as.numeric(HMDA$phist)-1)

# seguro hipotecario denegado
prop.table(table(HMDA$insurance))

# trabajadores por cuenta propia
prop.table(table(HMDA$selfemp))

# soltero
prop.table(table(HMDA$single))

# diploma de escuela secundaria
prop.table(table(HMDA$hschool))

# tasa de desempleo
mean(HMDA$unemp)

# condominio
prop.table(table(HMDA$condomin))

# negro
prop.table(table(HMDA$black))

# denegar
prop.table(table(HMDA$deny))
```

Se recomienda usar la función de ayuda de **R** para obtener más información sobre las variables contenidas en el conjunto de datos **HMDA**.

Antes de estimar los modelos, se debe transformar la relación préstamo-valor (**lvrat**) en una variable factorial, donde

\begin{align*}
  lvrat = 
  \begin{cases}
    \text{low} & \text{if} \ \ lvrat < 0.8, \\
    \text{medium} & \text{if} \ \ 0.8 \leq lvrat \leq 0.95, \\
    \text{high} & \text{if} \ \ lvrat > 0.95
  \end{cases}
\end{align*}

y convertir ambos puntajes de crédito en variables numéricas.

```{r, 634}
# definir una relación préstamo-valor baja, media y alta
HMDA$lvrat <- factor(
  ifelse(HMDA$lvrat < 0.8, "low",
  ifelse(HMDA$lvrat >= 0.8 & HMDA$lvrat <= 0.95, "medium", "high")),
  levels = c("low", "medium", "high"))

# convertir puntajes de crédito a numéricos
HMDA$mhist <- as.numeric(HMDA$mhist)
HMDA$chist <- as.numeric(HMDA$chist)
```

A continuación, se construyen los resultados de las estimaciones.

```{r, 635}
# estimar los 6 modelos para la probabilidad de negación
lpm_HMDA <- lm(deny ~ black + pirat + hirat + lvrat + chist + mhist + phist 
               + insurance + selfemp, data = HMDA)

logit_HMDA <- glm(deny ~ black + pirat + hirat + lvrat + chist + mhist + phist 
                  + insurance + selfemp, 
                  family = binomial(link = "logit"), 
                  data = HMDA)

probit_HMDA_1 <- glm(deny ~ black + pirat + hirat + lvrat + chist + mhist + phist 
                     + insurance + selfemp, 
                     family = binomial(link = "probit"), 
                     data = HMDA)

probit_HMDA_2 <- glm(deny ~ black + pirat + hirat + lvrat + chist + mhist + phist 
                     + insurance + selfemp + single + hschool + unemp, 
                     family = binomial(link = "probit"), 
                     data = HMDA)

probit_HMDA_3 <- glm(deny ~ black + pirat + hirat + lvrat + chist + mhist 
                     + phist + insurance + selfemp + single + hschool + unemp + condomin 
                     + I(mhist==3) + I(mhist==4) + I(chist==3) + I(chist==4) + I(chist==5) 
                     + I(chist==6), 
                     family = binomial(link = "probit"), 
                     data = HMDA)

probit_HMDA_4 <- glm(deny ~ black * (pirat + hirat) + lvrat + chist + mhist + phist 
                     + insurance + selfemp + single + hschool + unemp, 
                     family = binomial(link = "probit"), 
                     data = HMDA)
```

Al igual que en los capítulos anteriores, se almacenan los errores estándar robustos a la heterocedasticidad de los estimadores de coeficientes en un objeto **list** que luego se utiliza como argumento **se** en **stargazer()**.

```{r, 636, eval=FALSE}
rob_se <- list(sqrt(diag(vcovHC(lpm_HMDA, type = "HC1"))),
               sqrt(diag(vcovHC(logit_HMDA, type = "HC1"))),
               sqrt(diag(vcovHC(probit_HMDA_1, type = "HC1"))),
               sqrt(diag(vcovHC(probit_HMDA_2, type = "HC1"))),
               sqrt(diag(vcovHC(probit_HMDA_3, type = "HC1"))),
               sqrt(diag(vcovHC(probit_HMDA_4, type = "HC1"))))

stargazer(lpm_HMDA, logit_HMDA, probit_HMDA_1, 
          probit_HMDA_2, probit_HMDA_3, probit_HMDA_4,  
          digits = 3,
          type = "latex", 
          header = FALSE,
          se = rob_se,
          model.numbers = FALSE,
          column.labels = c("(1)", "(2)", "(3)", "(4)", "(5)", "(6)"))
```

<!--html_preserve-->

```{r hmdad, 637, message=F, warning=F, results='asis', echo=F, eval=my_output == "html"}
library(stargazer)

rob_se <- list(
  sqrt(diag(vcovHC(lpm_HMDA, type = "HC1"))),
  sqrt(diag(vcovHC(logit_HMDA, type = "HC1"))),
  sqrt(diag(vcovHC(probit_HMDA_1, type = "HC1"))),
  sqrt(diag(vcovHC(probit_HMDA_2, type = "HC1"))),
  sqrt(diag(vcovHC(probit_HMDA_3, type = "HC1"))),
  sqrt(diag(vcovHC(probit_HMDA_4, type = "HC1")))
)

stargazer(lpm_HMDA, logit_HMDA, probit_HMDA_1, probit_HMDA_2, probit_HMDA_3, probit_HMDA_4, 
          digits = 3,
          type = "html", 
          se = rob_se,
          header = FALSE,          
          dep.var.caption = "Variable dependiente: Denegación de solicitud de hipoteca",
          model.numbers = FALSE,
          column.labels = c("(1)", "(2)", "(3)", "(4)", "(5)", "(6)")
          )

stargazer_html_title("Datos HMDA: Modelos LPM, Probit y Logit", "hmdad")
```

<!--/html_preserve-->

```{r, 638, message=F, warning=F, results='asis', echo=F, eval=my_output == "latex"}
library(stargazer)

rob_se <- list(
  sqrt(diag(vcovHC(lpm_HMDA, type = "HC1"))),
  sqrt(diag(vcovHC(logit_HMDA, type = "HC1"))),
  sqrt(diag(vcovHC(probit_HMDA_1, type = "HC1"))),
  sqrt(diag(vcovHC(probit_HMDA_2, type = "HC1"))),
  sqrt(diag(vcovHC(probit_HMDA_3, type = "HC1"))),
  sqrt(diag(vcovHC(probit_HMDA_4, type = "HC1")))
)

stargazer(lpm_HMDA, logit_HMDA, probit_HMDA_1, probit_HMDA_2, probit_HMDA_3, probit_HMDA_4, 
          title = "\\label{tab:hmdad} Datos HMDA: Modelos LPM, Probit y Logit",
          digits = 3,
          type = "latex",
          float.env = "sidewaystable",
          column.sep.width = "-5pt",
          no.space = T,
          single.row = T,
          header = FALSE,
          se = rob_se,
          model.numbers = FALSE,
          column.labels = c("(1)", "(2)", "(3)", "(4)", "(5)", "(6)")
          )
```

En la tabla \@ref(tab:hmdad), los modelos (1), (2) y (3) son especificaciones de referencia que incluyen varias variables de control financiero. Se diferencian solo en la forma en que modelan la probabilidad de negación. El modelo (1) es un modelo de probabilidad lineal, el modelo (2) es una regresión Logit y el modelo (3) utiliza el enfoque Probit.

En el modelo lineal (1), los coeficientes tienen interpretación directa. Por ejemplo, se estima que un aumento en la calificación crediticia del consumidor en $1$ unidad aumentará la probabilidad de denegación de un préstamo en aproximadamente $0.031$ puntos porcentuales. Tener una relación préstamo-valor alta es perjudicial para la aprobación del crédito: El coeficiente para una relación préstamo-valor superior a $0.95$ es $0.189$, por lo que se estima que los clientes con esta propiedad enfrentan un riesgo de casi $19\%$ mayor de negación que aquellos con una baja relación préstamo-valor, ceteris paribus. El coeficiente estimado de la variable ficticia de raza es de $0.084$, lo que indica que la probabilidad de denegación para los afroamericanos es $8.4\%$ mayor que para los solicitantes blancos con las mismas características, excepto por la raza. Aparte de la relación entre gastos e ingresos de la vivienda y el puntaje de crédito hipotecario, todos los coeficientes son significativos.

Los modelos (2) y (3) proporcionan evidencia similar de que existe discriminación racial en el mercado hipotecario de EE. UU; dado que todos los coeficientes, excepto la relación entre gastos e ingresos de la vivienda (que no es significativamente diferente de cero), son significativos al nivel de $1\%$. Como se discutió anteriormente, la no linealidad hace que la interpretación de las estimaciones de los coeficientes sea más difícil que para el modelo (1). Para hacer una declaración sobre el efecto de ser negro, se necesita calcular la probabilidad de negación estimada para dos individuos que solo difieren en la raza. Para la comparación, se consideran dos individuos que comparten valores medios para todos los regresores numéricos. Para las variables cualitativas se asigna la propiedad que es más representativa para los datos disponibles. Por ejemplo, considere el trabajo por cuenta propia: Se ha visto que aproximadamente $88\%$ de todas las personas de la muestra no son trabajadores por cuenta propia, por lo que se establece **selfemp = no**. Con este enfoque, la estimación del efecto sobre la probabilidad de negación de ser afroamericano del modelo Logit (2) es de aproximadamente $4\%$. El siguiente fragmento de código muestra cómo aplicar este enfoque para los modelos (1) a (7) usando **R**.

```{r, 639}
# calcular valores de regresión para una persona negra promedio
new <- data.frame(
  "pirat" = mean(HMDA$pirat),
  "hirat" = mean(HMDA$hirat),
  "lvrat" = "low",
  "chist" = mean(HMDA$chist),
  "mhist" = mean(HMDA$mhist),
  "phist" = "no",
  "insurance" = "no",
  "selfemp" = "no",
  "black" = c("no", "yes"),
  "single" = "no",
  "hschool" = "yes",
  "unemp" = mean(HMDA$unemp),
  "condomin" = "no")

# diferencia predicha por el LPM
predictions <- predict(lpm_HMDA, newdata = new)
diff(predictions)

# diferencia predicha por el modelo logit
predictions <- predict(logit_HMDA, newdata = new, type = "response")
diff(predictions)

# diferencia predicha por el modelo probit (3)
predictions <- predict(probit_HMDA_1, newdata = new, type = "response")
diff(predictions)

# diferencia predicha por el modelo probit (4)
predictions <- predict(probit_HMDA_2, newdata = new, type = "response")
diff(predictions)

# diferencia predicha por el modelo probit (5)
predictions <- predict(probit_HMDA_3, newdata = new, type = "response")
diff(predictions)

# diferencia predicha por el modelo probit (6)
predictions <- predict(probit_HMDA_4, newdata = new, type = "response")
diff(predictions)
```

Las estimaciones del impacto sobre la probabilidad de negación de ser negro son similares para los modelos (2) y (3). Es interesante que la magnitud de los efectos estimados es mucho menor que para los modelos Probit y Logit que no controlan por características financieras (ver sección 11.2). Esto indica que estos modelos simples producen estimaciones sesgadas debido a variables omitidas.

Las regresiones (4) a (6) utilizan especificaciones de regresión que incluyen diferentes características del solicitante y variables indicadoras de calificación crediticia, así como interacciones. Sin embargo, la mayoría de los coeficientes correspondientes no son significativos y las estimaciones del coeficiente sobre **negro** obtenidas para estos modelos, así como la diferencia estimada en las probabilidades de negación, no difieren mucho de las obtenidas para especificaciones similares (2) y (3).

Una pregunta interesante relacionada con la discriminación racial se puede investigar utilizando el modelo Probit (6) donde las interacciones **blackyes:pirat** y **blackyes:hirat** se agregan al modelo (4). Si el coeficiente de **blackyes:pirat** fuera diferente de cero, el efecto de la relación pago-ingreso sobre la probabilidad de denegación sería diferente para los solicitantes blancos y negros. De manera similar, un coeficiente distinto de cero en **blackyes:hirat** indicaría que los oficiales de crédito evalúan el riesgo de quiebra asociado con una alta relación préstamo-valor de manera diferente para los solicitantes de hipotecas blancos y negros. Se puede probar si estos coeficientes son conjuntamente significativos al nivel de $5\%$ usando una prueba de $F$.

```{r, 640}
linearHypothesis(probit_HMDA_4,
                 test = "F",
                 c("blackyes:pirat=0", "blackyes:hirat=0"),
                 vcov = vcovHC, type = "HC1")
```

Dado que $p\text{-value} \approx 0.77$ para esta prueba, el valor nulo no se puede rechazar. No obstante, se puede rechazar la hipótesis de que no existe discriminación racial en absoluto, ya que la prueba $F$ correspondiente tiene un $p\text{-value}$ de aproximadamente $0.002$.

```{r, 641}
linearHypothesis(probit_HMDA_4,
                 test = "F",
                 c("blackyes=0", "blackyes:pirat=0", "blackyes:hirat=0"),
                 vcov = vcovHC, type = "HC1")
```

#### Resumen {-}

Los modelos (1) a (6) proporcionan evidencia de que existe un efecto de ser afroamericano en la probabilidad de denegación de una solicitud de hipoteca: En todas las especificaciones, se estima que el efecto es positivo (entre $4\%$ y $5\%$) y es significativamente diferente de cero en el nivel de $1\%$. Si bien el modelo de probabilidad lineal parece sobrestimar ligeramente este efecto, aún puede usarse como una aproximación a una relación intrínsecamente no lineal.

## Ejercicios {#Ejercicios-11}

```{r, 642, echo=F, purl=F, results='asis'}
if (my_output=="html") {
  cat('
<div  class = "DCexercise">

#### 1. Datos de supervivencia del Titanic {-}

El capítulo \\@ref(RPL) presentó tres enfoques para modelar la función de expectativa condicional de una variable dependiente binaria: El modelo de probabilidad lineal y la regresión Probit y Logit.

Los ejercicios de este capítulo utilizan datos sobre el destino de los pasajeros del océano lineal *Titanic*. El objetivo es explicar la supervivencia, una variable binaria, por variables socioeconómicas utilizando los enfoques anteriores.

En este ejercicio se comienza con el conjunto de datos agregados <tt>Titanic</tt>. Es parte del paquete <tt>datasets</tt> que es parte de la base de <tt>R</tt>. La siguiente cita de la [descripción](https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/Titanic.html) del conjunto de datos motiva el intento de predecir la *probabilidad* de supervivencia:

*El hundimiento del Titanic es un evento famoso y aún se están publicando nuevos libros al respecto. Muchos hechos bien conocidos, desde las proporciones de pasajeros de primera clase hasta la política de "las mujeres y los niños primero", y el hecho de que esa política no fue del todo exitosa para salvar a las mujeres y los niños de la tercera clase, lo que se refleja en la supervivencia a partir de las tarifas para varias clases de pasajeros.*

**Instrucciones:**

  + Asignar los datos del <tt>Titanic</tt> a <tt>Titanic_1</tt> y obtener una descripción general.

  + Visualizar las tasas de supervivencia condicional para la clase de viaje (<tt>Class</tt>), el género (<tt>Sex</tt>) y la edad (<tt>Age</tt>) usando <tt>mosaicplot()</tt>.

<iframe src="DCL/ex11_1.html" frameborder="0" scrolling="no" style="width:100%;height:340px"></iframe>

</div>') } else {
  cat("\\begin{center}\\textit{Esta parte interactiva del curso solo está disponible en la versión HTML.}\\end{center}")
}
```

```{r, 643, echo=F, purl=F, results='asis'}
if (my_output=="html") {
  cat('
<div  class = "DCexercise">

#### 2. Datos de supervivencia del Titanic --- Ctd. {-}

El conjunto de datos del <tt>Titanic</tt> del ejercicio 1 no es útil para el análisis de regresión porque está muy agregado. En este ejercicio se trabajará con <tt>titanic.csv</tt> que está disponible en la URL https://stanford.io/2O9RUCF.

Las columnas de <tt>titanic.csv</tt> contienen las siguientes variables:

  <tt>Survived</tt> --- El indicador de sobrevivido
    
  <tt>Pclass</tt> --- Clase de pasajero
    
  <tt>Name</tt> --- Nombre del pasajero
    
  <tt>Sex</tt> --- Género del pasajero
    
  <tt>Age</tt> --- Edad del pasajero
    
  <tt>Siblings</tt> --- Número de hermanos a bordo
    
  <tt>Parents.Children.Aboard</tt> --- Número de padres e hijos a bordo
    
  <tt>fare</tt> --- La tarifa pagada en libras esterlinas

**Instrucciones:**

  + Importar los datos de <tt>titanic.csv</tt> usando la función <tt>read.csv2()</tt>. Guardar el resultado en <tt>Titanic_2</tt>.

  + Asignar los siguientes nombres de columna a <tt>Titanic_2</tt>:
  
    <tt>Survived, Class, Name, Sex, Age, Siblings, Parents</tt> y <tt>Fare</tt>.

  + Obtener una descripción general del conjunto de datos. Soltar la columna <tt> Nombre </tt>.

  + Adjuntar los paquetes <tt>corrplot</tt> y <tt>dplyr</tt>. Verificar si existe multicolinealidad en los datos usando <tt>corrplot()</tt>.

<iframe src="DCL/ex11_2.html" frameborder="0" scrolling="no" style="width:100%;height:560px"></iframe>

**Sugerencias:**

  + <tt>read_csv()</tt> adivina la especificación de la columna así como los separadores usados en el archivo <tt>.csv</tt>. Siempre debe verificar si el resultado es correcto.

  + Puede usar <tt>select_if()</tt> del paquete <tt>dplyr</tt> para seleccionar todas las columnas numéricas del conjunto de datos.

</div>')}
```

```{r, 644, echo=F, purl=F, results='asis'}
if (my_output=="html") {
  cat('
<div  class = "DCexercise">

#### 3. Datos de supervivencia del Titanic --- Tasas de supervivencia {-}

Las tablas de contingencia similares a las proporcionadas por el conjunto de datos <tt>Titanic</tt> del ejercicio 1 pueden arrojar algo de luz sobre la distribución de las condiciones de supervivencia y los posibles determinantes de las mismas; por ejemplo, la clase de pasajeros. Las tablas de contingencia se crean fácilmente usando la función <tt>table</tt> que forma parte de la base <tt>R</tt>.

**Instrucciones:**

  + Generar una tabla de contingencia para <tt>Survived</tt> y <tt>Class</tt> usando <tt>table()</tt>. Guardar la tabla en <tt>t_abs</tt>.

  + <tt>t_abs</tt> reporta frecuencias absolutas. Transforme <tt>t_abs</tt> en una tabla que informe las frecuencias relativas (en relación con el número total de observaciones). Guardar el resultado en <tt>t_rel</tt>.

  + Visualizar las frecuencias relativas en <tt>t_rel</tt> usando <tt>barplot()</tt>. Usar diferentes colores para distinguir mejor entre la tasa de supervivencia y la no supervivencia (no importa qué colores use).

<iframe src="DCL/ex11_4_3.html" frameborder="0" scrolling="no" style="width:100%;height:320px"></iframe>

</div>')}
```


```{r, 645, echo=F, purl=F, results='asis'}
if (my_output=="html") {
  cat('
<div  class = "DCexercise">

#### 4. Datos de supervivencia del Titanic --- Distribuciones condicionales de <tt>Age</tt> {-}

Las tablas de contingencia son útiles para resumir la distribución de variables categóricas como <tt>Survived</tt> y <tt>Class</tt> en el Ejercicio 3. Sin embargo, no son útiles cuando la variable de interés toma muchos números enteros diferentes (e incluso son imposibles de generar cuando la variable es continua).

En este ejercicio se le pide que genere y visualice estimaciones de densidad de la distribución de <tt>Age</tt> condicionada a <tt>Survived</tt> para ver si existen indicaciones de cómo la edad se relaciona con la posibilidad de supervivencia (a pesar de que el conjunto de datos informa números enteros, aquí se trata <tt>Age</tt> como una variable continua). Por ejemplo, es interesante ver si la política de "las mujeres y los niños primero" fue eficaz.

El conjunto de datos <tt>Titanic_2</tt> de los ejercicios anteriores está disponible en el entorno de trabajo.

**Instrucciones:**

  + Obtener estimaciones de densidad de kernel de las distribuciones de <tt>Age</tt> tanto para los sobrevivientes como para los fallecidos.

  + Guardar los resultados en <tt>dens_age_surv</tt> (sobrevivió) y <tt>dens_age_died</tt> (murió).

  + Graficar ambas estimaciones de densidad de kernel (¡Superponerlas en una sola gráfica!). Usar diferentes colores de su elección para que las estimaciones sean distinguibles.

<iframe src="DCL/ex11_5_4.html" frameborder="0" scrolling="no" style="width:100%;height:330px"></iframe>

**Sugerencias:**

  + Las estimaciones de densidad de kernel se pueden obtener usando la función <tt>density()</tt>.

  + Utilizar <tt>plot()</tt> y <tt>lines()</tt> para trazar las estimaciones de densidad.

</div>')}
```

```{r, 646, echo=F, purl=F, results='asis'}
if (my_output=="html") {
  cat('
<div  class = "DCexercise">

#### 5. Datos de supervivencia del Titanic: un modelo de probabilidad lineal para <tt>Survival</tt> I {-}

¿Cómo influyen las características socioeconómicas de los pasajeros en la probabilidad de supervivencia? En particular, ¿existen diferencias sistemáticas entre las tres clases de pasajeros? ¿Los datos reflejan la política de "los niños y las mujeres primero"?

Es natural comenzar el análisis estimando un modelo de probabilidad lineal simple como (LMP) $$Survived_i = \\beta_0 + \\beta_1 Class2_i + \\beta_2 Class3_i + u_i$$ con variables ficticias $Class2_i$ y $Class3_i$.

El conjunto de datos <tt>Titanic_2</tt> de los ejercicios anteriores está disponible en el entorno de trabajo.

**Instrucciones:**

  + Asjuntar el paquete <tt>AER</tt>.

  + <tt>Class</tt> es de tipo <tt>int</tt> (integer), convertir <tt>Class</tt> en una variable factorial.

  + Estimar el modelo de probabilidad lineal y guardar el resultado en <tt>surv_mod</tt>.

  + Obtener un resumen robusto de los coeficientes del modelo.

  + Utilizar <tt>surv_mod</tt> para predecir la probabilidad de supervivencia de las tres clases de pasajeros.

<iframe src="DCL/ex11_lpm.html" frameborder="0" scrolling="no" style="width:100%;height:320px"></iframe>

**Sugerencias:**

  + Los modelos de probabilidad lineal se pueden estimar usando <tt>lm()</tt>.

  + Utilizar <tt>predict()</tt> para obtener las predicciones. Recuerde que se debe proporcionar un <tt>data.frame</tt> al argumento <tt>newdata</tt>.

</div>')}
```

```{r, 647, echo=F, purl=F, results='asis'}
if (my_output=="html") {
  cat('

<div  class = "DCexercise">

#### 6. Datos de supervivencia del Titanic: un modelo de probabilidad lineal para <tt>Survival</tt> II {-}

Considerar nuevamente el resultado del ejercicio 5:

$$\\widehat{Survived}_i = \\underset{(0.03)}{0.63} - \\underset{(0.05)}{0.16} Class2_i - \\underset{(0.04)}{0.39} Class3_i + u_i $$

(Los coeficientes estimados en este modelo están relacionados con las medias muestrales específicas de la clase de <tt>Survived</tt>. Se solicita que los calcule a continuación).

Los coeficientes altamente significativos indican que la probabilidad de supervivencia disminuye con la clase de pasajeros; es decir, los pasajeros de una clase menos lujosa tienen menos probabilidades de sobrevivir.

Este resultado podría verse afectado por el sesgo de la variable omitida que surge de la correlación de la clase de pasajero con los determinantes de la probabilidad de supervivencia no incluidos en el modelo. Por lo tanto, se aumenta el modelo de manera que incluya todas las variables restantes como regresores.

El conjunto de datos <tt>Titanic_2</tt> así como el modelo <tt>surv_mod</tt> de los ejercicios anteriores están disponibles en el entorno de trabajo. Se adjunta el paquete <tt>AER</tt>.

**Instrucciones:**

  + Utilizar el objeto modelo <tt>surv_mod</tt> para obtener las estimaciones específicas de la clase para la probabilidad de supervivencia. Guardar el resultado en <tt>surv_prob_c1</tt>, <tt>surv_prob_c2</tt> y <tt>surv_prob_c3</tt>.

  + Ajustar el LMP aumentado y asignar el resultado al objeto <tt>LPM_mod</tt>.

  + Obtener un resumen robusto de los coeficientes del modelo.

<iframe src="DCL/ex11_lpm2.html" frameborder="0" scrolling="no" style="width:100%;height:320px"></iframe>

**Sugerencia:**

  + Recuerde que la fórmula <tt>a ~.</tt> especifica una regresión de <tt>a</tt> en todas las demás variables en el conjunto de datos proporcionado como el argumento <tt>data</tt> en <tt>glm()</tt>. 

</div>')}
```

```{r, 648, echo=F, purl=F, results='asis'}
if (my_output=="html") {
  cat('
<div  class = "DCexercise">

#### 7. Datos de supervivencia del Titanic --- Regresión logística {-}

El capítulo \\@ref(RPL) presenta la regresión logística, también llamada regresión Logit, que es más adecuada que el LPM para modelar la función de probabilidad condicional de una variable de resultado dicotómica. La regresión logit usa una función de enlace no lineal que restringe los valores ajustados para que se encuentren entre $0$ y $1$: En la regresión logit, las *log-odds* del resultado se modelan como una combinación lineal de los predictores, mientras que el LPM asume que el condicional La función de probabilidad del resultado es lineal.

El conjunto de datos <tt>Titanic_2</tt> del ejercicio 2 está disponible en el entorno de trabajo. Se adjunta el paquete <tt>AER</tt>.

**Instrucciones:**

  + Use <tt>glm()</tt> para estimar el modelo

\\begin{align*}
\\log\\left(\\frac{P(survived_i = 1)}{1-P(survived_i = 1)}\\right) =& \\, \\beta_0 + \\beta_1 Class2_i + \\beta_2 Ckass3_i + \\beta_3 Sex_i \\\\ +& \\, \\beta_4 Age_i + \\beta_5 Siblings_i + \\beta_6 Perents_i + \\beta_7 Fare_i + u_i.
\\end{align*}

  + Obtener un resumen robusto de los coeficientes del modelo.

  + El marco de datos <tt>passengers</tt> contiene datos sobre tres pasajeros masculinos hipotéticos que difieren solo en su clase de pasajero (las otras variables se establecen en el promedio de la muestra respectiva). Utilizar <tt>Logit_mod</tt> para predecir la probabilidad de supervivencia de estos pasajeros.

<iframe src="DCL/ex11_3_6.html" frameborder="0" scrolling="no" style="width:100%;height:430px"></iframe>

**Sugerencias:**

  + Recuerde que la fórmula <tt>a ~.</tt> especifica una regresión de <tt>a</tt> en todas las demás variables en el conjunto de datos proporcionado como el argumento <tt>data</tt> en <tt>glm()</tt>.

  + Debe especificar el tipo correcto de predicción en <tt>predict()</tt>.

</div>')}
```

```{r, 649, echo=F, purl=F, results='asis'}
if (my_output=="html") {
  cat('
<div  class = "DCexercise">

#### 8. Datos de supervivencia del Titanic --- Regresión probit {-}

Repita el ejercicio 7, pero esta vez calcule el modelo Probit.

\\begin{align*}
P(Survived_i = 1\\vert Class2_i, Class3_i, \\dots, Fare_i) =& \\, \\Phi (\\beta_0 + \\beta_1 Class2_i + \\beta_2 Class3_i + \\beta_3 Sex_i \\\\ +& \\, \\beta_4 Age_i + \\beta_5 Siblings_i + \\beta_6 Parents_i + \\beta_7 Fare_i + u_i).
\\end{align*}

El conjunto de datos <tt>Titanic_2</tt> de los ejercicios anteriores así como el modelo Logit <tt>Logit_mod</tt> están disponibles en el entorno de trabajo. Se adjunta el paquete <tt>AER</tt>.

**Instrucciones: **

  + Utilizar <tt>glm()</tt> para estimar el modelo Probit anterior. Guardar el resultado en <tt>Probit_mod</tt>.

  + Obtener un resumen robusto de los coeficientes del modelo.

  + El marco de datos <tt>passengers</tt> contiene datos sobre tres pasajeros masculinos hipotéticos que difieren solo en su clase de pasajero (las otras variables se establecen en el promedio de la muestra respectiva). Utilizar <tt>Probit_mod</tt> para predecir la probabilidad de supervivencia de estos pasajeros.

<iframe src="DCL/ex11_8.html" frameborder="0" scrolling="no" style="width:100%;height:430px"></iframe>

**Sugerencias:**

  + Recuerde que la fórmula <tt>a ~.</tt> especifica una regresión de <tt>a</tt> en todas las demás variables en el conjunto de datos proporcionado como el argumento <tt>data</tt> en <tt>glm()</tt>.

  + Debe especificar el tipo correcto de predicción en <tt>predicts()</tt>.

</div>')}
```

<!--chapter:end:Capitulo_12.Rmd-->

# Regresión de variables instrumentales {#RVI}

```{r, echo = F}
options(knitr.duplicate.label = "allow")
```

```{r, 650, child="_setup.Rmd"}
```

```{r, 651, eval=knitr::opts_knit$get("rmarkdown.pandoc.to") == "html", results='asis', echo=FALSE}
cat('<hr style="background-color:#03193b;height:2px">')
```

Como se discutió en el Capítulo \@ref(EEBRM), los modelos de regresión pueden sufrir problemas como variables omitidas, errores de medición y causalidad simultánea. Si es así, el término de error se correlaciona con el regresor de interés y, por lo tanto, el coeficiente correspondiente se estima de manera inconsistente.

Hasta ahora se ha asumido que se pueden agregar las variables omitidas a la regresión para mitigar el riesgo de estimación sesgada del efecto causal de interés. Sin embargo, si los factores omitidos no se pueden medir o no están disponibles por otras razones, la regresión múltiple no puede resolver el problema.

El mismo problema surge si existe causalidad simultánea. Cuando la causalidad va de $X$ a $Y$ y viceversa, habrá un sesgo de estimación que no se puede corregir mediante regresión múltiple.

Una técnica general para obtener un estimador consistente del coeficiente de interés es la regresión de variables instrumentales (IV). El presente capítulo se enfoca en la herramienta de regresión VI llamada *mínimos cuadrados de dos etapas* (MC2E). Las primeras secciones recapitulan brevemente la mecánica general y los supuestos de la regresión VI y muestran cómo realizar la estimación MC2E usando **R**. A continuación, la regresión VI se utiliza para estimar la elasticidad de la demanda de cigarrillos, un ejemplo clásico en el que la regresión múltiple no funciona debido a la causalidad simultánea.

Al igual que en el capítulo anterior, los paquetes **AER** [@R-AER] y **stargazer** [@R-stargazer] son necesarios para reproducir el código presentado en este capítulo. Compruebe si el fragmento de código a continuación se ejecuta sin ningún mensaje de error.

```{r, 652, warning=FALSE, message=FALSE}
library(AER)
library(stargazer)
```

## El estimador de VI con un solo regresor y un solo instrumento {#EVIURUI}

Considere el modelo de regresión simple

\begin{align}
  Y_i = \beta_0 + \beta_1 X_i + u_i \ \ , \ \ i=1,\dots,n  (\#eq:srm12)
\end{align}

donde el término de error $u_i$ se correlaciona con el regresor $X_i$ ($X$ es *endógeno*) de modo que MCO es inconsistente para el verdadero $\beta_1$. En el caso más simple, la regresión VI usa una sola variable instrumental $Z$ para obtener un estimador consistente para $\beta_1$.

$Z$ debe cumplir dos condiciones para ser un instrumento válido:

**1. Condición de relevancia del instrumento**:

<center>$X$ y su instrumento $Z$ *deben estar* correlacionados: $\rho_{Z_i,X_i} \neq 0$.</center>

**2. Condición de exogeneidad del instrumento**:

<center>El instrumento $Z$ *no debe estar* correlacionado con el término de error $u$: $\rho_{Z_i,u_i} = 0$.</center>

#### El estimador de mínimos cuadrados en dos etapas {-}

Como se puede adivinar por su nombre, MC2E procede en dos etapas. En la primera etapa, la variación en el regresor endógeno $X$ se descompone en un componente libre de problemas que se explica por el instrumento $Z$ y un componente problemático que se correlaciona con el error $u_i$. La segunda etapa usa el componente libre de problemas de la variación en $X$ para estimar $\beta_1$.

El modelo de regresión de la primera etapa es $$X_i = \pi_0 + \pi_1 Z_i + \nu_i,$$ donde $\pi_0 + \pi_1 Z_i$ es el componente de $X_i$ que se explica por $Z_i$, mientras que $\nu_i$ es el componente que no puede ser explicado por $Z_i$ y exhibe correlación con $u_i$.

Usando las estimaciones de MCO$\widehat{\pi}_0$ y $\widehat{\pi}_1$ se obtienen los valores predichos $\widehat{X}_i, \ \ i=1,\dots,n$. Si $Z$ es un instrumento válido, los $\widehat{X}_i$ están libres de problemas en el sentido de que $\widehat{X}$ es exógeno en una regresión de $Y$ en $\widehat{X}$ que se realiza en la regresión de la segunda etapa. La segunda etapa produce $\widehat{\beta}_0^{MC2E}$ y $\widehat{\beta}_1^{MC2E}$, las estimaciones de MC2E de $\beta_0$ y $\beta_1$.

Para el caso de un solo instrumento, se puede demostrar que el estimador MC2E de $\beta_1$ es:

\begin{align}
\widehat{\beta}_1^{MC2E} = \frac{s_{ZY}}{s_{ZX}} = \frac{\frac{1}{n-1}\sum_{i=1}^n(Y_i - \overline{Y})(Z_i - \overline{Z})}{\frac{1}{n-1}\sum_{i=1}^n(X_i - \overline{X})(Z_i - \overline{Z})}, (\#eq:simpleMC2E)
\end{align}

que no es más que la relación de la covarianza muestral entre $Z$ y $Y$ y la covarianza muestral entre $Z$ y $X$.

Como se muestra, \@ref(eq:simpleMC2E) es un estimador consistente para $\beta_1$ en \@ref(eq:srm12) bajo el supuesto de que $Z$ es un instrumento válido. Al igual que para todos los demás estimadores MCO que se han considerado hasta ahora, el TLC implica que la distribución de $\widehat{\beta}_1^{MC2E}$ puede aproximarse mediante una distribución normal si el tamaño de la muestra es grande. Esto permite usar estadísticos $t$ e intervalos de confianza que también se calculan mediante ciertas funciones **R**.

#### Aplicación a la demanda de cigarrillos {-}

La relación entre la demanda y el precio de los productos básicos es un problema simple pero generalizado en economía. La economía de la salud se ocupa del estudio de cómo el sistema de atención de la salud y la política de regulación influyen en el comportamiento que afecta la salud de los individuos. Probablemente el ejemplo más destacado en los debates sobre políticas públicas sea el tabaquismo, ya que está relacionado con muchas enfermedades y externalidades negativas.

Es plausible que se pueda reducir el consumo de cigarrillos gravando más los cigarrillos. La pregunta es por *cuánto* deben aumentarse los impuestos para lograr una cierta reducción en el consumo de cigarrillos. Los economistas utilizan elasticidades para responder a este tipo de preguntas. Dado que se desconoce la elasticidad precio para la demanda de cigarrillos, debe estimarse. Como se discutió en otros capítulos, una regresión MCO de la cantidad logarítmica sobre el precio logarítmico no puede usarse para estimar el efecto de interés, ya que existe causalidad simultánea entre la oferta y la demanda. En su lugar, se puede utilizar la regresión VI.

Usando el conjunto de datos **CigarettesSW** que viene con el paquete **AER**. Es un conjunto de datos de panel que contiene observaciones sobre el consumo de cigarrillos y varios indicadores económicos para los 48 estados federales continentales de los EE. UU. desde 1985 a 1995. En este sentido, solo se consideran datos para la sección transversal de los estados en 1995.

Se comienza cargando el paquete, adjuntando el conjunto de datos y obteniendo una descripción general.

```{r, 653, warning=FALSE, message=FALSE}
# cargar el conjunto de datos y obtener una descripción general
library(AER)
data("CigarettesSW")
summary(CigarettesSW)
```

Utilizar `?CigarettesSW` para obtener una descripción detallada de las variables.

Se está interesado en estimar $\beta_1$ en

\begin{align}
  \log(Q_i^{cigarettes}) = \beta_0 + \beta_1 \log(P_i^{cigarettes}) + u_i, (\#eq:cigsMC2E)
\end{align}

donde $Q_i^{cigarrillos}$ es el número de paquetes de cigarrillos per cápita vendidos y $P_i^{cigarrillos}$ es el precio real promedio después de impuestos por paquete de cigarrillos en el estado $i$.

La variable instrumental que se va a utilizar para instrumentar el regresor endógeno $\log(P_i^{cigarrillos})$ es $\text{Impuesto sobre las ventas}$, la parte de los impuestos sobre los cigarrillos que se deriva del impuesto general sobre las ventas. $\text{Impuesto sobre las ventas}$ se mide en dólares por paquete. La idea es que $\text{Impuesto sobre las ventas}$ es un instrumento relevante, ya que está incluido en el precio promedio por paquete después de impuestos. Además, es plausible que $\text{Impuesto sobre las ventas}$ sea exógeno, ya que el impuesto a las ventas no influye en la cantidad vendida directamente, sino indirectamente a través del precio.

Se realizan algunas transformaciones con el fin de obtener datos de sección transversal deflactados para el año 1995.

También se calcula la correlación muestral entre el impuesto sobre las ventas y el precio por paquete. La correlación muestral es un estimador consistente de la correlación poblacional. La estimación de aproximadamente $0.614$ indica que $\text{Impuesto sobre las ventas}$ y $P_i^{cigarrillos}$ exhiben una correlación positiva que cumple con las expectativas: Mayores impuestos a las ventas conducen a precios más altos. Sin embargo, un análisis de correlación como este no es suficiente para comprobar si el instrumento es relevante. Más adelante se volverá al tema de verificar si un instrumento es relevante y exógeno.

```{r, 654}
# calcular precios reales per cápita
CigarettesSW$rprice <- with(CigarettesSW, price / cpi)

# calcular el impuesto sobre las ventas
CigarettesSW$salestax <- with(CigarettesSW, (taxs - tax) / cpi)

# comprobar la correlación entre el impuesto sobre las ventas y el precio
cor(CigarettesSW$salestax, CigarettesSW$price)

# generar un subconjunto para el año 1995
c1995 <- subset(CigarettesSW, year == "1995")
```

La regresión de la primera etapa es $$\log(P_i^{cigarrillos}) = \pi_0 + \pi_1 \text{Impuesto de venta}_i + \nu_i.$$ 

Se estima este modelo en **R** usando **lm()**. En la segunda etapa, se ejecuta una regresión de $\log(Q_i^{cigarrillos})$ en $\widehat{\log(P_i^{cigarrillos})}$ para obtener $\widehat{\beta}_0^{MC2E}$ y $\widehat{\beta}_1^{MC2E}$.

```{r, 655}
# realizar la regresión de la primera etapa
cig_s1 <- lm(log(rprice) ~ salestax, data = c1995)

coeftest(cig_s1, vcov = vcovHC, type = "HC1")
```

La regresión de la primera etapa es $$\widehat{\log(P_i^{cigarrillos})} = \underset{(0.03)}{4.62} + \underset{(0.005)}{0.031} \text{Impuesto de venta}_i$$ que predice la relación entre el precio del impuesto sobre las ventas por cigarrillos sea positivo. ¿Qué parte de la variación observada en $\log(P^{cigarrillos})$ se explica por el instrumento $\text{Impuesto de venta}$? Esto se puede responder observando los $R^2$ de la regresión, que establece que aproximadamente $47\%$ de la variación en los precios después de impuestos se explica por la variación del impuesto sobre las ventas entre los estados.

```{r, 656}
# inspeccionar el R^2 de la regresión de la primera etapa
summary(cig_s1)$r.squared
```

A continuación, se almacena $\widehat{\log(P_i^{cigarrillos})}$, los valores ajustados obtenidos por la regresión de la primera etapa **cig_s1**, en la variable **lcigp_pred**.

```{r, 657}
# almacenar los valores predichos
lcigp_pred <- cig_s1$fitted.values
```

A continuación, se ejecuta la regresión de la segunda etapa que da las estimaciones de MC2E que se buscan.

```{r, 658}
# ejecutar la regresión de la etapa 2
cig_s2 <- lm(log(c1995$packs) ~ lcigp_pred)
coeftest(cig_s2, vcov = vcovHC)
```

Estimando así el modelo \@ref(eq:cigsMC2E) usando rendimientos de MC2E

\begin{align}
  \widehat{\log(Q_i^{cigarettes})} = \underset{(1.70)}{9.72} + \underset{(0.36)}{1.08} \log(P_i^{cigarettes}), (\#eq:ecigsMC2E)
\end{align}

donde se escribió $\log(P_i^{cigarettes})$ en lugar de $\widehat{\log(P_i^{cigarettes})}$ para mantener la coherencia.

La función **ivreg()** del paquete **AER** realiza el procedimiento MC2E automáticamente. Se usa de manera similar a **lm()**. Los instrumentos se pueden agregar a la especificación habitual de la fórmula de regresión utilizando una barra vertical que separa la ecuación del modelo de los instrumentos. Por lo tanto, para la regresión en cuestión, la fórmula correcta es **log(packs) ~ log (rprice)|salestax**.

```{r, 659}
# realizar MC2E usando 'ivreg()'
cig_ivreg <- ivreg(log(packs) ~ log(rprice) | salestax, data = c1995)

coeftest(cig_ivreg, vcov = vcovHC, type = "HC1")
```

Se ha encontrado que las estimaciones de los coeficientes coinciden para ambos enfoques.

**Dos notas sobre el cálculo de errores estándar de MC2E**

1. Se ha demostrado que ejecutar las regresiones individuales para cada etapa de MC2E usando **lm()** conduce a las mismas estimaciones de coeficientes que cuando se usa **ivreg()**. Sin embargo, los errores estándar informados para la regresión de la segunda etapa, por ejemplo, mediante **coeftest()** o **summary()**, son *inválidos*: Ninguno se ajusta para usar predicciones de la regresión de la primera etapa como regresores en la regresión de la segunda etapa. Afortunadamente, **ivreg()** realiza el ajuste necesario automáticamente. Esta es otra ventaja sobre la estimación manual paso a paso que se ha realizado anteriormente para demostrar la mecánica del procedimiento.
  
2. Al igual que en la regresión múltiple, es importante calcular los errores estándar robustos a la heterocedasticidad como lo se hizo anteriormente usando **vcovHC()**.

La estimación de MC2E para $\beta1$ en \@ref(eq:ecigsMC2E) sugiere que un aumento en los precios de los cigarrillos en un uno por ciento reduce el consumo de cigarrillos en aproximadamente $1.08$ puntos porcentuales, lo cual es bastante elástico. Sin embargo, se debe tener en cuenta que esta estimación podría no ser confiable a pesar de que se usó la estimación de VI: Aún puede haber un sesgo debido a las variables omitidas. Por lo tanto, se necesita un enfoque de regresión de VI múltiple.

## El modelo de regresión VI general {#MRVIG}

El modelo de regresión VI simple se extiende fácilmente a un modelo de regresión múltiple al que se debe referir como modelo de regresión VI general. En este modelo se distingue entre cuatro tipos de variables: La variable dependiente, incluidas las variables exógenas, incluidas las endógenas y las instrumentales. El Concepto clave 12.1 resume el modelo y la terminología común.

```{r, 660, eval = my_output == "html", results='asis', echo=F, purl=F}
cat('
<div class = "keyconcept" id="KC12.1">
<h3 class = "right"> Concepto clave 12.1 </h3>
<h3 class = "left"> Terminología y modelo de regresión de variables instrumentales generales </h3>

\\begin{align}
  Y_i = \\beta_0 + \\beta_1 X_{1i} + \\dots + \\beta_k X_{ki} + \\beta_{k+1} W_{1i} + \\dots + \\beta_{k+r} W_{ri} + u_i, (\\#eq:givmodel)
\\end{align}

con $i=1,\\dots,n$ es el modelo de regresión de variables instrumentales generales donde:

- $Y_i$ es la variable dependiente.

- $\\beta_0,\\dots,\\beta_{k+1}$ son $1+k+r$ coeficientes de regresión desconocidos.

- $X_{1i},\\dots,X_{ki}$ son $k$ regresores endógenos.

- $W_{1i},\\dots,W_{ri}$ son $r$ regresores exógenos que no están correlacionados con $u_i$.

- $u_i$ es el término de error.

- $Z_{1i},\\dots,Z_{mi}$ son $m$ variables instrumentales.

Los coeficientes están sobreidentificados si $m>k$. Si $m<k$, los coeficientes están subidentificados y cuando $m=k$ están exactamente identificados. Para la estimación del modelo de regresión VI, se requiere una identificación exacta o una sobreidentificación.

</div>
')
```

```{r, 661, eval = my_output == "latex", results='asis', echo=F, purl=F}
cat('\\begin{keyconcepts}[Terminología y modelo de regresión de variables instrumentales generales]{12.1}

\\begin{align}
  Y_i = \\beta_0 + \\beta_1 X_{1i} + \\dots + \\beta_k X_{ki} + \\beta_{k+1} W_{1i} + \\dots + \\beta_{k+r} W_{ri} + u_i, \\label{eq:givmodel}
\\end{align}

con $i=1,\\dots,n$ es el modelo de regresión de variables instrumentales generales donde:\\newline

\\begin{itemize}
\\item $Y_i$ es la variable dependiente.
\\item $\\beta_0,\\dots,\\beta_{k+1}$ son $1+k+r$ coeficientes de regresión desconocidos.
\\item $X_{1i},\\dots,X_{ki}$ son $k$ regresores endógenos.
\\item $W_{1i},\\dots,W_{ri}$ son $r$ regresores exógenos que no están correlacionados con $u_i$.
\\item $u_i$ es el término de error.
\\item $Z_{1i},\\dots,Z_{mi}$ son $m$ variables instrumentales.
\\end{itemize}\\vspace{0.5cm}

Los coeficientes están sobreidentificados si $m>k$. Si $m<k$, los coeficientes están subidentificados y cuando $m=k$ están exactamente identificados. Para la estimación del modelo de regresión VI, se requiere una identificación exacta o una sobreidentificación.

\\end{keyconcepts}
')
```

Si bien calcular ambas etapas de MC2E individualmente no es un gran problema en \@ref(eq:srm12), el modelo de regresión simple con un solo regresor endógeno, el Concepto clave 12.2, aclara por qué recurrir a funciones de MC2E como **ivreg()** son más conveniente cuando el conjunto de regresores (e instrumentos) potencialmente endógenos es grande.

La estimación de modelos de regresión con MC2E utilizando múltiples instrumentos mediante **ivreg()** es sencilla. Sin embargo, existen algunas sutilezas al especificar correctamente la fórmula de regresión.

Suponga que desea estimar el modelo $$Y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + W_{1i} + u_i$$ donde $X_{1i}$ y $X_{2i}$ son regresores endógenos que serán instrumentados por $Z_{1i}$, $Z_{2i}$ y $Z_{3i}$ y $W_{1i}$  es un regresor exógeno. Los datos correspondientes están disponibles en un **data.frame** con nombres de columna **y**, **x1**, **x1**, **w1**, **z1**, **z2** y **z3**. Puede ser tentador especificar el argumento **formula** en la llamada de **ivreg()** como **y ~ x1 + x2 + w1 | z1 + z2 + z3** lo cual es incorrecto. Como se explica en la documentación de **ivreg()** (ver `?Ivreg`), también es necesario enumerar *todas las variables exógenas* como instrumentos; es decir, unirlas con **+** a la derecha de la barra vertical: **y ~ x1 + x2 + w1 | w1 + z1 + z2 + z3** donde **w1** se "instrumenta a sí misma".

Si existe una gran cantidad de variables exógenas, puede ser conveniente proporcionar una fórmula de actualización con **.** (esto incluye todas las variables excepto la variable dependiente) justo después de **|** y excluir todas las variables endógenas usando un **-**. Por ejemplo, si existe un regresor exógeno **w1** y un regresor endógeno **x1** con el instrumento **z1**, la fórmula apropiada sería **y ~ w1 + x1 | w1 + z1** que es equivalente a **y ~ w1 + x1 |. - x1 + z1**.

```{r, 662, eval = my_output == "html", results='asis', echo=F, purl=F}
cat('
<div class = "keyconcept" id="KC12.2">
<h3 class = "right"> Concepto clave 12.2 </h3>
<h3 class = "left"> Mínimos cuadrados de dos etapas </h3>

De manera similar al modelo de regresión VI simple, el modelo general VI \\@ref(eq:givmodel) se puede estimar usando el estimador de mínimos cuadrados de dos etapas:

1. **Regresión(es) de primera etapa** 

    Ejecutar una regresión MCO para cada una de las variables endógenas ($X_{1i},\\dots,X_{ki}$) en todas las variables instrumentales ($Z_{1i},\\dots,Z_{mi}$), todas variables exógenas ($W_{1i},\\dots,W_{ri}$) y una intersección. Calcular los valores ajustados ($\\widehat{X}_{1i},\\dots,\\widehat{X}_{ki}$).

2. **Regresión de segunda etapa**

    Regresar la variable dependiente en los valores predichos de todos los regresores endógenos, todas las variables exógenas y una intersección usando MCO. Esto da $\\widehat{\\beta}_{0}^{MC2E},\\dots,\\widehat{\\beta}_{k+r}^{MC2E}$, las estimaciones de MC2E de los coeficientes del modelo.

</div>
')
```

```{r, 663, eval = my_output == "latex", results='asis', echo=F, purl=F}
cat('\\begin{keyconcepts}[Mínimos cuadrados de dos etapas]{12.2}

De manera similar al modelo de regresión VI simple, el modelo general VI \\@ref(eq:givmodel) se puede estimar usando el estimador de mínimos cuadrados de dos etapas:\\newline

\\begin{itemize}
\\item \\textbf{Regresión(es) de primera etapa}\\newline Ejecutar una regresión MCO para cada una de las variables endógenas ($X_{1i},\\dots,X_{ki}$) en todas las variables instrumentales ($Z_{1i},\\dots,Z_{mi}$), todas variables exógenas ($W_{1i},\\dots,W_{ri}$) y una intersección. Calcular los valores ajustados ($\\widehat{X}_{1i},\\dots,\\widehat{X}_{ki}$).\\newline 
\\item \\textbf{Regresión de segunda etapa}\\newline Regresar la variable dependiente en los valores predichos de todos los regresores endógenos, todas las variables exógenas y una intersección usando MCO. Esto da $\\widehat{\\beta}_{0}^{MC2E},\\dots,\\widehat{\\beta}_{k+r}^{MC2E}$, las estimaciones de MC2E de los coeficientes del modelo.
\\end{itemize}
\\end{keyconcepts}
')
```

En el modelo de regresión general VI, los supuestos de relevancia y exogeneidad del instrumento son los mismos que en el modelo de regresión simple con un solo regresor endógeno y un solo instrumento. Consular el Concepto clave 12.3 para obtener un resumen utilizando la terminología de regresión general VI.

```{r, 664, eval = my_output == "html", results='asis', echo=F, purl=F}
cat('
<div class = "keyconcept" id="KC12.3">
<h3 class = "right"> Concepto clave 12.3 </h3>
<h3 class = "left"> Dos condiciones para instrumentos válidos </h3>

Para que $Z_{1i},\\dots,Z_{mi}$ sea un conjunto de instrumentos válidos, se deben cumplir las dos condiciones siguientes:

1. **Relevancia del instrumento**:

    Si existen $k$ variables endógenas, $r$ variables exógenas y $m\\geq k$ instrumentos $Z$ y $\\widehat{X}_{1i}^*,\\dots,\\widehat{X}_{ki}^*$ son los valores predichos de las regresiones de la primera etapa de la población $k$, debe sostenerse que $$(\\widehat{X}_{1i}^*,\\dots,\\widehat{X}_{ki}^*, W_{1i}, \\dots, W_{ri},1)$$ no son perfectamente multicolineales. $1$ denota el regresor constante que es igual a $1$ para todas las observaciones.

    *Nota*: Si solo hay un regresor endógeno $X_i$, debe haber al menos un coeficiente distinto de cero en el $Z$ y el $W$ en la regresión poblacional para que esta condición sea válida: Si todos los los coeficientes son cero, todos los $\\widehat{X}^*_i$ son solo la media de $X$, de modo que existe una multicolinealidad perfecta.

2. **Exogeneidad del instrumento**:

    Todos los instrumentos de $m$ no deben estar correlacionados con el término de error, $$\\rho_{Z_{1i},u_i} = 0,\\dots,\\rho_{Z_{mi},u_i} = 0.$$

</div>
')
```

```{r, 665, eval = my_output == "latex", results='asis', echo=F, purl=F}
cat('\\begin{keyconcepts}[Dos condiciones para instrumentos válidos]{12.3}

Para que $Z_{1i},\\dots,Z_{mi}$ sea un conjunto de instrumentos válidos, se deben cumplir las dos condiciones siguientes:\\newline

\\begin{enumerate}
\\item \\textbf{Relevancia del instrumento}\\newline Si existen $k$ variables endógenas, $r$ variables exógenas y $m\\geq k$ instrumentos $Z$ y $\\widehat{X}_{1i}^*,\\dots,\\widehat{X}_{ki}^*$ son los valores predichos de las regresiones de la primera etapa de la población $k$, debe sostenerse que $$(\\widehat{X}_{1i}^*,\\dots,\\widehat{X}_{ki}^*, W_{1i}, \\dots, W_{ri},1)$$ no son perfectamente multicolineales. $1$ denota el regresor constante que es igual a $1$ para todas las observaciones.\\newline

\\textit{Nota}: Si solo hay un regresor endógeno $X_i$, debe haber al menos un coeficiente distinto de cero en el $Z$ y el $W$ en la regresión poblacional para que esta condición sea válida: Si todos los los coeficientes son cero, todos los $\\widehat{X}^*_i$ son solo la media de $X$, de modo que existe una multicolinealidad perfecta.\\newline

\\item \\textbf{Exogeneidad del instrumento}\\newline

Todos los instrumentos de $m$ no deben estar correlacionados con el término de error, $$\\rho_{Z_{1i},u_i} = 0,\\dots,\\rho_{Z_{mi},u_i} = 0.$$

\\end{enumerate}
\\end{keyconcepts}
')
```

Se puede demostrar que si se cumplen los supuestos de regresión VI presentados en el Concepto clave 12.4, el estimador MC2E en \@ref(eq:givmodel) es consistente y se distribuye normalmente cuando el tamaño de la muestra es grande. El razonamiento detrás de esto se traslada al modelo VI general.

Para el propósito actual, es suficiente tener en cuenta que la validez de los supuestos establecidos en el Concepto clave 12.4 permiten obtener inferencias estadísticas válidas utilizando funciones **R** que calculan las pruebas $t$ y $F$, así como los intervalos de confianza para los coeficientes del modelo.

```{r, 666, eval = my_output == "html", results='asis', echo=F, purl=F}
cat('
<div class = "keyconcept" id="KC12.4">
<h3 class = "right"> Concepto clave 12.4 </h3>
<h3 class = "left"> Los supuestos de regresión VI </h3>

Para el modelo de regresión general VI en el Concepto clave 12.1 se asumió lo siguiente:

1. $E(u_i\\vert W_{1i}, \\dots, W_{ri}) = 0.$

2. $(X_{1i},\\dots,X_{ki},W_{1i},\\dots,W_{ri},Z_{1i},\\dots,Z_{mi})$ son i.i.d. a partir de su distribución conjunta.

3. Todas las variables tienen cuartos momentos finitos distintos de cero; es decir, es poco probable que existan valores atípicos.

4. Los $Z$ son instrumentos válidos (ver Concepto clave 12.3).

</div>
')
```

```{r, 667, eval = my_output == "latex", results='asis', echo=F, purl=F}
cat('\\begin{keyconcepts}[Los supuestos de regresión VI]{12.4}

Para el modelo de regresión general VI en el Concepto clave 12.1 se asumió lo siguiente:\\newline

\\begin{enumerate}
\\item $E(u_i\\vert W_{1i}, \\dots, W_{ri}) = 0.$
\\item $(X_{1i},\\dots,X_{ki},W_{1i},\\dots,W_{ri},Z_{1i},\\dots,Z_{mi})$ son i.i.d. a partir de su distribución conjunta.
\\item Todas las variables tienen cuartos momentos finitos distintos de cero; es decir, es poco probable que existan valores atípicos.
\\item Los $Z$ son instrumentos válidos (ver Concepto clave 12.3).
\\end{enumerate}

\\end{keyconcepts}
')
```

#### Aplicación a la demanda de cigarrillos {-}

La elasticidad estimada de la demanda de cigarrillos en \@ref(eq:srm12) es $1.08$. Aunque \@ref(eq:srm12) se estimó mediante la regresión VI, es plausible que la estimación VI esté sesgada: En este modelo, el estimador MC2E es inconsistente para el verdadero $\beta_1$ si el instrumento (el impuesto real sobre las ventas por paquete) se correlaciona con el término de error. Es probable que este sea el caso, ya que existen factores económicos, como los ingresos estatales, que afectan la demanda de cigarrillos y se correlacionan con el impuesto a las ventas. Los estados con altos ingresos personales tienden a generar ingresos fiscales mediante impuestos sobre la renta y menos mediante impuestos sobre las ventas. En consecuencia, los ingresos estatales deben incluirse en el modelo de regresión.

\begin{align}
  \log(Q_i^{cigarettes}) = \beta_0 + \beta_1 \log(P_i^{cigarettes}) + \beta_2 \log(income_i) + u_i (\#eq:mcigsMC2E1)
\end{align}

Antes de estimar \@ref(eq:mcigsMC2E1) usando **ivreg()**, se define $income$ como ingresos reales per cápita **rincome** y los se agrega al conjunto de datos **CigarettesSW**.

```{r, 668}
# agregar rincome al conjunto de datos
CigarettesSW$rincome <- with(CigarettesSW, income / population / cpi)

c1995 <- subset(CigarettesSW, year == "1995")
```

```{r, 669}
# estimar el modelo
cig_ivreg2 <- ivreg(log(packs) ~ log(rprice) + log(rincome) | log(rincome) + 
                    salestax, data = c1995)

coeftest(cig_ivreg2, vcov = vcovHC, type = "HC1")
```

Se obtiene:

\begin{align}
  \widehat{\log(Q_i^{cigarettes})} = \underset{(1.26)}{9.42} - \underset{(0.37)}{1.14} \log(P_i^{cigarettes}) + \underset{(0.31)}{0.21} \log(income_i). (\#eq:emcigsMC2E2)
\end{align}

Se agregan los impuestos específicos a los cigarrillos ($cigtax_i$) como una variable instrumental adicional y se estima nuevamente usando MC2E.

```{r, 670}
# agregar cigtax al conjunto de datos
CigarettesSW$cigtax <- with(CigarettesSW, tax/cpi)

c1995 <- subset(CigarettesSW, year == "1995")
```

```{r, 671}
# estimar el modelo
cig_ivreg3 <- ivreg(log(packs) ~ log(rprice) + log(rincome) | 
                    log(rincome) + salestax + cigtax, data = c1995)

coeftest(cig_ivreg3, vcov = vcovHC, type = "HC1")
```

Usando los dos instrumentos $salestax_i$ y $cigtax_i$ se tiene que $m = 2$ y $k = 1$, por lo que el coeficiente del regresor endógeno $\log(P_i^{cigarettes})$ es *sobreidentificado*. La estimación de MC2E de \@ref(eq:mcigsMC2E1) es

\begin{align}
  \widehat{\log(Q_i^{cigarettes})} = \underset{(0.96)}{9.89} - \underset{(0.25)}{1.28} \log(P_i^{cigarettes}) + \underset{(0.25)}{0.28} \log(income_i). (\#eq:emcigsMC2E3)
\end{align}

¿Debería confiar en las estimaciones presentadas en \@ref(eq:emcigsMC2E2) o más bien confiar en \@ref(eq:emcigsMC2E3)? Las estimaciones obtenidas con ambos instrumentos son más precisas ya que en \@ref(eq:emcigsMC2E3) todos los errores estándar reportados son menores que en \@ref(eq:emcigsMC2E2). De hecho, el error estándar para la estimación de la elasticidad de la demanda es solo dos tercios del error estándar cuando el impuesto a las ventas es el único instrumento utilizado. Esto se debe a que se está utilizando más información en la estimación \@ref(eq:emcigsMC2E3). *Si* los instrumentos son válidos, \@ref(eq:emcigsMC2E3) puede considerarse más confiable.

Sin embargo, sin conocimientos sobre la validez de los instrumentos, no es sensato hacer tal afirmación. Esto enfatiza por qué es esencial verificar la validez del instrumento. El capítulo \@ref(CVI) analiza brevemente las pautas para verificar la validez de un instrumento y presenta enfoques que permiten probar la relevancia y exogeneidad del instrumento bajo ciertas condiciones. Luego se utilizan en una aplicación a la demanda de cigarrillos en el Capítulo \@ref(ADC).

## Comprobación de la validez del instrumento {#CVI}

#### Relevancia del instrumento {-}

Los instrumentos que explican poca variación en el regresor endógeno $X$ se denominan *instrumentos débiles*. Los instrumentos débiles proporcionan poca información sobre la variación en $X$ que es aprovechada por la regresión VI para estimar el efecto de interés: La estimación del coeficiente sobre el regresor endógeno se estima de manera inexacta. Además, los instrumentos débiles hacen que la distribución del estimador se desvíe considerablemente de una distribución normal incluso en muestras grandes, de modo que los métodos habituales para obtener inferencias sobre el coeficiente verdadero en $X$ pueden producir resultados erróneos.

```{r, 672, eval = my_output == "html", results='asis', echo=F, purl=F}
cat('
<div class = "keyconcept" id="KC12.5">
<h3 class = "right"> Concepto clave 12.5 </h3>
<h3 class = "left"> Una regla general para comprobar si hay instrumentos débiles </h3>

Considere el caso de un solo regresor endógeno $X$ y $m$ instrumentos $Z_1,\\dots,Z_m$. Si los coeficientes de todos los instrumentos en la regresión de la primera etapa de la población de una estimación MC2E son cero, los instrumentos no explican ninguna de las variaciones en el $X$ que claramente viola el supuesto 1 del Concepto clave 12.2. Si bien es poco probable que este último caso se encuentre en la práctica, se debería preguntar "en qué medida" debe cumplirse el supuesto de pertinencia del instrumento.

Si bien esto es difícil de responder para la regresión VI general, en el caso de un regresor endógeno *único* $X$ uno puede usar la siguiente regla empírica:

Calcular el estadístico $F$ que corresponde a la hipótesis de que los coeficientes en $Z_1,\\dots,Z_m$ son todos cero en la regresión de la primera etapa. Si el estadístico $F$ es menor que $10$, los instrumentos son débiles de tal manera que la estimación MC2E del coeficiente en $X$ está sesgada y no se puede hacer una inferencia estadística válida sobre su valor real.

</div>
')
```

```{r, 673, eval = my_output == "latex", results='asis', echo=F, purl=F}
cat('\\begin{keyconcepts}[Una regla general para comprobar si hay instrumentos débiles]{12.5}
Consider the case of a single endogenous regressor $X$ and $m$ instruments $Z_1,\\dots,Z_m$. If the coefficients on all instruments in the population first-stage regression of a MC2E estimation are zero, the instruments do not explain any of the variation in the $X$ which clearly violates assumption 1 of Key Concept 12.2. Although the latter case is unlikely to be encountered in practice, we should ask ourselves to what extent the assumption of instrument relevance should be fulfilled.\\newline 

Si bien esto es difícil de responder para la regresión VI general, en el caso de un regresor endógeno \\textit{único} $X$ uno puede usar la siguiente regla empírica:\\newline

Calcular el estadístico $F$ que corresponde a la hipótesis de que los coeficientes en $Z_1,\\dots,Z_m$ son todos cero en la regresión de la primera etapa. Si el estadístico $F$ es menor que $10$, los instrumentos son débiles de tal manera que la estimación MC2E del coeficiente en $X$ está sesgada y no se puede hacer una inferencia estadística válida sobre su valor real.

\\end{keyconcepts}
')
```

La regla general del Concepto clave 12.5 se implementa fácilmente en **R**. Ejecute la regresión de la primera etapa usando **lm()** y luego calcule el estadístico $F$ robusto a la heterocedasticidad mediante **linearHypothesis()**. Esto es parte de la aplicación a la demanda de cigarrillos discutida en el Capítulo \@ref(ADC).

#### Si los instrumentos son débiles {-}

Existen dos formas de proceder si los instrumentos son débiles:

1. Desechar los instrumentos débiles y/o buscar instrumentos más fuertes. Mientras que lo primero es solo una opción si los coeficientes desconocidos permanecen identificados cuando se descartan los instrumentos débiles, lo segundo puede ser muy difícil e incluso puede requerir un rediseño de todo el estudio.

2. Seguir con los instrumentos débiles pero usando métodos que mejoren el MC2E en este escenario; por ejemplo, estimación de máxima verosimilitud de información limitada.

#### Cuando se infringe la suposición de exogeneidad del instrumento {-}

Si hay correlación entre un instrumento y el término de error, la regresión VI no es consistente. La prueba de restricciones de sobreidentificación (también llamada prueba $J$) es un enfoque para probar la hipótesis de que los instrumentos *adicionales* son exógenos. Para que la prueba $J$ sea aplicable es necesario que haya *más* instrumentos que regresores endógenos. La prueba $J$ se resume en el Concepto clave 12.5.

```{r, 674, eval = my_output == "html", results='asis', echo=F, purl=F}
cat('
<div class = "keyconcept" id="KC12.6">
<h3 class = "right"> Concepto clave 12.6 </h3>
<h3 class = "left"> Estadístico $J$ / Prueba de sobreidentificación de restricciones </h3>

Tome $\\widehat{u}_i^{MC2E} \\ , \\ i = 1,\\dots,n$, los residuos de la estimación MC2E del modelo de regresión general VI \\@ref(eq:givmodel). Ejecute la regresión MCO.

\\begin{align}
  \\widehat{u}_i^{MC2E} =& \\, \\delta_0 + \\delta_1 Z_{1i} + \\dots + \\delta_m Z_{mi} + \\delta_{m+1} W_{1i} + \\dots + \\delta_{m+r} W_{ri} + e_i (\\#eq:jstatreg)
\\end{align}

y probar la hipótesis conjunta $$H_0: \\delta_1 = 0, \\dots, \\delta_{m} = 0$$ que establece que todos los instrumentos son exógenos. Esto se puede hacer usando el estadístico $F$ correspondiente calculando  $$J = mF.$$ Esta prueba es la prueba de restricciones de sobreidentificación y la estadística se llama estadístico $J$ con $$J \\sim \\chi^2_{m-k}$$ en muestras grandes bajo la hipótesis nula y el supuesto de homocedasticidad. Los grados de libertad $m-k$ indican el grado de sobreidentificación, ya que este es el número de instrumentos $m$ menos el número de regresores endógenos $k$.

</div>
')
```

```{r, 675, eval = my_output == "latex", results='asis', echo=F, purl=F}
cat('\\begin{keyconcepts}[Estadístico $J$ / Prueba de sobreidentificación de restricciones]{12.6}

Tome $\\widehat{u}_i^{MC2E} \\ , \\ i = 1,\\dots,n$, los residuos de la estimación MC2E del modelo de regresión general VI \\@ref(eq:givmodel). Ejecute la regresión MCO.

\\begin{align}
  \\widehat{u}_i^{MC2E} =& \\, \\delta_0 + \\delta_1 Z_{1i} + \\dots + \\delta_m Z_{mi} + \\delta_{m+1} W_{1i} + \\dots + \\delta_{m+r} W_{ri} + e_i \\label{eq:jstatreg}
\\end{align}

y probar la hipótesis conjunta $$H_0: \\delta_1 = 0, \\dots, \\delta_{m} = 0$$ que establece que todos los instrumentos son exógenos. Esto se puede hacer usando el estadístico $F$ correspondiente calculando  $$J = mF.$$ Esta prueba es la prueba de restricciones de sobreidentificación y la estadística se llama estadístico $J$ con $$J \\sim \\chi^2_{m-k}$$ en muestras grandes bajo la hipótesis nula y el supuesto de homocedasticidad. Los grados de libertad $m-k$ indican el grado de sobreidentificación, ya que este es el número de instrumentos $m$ menos el número de regresores endógenos $k$.

\\end{keyconcepts}
')
```

Es importante notar que el estadístico $J$ discutido en el Concepto clave 12.6 solo está distribuido en $\chi^2_{m-k}$ cuando el término de error $e_i$ en la regresión \@ref(eq:jstatreg) es homoscedástico. Una discusión del estadístico $J$ robusto a la heterocedasticidad está más allá del alcance de este capítulo. 

En cuanto al procedimiento que se muestra en el Concepto clave 12.6, la aplicación en la siguiente sección muestra cómo aplicar la prueba $J$ usando **linearHypothesis()**.

## Aplicación a la demanda de cigarrillos {#ADC}

¿Son válidos el impuesto general sobre las ventas y el impuesto específico a los cigarrillos? De lo contrario, MC2E no es útil para estimar la elasticidad de la demanda de cigarrillos discutida en el Capítulo \@ref(MRVIG). Como se discutió en el Capítulo \@ref(EVIURUI), es probable que ambas variables sean relevantes, pero si son exógenas es una cuestión diferente.

Se sostiene que los impuestos específicos a los cigarrillos podrían ser endógenos porque podrían haber factores históricos específicos del estado, como la importancia económica del cultivo de tabaco y la industria de producción de cigarrillos, que presionan para obtener impuestos bajos específicos de los cigarrillos. Dado que es plausible que los estados productores de tabaco tengan tasas de tabaquismo más altas que otros, esto conduciría a la endogeneidad de los impuestos específicos a los cigarrillos. Si se tuvieran datos sobre el tamaño de la industria del tabaco y los cigarrillos, se podría resolver este problema potencial al incluir la información en la regresión. Desafortunadamente, este no es el caso.

Sin embargo, dado que el papel de la industria del tabaco y los cigarrillos es un factor que se puede asumir que difiere entre los estados, pero no con el tiempo; en su lugar, se puede aprovechar la estructura del panel de **CigarettesSW**: Como se muestra en el Capítulo \@ref(DPDPTCAD), cuando la regresión usa datos sobre *cambios* entre dos períodos de tiempo elimina tales efectos específicos del estado e invariantes en el tiempo. Se consideran los cambios en las variables entre 1985 y 1995. Es decir, interesa estimar la *elasticidad a largo plazo* de la demanda de cigarrillos.

El modelo que será estimado por MC2E utiliza el impuesto general a las ventas y el impuesto a las ventas específicas de cigarrillos como instrumentos, por lo tanto, es

\begin{align}
\begin{split}
  \log(Q_{i,1995}^{cigarettes}) - \log(Q_{i,1995}^{cigarettes}) =& \, \beta_0 + \beta_1 \left[\log(P_{i,1995}^{cigarettes}) - \log(P_{i,1985}^{cigarettes}) \right] \\ &+ \beta_2 \left[\log(income_{i,1995}) - \log(income_{i,1985})\right] + u_i. \end{split}(\#eq:diffivreg)
\end{align}

Primero se crean diferencias de 1985 a 1995 para la variable dependiente, los regresores y ambos instrumentos.

```{r, 676}
# subconjunto de los datos para el año 1985
c1985 <- subset(CigarettesSW, year == "1985")

# definir diferencias en variables
packsdiff <- log(c1995$packs) - log(c1985$packs)

pricediff <- log(c1995$price/c1995$cpi) - log(c1985$price/c1985$cpi)

incomediff <- log(c1995$income/c1995$population/c1995$cpi) -
log(c1985$income/c1985$population/c1985$cpi)

salestaxdiff <- (c1995$taxs - c1995$tax)/c1995$cpi - (c1985$taxs - c1985$tax)/c1985$cpi

cigtaxdiff <- c1995$tax/c1995$cpi - c1985$tax/c1985$cpi
```

Ahora se realizan tres estimaciones de VI diferentes de \@ref(eq:diffivreg) usando **ivreg()**:

1. MC2E utilizando solo la diferencia en los impuestos sobre las ventas entre 1985 y 1995 como instrumento.

2. MC2E utilizando solo la diferencia en los impuestos a las ventas específicos de cigarrillos 1985 y 1995 como instrumento.

3. MC2E utilizando como instrumentos tanto la diferencia en los impuestos a las ventas de 1985 y 1995 como la diferencia en los impuestos a las ventas específicas de cigarrillos de 1985 y 1995.

```{r, 677}
# estimar los tres modelos
cig_ivreg_diff1 <- ivreg(packsdiff ~ pricediff + incomediff | incomediff + 
                         salestaxdiff)

cig_ivreg_diff2 <- ivreg(packsdiff ~ pricediff + incomediff | incomediff + 
                         cigtaxdiff)

cig_ivreg_diff3 <- ivreg(packsdiff ~ pricediff + incomediff | incomediff + 
                         salestaxdiff + cigtaxdiff)
```

Como de costumbre, se usa **coeftest()** junto con **vcovHC()** para obtener resúmenes robustos de coeficientes para todos los modelos.

```{r, 678}
# resumen robusto de coeficientes para 1.
coeftest(cig_ivreg_diff1, vcov = vcovHC, type = "HC1")

# resumen robusto de coeficientes para 2.
coeftest(cig_ivreg_diff2, vcov = vcovHC, type = "HC1")

# resumen robusto de coeficientes para 3.
coeftest(cig_ivreg_diff3, vcov = vcovHC, type = "HC1")
```

Se procede a generar un resumen tabulado de los resultados de la estimación utilizando **stargazer())**.

```{r, 679, eval = F}
# recopilar errores estándar robustos en una lista
rob_se <- list(sqrt(diag(vcovHC(cig_ivreg_diff1, type = "HC1"))),
               sqrt(diag(vcovHC(cig_ivreg_diff2, type = "HC1"))),
               sqrt(diag(vcovHC(cig_ivreg_diff3, type = "HC1"))))

# generar tabla
stargazer(cig_ivreg_diff1, cig_ivreg_diff2,cig_ivreg_diff3,
  header = FALSE, 
  type = "html",
  omit.table.layout = "n",
  digits = 3, 
  column.labels = c("IV: Impuesto sobre las ventas", "IV: Impuesto sobre los cigarros", "IV: Impuesto sobre las ventas, impuesto sobre el consumo"),
  dep.var.labels.include = FALSE,
  dep.var.caption = "Variable dependiente: Diferencia 1985-1995 en el precio de registro por paquete",
  se = rob_se)
```

<!--html_preserve-->

```{r, 680, message=F, warning=F, results='asis', echo=F, purl=F, eval=my_output == "html"}
# recopilar errores estándar robustos en una lista
rob_se <- list(sqrt(diag(vcovHC(cig_ivreg_diff1, type = "HC1"))),
               sqrt(diag(vcovHC(cig_ivreg_diff2, type = "HC1"))),
               sqrt(diag(vcovHC(cig_ivreg_diff3, type = "HC1"))))

stargazer(cig_ivreg_diff1, cig_ivreg_diff2,cig_ivreg_diff3,
  header = FALSE, 
  type = "html",
  digits = 3, 
  column.labels = c("IV: Impuesto sobre las ventas", "IV: Impuesto sobre los cigarros", "IV: Impuesto sobre las ventas, impuesto sobre el consumo"),
  dep.var.labels.include = FALSE,
  dep.var.caption = "Variable dependiente: Diferencia 1985-1995 en el precio de registro por paquete",
  se = rob_se)

stargazer_html_title("Estimaciones de MC2E de la elasticidad a largo plazo de la demanda de cigarrillos utilizando datos de panel", "MC2Eeotlteotdfcupd")
```

<!--/html_preserve-->

```{r, 681, message=F, warning=F, results='asis', eval=my_output == "latex", echo=F, purl=F}
library(stargazer)

stargazer(cig_ivreg_diff1, cig_ivreg_diff2,cig_ivreg_diff3,
  title = "\\label{tab:MC2Eeotlteotdfcupd} Estimaciones de MC2E de la elasticidad a largo plazo de la demanda de cigarrillos utilizando datos de panel",
  header = F, 
  digits = 3,
  type = "latex",
  no.space = T,
  column.sep.width = "35pt",
  omit.table.layout = "n",
  column.labels = c("IV: Impuesto sobre las ventas", "IV: Impuesto sobre los cigarros", "IV: Impuesto sobre las ventas, impuesto sobre el consumo"),
  dep.var.labels.include = FALSE,
  dep.var.caption = "Variable dependiente: Diferencia 1985-1995 en el precio de registro por paquete",
  se = rob_se)
```

La tabla \@ref(pestaña:MC2Eeotlteotdfcupd) informa estimaciones negativas del coeficiente de **priceiff** que son bastante diferentes en magnitud. ¿En cuál se debe confiar? Esto depende de la validez de los instrumentos utilizados. Para evaluar esto, se calculan los estadísticos $F$ para las regresiones de la primera etapa de los tres modelos para verificar la relevancia del instrumento.

```{r, 682}
# regresiones de primera etapa
mod_relevance1 <- lm(pricediff ~ salestaxdiff + incomediff)
mod_relevance2 <- lm(pricediff ~ cigtaxdiff + incomediff)
mod_relevance3 <- lm(pricediff ~ incomediff + salestaxdiff + cigtaxdiff)
```

```{r, 683}
# comprobar la relevancia del instrumento para el modelo (1)
linearHypothesis(mod_relevance1, 
                 "salestaxdiff = 0", 
                 vcov = vcovHC, type = "HC1")
```

```{r, 684}
# comprobar la relevancia del instrumento para el modelo (2)
linearHypothesis(mod_relevance2, 
                 "cigtaxdiff = 0", 
                 vcov = vcovHC, type = "HC1")
```

```{r, 685}
# comprobar la relevancia del instrumento para el modelo (3)
linearHypothesis(mod_relevance3, 
                 c("salestaxdiff = 0", "cigtaxdiff = 0"), 
                 vcov = vcovHC, type = "HC1")
```

También se realiza la prueba de restricciones de sobreidentificación para el modelo tres, que es el único modelo en que el coeficiente de la diferencia en los logaritmos de los precios está sobreidentificado ($m = 2$, $k = 1$) de modo que se pueda calcular el estadístico de $J$. Para hacer esto, se toman los residuos almacenados en **cig_ivreg_diff3** y se retroceden en ambos instrumentos y en el regresor presuntamente exógeno **incomediff**. De nuevo, se usa **linearHypothesis()** para probar si los coeficientes en ambos instrumentos son cero, lo cual es necesario para que se cumpla el supuesto de exogeneidad. Se debe tener en cuenta que con **test = "Chisq"** se obtiene una estadística de prueba distribuida chi-cuadrada en lugar de un estadístico de $F$.

```{r, 686}
# calcular el estadístico J
cig_iv_OR <- lm(residuals(cig_ivreg_diff3) ~ incomediff + salestaxdiff + cigtaxdiff)

cig_OR_test <- linearHypothesis(cig_iv_OR, 
                               c("salestaxdiff = 0", "cigtaxdiff = 0"), 
                               test = "Chisq")
cig_OR_test
```

**Precaución**: En este caso, el valor $p$ informado por **linearHypothesis()** es incorrecto porque los grados de libertad se establecen en $2$. Esto difiere del grado de sobreidentificación ($mk = 2-1 = 1$) por lo que el estadístico $J$ esta distribuido $\chi^2_1$, en lugar de seguir una distribución $\chi^2_2$ como se supone de forma predeterminada por **linearHypothesis()**. Se puede calcular el valor $p$ correcto usando **pchisq()**.

```{r, 687}
# calcular el valor p correcto para el estadístico J
pchisq(cig_OR_test[2, 5], df = 1, lower.tail = FALSE)
```

Dado que este valor es menor que $0.05$, se rechaza la hipótesis de que ambos instrumentos son exógenos al nivel de $5\%$. Esto implica uno de los siguientes:

1. El impuesto sobre las ventas es un instrumento inválido para el precio por paquete.
2. El impuesto sobre las ventas específico de cigarrillos es un instrumento inválido para el precio por paquete.
3. Ambos instrumentos son inválidos.

Se puede sostener que es más probable que se mantenga el supuesto de exogeneidad del instrumento para el impuesto general a las ventas, de modo que la estimación VI de la elasticidad a largo plazo de la demanda de cigarrillos que se considera más fiable es $-0.94$, la estimación MC2E obtenida utilizando el impuesto general a las ventas como único instrumento.

La interpretación de esta estimación es que durante un período de 10 años, se espera que un aumento en el precio promedio por paquete en un uno por ciento disminuya el consumo en aproximadamente $0.94$ puntos porcentuales. Esto sugiere que, a largo plazo, los aumentos de precios pueden reducir considerablemente el consumo de cigarrillos.

## ¿De dónde provienen los instrumentos válidos?

Existen muchos enfoques para encontrar instrumentos válidos en la práctica. A continuación se ejemplifican tres preguntas de investigación:

+ ¿Poner a los criminales en la cárcel reduce el crimen?
+ ¿Reducir el tamaño de las clases aumenta los puntajes de las pruebas?
+ ¿El tratamiento agresivo de los infartos prolonga la vida?

Esta sección no está directamente relacionada con las aplicaciones en **R**, por lo que no se discute el contenido aquí. Se le invita a trabajar en estas preguntas por su cuenta.

#### Resumen {-}

La función **ivreg()** del paquete **AER** proporciona funcionalidades convenientes para estimar modelos de regresión VI en **R**. Es una implementación del enfoque de estimación MC2E.

Además de tratar la estimación de VI, también se ha discutido cómo probar instrumentos débiles y cómo realizar una prueba de restricciones de sobreidentificación cuando existan más instrumentos que regresores endógenos que usan **R**.

Una aplicación empírica ha demostrado cómo **ivreg()** se puede utilizar para estimar la elasticidad a largo plazo de la demanda de cigarrillos en función de **CigarettesSW**, un conjunto de datos de panel sobre el consumo de cigarrillos y los indicadores económicos de los 48 estados continentales de EE. UU. para 1985 y 1995. Se utilizaron diferentes conjuntos de instrumentos y se ha argumentado por qué utilizar el impuesto general sobre las ventas como único instrumento es la opción preferida. La estimación de la elasticidad de la demanda que se considera más confiable es de $-0.94$. Esta estimación sugiere que existe un notable efecto negativo a largo plazo sobre el consumo de cigarrillos por el aumento de los precios.

## Ejercicios {#Ejercicios-12}

```{r, 688, echo=F, purl=F, results='asis'}
if (my_output == "html") {
  cat('
<div  class = "DCexercise">

#### 1. Los datos de distancia de la universidad {-}

Existen muchos estudios en economía laboral que tratan el tema de la estimación de las funciones de ingresos del capital humano que establecen cómo los ingresos salariales están determinados por la educación y la experiencia laboral. Un ejemplo destacado es @card1993, que investiga el rendimiento económico de la educación y utiliza la proximidad a la universidad como variable instrumental.

Los ejercicios de este capítulo tratan con el conjunto de datos <tt>CollegeDistance</tt> que es similar a los datos utilizados por @card1993. Proviene de una encuesta de graduados de preparatoria con variables codificadas como salario, educación, matrícula promedio y una serie de medidas socioeconómicas. El conjunto de datos también incluye la distancia desde una universidad mientras los participantes de la encuesta estaban en la escuela preparatoria. <tt>CollegeDistance</tt> viene con el paquete <tt>AER</tt>.

**Instrucciones:**

  + Adjuntar el paquete <tt>AER</tt> y cargar los datos de <tt>CollegeDistance</tt>.

  + Obtener una descripción general del conjunto de datos.

  + La variable <tt>distance</tt> (la distancia a la universidad de 4 años más cercana en 10 millas) servirá como instrumento en ejercicios posteriores. Utilizar un histograma para visualizar la distribución de <tt>distance</tt>.

<iframe src="DCL/ex12_1.html" frameborder="0" scrolling="no" style="width:100%;height:330px"></iframe>

**Sugerencias:**

  + Usar <tt>data()</tt> para adjuntar el conjunto de datos.

  + La función <tt>hist()</tt> se puede utilizar para generar histogramas.

</div>')}
```


```{r, 689, echo=F, purl=F, results='asis'}
if (my_output == "html") {
  cat('
<div  class = "DCexercise">

#### 2. El problema de la selección {-}

Hacer una regresión de <tt>wage</tt> en <tt>education</tt> y las variables de control para estimar la función de ingresos del capital humano es problemático porque la educación no se asigna al azar entre los encuestados: Los individuos toman sus propias decisiones de educación y, por lo tanto, medir las diferencias de las ganancias entre individuos con diferentes niveles de educación dependen de cómo se tomen dichas decisiones. En la literatura, esto se conoce como un *problema de selección*. Este problema de selección implica que <tt>education</tt> es *endógena* por lo que la estimación de MCO estará sesgada y no se puede hacer una inferencia válida respecto al coeficiente verdadero.

En este ejercicio se pide estimar dos regresiones que no arrojan estimaciones confiables del coeficiente de educación debido al problema esbozado anteriormente. A continuación, se compararán los resultados con los obtenidos mediante el enfoque de variables instrumentales aplicado por @card1993.

Se ha adjuntado el paquete <tt>AER</tt>. El conjunto de datos <tt>CollegeDistance</tt> está disponible en el entorno global.

**Instrucciones:**

  + Hacer una regresión del *logaritmo* de <tt>wage</tt> sobre <tt>education</tt>; es decir, estimar el modelo $$\\log(wage_i) = \\beta_0 + \\beta_1 education_i + u_i$$. Guardar el resultado en <tt>wage_mod_1</tt>.

  + Aumentar el modelo incluyendo los regresores <tt>unemp</tt>, <tt>hispanic</tt>, <tt>af-am</tt>, <tt>female</tt> y <tt>urban</tt>. Guardar el resultado en <tt>wage_mod_2</tt>

  + Obtener resúmenes de los coeficientes estimados en ambos modelos.

<iframe src="DCL/ex12_2.html" frameborder="0" scrolling="no" style="width:100%;height:330px"></iframe>

</div>')}
```

```{r, 690, echo=F, purl=F, results='asis'}
if (my_output == "html") {
  cat('
<div  class = "DCexercise">

#### 3. Enfoques de regresión de variables instrumentales --- I {-}

El problema de selección discutido anteriormente hace que las estimaciones de regresión en el ejercicio 2 sean inverosímiles, por lo que @card1993 sugiere una regresión de variables instrumentales que utiliza la distancia de la universidad como un instrumento para la educación.

¿Por qué utilizar la distancia universitaria como instrumento? La lógica detrás de esto es que la distancia de una universidad estará correlacionada con la decisión de obtener un título universitario (relevancia), pero es posible que no prediga los salarios aparte del aumento de la educación (exogeneidad), por lo que la proximidad a la universidad podría considerarse un instrumento válido (recuerde la definición de un instrumento válido indicado al comienzo del Capítulo \\@ref(TIVEWASRAASI)).

Se ha adjuntado el paquete <tt>AER</tt>. El conjunto de datos <tt>CollegeDistance</tt> está disponible en el entorno global.

**Instrucciones:**

  + Calcular las correlaciones del instrumento <tt>distance</tt> con el regresor endógeno <tt>education</tt> y la variable dependiente <tt>wage</tt>.

  + ¿Qué proporción de la variación en <tt>education</tt> se explica por la *regresión de la primera etapa* que usa <tt>distance</tt> como regresor? Guardar el resultado en <tt>R2</tt>.

  + Repetir el ejercicio 2 con la regresión VI; es decir, utilizar <tt>distance</tt> como instrumento para <tt>education</tt> en ambas regresiones usando <tt>ivreg()</tt>. Guardar los resultados en <tt>wage_mod_iv1</tt> y <tt>wage_mod_iv2</tt>. Obtenga resúmenes robustos de coeficientes para ambos modelos.

<iframe src="DCL/ex12_3.html" frameborder="0" scrolling="no" style="width:100%;height:410px"></iframe>

</div>')}
```


```{r, 691, echo=F, purl=F, results='asis'}
if (my_output == "html") {
  cat('
<div  class = "DCexercise">

#### 4. Enfoques de regresión de variables instrumentales --- II {-}

Convénzase usted mismo de que <tt>ivreg()</tt> funciona como se esperaba implementando el algoritmo MC2E presentado en el Concepto clave 12.2 para un solo instrumento, consulte el Capítulo \\@ref(MRVIG).

**Instrucciones:**

  + Completar la función <tt>MC2E()</tt> de manera que implemente el estimador MC2E.

  + Usar <tt>MC2E()</tt> para reproducir las estimaciones de coeficientes obtenidas usando <tt>ivreg()</tt> para ambos modelos del ejercicio 3.

<iframe src="DCL/ex12_4.html" frameborder="0" scrolling="no" style="width:100%;height:460px"></iframe>

**Sugerencias:**

  + La finalización de la función se reduce a reemplazar el <tt>. . .</tt> con argumentos apropiados.

  + Además del conjunto de datos (<tt>data</tt>), la función espera la variable dependiente (<tt>Y</tt>), los regresores exógenos (<tt>W</tt>), los regresores endógenos (<tt>X</tt>) y un instrumento (<tt>Z</tt>) como argumentos. Todos estos deben ser de clase <tt>character</tt>.
  
  + Incluir <tt>W = NULL</tt> en el encabezado de la definición de función asegura que el conjunto de variables exógenas esté vacío, por defecto.

</div>')}
```

```{r, 692, echo=F, purl=F, results='asis'}
if (my_output == "html") {
  cat('
<div  class = "DCexercise">

#### 5. ¿Se debe confiar en los resultados? {-}

Este no es un ejercicio de código real (no existen pruebas de corrección de envío para verificar su código). En su lugar, utilice el widget a continuación para comparar los resultados obtenidos usando las regresiones MCO del Ejercicio 2 con los de las regresiones VI del Ejercicio 3.

El conjunto de datos <tt>CollegeDistance</tt> y todos los objetos del modelo de los Ejercicios 2 y 3 están disponibles en el entorno global.

**Instrucciones:**

  Convénzase de lo siguiente:

  1. Es probable que el sesgo del coeficiente estimado de <tt>education</tt> en el modelo de regresión simple <tt>wage_mod_1</tt> sea sustancial porque el regresor es endógeno debido a la omisión de variables del modelo que se correlacionan con <tt>education</tt> e impactan los ingresos salariales.

  2. Debido al problema de selección descrito en el ejercicio 2, la estimación del coeficiente de interés no es confiable incluso en el modelo de regresión múltiple <tt>wage_mod_2</tt> que incluye varias variables de control socioeconómico. El coeficiente de <tt>education</tt> no es significativo y su estimación es cercana a cero).

  3. Instrumentar la educación por la distancia de la universidad como se hizo en <tt>wage_mod_iv1</tt> produce la estimación VI del coeficiente de interés. Sin embargo, el resultado no debe considerarse confiable porque este modelo simple probablemente adolece de sesgo de variables omitidas al igual que el modelo de regresión múltiple <tt>wage_mod_2</tt> del Ejercicio 2, ver 1. Nuevamente, el coeficiente en <tt>education</tt> no es significativo, su estimación es bastante pequeña.

  4. En el modelo de regresión múltiple <tt>wage_mod_iv2</tt>, donde se incluyeron variables de control demográfico y se instrumenta <tt>education</tt> por <tt>distance</tt> ofrece la estimación más confiable del impacto de la educación sobre la renta salarial entre todos los modelos considerados. El coeficiente es muy significativo y la estimación es de aproximadamente $0.067$. Siguiendo el Concepto clave 8.2, la interpretación es que se espera que un año adicional de escolaridad aumente los ingresos salariales en aproximadamente $0.067 \\cdot 100\\% = 6.7\\%$.

  5. ¿Es confiable la estimación del coeficiente de educación reportada por <tt>wage_mod_iv2</tt>? Esta pregunta no es fácil de responder. En cualquier caso, se debe tener en cuenta que utilizar un enfoque de variables instrumentales es problemático cuando el instrumento es *débil*. Este podría ser el caso aquí: Las familias con una fuerte preferencia por la educación pueden mudarse a vecindarios cercanos a las universidades. Además, los vecindarios cercanos a las universidades pueden tener mercados laborales más fuertes reflejados en ingresos más altos. Tales características invalidarían el instrumento, ya que introducen variables no observadas que influyen en los ingresos pero que no pueden ser capturadas por años de escolaridad, la medida de educación.

<iframe src="DCL/ex12_5.html" frameborder="0" scrolling="no" style="width:100%;height:340px"></iframe>

</div>')}
```

<!--chapter:end:Capitulo_13.Rmd-->

# Experimentos y cuasiexperimentos {#EC}

```{r, echo = F}
options(knitr.duplicate.label = "allow")
```

```{r, 693, child="_setup.Rmd"}
```

```{r, 694, eval=knitr::opts_knit$get("rmarkdown.pandoc.to") == "html", results='asis', echo=FALSE}
cat('<hr style="background-color:#03193b;height:2px">')
```

Este capítulo analiza las herramientas estadísticas que se aplican comúnmente en la evaluación de programas, donde el interés radica en medir los efectos causales de programas, políticas u otras intervenciones. Un diseño de investigación óptimo para este propósito es lo que los estadísticos llaman un experimento controlado aleatorio ideal. La idea básica es asignar sujetos aleatoriamente a dos grupos diferentes, uno que recibe el tratamiento (el grupo de tratamiento) y otro que no (el grupo de control) y comparar los resultados de ambos grupos para obtener una estimación del efecto promedio del tratamiento.

Estos datos *experimentales* son fundamentalmente diferentes de los datos *observacionales*. Por ejemplo, se podría usar un experimento controlado aleatorio para medir cuánto difiere el desempeño de los estudiantes en una prueba estandarizada entre dos clases en las que una tiene una proporción de estudiantes por maestro "regular" y la otra tiene menos estudiantes. Los datos producidos por tal experimento sería diferente de, por ejemplo, los datos de sección transversal observados sobre el desempeño de los estudiantes utilizados en los Capítulos \@ref(RLR) a \@ref(FRNL) donde los tamaños de las clases no se asignan al azar a los estudiantes, sino que son los resultados de una decisión económica donde se equilibraron los objetivos educativos y los aspectos presupuestarios.

Para los economistas, los experimentos controlados aleatorios a menudo son difíciles o incluso irrealizables de implementar. Por ejemplo, debido a razones éticas, morales y legales, es prácticamente imposible para el propietario de una empresa estimar el efecto causal sobre la productividad de los trabajadores de ponerlos bajo estrés psicológico mediante un experimento en el que los trabajadores se asignan aleatoriamente al grupo de tratamiento que se encuentra bajo presión de tiempo o al grupo de control donde el trabajo se realiza en condiciones regulares, en el mejor de los casos sin el conocimiento de estar en un experimento (ver el *El efecto Hawthorne*).

Sin embargo, a veces las circunstancias externas producen lo que se llama un *cuasi-experimento* o *experimento natural*. Esta aleatoriedad "si permite estimar los efectos causales" que son de interés para los economistas que utilizan herramientas muy similares a las válidas para experimentos controlados aleatorios ideales. Estas herramientas se basan en gran medida en la teoría de la regresión múltiple y también en la regresión IV (consulte el Capítulo \@ref(RVI)). Se revisarán los aspectos centrales de estos métodos y se demostrará cómo aplicarlos en **R** utilizando el conjunto de datos **STAR** (consulte la [descripción](https://dataverse.harvard.edu/dataset.xhtml?persistentId=hdl:1902.1/10766) del conjunto de datos).

Los siguientes paquetes y sus dependencias son necesarias para la reproducción de los fragmentos de código presentados a lo largo de este capítulo:

+ **AER** [@R-AER]
+ **dplyr** [@R-dplyr]
+ **MASS** [@R-MASS]
+ **mvtnorm** [@R-mvtnorm]
+ **rddtools** [@R-rddtools]
+ **scales** [@R-scales]
+ **stargazer** [@R-stargazer]
+ **tidyr** [@R-tidyr]

Asegúrese de que el siguiente fragmento de código se ejecute sin errores.

```{r, 695, warning=FALSE, message=FALSE, eval=FALSE}
if (!require('remotes')) install.packages('remotes')
remotes::install_github( "bquast/rddtools" )
```

```{r, 696, warning=FALSE, message=FALSE, eval=FALSE}
library(AER)
library(dplyr)
library(MASS)
library(mvtnorm)
library(rddtools)
library(scales)
library(stargazer)
library(tidyr)
```

## Resultados potenciales, efectos causales y experimentos idealizados {#RPECEI}

Ahora se recapitula brevemente la idea del efecto causal promedio y cómo se puede estimar usando el *estimador de diferencias*.

#### Resultados potenciales y efecto causal promedio {-}

Un *resultado potencial* es el resultado de un individuo bajo un tratamiento potencial. Para este individuo, el efecto causal del tratamiento es la diferencia entre el resultado potencial si el individuo recibe el tratamiento y el resultado potencial si no lo recibe. Dado que este efecto causal puede ser diferente para diferentes individuos y no es posible medir el efecto causal para un solo individuo, uno está interesado en estudiar el *efecto causal promedio* del tratamiento, por lo que también se denomina *efecto promedio del tratamiento*.

En un experimento controlado aleatorio ideal se cumplen las siguientes condiciones:

1. Los sujetos se seleccionan al azar de la población.
2. Los sujetos se asignan aleatoriamente al grupo de tratamiento y control.

La condición 1. garantiza que los resultados potenciales de los sujetos se extraigan aleatoriamente de la misma distribución de modo que el valor esperado del efecto causal en la muestra sea igual al efecto causal promedio en la distribución. La condición 2. asegura que la recepción del tratamiento sea independiente de los posibles resultados de los sujetos. Si se cumplen ambas condiciones, el efecto causal esperado es el resultado esperado en el grupo de tratamiento menos el resultado esperado en el grupo de control. Usando expectativas condicionales se tiene $$\text{Efecto causal promedio} = E(Y_i\vert X_i=1) -  E(Y_i\vert X_i=0),$$ donde $X_i$ es un indicador de tratamiento binario.

El efecto causal promedio se puede estimar usando el *estimador de diferencias*, que no es más que el estimador MCO en el modelo de regresión simple:
 
\begin{align}
  Y_i = \beta_0 + \beta_1 X_i + u_i \ \ , \ \ i=1,\dots,n, (\#eq:diffest)
\end{align}

donde la asignación aleatoria asegura que $E(u_i\vert X_i) = 0$.

El estimador MCO en el modelo de regresión: 

\begin{align}
  Y_i = \beta_0 + \beta_1 X_i + \beta_2 W_{1i} + \dots + \beta_{1+r} W_{ri} + u_i \ \ , \ \ i=1,\dots,n (\#eq:diffestwar)
\end{align}

con regresores adicionales $W_1,\dots,W_r$ se denomina *estimador de diferencias con regresores adicionales*. Se supone que el tratamiento $X_i$ se asigna aleatoriamente para que sea independiente de la característica previa al tratamiento $W_i$. Esta suposición se llama *independencia media condicional* e implica $$E(u_i\vert X_i , W_i) = E(u_i\vert W_i) = 0,$$ por lo que la expectativa condicional del error $u_i$ dado el indicador de tratamiento $X_i$ y la característica de pretratamiento $W_i$ no depende de $X_i$. La independencia media condicional reemplaza el primer supuesto de mínimos cuadrados en el Concepto clave 6.4 y, por lo tanto, asegura que el estimador de diferencias de $\beta_1$ sea insesgado. El *estimador de diferencias con regresores adicionales* es más eficiente que el *estimador de diferencias* si los regresores adicionales explican parte de la variación en $Y_i$.

## Amenazas a la validez de los experimentos

Los conceptos de validez interna y externa discutidos en el Concepto clave 9.1 también son aplicables para estudios basados en datos experimentales y cuasi-experimentales. Se proporciona una explicación detallada de las amenazas particulares a la validez interna y externa de los experimentos, incluidos ejemplos. En consecuencia, el capítulo aborda una breve repetición de las amenazas.

#### Amenazas a la validez interna {-}

1. **Falta de aleatorización**

    Si los sujetos no se asignan al azar al grupo de tratamiento, los resultados se contaminarán con el efecto de las características o preferencias individuales de los sujetos y no es posible obtener una estimación no sesgada del efecto del tratamiento. Se puede probar la asignación no aleatoria usando una prueba de significancia (prueba $F$) en los coeficientes en el modelo de regresión $$X_i = \beta_0 + \beta_1 W_{1i} + \dots +\beta_2 W_{ri} + u_i \ \ , \ \ i=1,\dots,n.$$
    
2. **Incumplimiento del protocolo de tratamiento**

    Si los sujetos no siguen el protocolo de tratamiento; es decir, algunos sujetos del grupo de tratamiento logran evitar recibir el tratamiento y/o algunos sujetos del grupo de control logran recibir el tratamiento (*cumplimiento parcial*), existe correlación entre $X_i$ y $u_i$ de manera que el estimador MCO del efecto promedio del tratamiento está sesgado. Si existen datos sobre *ambos*, el tratamiento realmente recibido ($X_i$) y la asignación aleatoria inicial ($Z_i$), la regresión IV de los modelos \@ref(eq:diffest) y \@ref(eq:diffestwar) es un remedio.

3. **Desgaste**

    El desgaste puede resultar en una muestra no seleccionada al azar. Si los sujetos abandonan sistemáticamente el estudio después de haber sido asignados al grupo de control o al grupo de tratamiento (sistemático significa que el motivo del abandono está relacionado con el tratamiento), habrá correlación entre $X_i$ y $u_i$. Por lo tanto, existe sesgo en el estimador de MCO sobre el efecto del tratamiento.

4. **Efectos experimentales**
  
    Si los sujetos humanos en el grupo de tratamiento y/o el grupo de control saben que están en un experimento, podrían adaptar su comportamiento de una manera que evite la estimación no sesgada del efecto del tratamiento.

5. **Tamaños de muestra pequeños**

    Como se sabe por la teoría de la regresión lineal, los tamaños de muestra pequeños conducen a una estimación imprecisa de los coeficientes y, por lo tanto, implican una estimación imprecisa del efecto causal. Además, los intervalos de confianza y la prueba de hipótesis pueden producir inferencias erróneas cuando el tamaño de la muestra es pequeño.

#### Amenazas a la validez externa {-}

1. **Muestra no representativa**

    Si la población estudiada y la población de interés no son suficientemente similares, no existe justificación para generalizar los resultados.

2. **Programa o política no representativa**

    Si el programa o política para la población estudiada difiere considerablemente del programa (que se) aplicará a la(s) población(es) de interés, los resultados no se pueden generalizar. Por ejemplo, un programa a pequeña escala con poca financiación puede tener efectos diferentes a los de un programa ampliado ampliamente disponible que se implementa realmente. Existen otros factores como la duración y el alcance del seguimiento que deben considerarse aquí.
    
3. **Efectos de equilibrio general**

    Si las condiciones de mercado y/o ambientales no pueden mantenerse constantes cuando un programa válido internamente se implementa de manera amplia, la validez externa puede ser dudosa.

## Estimaciones experimentales del efecto de las reducciones del tamaño de las clases

### Diseño experimental y conjunto de datos {-}

El Proyecto *Student-Teacher Achievement Ratio* (STAR) fue un gran experimento controlado aleatorio con el objetivo de afirmar si una reducción del tamaño de la clase es efectiva para mejorar los resultados educativos. Se llevó a cabo en 80 escuelas primarias de Tennessee durante un período de cuatro años durante la década de 1980 por el Departamento de Educación del Estado en EE. UU.

En el primer año, alrededor de 6400 estudiantes fueron asignados aleatoriamente a una de tres intervenciones: Clase pequeña (13 a 17 estudiantes por maestro), clase regular (22 a 25 estudiantes por maestro) y clase regular con ayudante (22 a 25 estudiantes por maestro y estudiantes con un asistente de maestro de tiempo completo). Los maestros también fueron asignados al azar a las clases que impartían. Las intervenciones se iniciaron cuando los estudiantes ingresaron a la escuela en el jardín de infantes y continuaron hasta el tercer grado. Los grupos de control y tratamiento en todos los grados se resumen en la Tabla \@ref(tab:starstructure).

|                | K                        | 1                        | 2                        | 3                        |
|:---------------|:------------------------:|:------------------------:|:------------------------:|:------------------------:|
| Tratamiento 1  | Clase pequeña            | Clase pequeña            | Clase pequeña            | Clase pequeña            |
| Tratamiento 2  | Clase regular + Ayudante | Clase regular + Ayudante | Clase regular + Ayudante | Clase regular + Ayudante |
| Control        | Clase regular            | Clase regular            | Clase regular            | Clase regular            |

Table: (\#tab:starstructure) Grupos de control y tratamiento en el experimento STAR

Cada año, el progreso de aprendizaje de los estudiantes se evaluó mediante la suma de los puntos obtenidos en las partes de matemáticas y lectura de una prueba estandarizada (la [Prueba de rendimiento de Stanford](https://en.wikipedia.org/wiki/Stanford_Achievement_Test_Series)).

El conjunto de datos **STAR** es parte del paquete **AER**.

```{r, 697, message=FALSE}
# cargar el paquete AER y el conjunto de datos STAR
library(AER)
data(STAR)
```

**head(STAR)** muestra que existe una variedad de variables de factores que describen las características de los estudiantes y maestros, así como varios indicadores de la escuela, todos los cuales se registran por separado para los cuatro grados diferentes. Los datos están en *formato ancho*; es decir, cada variable tiene su propia columna y, para cada alumno, las filas contienen observaciones sobre estas variables. Usando **dim(STAR)** se encontró que existe un total de 11598 observaciones en 47 variables.

```{r, 698}
# obtener una descripción general
head(STAR, 2)
dim(STAR)
```

```{r, 699}
# obtener los nombres de las variables
names(STAR)
```

La mayoría de los nombres de las variables contienen un sufijo (**k**, **1**, **2** o **3**) que indica el grado al que se refiere la variable respectiva. Esto facilita el análisis de regresión porque permite ajustar el argumento **formula** en **lm()** para cada grado simplemente cambiando los sufijos de las variables de acuerdo con el objetivo.

El resultado producido por **head()** muestra que algunos valores registrados son **NA** y **<NA>**; es decir, no existen datos sobre esta variable para el alumno en consideración. Esto radica en la naturaleza de los datos; por ejemplo, al tomar la primera observación **"STAR[1,]"**.

En la salida de `head(STAR, 2)` se encontró que el estudiante ingresó al experimento en tercer grado en una clase regular, por lo que el tamaño de la clase se registra en **star3** y las otras variables indicadoras del tipo de clase son **<NA>**. Por la misma razón, no existen registros de su puntaje en matemáticas y lectura, excepto para el tercer grado. Es sencillo obtener un conjunto de datos que no contenga todos los *NA* / *<NA>*: Simplemente se necesitan pedir los **NA** usando **!is.na()**.

```{r, 700}
# pedir los datos de NA para la primera observación e imprimir los resultados en la consola
STAR[1, !is.na(STAR[1, ])]
```

`is.na(STAR[1, ])` devuelve un vector lógico con **TRUE** en posiciones que corresponden a entradas **<NA>** para la primera observación. El operador **!** se usa para invertir el resultado de modo que se obtengan solo entradas que no sean **<NA>** para las primeras observaciones.

En general, no es necesario eliminar filas con datos faltantes porque **lm()** lo hace de forma predeterminada. La falta de datos puede implicar un tamaño de muestra pequeño y, por lo tanto, puede conducir a una estimación imprecisa y a una inferencia incorrecta. Sin embargo, esto no es un problema para el estudio en cuestión ya que, como se vera a continuación, los tamaños de muestra superan las 5000 observaciones para cada regresión realizada.

### Análisis de los datos STAR {-}

Como se puede ver en la Tabla \@ref(tab:starstructure), existen dos grupos de tratamiento en cada grado, clases pequeñas con solo 13 a 17 estudiantes y clases regulares con 22 a 25 estudiantes y un ayudante de enseñanza. Por lo tanto, se introducen dos variables binarias, cada una de las cuales es un indicador para el grupo de tratamiento respectivo, con el objetivo de que el estimador de diferencias capture el efecto del tratamiento para cada grupo de tratamiento por separado. Esto produce el modelo de regresión poblacional:

\begin{align}
  Y_i = \beta_0 + \beta_1 SmallClass_i + \beta_2 RegAide_i + u_i, (\#eq:starpopreg)
\end{align}

con puntaje de prueba $Y_i$, el indicador de clase pequeña $SmallClass_i$ y $RegAide_i$, el indicador de una clase regular con ayudante.

Se reproducen los resultados presentados en la Tabla 13.1 realizando la regresión \@ref(eq:starpopreg) para cada grado por separado. En el caso de cada estudiante, la variable dependiente es simplemente la suma de los puntos obtenidos en las partes de matemáticas y lectura, construida usando **I()**.

```{r, 701}
# calcular estimaciones de diferencias para cada grado
fmk <- lm(I(readk + mathk) ~ stark, data = STAR)
fm1 <- lm(I(read1 + math1) ~ star1, data = STAR)
fm2 <- lm(I(read2 + math2) ~ star2, data = STAR)
fm3 <- lm(I(read3 + math3) ~ star3, data = STAR)
```

```{r, 702}
# obtener una matriz de coeficientes utilizando errores estándar robustos
coeftest(fmk, vcov = vcovHC, type= "HC1")
coeftest(fm1, vcov = vcovHC, type= "HC1")
coeftest(fm2, vcov = vcovHC, type= "HC1")
coeftest(fm3, vcov = vcovHC, type= "HC1")
```

Se recopilan los resultados y se presentan en una tabla usando **stargazer()**.

```{r, 703}
# calcular errores estándar robustos para cada modelo y reunirlos en una lista
rob_se_1 <- list(sqrt(diag(vcovHC(fmk, type = "HC1"))),
                 sqrt(diag(vcovHC(fm1, type = "HC1"))),
                 sqrt(diag(vcovHC(fm2, type = "HC1"))),
                 sqrt(diag(vcovHC(fm2, type = "HC1"))))
```

```{r, 704, message=F, warning=F, results='asis', eval=FALSE}
library(stargazer)

stargazer(fmk,fm1,fm2,fm3,
  title = "Proyecto STAR: Estimaciones de diferencias",
  header = FALSE, 
  type = "latex",
  model.numbers = F,
  omit.table.layout = "n",
  digits = 3, 
  column.labels = c("K", "1", "2", "3"),
  dep.var.caption  = "Variable dependiente: Grado",
  dep.var.labels.include = FALSE,
  se = rob_se_1)
```

<!--html_preserve-->

```{r, 705, message=F, warning=F, results='asis', echo=F, purl=F, eval=my_output=="html"}
library(stargazer)

stargazer(fmk,fm1,fm2,fm3,
  header = FALSE, 
  type = "html",
  model.numbers = F,
  omit.table.layout = "n",
  digits = 2, 
  column.labels = c("K", "1", "2", "3"),
  dep.var.caption  = "Variable dependiente: Grado",
  dep.var.labels.include = FALSE,
  se = rob_se_1)

stargazer_html_title("Proyecto STAR - Estimaciones de diferencias", "psde")
```

<!--/html_preserve-->

```{r, 706, message=F, warning=F, results='asis', echo=F, purl=F, eval=my_output=="latex"}
library(stargazer)

stargazer(fmk,fm1,fm2,fm3,
  title = "\\label{tab:psde} Proyecto STAR - Estimaciones de diferencias",
  header = FALSE, 
  digits = 3,
  type = "latex",
  float.env = "sidewaystable",
  column.sep.width = "-3pt",
  model.numbers = F,
  omit.table.layout = "n",
  column.labels = c("K", "1", "2", "3"),
  dep.var.caption  = "Variable dependiente: Grado",
  dep.var.labels.include = FALSE,
  se = rob_se_1) 
```

Las estimaciones presentadas en la Tabla \@ref(tab:psde) sugieren que la reducción del tamaño de la clase mejora el rendimiento de los estudiantes. A excepción del grado 1, las estimaciones del coeficiente en $SmallClass$ son aproximadamente de la misma magnitud (las estimaciones se encuentran entre 13.90 y 19.39 puntos) y son estadísticamente significativas a $1\%$. Además, un ayudante de enseñanza tiene poco efecto, posiblemente nulo, en el desempeño de los estudiantes.

Se aumenta el modelo de regresión \@ref(eq:starpopreg) a partir de diferentes conjuntos de regresores por dos razones:

1. Si los regresores adicionales explican parte de la variación observada en la variable dependiente, se obtienen estimaciones más eficientes de los coeficientes de interés.
2. Si el tratamiento no se recibe al azar debido a fallas en el seguimiento del protocolo de tratamiento, las estimaciones obtenidas usando \@ref(eq:starpopreg) pueden estar sesgadas. Agregar regresores adicionales puede resolver o mitigar este problema.

En particular, se consideran las siguientes características de estudiantes y profesores:

- $experience$ --- Años de experiencia del maestro.
- $boy$ --- El estudiante es un niño (ficticia).
- $lunch$ --- Elegibilidad para almuerzo gratis (ficticia).
- $black$ --- El estudiante es afroamericano (ficticia).
- $race$ --- La raza del estudiante no es blanca ni negra (ficticia).
- $\text{schoolid}$ --- Variables indicadoras de la escuela.

en las cuatro especificaciones de regresión poblacional

\begin{align}
Y_i =& \beta_0 + \beta_1 SmallClass_i + \beta_2 RegAide_i + u_i, (\#eq:augstarpopreg1) \\
Y_i =& \beta_0 + \beta_1 SmallClass_i + \beta_2 RegAide_i + \beta_3 experience_i + u_i, (\#eq:augstarpopreg2) \\
Y_i =& \beta_0 + \beta_1 SmallClass_i + \beta_2 RegAide_i + \beta_3 experience_i + schoolid + u_i, (\#eq:augstarpopreg3)
\end{align}

y

\begin{align}
Y_i =& \beta_0 + \beta_1 SmallClass_i + \beta_2 RegAide_i + \beta_3 experience_i + \beta_4 boy + \beta_5 lunch \\ 
& + \beta_6 black + \beta_7 race + schoolid + u_i. (\#eq:augstarpopreg4)
\end{align}

Antes de la estimación, se crean algunos subconjuntos y ordenan los datos utilizando funciones de los paquetes **dplyr** y **tidyr**. Ambos son parte de **tidyverse**, una colección de paquetes **R** diseñados para la ciencia de datos y el manejo de grandes conjuntos de datos (consulte el [sitio oficial](https://www.tidyverse.org/) para obtener más información sobre los **paquetes de tidyverse**). Las funciones **%>%**, **transmute()** y **mutate()** son suficientes para nosotros aquí:

+ **%>%** permite encadenar llamadas a funciones.
+ **transmute()** permite subdividir el conjunto de datos nombrando las variables que se van a mantener.
+ **mutate()** es conveniente para agregar nuevas variables basadas en las existentes conservando estas últimas.

Los modelos de regresión \@ref(eq:augstarpopreg1) a \@ref(eq:augstarpopreg4) requieren las variables **gender**, **ethnicity**, **stark**, **readk**, **mathk**, **lunchk**, **experiencek** y **schoolidk**. Después de eliminar las variables restantes usando **transmute()**, se usa **mutate()** para agregar tres variables binarias adicionales que son derivadas de las existentes: **black**, **race** y **boy**. Se generan usando declaraciones lógicas dentro de la función **ifelse()**.

```{r, 707, message=FALSE, warning=FALSE}
# cargar los paquetes 'dplyr' y 'tidyr' para las funcionalidades de gestión de datos
library(dplyr)
library(tidyr)

# generar subconjunto con datos de jardín de infantes
STARK <- STAR %>% 
      transmute(gender,
                ethnicity,
                stark,
                readk,
                mathk,
                lunchk,
                experiencek,
                schoolidk) %>% 
      mutate(black = ifelse(ethnicity == "afam", 1, 0),
             race = ifelse(ethnicity == "afam" | ethnicity == "cauc", 1, 0),
             boy = ifelse(gender == "male", 1, 0))
```

```{r, 708, message=FALSE, warning=FALSE}
# estimar los modelos
gradeK1 <- lm(I(mathk + readk) ~ stark + experiencek, 
              data = STARK)

gradeK2 <- lm(I(mathk + readk) ~ stark + experiencek + schoolidk, 
              data = STARK)

gradeK3 <- lm(I(mathk + readk) ~ stark + experiencek + boy + lunchk 
              + black + race + schoolidk, 
              data = STARK)
```

En aras de la brevedad, se excluyen los coeficientes de las variables ficticias del indicador en la salida de **coeftest()** subconjuntando las matrices.

```{r, 709, message=FALSE, warning=FALSE}
# obtener una inferencia robusta sobre la significancia de los coeficientes
coeftest(gradeK1, vcov. = vcovHC, type = "HC1")
coeftest(gradeK2, vcov. = vcovHC, type = "HC1")[1:4, ]
coeftest(gradeK3, vcov. = vcovHC, type = "HC1")[1:7, ]
```

Ahora se usa **stargazer()** para recopilar toda la información relevante en una tabla estructurada.

```{r, 710}
# calcular errores estándar robustos para cada modelo y reunirlos en una lista
rob_se_2 <- list(sqrt(diag(vcovHC(fmk, type = "HC1"))),
                 sqrt(diag(vcovHC(gradeK1, type = "HC1"))),
                 sqrt(diag(vcovHC(gradeK2, type = "HC1"))),
                 sqrt(diag(vcovHC(gradeK3, type = "HC1"))))
```

```{r, 711, message=F, warning=F, results='asis', eval=FALSE}
stargazer(fmk, fm1, fm2, fm3,
  title = "Proyecto STAR - Estimación de diferencias con regresores adicionales para jardín de infantes",
  header = FALSE, 
  type = "latex",
  model.numbers = F,
  omit.table.layout = "n",
  digits = 3, 
  column.labels = c("(1)", "(2)", "(3)", "(4)"),
  dep.var.caption  = "Variable dependiente: Puntaje de la prueba en el jardín de infantes",
  dep.var.labels.include = FALSE,
  se = rob_se_2) 
```

<!--html_preserve-->

```{r, 712, message=F, warning=F, results='asis', echo=F, purl=F, eval=my_output=="html"}
stargazer(fmk,gradeK1,gradeK2,gradeK3,
  header = FALSE, 
  type = "html",
  model.numbers = F,
  omit.table.layout = "n",
  digits = 3, 
  column.labels = c("(1)", "(2)", "(3)", "(4)"),
  dep.var.caption  = "Variable dependiente: Puntaje de la prueba en el jardín de infantes",
  dep.var.labels.include = FALSE,
  se = rob_se_2,
  omit = "schoolid",
  nobs = TRUE,
  add.lines = list(
                   c("School indicators?", "no", "no", "yes", "yes")
                   )
  ) 

stargazer_html_title("Proyecto STAR - Estimación de diferencias con regresores adicionales para jardín de infantes", "psdewarfk")
```

<!--/html_preserve-->

```{r, 713, message=F, warning=F, results='asis', echo=F, purl=F, eval=my_output=="latex"}
stargazer(fmk,gradeK1,gradeK2,gradeK3,
  title = "\\label{tab:psdewarfk} Proyecto STAR - Estimación de diferencias con regresores adicionales para jardín de infantes",
  header = FALSE, 
  digits = 3,
  type = "latex",
  float.env = "sidewaystable",
  column.sep.width = "-5pt",
  model.numbers = F,
  omit.table.layout = "n",
  column.labels = c("(1)", "(2)", "(3)", "(4)"),
  dep.var.caption  = "Variable dependiente: Puntaje de la prueba en el jardín de infantes",
  dep.var.labels.include = FALSE,
  se = rob_se_2,
  omit = "schoolid",
  nobs = TRUE,
  add.lines = list(
                   c("School indicators?", "no", "no", "yes", "yes")
                   )
  ) 
```

Los resultados en la columna (1) de la Tabla \@ref(tab:psdewarfk) simplemente repiten los resultados obtenidos para \@ref(eq:starpopreg). Las columnas (2) a (4) revelan que la suma de las características de los estudiantes y los efectos fijos de la escuela no conducen a estimaciones sustancialmente diferentes de los efectos del tratamiento. Este resultado hace que sea más plausible que las estimaciones de los efectos obtenidos utilizando el modelo \@ref(eq:starpopreg) no adolezcan de fallas en la asignación aleatoria. Existe una disminución en los errores estándar y un aumento en $\bar{R}^2$, lo que implica que las estimaciones son más precisas.

Debido a que los maestros fueron asignados aleatoriamente en las clases, la inclusión del efecto fijo de la escuela nos permite estimar el efecto causal de la experiencia de un maestro en los puntajes de las pruebas de los estudiantes de jardín de infantes. La regresión (3) predice que el efecto promedio de 10 años de experiencia en los puntajes de las pruebas será de $10\cdot 0.74=7.4$ puntos. Se debe tener en cuenta que las otras estimaciones sobre las características de los estudiantes en la regresión (4) *no tienen una interpretación causal* debido a una asignación no aleatoria.

¿Los efectos estimados presentados en la Tabla \@ref(tab:psdewarfk) son grandes o pequeños en un sentido práctico? Traduciendo los cambios predichos en los puntajes de las pruebas a unidades de desviación estándar para permitir una comparación (consulte la Sección \@ref(EPETC) para obtener un argumento similar).

```{r, 714}
# calcular las desviaciones estándar de la muestra de los puntajes de las pruebas
SSD <- c("K" = sd(na.omit(STAR$readk + STAR$mathk)),
         "1" = sd(na.omit(STAR$read1 + STAR$math1)),
         "2" = sd(na.omit(STAR$read2 + STAR$math2)),
         "3" = sd(na.omit(STAR$read3 + STAR$math3)))

# traducir los efectos de clases pequeñas a desviaciones estándar
Small <- c("K" = as.numeric(coef(fmk)[2]/SSD[1]),
           "1" = as.numeric(coef(fm1)[2]/SSD[2]),
           "2" = as.numeric(coef(fm2)[2]/SSD[3]),
           "3" = as.numeric(coef(fm3)[2]/SSD[4]))

# ajustar los errores estándar
SmallSE <- c("K" = as.numeric(rob_se_1[[1]][2]/SSD[1]),
             "1" = as.numeric(rob_se_1[[2]][2]/SSD[2]),
             "2" = as.numeric(rob_se_1[[3]][2]/SSD[3]),
             "3" = as.numeric(rob_se_1[[4]][2]/SSD[4]))

# traducir los efectos de las clases regulares con ayuda a las desviaciones estándar
RegAide<- c("K" = as.numeric(coef(fmk)[3]/SSD[1]),
            "1" = as.numeric(coef(fm1)[3]/SSD[2]),
            "2" = as.numeric(coef(fm2)[3]/SSD[3]),
            "3" = as.numeric(coef(fm3)[3]/SSD[4]))

# ajustar los errores estándar
RegAideSE <- c("K" = as.numeric(rob_se_1[[1]][3]/SSD[1]),
               "1" = as.numeric(rob_se_1[[2]][3]/SSD[2]),
               "2" = as.numeric(rob_se_1[[3]][3]/SSD[3]),
               "3" = as.numeric(rob_se_1[[4]][3]/SSD[4]))

# recopilar los resultados en un data.frame y redondear
df <- t(round(data.frame(
                        Small, SmallSE, RegAide, RegAideSE, SSD),
                        digits =  2))
```

Es bastante fácil convertir **data.frame** o **df** en una tabla.

```{r, 715, eval=FALSE}
# generar una tabla simple usando Stargazer
stargazer(df,
          title = "Efectos estimados del tamaño de la clase (en unidades de desviaciones estándar)",
          type = "html", 
          summary = FALSE,
          header = FALSE
          )
```

<!--html_preserve-->

```{r, 716, message=F, warning=F, results='asis', echo=F, purl=F, eval=my_output=="html"}
stargazer(df,
          type = "html",
          header = FALSE,
          summary = FALSE)

stargazer_html_title("Efectos estimados del tamaño de la clase (en unidades de desviaciones estándar)", "ecse")
```

<!--/html_preserve-->

```{r, 717, message=F, warning=F, results='asis', echo=F, purl=F, eval=my_output=="latex"}
stargazer(df,
          title = "\\label{tab:ecse} Efectos estimados del tamaño de la clase (en unidades de desviaciones estándar)",
          type = "latex",
          header = FALSE,
          summary = FALSE)
```

El efecto estimado de clases pequeñas es mayor para el primer grado. Lo anterior probablemente se deba a que los estudiantes del grupo de control del primer grado obtuvieron malos resultados en la prueba por alguna razón desconocida o simplemente debido a una variación aleatoria. La diferencia entre el efecto estimado de estar en una clase pequeña y estar en clases regulares con un asistente es aproximadamente 0.2 desviaciones estándar para todos los grados. Esto lleva a la conclusión de que el efecto de estar en una clase de tamaño regular con un asistente es cero y el efecto de estar en una clase pequeña es aproximadamente el mismo para todos los grados.

Hasta qué punto estas estimaciones experimentales son comparables con las estimaciones observacionales obtenidas utilizando datos sobre distritos escolares en California y Massachusetts en el Capítulo \@ref(EEBRM). Resulta que, de hecho, las estimaciones son muy similares. Consulte la sección mencionada anteriormente en el libro para obtener una discusión más detallada.

## Cuasi experimentos {#CE}

En los cuasiexperimentos, se explota "como si" la aleatoriedad para utilizar métodos similares a los que se han analizado en el capítulo anterior. Existen dos tipos de cuasiexperimentos:

1. Las variaciones aleatorias en circunstancias individuales permiten ver el tratamiento "como si" se hubiera determinado al azar.

2. El tratamiento está determinado sólo parcialmente por una variación aleatoria "como si".

El primero permite estimar el efecto usando cualquier modelo \@ref(eq:diffestwar); es decir, el *estimador de diferencias con regresores adicionales*, o, si hay duda de que la aleatoriedad "como si" no asegura completamente que haya sin diferencias sistemáticas entre el grupo de control y el grupo de tratamiento, utilizando el estimador *diferencias en diferencias* (DED). En el último caso, se puede aplicar un enfoque IV para la estimación de un modelo como \@ref(eq:diffestwar) que utiliza la fuente de aleatoriedad "como si" en la asignación de tratamientos como instrumento.

Algunas técnicas más avanzadas que son útiles en entornos donde la asignación de tratamiento está (parcialmente) determinada por un umbral en una de las llamadas variables de ejecución son *diseño de discontinuidad de regresión aguda* (DDRA) y *diseño de discontinuidad de regresión difusa* (DDRD).

Se revisa brevemente estas técnicas y se usarán datos simulados en un ejemplo mínimo para discutir cómo se pueden aplicar DED, DDRA y DDRD en **R**.

### El Estimador de diferencias en diferencias {-}

En los cuasiexperimentos, la fuente de aleatoriedad "como si" en la asignación de tratamientos a menudo no puede evitar por completo las diferencias sistemáticas entre los grupos de control y de tratamiento. Este problema fue encontrado por @card1994, que usa la geografía como la asignación de tratamiento aleatorio "como si" para estudiar el efecto en el empleo en restaurantes de comida rápida causado por un aumento en el salario mínimo estatal en Nueva Jersey en el año de 1992. Su idea era utilizar el hecho de que el aumento del salario mínimo se aplicaba a los empleados de Nueva Jersey (grupo de tratamiento), pero no a los que vivían en la vecina Pensilvania (grupo de control).

Es bastante concebible que tal aumento salarial no esté correlacionado con otros determinantes del empleo. Sin embargo, todavía puede haber algunas diferencias específicas de cada estado y, por lo tanto, diferencias entre el grupo de control y el grupo de tratamiento. Esto haría que el *estimador de diferencias* fuese sesgado e inconsistente. @card1994 resolvió esto utilizando un estimador DED: Recopilaron datos en febrero de 1992 (antes del tratamiento) y noviembre de 1992 (después del tratamiento) para los mismos restaurantes y estimaron el efecto del aumento salarial analizando las diferencias en las diferencias en el empleo para Nueva Jersey y Pensilvania antes y después del aumento.^[Se recomuenda consultar el artículo para identificar *¿Cuál es el efecto sobre el empleo del salario mínimo?*] El estimador DED es:

\begin{align}
  \widehat{\beta}_1^{\text{diffs-in-diffs}} =& \, (\overline{Y}^{\text{treatment,after}} - \overline{Y}^{\text{treatment,before}}) - (\overline{Y}^{\text{control,after}} - \overline{Y}^{\text{control,before}}) \\
  =& \Delta \overline{Y}^{\text{treatment}} - \Delta \overline{Y}^{\text{control}} (\#eq:DID)
\end{align}

con

+ $\overline{Y}^{\text{tratamiento, antes}}$ - la media muestral en el grupo de tratamiento antes del tratamiento.

+ $\overline{Y}^{\text{tratamiento, después}}$ - la media muestral en el grupo de tratamiento después del tratamiento.

+ $\overline{Y}^{\text{tratamiento, antes}}$ - la media muestral en el grupo de control antes del tratamiento.

+ $\overline{Y}^{\text{tratamiento, después}}$ - la media muestral en el grupo de control después del tratamiento.

Ahora se usa **R** para crear una gráfica de vital importancia.

```{r, 718, fig.align='center'}
# inicializar la gráfica y agregar un grupo de control
plot(c(0, 1), c(6, 8), 
     type = "p",
     ylim = c(5, 12),
     xlim = c(-0.3, 1.3),
     main = "El estimador de diferencias en diferencias",
     xlab = "Período",
     ylab = "Y",# agregar anotaciones
     col = "steelblue",
     pch = 20,
     xaxt = "n",
     yaxt = "n")

axis(1, at = c(0, 1), labels = c("Antes", "Después"))
axis(2, at = c(0, 13))
# agregar grupo de tratamiento
points(c(0, 1, 1), c(7
, 9, 11), 
       col = "darkred",
       pch = 20)


# agregar segmentos de línea
lines(c(0, 1), c(7, 11), col = "darkred")
lines(c(0, 1), c(6, 8), col = "steelblue")
lines(c(0, 1), c(7, 9), col = "darkred", lty = 2)
lines(c(1, 1), c(9, 11), col = "black", lty = 2, lwd = 2)


# agregar anotaciones
text(1, 10, expression(hat(beta)[1]^{DID}), cex = 0.8, pos = 4)
text(0, 5.5, "media muestral de control", cex = 0.8 , pos = 4)
text(0, 6.8, "media muestral de tratamiento", cex = 0.8 , pos = 4)
text(1, 7.9, "media muestral control", cex = 0.8 , pos = 4)
text(1, 11.1, "media muestral tratamiento", cex = 0.8 , pos = 4)
```

El estimador DID \@ref(eq:DID) también se puede escribir en notación de regresión: $\widehat{\beta}_1^{\text{DID}}$ es el estimador MCO de $\beta_1$ en

\begin{align}
  \Delta Y_i = \beta_0 + \beta_1 X_i + u_i, (\#eq:did)
\end{align}

donde $\Delta Y_i$ denota la diferencia en los resultados previos y posteriores al tratamiento del individuo $i$, mientras que $X_i$ es el indicador de tratamiento.

Añadiendo regresores adicionales que miden las características previas al tratamiento a \@ref(eq:did) se obtiene:

\begin{align}
  \Delta Y_i = \beta_0 + \beta_1 X_i + \beta_2 W_{1i} + \dots + \beta_{1+r} W_{ri} + u_i, (\#eq:didwar)
\end{align}

el *estimador de diferencias en diferencias* con regresores adicionales. Los regresores adicionales pueden llevar a una estimación más precisa de $\beta_1$. 

Se mantienen las cosas simples y se enfoca en la estimación del efecto del tratamiento usando DID en el caso más simple; es decir, un grupo de control y un grupo de tratamiento observados durante dos períodos de tiempo, uno antes y otro después del tratamiento. En particular, se verá que existen tres formas diferentes de proceder.

Primero, se simulan los datos previos y posteriores al tratamiento utilizando **R**.

```{r, 719}
# establecer tamaño de muestra
n <- 200

# definir el efecto del tratamiento
TEffect <- 4

# generar ficticias de tratamiento
TDummy <- c(rep(0, n/2), rep(1, n/2))

# simular valores previos y posteriores al tratamiento de la variable dependiente
y_pre <- 7 + rnorm(n)
y_pre[1:n/2] <- y_pre[1:n/2] - 1
y_post <- 7 + 2 + TEffect * TDummy + rnorm(n)
y_post[1:n/2] <- y_post[1:n/2] - 1 
```

A continuación, se grafican los datos. La función **jitter()** se usa para agregar algo de dispersión artificial en el componente horizontal de los puntos para que haya menos sobretrazado. La función **alpha()** del paquete **scales** permite ajustar la opacidad de los colores utilizados en los gráficos.

```{r, 720, fig.align='center'}
library(scales)

pre <- rep(0, length(y_pre[TDummy==0]))
post <- rep(1, length(y_pre[TDummy==0]))

# graficar grupo de control en t = 1
plot(jitter(pre, 0.6), 
     y_pre[TDummy == 0], 
     ylim = c(0, 16), 
     col = alpha("steelblue", 0.3),
     pch = 20, 
     xlim = c(-0.5, 1.5),
     ylab = "Y",
     xlab = "Período",
     xaxt = "n",
     main = "Datos artificiales para la estimación DID")

axis(1, at = c(0, 1), labels = c("Antes", "Después"))

# agregar grupo de tratamiento en t = 1
points(jitter(pre, 0.6), 
       y_pre[TDummy == 1], 
       col = alpha("darkred", 0.3), 
       pch = 20)

# agregar grupo de control en t = 2
points(jitter(post, 0.6),
       y_post[TDummy == 0], 
       col = alpha("steelblue", 0.5),
       pch = 20)

# agregar grupo de tratamiento en t = 2
points(jitter(post, 0.6), 
       y_post[TDummy == 1], 
       col = alpha("darkred", 0.5),
       pch = 20)
```

Las observaciones del grupo de control y de tratamiento tienen una media más alta después del tratamiento, pero el aumento es más fuerte para el grupo de tratamiento. Usando DID se puede estimar qué parte de esa diferencia se debe al tratamiento.

Es sencillo calcular la estimación DID en la forma de \@ref(eq:DID).

```{r, 721}
# calcular el estimador DID para el efecto del tratamiento 'a mano'
mean(y_post[TDummy == 1]) - mean(y_pre[TDummy == 1]) - 
(mean(y_post[TDummy == 0]) - mean(y_pre[TDummy == 0]))
```

Se debe tener en cuenta que la estimación es cercana a $4$, el valor elegido como el efecto del tratamiento **TEffect** arriba. Dado que \@ref(eq:did) es un modelo lineal simple, se puede realizar una estimación MCO de esta especificación de regresión usando **lm()**.

```{r, 722}
# calcular el estimador DID usando un modelo lineal
lm(I(y_post - y_pre) ~ TDummy)
```

Se encontró que las estimaciones coinciden. Además, se puede demostrar que la estimación DID obtenida al estimar la especificación \@ref(eq:did) MCO es la misma que la estimación MCO de $\beta_{TE}$ en:

\begin{align}
  Y_i =& \beta_0 + \beta_1 D_i + \beta_2 Period_i + \beta_{TE} (Period_i \times D_i) + \varepsilon_i (\#eq:DIDint)
\end{align}

donde $D_i$ es el indicador de tratamiento binario, $Period_i$ es un indicador binario para el período de postratamiento y $Period_i \times D_i$  es la interacción de ambos.

En cuanto a \@ref(eq:did), la estimación de \@ref(eq:DIDint) usando **R** es sencilla. Consulte el Capítulo \@ref(FRNL) para ver una discusión de los términos de interacción.

```{r, 723}
# preparar datos para la regresión DID usando el término de interacción
d <- data.frame("Y" = c(y_pre,y_post),
                "Treatment" = TDummy, 
                "Period" = c(rep("1", n), rep("2", n)))

# estimar el modelo
lm(Y ~ Treatment * Period, data = d)
```

Como era de esperar, la estimación del coeficiente de interacción de la variable ficticia de tratamiento y la variable temporal coinciden con las estimaciones obtenidas utilizando \@ref(eq:DID) y la estimación MCO de \@ref(eq:did).

### Estimadores de discontinuidad de regresión {-}

Considerar el modelo

\begin{align}
  Y_i =& \beta_0 + \beta_1 X_i + \beta_2 W_i + u_i (\#eq:SRDDsetting)
\end{align}

y sea

\begin{align*}
X_i =& 
  \begin{cases}
    1, & W_i \geq c \\
    0, & W_i < c
  \end{cases}
\end{align*}

de modo que la recepción del tratamiento, $X_i$, está determinada por algún umbral $c$ de una variable continua $W_i$, la denominada variable de ejecución. La idea del *diseño de discontinuidad de regresión* es usar observaciones con un $W_i$ cercano a $c$ para la estimación de $\beta_1$. $\beta_1$ es el efecto del tratamiento promedio para individuos con $W_i = c$, que se supone que es una buena aproximación al efecto del tratamiento en la población. \@ref(eq:SRDDsetting) se denomina *diseño de discontinuidad de regresión aguda* porque la asignación de tratamiento es determinista y discontinua en el punto de corte: Todas las observaciones con $W_i < c$ no reciben tratamiento y todas las observaciones donde $W_i \geq c$ son tratados.

Los siguientes fragmentos de código muestran cómo estimar un SRDD lineal usando **R** y cómo producir el gráfico.

```{r, 724, message=FALSE}
# generar algunos datos de muestra
W <- runif(1000, -1, 1)
y <- 3 + 2 * W + 10 * (W>=0) + rnorm(1000)
```

```{r, 725, fig.align='center', message=FALSE}
# cargar el paquete 'rddtools'
library(rddtools)

# construir rdd_data
data <- rdd_data(y, W, cutpoint = 0)

# graficar los datos de la muestra
plot(data,
     col = "steelblue",
     cex = 0.35, 
     xlab = "W", 
     ylab = "Y")
```

El argumento **nbins** establece el número de contenedores en los que se divide la variable en ejecución para la agregación. Los puntos representan los promedios del contenedor de la variable de resultado.

Se puede usar la función **rdd_reg_lm()** para estimar el efecto del tratamiento usando el model \@ref(eq:SRDDsetting) para los datos artificiales generados anteriormente. Al elegir **slope = "same"**, se restringen las pendientes de la función de regresión estimada para que sean iguales en ambos lados del salto en el punto de corte $W=0$.

```{r, 726, message=FALSE, warning=FALSE}
# estimar el modelo de RDD agudo o nítido
rdd_mod <- rdd_reg_lm(rdd_object = data, 
                      slope = "same")
summary(rdd_mod)
```

La estimación del coeficiente de interés se etiqueta **D**. La estimación está muy cerca del efecto del tratamiento elegido en el DGP anterior.

Es fácil visualizar el resultado: Simplemente llamar a **plot()** en el objeto del modelo estimado.

```{r, 727, fig.align='center'}
# graficar el modelo RDD junto con observaciones agrupadas
plot(rdd_mod,
     cex = 0.35, 
     col = "steelblue", 
     xlab = "W", 
     ylab = "Y")
```

Como arriba, los puntos representan promedios de observaciones agrupadas.

Hasta ahora se asume que el cruce del umbral determina la recepción del tratamiento, de modo que el salto de las funciones de regresión de la población en el umbral puede considerarse como el efecto causal del tratamiento.

Cuando cruzar el umbral $c$ no es la única causa para recibir el tratamiento, el tratamiento no es una función determinista de $W_i$. En cambio, es útil pensar en $c$ como un umbral donde la *probabilidad* de recibir el salto de tratamiento.

Este salto puede deberse a variables inobservables que repercuten en la probabilidad de ser tratado. Por lo tanto, $X_i$ en \@ref(eq:SRDDsetting) se correlacionará con el error $u_i$ y será más difícil estimar consistentemente el efecto del tratamiento. En este escenario, usar un *diseño de discontinuidad de regresión difusa* que se base en un enfoque IV puede ser un remedio: Tomar la variable binaria $Z_i$ como un indicador para cruzar el umbral,

\begin{align*}
  Z_i = \begin{cases}
    1, & W_i \geq c \\
    0, & W_i < c.
  \end{cases}
\end{align*}

y suponga que $Z_i$ se relaciona con $Y_i$ solo a través del indicador de tratamiento $X_i$. Entonces $Z_i$ y $u_i$ no están correlacionados, pero $Z_i$ influye en la recepción del tratamiento, por lo que está correlacionado con $X_i$. Por lo tanto, $Z_i$ es un instrumento válido para $X_i$ y \@ref(eq:SRDDsetting) se puede estimar usando TSLS.

El siguiente fragmento de código genera datos de muestra donde las observaciones con un valor de la variable en ejecución $W_i$ por debajo del límite $c = 0$ no reciben tratamiento y las observaciones con $W_i \geq 0$ sí reciben tratamiento con una probabilidad de $80\%$ de modo que el estado del tratamiento solo esté determinado parcialmente por la variable en ejecución y el corte. El tratamiento conduce a un aumento de $Y$ en $2$ unidades. Las observaciones con $W_i \geq 0$ que no reciben tratamiento se denominan *no-shows*: Piense en una persona que fue asignada para recibir el tratamiento pero de alguna manera logra evitarlo.

```{r, 728,  message=FALSE}
library(MASS)

# generar datos de muestra
mu <- c(0, 0)
sigma <- matrix(c(1, 0.7, 0.7, 1), ncol = 2)

set.seed(1234)
d <- as.data.frame(mvrnorm(2000, mu, sigma))
colnames(d) <- c("W", "Y")

# introducir borrosidad
d$treatProb <- ifelse(d$W < 0, 0, 0.8)

fuzz <- sapply(X = d$treatProb, FUN = function(x) rbinom(1, 1, prob = x))

# efecto de tratamiento
d$Y <- d$Y + fuzz * 2
```

**sapply()** aplica la función proporcionada a **FUN** a cada elemento del argumento **X**. Aquí, **d$treatProb** es un vector y el resultado también es un vector.

Se grafican todas las observaciones, usando el color azul para marcar a los individuos que no recibieron el tratamiento y el color rojo para los que recibieron el tratamiento.

```{r, 729,fig.align='center'}
# generar una gráfica coloreada del grupo de tratamiento y control
plot(d$W, d$Y,
     col = c("steelblue", "darkred")[factor(fuzz)], 
     pch= 20, 
     cex = 0.5,
     xlim = c(-3, 3),
     ylim = c(-3.5, 5),
     xlab = "W",
     ylab = "Y")

# agregar una línea vertical discontinua en el corte
abline(v = 0, lty = 2)
```

Obviamente, la recepción de tratamiento ya no es una función determinista de la variable de ejecución $W$. Algunas observaciones con $W\geq0$ *no* recibieron el tratamiento. Se puede estimar un FRDD estableciendo adicionalmente **treatProb** como la variable de asignación **z** en **rdd_data()**. Luego, **rdd_reg_lm()** aplica el siguiente procedimiento TSLS: El tratamiento se predice usando $W_i$ y la variable ficticia de corte $Z_i$, la variable instrumental, en la regresión de la primera etapa. Los valores ajustados de la regresión de la primera etapa se utilizan para obtener una estimación coherente del efecto del tratamiento utilizando la segunda etapa en la que el resultado $Y$ se retrocede sobre los valores ajustados y la variable corriente $W$.

```{r, 730}
# estimar la Fuzzy RDD
data <- rdd_data(d$Y, d$W, 
                 cutpoint = 0, 
                 z = d$treatProb)

frdd_mod <- rdd_reg_lm(rdd_object = data, 
                       slope = "same")
frdd_mod
```

La estimación es cercana a $2$, el efecto del tratamiento poblacional. Se puede llamar **plot()** en el objeto del modelo para obtener una figura que consta de datos agrupados y la función de regresión estimada.

```{r, 731, fig.align='center'}
# graficar una función FRDD estimada
plot(frdd_mod, 
     cex = 0.5, 
     lwd = 0.4,
     xlim = c(-4, 4),
     ylim = c(-3.5, 5),
     xlab = "W",
     ylab = "Y")
```

¿Qué pasa si se usa un SRDD en su lugar, ignorando así el hecho de que el tratamiento no está perfectamente determinado por el límite en $W$? Se puede tener una impresión de las consecuencias estimando un SRDD utilizando los datos simulados previamente.

```{r, 732}
# estimar SRDD
data <- rdd_data(d$Y, 
                 d$W, 
                 cutpoint = 0)

srdd_mod <- rdd_reg_lm(rdd_object = data, 
                       slope = "same")
srdd_mod
```

La estimación obtenida utilizando un SRDD sugiere un sesgo sustancial a la baja. De hecho, este procedimiento es inconsistente para el verdadero efecto causal, por lo que aumentar la muestra no aliviaría el sesgo.

Existen varios problemas potenciales con los cuasiexperimentos. Como ocurre con todos los estudios empíricos, estos problemas potenciales están relacionados con la validez interna y externa. De igual, es necesario discutir las técnicas sobre la estimación del efecto del tratamiento cuando el efecto causal del tratamiento es heterogéneo en la población. 

#### Resumen {-}

Este capítulo ha introducido el concepto de efectos causales en experimentos controlados aleatorios y cuasiexperimentos en los que las variaciones en las circunstancias o los accidentes de la naturaleza se tratan como fuentes de asignación al tratamiento "como si". También se han analizado métodos que permiten una estimación coherente de estos efectos en ambos entornos. Estos incluyeron el *estimador de diferencias*, el *estimador de diferencias en diferencias* así como los estimadores *agudos* y el *diseño de regresión discontinua difusa*. Se mostró cómo aplicar estas técnicas de estimación en **R**.

En una aplicación empírica se ha mostrado cómo replicar los resultados del análisis de los datos STAR usando **R**. Este estudio utiliza un experimento controlado aleatorio para evaluar si las clases más pequeñas mejoran el desempeño de los estudiantes en las pruebas estandarizadas. Al estar relacionados con un experimento controlado aleatorio, los datos de este estudio son fundamentalmente diferentes a los utilizados en los estudios de corte transversal en los capítulos \@ref(RLR) a \@ref(FRNL). Por lo tanto, se ha motivado el uso de un *estimador de diferencias*.

El capítulo \@ref(ADC) demostró cómo se pueden obtener estimaciones de los efectos del tratamiento cuando el diseño del estudio es un cuasi-experimento que permite *diferencias en diferencias* o *estimadores de regresión discontinua*. En particular, se han introducido funciones del paquete **rddtools** que son convenientes tanto para la estimación como para el análisis gráfico al estimar un diseño de regresión discontinua.

## Ejercicios {#Ejercicios-13}

Los siguientes ejercicios guiarán en la reproducción de algunos de los resultados presentados en uno de los estudios DID más famosos de @card1994. Los autores utilizan la geografía como la asignación de tratamiento aleatorio "como si" para estudiar el efecto sobre el empleo en los restaurantes de comida rápida causado por un aumento en el salario mínimo estatal en Nueva Jersey en el año de 1992, ver Capítulo \@ref(CE).

El estudio se basa en datos de encuestas recopilados en febrero de 1992 y en noviembre de 1992, después de que el salario mínimo de Nueva Jersey aumentara $\$0.80$ de $\$4.25$ a $\$5.05$ en abril de 1992.

Estimar el efecto del aumento salarial simplemente calculando el cambio en el empleo en Nueva Jersey (como se pide que haga en el ejercicio 3) fallaría en controlar las variables omitidas. Al usar Pensilvania como control en un modelo de diferencias en diferencias (DID), se pueden controlar las variables con una influencia común en Nueva Jersey (grupo de tratamiento) y Pensilvania (grupo de control). Esto reduce enormemente el riesgo de sesgo de variables omitidas e incluso funciona cuando estas variables no se observan.

Para que el enfoque DID funcione, se debe asumir que Nueva Jersey y Pensilvania tienen tendencias paralelas a lo largo del tiempo; es decir, se asume que los factores (no observados) influyen en el empleo en Pensilvania y Nueva Jersey de la misma manera. Esto permite interpretar un cambio observado en el empleo en Pensilvania como el cambio que habría experimentado Nueva Jersey si no hubiera un aumento en el salario mínimo (y viceversa).

En contra de lo que sugeriría la teoría económica estándar, los autores no encontraron evidencia de que el aumento del salario mínimo indujera un aumento del desempleo en Nueva Jersey utilizando el enfoque DID: Todo lo contrario, sus resultados sugieren que el aumento del salario mínimo de $\$0.80$ en Nueva Jersey dio lugar a un aumento de $2.75$ equivalente a tiempo completo (FTE) en el empleo.

```{r, 733, echo=F, purl=F, results='asis'}
if (my_output == "html") {
  cat('
<div  class = "DCexercise">

#### 1. Los datos de Card y Krueger (1994) {-}

<tt>fastfood.dat</tt>, el conjunto de datos utilizado por Card & Krueger (1994) se puede descargar [aquí](http://www.stat.ucla.edu/projects/datasets/fastfood.dta). Consulte este [enlace](http://www.stat.ucla.edu/projects/datasets/fastfood-explanation.html) para obtener una explicación detallada de las variables.

Este ejercicio pide que importe el conjunto de datos en <tt>R</tt> y que realice algunos formatos necesarios para el análisis posterior. Esto puede ser tedioso usando las funciones base <tt>R</tt>, pero se hace fácilmente usando el paquete <tt>dplyr</tt> presentado en el Capítulo \\@ref(UABGI).

La URL del conjunto de datos se guarda en <tt>data_URL</tt>.

**Instrucciones:**

  - Adjunte los paquetes <tt>dplyr</tt> y <tt>foreign</tt>.

  - Lea el conjunto de datos <tt>fastfood.dta</tt> usando <tt>data_URL</tt> y asignarlo a un <tt>data.frame</tt> llamado <tt>dat</tt>.

  En su estudio, Card y Krueger (1994) miden el empleo en equivalentes de tiempo completo que definen como el número de empleados a tiempo completo (<tt>empft</tt> y <tt>empft2</tt>) más el número de gerentes (<tt>nmgrs</tt> y <tt>nmgrs2</tt>) más 0.5 veces el número de empleados a tiempo parcial (<tt>emppt</tt> / <tt>emppt2</tt>).

  - Definir el empleo a tiempo completo antes (<tt>FTE</tt>) y después del aumento salarial (<tt>FTE2</tt>) y agregar ambas variables a <tt>dat</tt>.

<iframe src="DCL/ex13_1.html" frameborder="0" scrolling="no" style="width:100%;height:340px"></iframe>

**Sugerencias:**

  - <tt>read.dta()</tt> del paquete <tt>foreign</tt> para leer archivos <tt>.dta</tt>, un formato utilizado por el paquete de software estadístico *STATA*.

  - <tt>mutate()</tt> genera nuevas columnas usando las existentes.

</div>')}
```

```{r, 734, echo=F, purl=F, results='asis'}
if (my_output == "html") {
  cat('
<div  class = "DCexercise"> 

#### 2. Estimaciones estatales específicas de empleo a tiempo completo --- I {-}

Este ejercicio pide realizar un cálculo rápido de las medias muestrales específicas del estado para verificar si los datos sobre el empleo a tiempo completo están alineados con los datos utilizados por Card y Krueger (1994).

**Instrucciones:**

  - Generar subconjuntos de <tt>dat</tt> para separar las observaciones de Nueva Jersey y Pensilvania. Guárdarlos como <tt>dat_NJ</tt> y <tt>dat_PA</tt>.

  - Calcular las medias muestrales de los equivalentes de empleo a tiempo completo para Nueva Jersey y Pensilvania, tanto antes como después del aumento del salario mínimo en Nueva Jersey. Es suficiente si el código imprime los valores correctos en la consola.

<iframe src="DCL/ex13_2.html" frameborder="0" scrolling="no" style="width:100%;height:340px"></iframe>

**Sugerencias:**

  - Se puede usar <tt>group_by()</tt> junto con <tt>summary()</tt> para calcular las medias grupales. Ambas funciones vienen con el paquete <tt>dplyr</tt>.

</div>')}
```


```{r, 735, echo=F, purl=F, results='asis'}
if (my_output == "html") {
  cat('
<div  class = "DCexercise"> 

#### 3. Estimaciones estatales específicas de empleo a tiempo completo --- II {-}
  
Un enfoque ingenuo para investigar el impacto del aumento del salario mínimo en el empleo es utilizar la diferencia estimada en el empleo medio antes y después del aumento salarial para los restaurantes de comida rápida de Nueva Jersey.

Este ejercicio le pide que haga lo antes mencionado y además pruebe si la diferencia estimada es significativamente diferente de cero usando una prueba *robusta* $t$.

Los subconjuntos <tt>dat_NJ</tt> y <tt>dat_PA</tt> del ejercicio anterior están disponibles en el entorno de trabajo.

**Instrucciones:**

  - Usar <tt>dat_NJ</tt> para una prueba sólida de la hipótesis de que no existe diferencia en el empleo a tiempo completo antes y después del aumento salarial en Nueva Jersey al nivel de $5\\%$.

<iframe src="DCL/ex13_3.html" frameborder="0" scrolling="no" style="width:100%;height:340px"></iframe>

**Sugerencias:**

  - El problema de prueba equivale a una prueba $t$ de dos muestras que se realiza convenientemente usando <tt>t.test()</tt>.

</div>')}
```

```{r, 736, echo=F, purl=F, results='asis'}
if (my_output == "html") {
  cat('
<div  class = "DCexercise"> 

#### 4. Preparación de los datos para la regresión {-}

Se puede demostrar que las estimaciones realizadas en el Ejercicio 3 y el enfoque de diferencias en diferencias con el que se está trabajando producen los mismos resultados MCO aplicados a modelos de regresión específicos, ver Capítulos \\@ref(RPECEI) y \\@ref(UABGI).

Este ejercicio le pide que construya un conjunto de datos que sea más conveniente para este propósito que el conjunto de datos <tt>dat</tt>.

**Instrucciones:**

Generar el conjunto de datos <tt>reg_dat</tt> a partir de <tt>dat</tt> en *formato largo*; es decir, asegúrese de que para cada restaurante (identificado por <tt>sheet</tt>) una observación antes y una después del aumento del salario mínimo (identificado por <tt>D</tt>) se incluyen.

Solo considere las siguientes variables:

  - <tt>id</tt>: Número de sheet (id único de tienda)

  - <tt>chain</tt>: Cadena 1 = Burger King; 2 = KFC; 3 = Roy Rogers; 4 = Wendys

  - <tt>state</tt>: 1 si es Nueva Jersey; 0 si Pensilvania

  - <tt>empl</tt>: Medida del empleo a tiempo completo (<tt>FTE</tt>/<tt>FTE2</tt>)

  - <tt>D</tt>: Variable ficticia que indica si la observación se realizó antes o después del aumento del salario mínimo en Nueva Jersey.

<iframe src="DCL/ex13_4.html" frameborder="0" scrolling="no" style="width:100%;height:340px"></iframe>

**Sugerencias:**

  - El conjunto de datos original <tt>dat</tt> tiene 410 observaciones de 48 variables (verificar esto usando <tt>dim(dat)</tt>). El conjunto de datos <tt>reg_dat</tt> que se le pide generar debe constar de 820 observaciones de las variables enumeradas anteriormente.

  - Es sencillo generar un <tt>data.frame</tt> a partir de las columnas de otro <tt>data.frame</tt> usando <tt>data.frame(...)</tt>.

  - Utilizar <tt>rbind()</tt> para combinar dos objetos de tipo <tt>data.frame</tt> por fila.

</div>')}
```

```{r, 737, echo=F, purl=F, results='asis'}
if (my_output == "html") {
  cat('
<div  class = "DCexercise">   

#### 5. Una estimación de la diferencia utilizando datos de Card & Krueger (1994) --- II {-}

<tt>reg_dat</tt> del ejercicio 4 es un *conjunto de datos de panel*, ya que tiene dos observaciones para cada restaurante de comida rápida  $i=1,\\dots,410$, en períodos de tiempo $t=0,1$. 

Por tanto, se puede escribir el modelo de regresión simple

$$employment_{i,t} = \\beta_0 + \\beta_1 D_t + \\varepsilon_{i,t},$$

donde $D_t$ es una variable ficticia que es igual a $0$ si la observación se realizó antes del cambio de salario mínimo ($t = 0$) y $1$ después del cambio de salario mínimo ($t = 1$); es decir,

\\begin{align*}
D_t = \\begin{cases}
0, & \\, \\text{if $t=0$ (antes del cambio de salario),} \\\\
1, & \\, \\text{if $t=1$ (después del cambio de salario)}
\\end{cases}
\\end{align*}

y suponga que las observaciones de *restaurantes de Nueva Jersey solamente* se utilizan para calcular  $\\hat\\beta_1$, el estimador MCO de $\\beta_1$, que también se denomina *estimador de diferencias*.

El conjunto de datos <tt>reg_dat</tt> del ejercicio 4 y el subconjunto de Nueva Jersey <tt>dat_NJ</tt> están disponibles en el entorno de trabajo.

**Instrucciones:**

  - Estimar $\\beta_1$ en el modelo anterior usando MCO. Guardar el modelo estimado en <tt>emp_mod</tt>.

  - Obtener un resumen sólido de los resultados e interprete los hallazgos.

<iframe src="DCL/ex13_5.html" frameborder="0" scrolling="no" style="width:100%;height:340px"></iframe>

**Sugerencias:**

  - Recuerde que las dependencias del paquete <tt>AER</tt> incluyen funciones para una inferencia robusta en modelos de regresión.

  - El argumento <tt>subset</tt> en <tt>lm()</tt> toma un vector lógico que identifica las observaciones utilizadas para la estimación.

</div>')}
```

```{r, 738, echo=F, purl=F, results='asis'}
if (my_output == "html") {
  cat('
<div  class = "DCexercise">

#### 6. Una estimación de la diferencia utilizando datos de Card & Krueger (1994) --- II {-}

La estimación obtenida usando <tt>t.test()</tt> en el subconjunto de Nueva Jersey en el ejercicio 3 y la estimación de MCO de $\\hat\\beta_1$ en el ejercicio 5 son numéricamente iguales. Esto también es válido para las estadísticas de $t$ informadas si se utilizan las mismas fórmulas de error estándar (<tt>t.test(..., var.equal = T)</tt> y <tt>coeftest(... , vcov. = vcovHC, type = "HC1")</tt>).

Este ejercicio le pide que compruebe que la afirmación anterior sea cierta.

Los datos de los ejercicios anteriores, el resultado de <tt>t.test(...)</tt> del Ejercicio 3 así como el objeto del modelo de regresión <tt>emp_mod</tt> del Ejercicio 5 están disponibles en su ambiente de trabajo. Se ha adjuntado el paquete <tt>AER</tt>.

*No se realizan pruebas de corrección de envío.*

**Instrucciones:**

  - Verificar que la estimación de $\\beta_1$ en el ejercicio 5 sea igual a la diferencia estimada en el empleo medio de los restaurantes de comida rápida de Nueva Jersey antes y después del aumento del salario mínimo del ejercicio 3.

  - Convénzase de que las estadísticas $t$ reportadas por <tt>coeftest(...)</tt> en el ejercicio 5 y <tt>t.test(...)</tt> en el ejercicio 3 coinciden.

<iframe src="DCL/ex13_6.html" frameborder="0" scrolling="no" style="width:100%;height:340px"></iframe>

</div>')}
```

```{r, 739, echo=F, purl=F, results='asis'}
if (my_output == "html") {
  cat('
<div  class = "DCexercise">

#### 7. Una estimación de diferencias en diferencias --- II {-}

Como se mencionó en el Capítulo \\@ref(UABGI), el enfoque discutido en los Ejercicios 5 y 6 es ingenuo: $\\hat\\beta_1$ es una estimación sesgada del efecto promedio del aumento del salario mínimo sobre el empleo porque no se puede controlar para otros determinantes del empleo que se correlacionan con $D_t$. Como ejemplo, piense en los desarrollos macroeconómicos que tienen un impacto positivo en el mercado laboral, de manera que el empleo es mayor en el período posterior al aumento del salario mínimo. Es probable que $D_t$ se correlacione positivamente con el término de error de manera que $\\hat\\beta_1$ *sobreestime* el efecto del aumento salarial en el empleo.

Esto motiva el uso del estimador de diferencias en diferencias (DID) descrito en el Capítulo \\@ref(UABGI).

Considere el modelo de regresión lineal:

$$employment_{i,t} = \\beta_0 + \\beta_1 D_t + \\beta_2 state_i + \\beta_3 (D_t \\times state_i) + \\varepsilon_{i,t},$$

donde se usan índices $i$ y $t$, tal como en el modelo de regresión simple del ejercicio 5.

En este modelo, $\\beta_3$ es el coeficiente que interesa, ya que se interpreta como la diferencia promedio en el empleo de los restaurantes de comida rápida de Nueva Jersey antes y después del aumento salarial después de controlar los elementos inobservables que son comunes en Nueva Jersey y Pensilvania, el grupo de control. El estimador MCO de $\\beta_3$ se llama estimador DID.

**Instrucciones:**

- Estime el modelo anterior utilizando MCO y obtenga un resumen sólido.

- Interprete los hallazgos.

<iframe src="DCL/ex13_7.html" frameborder="0" scrolling="no" style="width:100%;height:340px"></iframe>

</div>')}
```

<!--chapter:end:Capitulo_14.Rmd-->

# Introducción a la regresión de series de tiempo y pronóstico {#IRSTP}

```{r, echo = F}
options(knitr.duplicate.label = "allow")
```

```{r, 740, child="_setup.Rmd"}
```

```{r, 741, eval=knitr::opts_knit$get("rmarkdown.pandoc.to") == "html", results='asis', echo=FALSE}
cat('<hr style="background-color:#03193b;height:2px">')
```

```{r, 742, warnings = FALSE, message=FALSE, echo=F, purl=F}
library(dynlm)
library(stargazer)
library(scales)
library(readxl)
library(urca)
```

Los datos de series de tiempo son datos que se recopilan para una sola entidad a lo largo del tiempo. Esto es fundamentalmente diferente de los datos de sección transversal, que son datos sobre múltiples entidades en el mismo momento. Los datos de series de tiempo permiten estimar el efecto en $Y$ de un cambio en $X$ *a lo largo del tiempo*. Esto es lo que los econometristas llaman un *efecto causal dinámico*. Volviendo a la aplicación al consumo de cigarrillos del capítulo \@ref(RVI) donde interesaba estimar el efecto sobre la demanda de cigarrillos de un aumento de precio causado por un aumento del impuesto general a las ventas. Se podrían utilizar datos de series de tiempo para evaluar el efecto causal de un aumento de impuestos sobre el tabaquismo tanto inicialmente como en períodos posteriores.

Otra aplicación de los datos de series de tiempo es la previsión. Por ejemplo, los servicios meteorológicos utilizan datos de series de tiempo para predecir la temperatura del mañana, entre otras cosas, utilizando la temperatura actual y las temperaturas del pasado. Para motivar un ejemplo económico, los bancos centrales están interesados en pronosticar las tasas de desempleo del próximo mes.

El resto de los capítulos del curso trata de las técnicas econométricas para el análisis de datos de series de tiempo y las aplicaciones para pronosticar y estimar los efectos causales dinámicos. Esta sección cubre los conceptos básicos de las series de tiempo, en las que se explica cómo visualizar datos de series de tiempo y se demuestra cómo estimar modelos autorregresivos simples, donde los regresores son valores pasados de la variable dependiente u otras variables. En este contexto también se discute el concepto de estacionariedad, una propiedad importante que tiene consecuencias de gran alcance.

La mayoría de las aplicaciones empíricas de este capítulo se refieren a pronosticar y utilizar datos sobre indicadores macroeconómicos o series de tiempo financieras de EE. UU. como el Producto Interno Bruto (PIB), la tasa de desempleo o el exceso de rendimiento de las acciones.

Los siguientes paquetes y sus dependencias son necesarios para la reproducción de los fragmentos de código presentados a lo largo de este capítulo:

+ **AER** [@R-AER]
+ **dynlm** [@R-dynlm]
+ **forecast** [@R-forecast]
+ **readxl** [@R-readxl]
+ **stargazer** [@R-stargazer]
+ **scales** [@R-scales]
+ **quantmod** [@R-quantmod]
+ **urca** [@R-urca]

Verifique que el siguiente fragmento de código se ejecute en su máquina sin ningún error:

```{r, 743, warning=FALSE, message=FALSE, eval=FALSE}
library(AER)
library(dynlm)
library(forecast)
library(readxl)
library(stargazer)
library(scales)
library(quantmod)
library(urca)
```

## Uso de modelos de regresión para la previsión

¿Cuál es la diferencia entre la estimación de modelos para la evaluación de efectos causales y la previsión? Considere nuevamente el ejemplo simple de estimar el efecto casual de la proporción alumno-maestro en los puntajes de las pruebas presentado en el Capítulo \@ref(RLR).

```{r, 744, warning = FALSE, message=FALSE}
library(AER)
data(CASchools)   
CASchools$STR <- CASchools$students/CASchools$teachers       
CASchools$score <- (CASchools$read + CASchools$math)/2

mod <- lm(score ~ STR, data = CASchools)
mod
```

Como se enfatizó en el Capítulo \@ref(MRVR), la estimación del coeficiente de la razón alumno-maestro no tiene una interpretación causal debido al sesgo de la variable omitida. Sin embargo, en términos de decidir a qué escuela enviar a su hijo, podría ser atractivo para un padre usar **mod** para pronosticar los puntajes de las pruebas en los distritos escolares donde no existen datos públicos disponibles sobre los puntajes.

Como ejemplo, suponga que la clase promedio en un distrito tiene $25$ estudiantes. Este no es un pronóstico perfecto, pero la siguiente serie puede ser útil para que los padres decidan.

```{r, 745}
predict(mod, newdata = data.frame("STR" = 25))
```

En un contexto de series de tiempo, el padre podría usar datos sobre los puntajes de las pruebas del año presente y pasado para pronosticar los puntajes de las pruebas del próximo año, una aplicación típica para un modelo autorregresivo.

## Datos de series de tiempo y correlación serial {#DSTCS}

El PIB se define comúnmente como el valor de los bienes y servicios producidos durante un período de tiempo determinado. El conjunto de datos **us_macro_quarterly.xlsx** es proporcionado por los autores y se puede descargar [aquí](http://wps.pearsoned.co.uk/wps/media/objects/16103/16489878/data3eu/us_macro_quarterly.xlsx). Proporciona datos trimestrales sobre el PIB real de Estados Unidos (es decir, ajustado por inflación) desde 1947 hasta 2004.

Como antes, un buen punto de partida es graficar los datos. El paquete **quantmod** proporciona algunas funciones convenientes para trazar y calcular con datos de series de tiempo. También se carga el paquete **readxl** para leer los datos en **R**.

```{r, 746, warning=FALSE, message=FALSE}
# adjuntar el paquete 'quantmod'
library(quantmod)
```

Se comienza importando el conjunto de datos.

```{r, 747}
# cargar datos macroeconómicos de EE. UU.
USMacroSWQ <- read_xlsx("Data/us_macro_quarterly.xlsx",
                         sheet = 1,
                         col_types = c("text", rep("numeric", 9)))

# formatear columna de fecha
USMacroSWQ$...1 <- as.yearqtr(USMacroSWQ$...1, format = "%Y:0%q")

# ajustar los nombres de las columnas
colnames(USMacroSWQ) <- c("Date", "GDPC96", "JAPAN_IP", "PCECTPI", 
                          "GS10", "GS1", "TB3MS", "UNRATE", "EXUSUK", "CPIAUCSL")
```

La primera columna de **us_macro_quarterly.xlsx** contiene texto y las restantes son numéricas. Usando **col_types = c (" text ", rep (" numeric ", 9))** se indica a **read_xlsx()** tener esto en cuenta al importar los datos.

Es útil trabajar con objetos de series de tiempo que realizan un seguimiento de la frecuencia de los datos y son extensibles. En lo que sigue se usarán objetos de la clase **xts**, ver `?Xts`. Dado que los datos en **USMacroSWQ** están en frecuencia trimestral, se convierte la primera columna al formato **yearqtr** antes de generar el **xts** objeto **GDP**.

```{r, 748}
# serie de PIB como objeto xts
GDP <- xts(USMacroSWQ$GDPC96, USMacroSWQ$Date)["1960::2013"]

# serie de crecimiento del PIB como objeto xts
GDPGrowth <- xts(400 * log(GDP/lag(GDP)))
```

Los siguientes fragmentos de código reproducen un gráfico con los datos:

```{r, 749, fig.align='center'}
# gráfico (a)
plot(log(as.zoo(GDP)),
     col = "steelblue",
     lwd = 2,
     ylab = "Logaritmo",
     xlab = "Fecha",
     main = "PIB real trimestral de EE. UU.")
```

```{r, 750, fig.align='center'}
# gráfico (b)
plot(as.zoo(GDPGrowth),
     col = "steelblue",
     lwd = 2,
     ylab = "Logaritmo",
     xlab = "Fecha",
     main = "Tasas de crecimiento del PIB real de EE. UU.")
```

### Notación, retrasos, diferencias, logaritmos y tasas de crecimiento {-}

Para las observaciones de una variable $Y$ registradas a lo largo del tiempo, $Y_t$ denota el valor observado en el momento $t$. El período entre dos observaciones secuenciales $Y_t$ y $Y_{t-1}$ es una unidad de tiempo: Horas, días, semanas, meses, trimestres, años, entre otros. El Concepto clave 14.1 presenta la terminología y la notación esenciales para los datos de series de tiempo que se usan en las siguientes secciones.

```{r, 751, eval = my_output == "html", results='asis', echo=F, purl=F}
cat('
<div class = "keyconcept" id="KC14.1">
<h3 class = "right"> Concepto clave 14.1 </h3>
<h3 class = "left"> Retrasos, primeras diferencias, logaritmos y tasas de crecimiento </h3>

- Los valores anteriores de una serie de tiempo se denominan *rezagos*. El primer desfase de $Y_t$ es $Y_{t-1}$. El retraso $j^{th}$ de $Y_t$ es $Y_{t-j}$. En <tt>R</tt>, los retrasos de los objetos de series de tiempo univariados o multivariados se calculan convenientemente mediante <tt>lag()</tt>, consulte <tt>?Lag</tt>.

- A veces se trabaja con series diferenciadas. La primera diferencia de una serie es $\\Delta Y_{t} = Y_t - Y_{t-1}$, la diferencia entre los períodos $t$ y $t-1$. Si <tt>Y</tt> es una serie de tiempo, la serie de las primeras diferencias se calcula como <tt>diff(Y)</tt>.

- Puede resultar conveniente trabajar con la primera diferencia en logaritmos de una serie. Se denota esto por $\\Delta \\log(Y_t) = \\log(Y_t) - \\log(Y_{t-1})$. Para una serie de tiempo <tt>Y</tt>, esto se obtiene usando <tt>log(Y/lag(Y))</tt>.

- $100 \\Delta \\log (Y_t)$ es una aproximación del cambio porcentual entre $Y_t$ y $Y_{t-1}$.

</div>
')
```

```{r, 752, eval = my_output == "latex", results='asis', echo=F, purl=F}
cat('\\begin{keyconcepts}[Retrasos, primeras diferencias, logaritmos y tasas de crecimiento]{14.1}
\\begin{itemize}
\\item Los valores anteriores de una serie de tiempo se denominan \\textit{rezagos}. El primer desfase de $Y_t$ es $Y_{t-1}$. El retraso $j^{th}$ de $Y_t$ es $Y_{t-j}$. En \\texttt{R}, los retrasos de los objetos de series de tiempo univariados o multivariados se calculan convenientemente mediante \\texttt{lag()}, consulte \\texttt{?lag}.
\\item A veces se trabaja con series diferenciadas. La primera diferencia de una serie es $\\Delta Y_{t} = Y_t - Y_{t-1}$, la diferencia entre los períodos $t$ y $t-1$. Si \\texttt{Y} es una serie de tiempo, la serie de las primeras diferencias se calcula como \\texttt{diff(Y)}.
\\item Puede resultar conveniente trabajar con la primera diferencia en logaritmos de una serie. Se denota esto por $\\Delta \\log(Y_t) = \\log(Y_t) - \\log(Y_{t-1})$. Para una serie de tiempo \\texttt{Y}, esto se obtiene usando \\texttt{log(Y/lag(Y))}.
\\item $100 \\Delta \\log (Y_t)$ es una aproximación del cambio porcentual entre $Y_t$ y $Y_{t-1}$.
\\end{itemize}
\\end{keyconcepts}
')
```

Las definiciones hechas en el Concepto clave 14.1 son útiles debido a dos propiedades que son comunes a muchas series de tiempo económicas:

- Crecimiento exponencial: Algunas series económicas crecen aproximadamente exponencialmente de tal manera que su logaritmo es aproximadamente lineal.

- La desviación estándar de muchas series de tiempo económicas es aproximadamente proporcional a su nivel. Por lo tanto, la desviación estándar del logaritmo de tal serie es aproximadamente constante.

Además, es común informar las tasas de crecimiento en series macroeconómicas, por lo que a menudo se utilizan diferencias $\log$.

A continuación se presenta la serie temporal del PIB trimestral de EE. UU; su logaritmo, la tasa de crecimiento anualizada y el primer rezago de la serie de la tasa de crecimiento anualizada para el período 2012:Q1 - 2013:Q1. La función simple **serie** se puede utilizar para calcular estas cantidades para una serie de tiempo trimestral.

```{r, 753}
# calcular logaritmos, tasas de crecimiento anual y el primer rezago de las tasas de crecimiento
quants <- function(series) {
  s <- series
  return(
    data.frame("Nivel" = s,
               "Logaritmo" = log(s),
               "Tasa anual de crecimiento" = 400 * log(s / lag(s)),
               "Primer rezago de la tasa de crecimiento anual" = lag(400 * log(s / lag(s))))
    )
}
```

La tasa de crecimiento anual se calcula usando la aproximación $$Annual Growth Y_t = 400 \cdot\Delta\log(Y_t)$$, ya que $100\cdot\Delta\log(Y_t)$ es una aproximación de los cambios porcentuales trimestrales, ver Concepto clave 14.1.

Se llama **quants()** a las observaciones para el período 2011:Q3 - 2013:Q1.

```{r, 754}
# obtain a data.frame with level, logarithm, annual growth rate and its 1st lag of GDP
quants(GDP["2011-07::2013-01"])
```

#### Autocorrelación {-}

Las observaciones de una serie temporal suelen estar correlacionadas. Este tipo de correlación se llama *autocorrelación* o *correlación en serie*. El Concepto clave 14.2 resume los conceptos de autocovarianza poblacional y autocorrelación poblacional. De igual forma, muestra cómo calcular sus equivalentes muestrales.

```{r, 755, eval = my_output == "html", results='asis', echo=F, purl=F}
cat('
<div class = "keyconcept" id="KC14.2">
<h3 class = "right"> Concepto clave 14.2 </h3>
<h3 class = "left"> Autocorrelación y autocovarianza </h3>

La covarianza entre $Y_t$ y su rezago $j^{th}$, $Y_{t-j}$, se denomina $j^{th}$ *autocovarianza* de la serie $Y_t$. El $j^{th}$ *coeficiente de autocorrelación*, también llamado *coeficiente de correlación serial*, mide la correlación entre $Y_t$ y $Y_{t-j}$. Así se tiene:

\\begin{align*}
  j^{th} \\text{autocovarianza} =& \\, Cov(Y_t,Y_{t-j}), \\\\
  j^{th} \\text{autocorrelación} = \\rho_j =& \\, \\rho_{Y_t,Y_{t-j}} = \\frac{Cov(Y_t,Y_{t-j)}}{\\sqrt{Var(Y_t)Var(Y_{t-j})}}.
\\end{align*}

La autocovarianza de la población y la autocorrelación de la población se pueden estimar mediante $\\widehat{Cov(Y_t,Y_{t-j})}$, la autocovarianza de la muestra y $\\widehat{\\rho}_j$, la autocorrelación de la muestra:

\\begin{align*}
  \\widehat{Cov(Y_t,Y_{t-j})} =& \\, \\frac{1}{T} \\sum_{t=j+1}^T (Y_t - \\overline{Y}_{j+1:T})(Y_{t-j} - \\overline{Y}_{1:T-j}), \\\\
  \\widehat{\\rho}_j =& \\, \\frac{\\widehat{Cov(Y_t,Y_{t-j})}}{\\widehat{Var(Y_t)}}
\\end{align*}

$\\overline{Y}_{j+1:T}$ denota el promedio de $Y_{j+1}, Y{j+2}, \\dots, Y_T$.

En <tt>R</tt> la función <tt>acf()</tt> del paquete <tt>stats</tt> calcula la autocovarianza de muestra o la función de autocorrelación de muestra.

</div>
')
```

```{r, 756, eval = my_output == "latex", results='asis', echo=F, purl=F}
cat('\\begin{keyconcepts}[Autocorrelation and Autocovariance]{14.2}

La covarianza entre $Y_t$ y su rezago $j^{th}$, $Y_{t-j}$, se denomina $j^{th}$ \\textit{autocovarianza} de la serie $Y_t$. El $j^{th}$ \\textit{coeficiente de autocorrelación}, también llamado \\textit{coeficiente de correlación serial}, mide la correlación entre $Y_t$ y $Y_{t-j}$. Así se tiene:

\\begin{align*}
  j^{th} \\text{autocovarianza} =& \\, Cov(Y_t,Y_{t-j}), \\\\
  j^{th} \\text{autocorrelación} = \\rho_j =& \\, \\rho_{Y_t,Y_{t-j}} = \\frac{Cov(Y_t,Y_{t-j)}}{\\sqrt{Var(Y_t)Var(Y_{t-j})}}.
\\end{align*}

La autocovarianza de la población y la autocorrelación de la población se pueden estimar mediante $\\widehat{Cov(Y_t,Y_{t-j})}$, la autocovarianza de la muestra y $\\widehat{\\rho}_j$, la autocorrelación de la muestra:

\\begin{align*}
  \\widehat{Cov(Y_t,Y_{t-j})} =& \\, \\frac{1}{T} \\sum_{t=j+1}^T (Y_t - \\overline{Y}_{j+1:T})(Y_{t-j} - \\overline{Y}_{1:T-j}), \\\\
  \\widehat{\\rho}_j =& \\, \\frac{\\widehat{Cov(Y_t,Y_{t-j})}}{\\widehat{Var(Y_t)}}.
\\end{align*}\\vspace{0.5cm}

$\\overline{Y}_{j+1:T}$ denota el promedio de $Y_{j+1}, Y{j+2}, \\dots, Y_T$.\\newline

En \\texttt{R} la función \\texttt{acf()} del paquete \\texttt{stats} calcula la autocovarianza de muestra o la función de autocorrelación de muestra.

\\end{keyconcepts}
')
```

Usando **acf()** es sencillo calcular las primeras cuatro autocorrelaciones de muestra de la serie **GDPGrowth**.

```{r, 757}
acf(na.omit(GDPGrowth), lag.max = 4, plot = F)
```

Esto es evidencia de que existe una leve autocorrelación positiva en el crecimiento del PIB: Si el PIB crece más rápido que el promedio en un período, existe una tendencia que crece más rápido que el promedio en los siguientes períodos.

#### Otros ejemplos de series de tiempo económicas {-}

Se presentan cuatro gráficos: La tasa de desempleo de EE. UU; el tipo de cambio dólar estadounidense/libra esterlina, el logaritmo del índice de producción industrial japonés, así como los cambios diarios en el índice de precios de las acciones de Wilshire 5000, una serie de tiempo financiera. El siguiente fragmento de código reproduce los gráficos de las tres series macroeconómicas y agrega cambios porcentuales en los valores diarios del índice compuesto de la Bolsa de Nueva York como cuarto (el conjunto de datos **NYSESW** viene con el paquete **AER**).

```{r, 758}
# definir series como objetos xts
USUnemp <- xts(USMacroSWQ$UNRATE, USMacroSWQ$Date)["1960::2013"]

DollarPoundFX <- xts(USMacroSWQ$EXUSUK, USMacroSWQ$Date)["1960::2013"]
  
JPIndProd <- xts(log(USMacroSWQ$JAPAN_IP), USMacroSWQ$Date)["1960::2013"]

# adjuntar datos de NYSESW
data("NYSESW")  
NYSESW <- xts(Delt(NYSESW))
```

```{r, 759, fig.align='center'}
# dividir el área de graficado en una matriz de 2x2
par(mfrow = c(2, 2))

# graficar la serie
plot(as.zoo(USUnemp),
     col = "steelblue",
     lwd = 2,
     ylab = "Porcentaje",
     xlab = "Fecha",
     main = "Tasa de desempleo de EE. UU.",
     cex.main = 1)

plot(as.zoo(DollarPoundFX),
     col = "steelblue",
     lwd = 2,
     ylab = "Dólar por libra",
     xlab = "Fecha",
     main = "Tipo de cambio del dólar estadounidense / libra esterlina",
     cex.main = 1)

plot(as.zoo(JPIndProd),
     col = "steelblue",
     lwd = 2,
     ylab = "Logaritmo",
     xlab = "Fecha",
     main = "Producción industrial japonesa",
     cex.main = 1)

plot(as.zoo(NYSESW),
     col = "steelblue",
     lwd = 2,
     ylab = "Porcentaje por día",
     xlab = "Fecha",
     main = "Índice compuesto de la bolsa de valores de Nueva York",
     cex.main = 1)
```

La serie presenta características bastante diferentes. La tasa de desempleo aumenta durante las recesiones y disminuye durante la recuperación económica y el crecimiento. El tipo de cambio dólar/libra muestra un patrón determinista hasta el final del sistema de Bretton Woods. La producción industrial de Japón muestra una tendencia ascendente y un crecimiento decreciente. Los cambios diarios en el índice compuesto de la Bolsa de Nueva York parecen fluctuar aleatoriamente alrededor de la línea cero. Las autocorrelaciones de muestra apoyan esta conjetura.

```{r, 760}
# calcular la autocorrelación de la muestra para la serie NYSESW
acf(na.omit(NYSESW), plot = F, lag.max = 10)
```

Los primeros 10 coeficientes de autocorrelación de muestra están muy cerca de cero. El gráfico predeterminado generado por `acf()` proporciona más evidencia.

```{r, 761, fig.align='center'}
# graficar la autocorrelación de la muestra para la serie NYSESW
acf(na.omit(NYSESW), main = "Sample Autocorrelation for NYSESW Data")
```

Las bandas de trazos azules representan valores más allá de los cuales las autocorrelaciones son significativamente diferentes de cero al nivel de $5\%$. Incluso cuando las verdaderas autocorrelaciones son cero, se debe esperar algunas superaciones; recuerde la definición de error de tipo I del Concepto clave 3.5.

Para la mayoría de los rezagos, se puede ver que la autocorrelación de la muestra no excede las bandas y solo existen unos pocos casos que se encuentran marginalmente más allá de los límites.

Además, la serie **NYSESW** muestra lo que los econometristas llaman *agrupamiento de volatilidad*: Existen períodos de alta y baja variación. Esto es común para muchas series de tiempo financieras.

## Autoregresiones

Los modelos autorregresivos se utilizan mucho en la previsión económica. Un modelo autorregresivo relaciona una variable de serie temporal con sus valores pasados. En esta sección se analizan las ideas básicas de los modelos de autorregresiones, se muestra cómo se estiman y se analiza una aplicación para pronosticar el crecimiento del PIB utilizando **R**.

#### El modelo autorregresivo de primer orden {-}

Es intuitivo que el pasado inmediato de una variable debería tener el poder de predecir su futuro cercano. El modelo autorregresivo más simple utiliza solo el resultado más reciente de la serie temporal observada para predecir valores futuros. Para una serie de tiempo $Y_t$, dicho modelo se denomina modelo autorregresivo de primer orden, a menudo abreviado AR(1), donde el 1 indica que el orden de autorregresión es uno:

\begin{align*}
  Y_t = \beta_0 + \beta_1 Y_{t-1} + u_t
\end{align*}

es el modelo de población AR(1) de una serie de tiempo $Y_t$.

Para la serie de crecimiento del PIB, un modelo autorregresivo de primer orden utiliza solo la información sobre el crecimiento del PIB observado en el último trimestre para predecir una tasa de crecimiento futura. El modelo de autorregresión de primer orden del crecimiento del PIB se puede estimar calculando estimaciones de MCO en la regresión de $GDPGR_t$ sobre $GDPGR_{t-1}$,

\begin{align}
  \widehat{GDPGR}_t = \hat\beta_0 + \hat\beta_1  GDPGR_{t-1}. (\#eq:GDPGRAR1)
\end{align}

Se usan datos de 1962 a 2012 para estimar \@ref(eq:GDPGRAR1). Esto se hace fácilmente con la función **ar.ols()** del paquete **stats**.

```{r, 762}
# datos de subconjunto
GDPGRSub <- GDPGrowth["1962::2012"]

# estimar el modelo
ar.ols(GDPGRSub, 
       order.max = 1, 
       demean = F, 
       intercept = T)
```

Se puede comprobar que los cálculos realizados por **ar.ols()** son los mismos que los realizados por **lm()**.

```{r, 763}
# longitud del conjunto de datos
N <-length(GDPGRSub)

GDPGR_level <- as.numeric(GDPGRSub[-1])
GDPGR_lags <- as.numeric(GDPGRSub[-N])

# estimar el modelo
armod <- lm(GDPGR_level ~ GDPGR_lags)
armod
```

Como de costumbre, se puede usar **coeftest()** para obtener un resumen robusto de los coeficientes de regresión estimados.

```{r, 764}
# resumen robusto
coeftest(armod, vcov. = vcovHC, type = "HC1")
```

Por tanto, el modelo estimado es

\begin{align}
  \widehat{GDPGR}_t = \underset{(0.351)}{1.995} + \underset{(0.076)}{0.338} GDPGR_{t-1} (\#eq:gdpgrar1).
\end{align}

Se ha omitido la primera observación para $GDPGR_{1962 \ Q1}$ del vector de la variable dependiente, ya que $GDPGR_{1962 \ Q1 - 1} = GDPGR_{1961 \ Q4}$, no está incluido en la muestra. De manera similar, la última observación, $GDPGR_{2012 \ Q4}$, se excluye del vector predictor ya que los datos no incluyen $GDPGR_{2012 \ Q4 + 1} = GDPGR_{2013 \ Q1}$. Dicho de otra manera, al estimar el modelo, se pierde una observación debido a la estructura de series de tiempo de los datos.

#### Pronósticos y errores de pronóstico {-}

Suponga que $Y_t$ sigue un modelo AR(1) con una intersección y que tiene una estimación de MCO del modelo sobre la base de observaciones para períodos de $T$. Luego puede usar el modelo AR(1) para obtener $\widehat{Y}_{T+1\vert T}$, un pronóstico para $Y_{T+1}$ usando datos hasta el período $T$ donde

\begin{align*}
  \widehat{Y}_{T+1\vert T} = \hat{\beta}_0 + \hat{\beta}_1 Y_T.
\end{align*}

El error de previsión es

\begin{align*}
  \text{Error de previsión} = Y_{T+1} - \widehat{Y}_{T+1\vert T}.
\end{align*}

#### Pronósticos y valores previstos {-}

Los valores pronosticados de $Y_t$ *no* son lo que se llama *valores predichos de MCO* de $Y_t$. Además, el error de pronóstico *no* es un residuo de MCO. Los pronósticos y los errores de pronóstico se obtienen utilizando valores *fuera de la muestra*, mientras que los valores predichos y los residuos se calculan para los valores *dentro de la muestra* que se observaron y utilizaron realmente para estimar el modelo.

El error de pronóstico de la raíz cuadrada media (EPRCM) mide el tamaño típico del error de pronóstico y se define como

\begin{align*}
  EPRCM = \sqrt{E\left[\left(Y_{T+1} - \widehat{Y}_{T+1\vert T}\right)^2\right]}.
\end{align*}

El $EPRCM$ está compuesto por los errores futuros $u_t$ y el error cometido al estimar los coeficientes. Cuando el tamaño de la muestra es grande, el primero puede ser mucho mayor que el segundo, de modo que $EPRCM \approx \sqrt{Var()u_t}$ que puede estimarse mediante el error estándar de la regresión.

#### Aplicación al crecimiento del PIB {-}

Utilizando \@ref(eq:gdpgrar1), el modelo estimado AR(1) de crecimiento del PIB, se realiza el pronóstico de crecimiento del PIB para 2013:Q1 (recuerde que el modelo se estimó utilizando datos para los períodos 1962:Q1 - 2012:Q4, por lo que 2013, el primer trimestre, es un período fuera de la muestra). Conectando $GDPGR_{2012:Q4} \approx 0.15$ en \@ref(eq:gdpgrar1),

\begin{align*}
  \widehat{GDPGR}_{2013:Q1} = 1.995 + 0.348 \cdot 0.15 = 2.047.
\end{align*}

La función **forecast()** del paquete **forecast** tiene algunas características útiles para pronosticar datos de series de tiempo.

```{r, 765, message=FALSE}
library(forecast)

# asignar tasa de crecimiento del PIB en 2012:Q4
new <- data.frame("GDPGR_lags" = GDPGR_level[N-1])

# pronosticar la tasa de crecimiento del PIB para 2013:Q1
forecast(armod, newdata = new)
```

El uso de **forecast()** produce el mismo pronóstico puntual de aproximadamente 2.0, junto con intervalos de pronóstico de $80\%$ y $95\%$, consulte la sección \@ref(PAMADL). Se concluye con que el modelo AR(1) prevé un crecimiento del PIB de $2\%$ en 2013:Q1.

¿Qué tan preciso es este pronóstico? El error de pronóstico es bastante grande: $GDPGR_{2013:Q1} \approx 1.1\%$ mientras que el pronóstico es $2\%$.

En segundo lugar, al llamar `summary(armod)` se muestra que el modelo explica solo una pequeña parte de la variación en la tasa de crecimiento del PIB y el $SER$ es de aproximadamente $3.16$. Dejando de lado la incertidumbre del pronóstico debido a la estimación de los coeficientes del modelo $\beta_0$ y $\beta_1$, el $EPRCM$ debe ser al menos $3.16\%$, la estimación de la desviación estándar de los errores. Se concluye que este pronóstico es bastante inexacto.

```{r, 766}
# calcular el error de pronóstico
forecast(armod, newdata = new)$mean - GDPGrowth["2013"][1]

# R^2
summary(armod)$r.squared

# SER
summary(armod)$sigma
```

### Modelos autorregresivos de orden $p$ {-}

Para pronosticar el crecimiento del PIB, el modelo AR($1$) \@ref(eq:gdpgrar1) descarta cualquier información del pasado de la serie que esté más distante que un período. Un modelo AR($p$) incorpora la información de $p$ rezagos de la serie. La idea se explica en el Concepto clave 14.3.

```{r, 767, eval = my_output == "html", results='asis', echo=F, purl=F}
cat('
<div class = "keyconcept" id="KC14.3">
<h3 class = "right"> Concepto clave 14.3 </h3>
<h3 class = "left"> Autorregresiones </h3>
<p>

Un modelo AR($p$) supone que una serie de tiempo $Y_t$ puede modelarse mediante una función lineal de los primeros $p$ de sus valores rezagados.

\\begin{align*}
  Y_t = \\beta_0 + \\beta_1 Y_{t-1} + \\beta_2 Y_{t-2} + \\dots + \\beta_p Y_{t-p} + u_t
\\end{align*}

es un modelo autorregresivo de orden $p$ donde $E(u_t\\vert Y_{t-1}, Y_{t-2}, \\dots,Y_{t-p})=0$.

</p>
</div>
')
```

```{r, 768, eval = my_output == "latex", results='asis', echo=F, purl=F}
cat('\\begin{keyconcepts}[Autorregresiones]{14.3}

Un modelo AR($p$) supone que una serie de tiempo $Y_t$ puede modelarse mediante una función lineal de los primeros $p$ de sus valores rezagados.

\\begin{align*}
  Y_t = \\beta_0 + \\beta_1 Y_{t-1} + \\beta_2 Y_{t-2} + \\dots + \\beta_p Y_{t-p} + u_t
\\end{align*}

es un modelo autorregresivo de orden $p$ donde $E(u_t\\vert Y_{t-1}, Y_{t-2}, \\dots,Y_{t-p})=0$.

\\end{keyconcepts}
')
```

Se estima un modelo AR($2$) de la serie de crecimiento del PIB desde 1962:Q1 hasta 2012:Q4.

```{r, 769}
# estimar el modelo AR(2)
GDPGR_AR2 <- dynlm(ts(GDPGR_level) ~ L(ts(GDPGR_level)) + L(ts(GDPGR_level), 2))

coeftest(GDPGR_AR2, vcov. = sandwich)
```

La estimación rinde

\begin{align}
  \widehat{GDPGR}_t = \underset{(0.40)}{1.63} + \underset{(0.08)}{0.28} GDPGR_{t-1} + \underset{(0.08)}{0.18} GDPGR_{t-1}. (\#eq:GDPGRAR2)
\end{align}

Se puede ver que el coeficiente en el segundo rezago es significativamente diferente de cero. El ajuste mejora ligeramente: $\bar{R}^2$ crece de $0.11$ para el modelo AR($1$) a, aproximadamente, $0.14$ y el $SER$ se reduce a $3.13$.

```{r, 770}
# R^2
summary(GDPGR_AR2)$r.squared

# SER
summary(GDPGR_AR2)$sigma
```

Se puede utilizar el modelo AR($2$) para obtener un pronóstico del crecimiento del PIB en 2013:Q1 de la misma manera que para el modelo AR(1).

```{r, 771}
# pronóstico AR(2) de crecimiento del PIB en 2013:Q1 
forecast <- c("2013:Q1" = coef(GDPGR_AR2) %*% c(1, GDPGR_level[N-1], GDPGR_level[N-2]))
```

Esto conduce a un error de pronóstico de aproximadamente $-1\%$.

```{r, 772}
# calcular el error de pronóstico AR(2)
GDPGrowth["2013"][1] - forecast
```

## ¿Puede ganarle al mercado? (Parte I) {#PGMPI}

La teoría de los mercados de capital eficientes establece que los precios de las acciones incorporan toda la información disponible actualmente. Si esta hipótesis se cumple, no debería ser posible estimar un modelo útil para pronosticar los rendimientos futuros de las acciones utilizando información disponible públicamente sobre los rendimientos pasados (esto también se conoce como la hipótesis de eficiencia de forma débil): Si fuera posible pronosticar el mercado, los comerciantes podrían arbitrar; por ejemplo, al confiar en un modelo AR($2$), usarían información que aún no está incluida en el precio, lo que empujaría los precios hasta que el rendimiento esperado sea cero.

Esta sección reproduce los resultados de las estimaciones. Comenzando por importar los datos mensuales desde 1931:1 hasta 2002:12 sobre el exceso de rendimiento de un índice de precios de acciones de base amplia, el índice ponderado por valor CRSP. Los datos son proporcionados como una hoja de Excel que se puede descargar [aquí](http://wps.aw.com/wps/media/objects/11422/11696965/data3eu/Stock_Returns_1931_2002.xlsx).

```{r, 773}
# leer datos sobre el rendimiento de las acciones
SReturns <- read_xlsx("Data/Stock_Returns_1931_2002.xlsx",
                      sheet = 1,
                      col_types = "numeric")
```

Se continua convirtiendo los datos en un objeto de clase **ts**.

```{r, 774}
# convertir los datos en un objeto ts
StockReturns <- ts(SReturns[, 3:4], 
                   start = c(1931, 1), 
                   end = c(2002, 12), 
                   frequency = 12)
```

A continuación, se estiman los modelos AR($1$), AR($2$) y AR($4$) del exceso de rendimientos para el período de tiempo 1960:1 a 2002:12.

```{r, 775}
# estimar modelos AR:

# AR(1)
SR_AR1 <- dynlm(ExReturn ~ L(ExReturn), 
      data = StockReturns, start = c(1960, 1), end = c(2002, 12))

# AR(2)
SR_AR2 <- dynlm(ExReturn ~ L(ExReturn) + L(ExReturn, 2), 
      data = StockReturns, start = c(1960, 1), end = c(2002, 12))

# AR(4)
SR_AR4 <- dynlm(ExReturn ~ L(ExReturn) + L(ExReturn, 1:4), 
      data = StockReturns, start = c(1960, 1), end = c(2002, 12))
```

Después de calcular errores estándar robustos, se recopilan los resultados en una tabla generada por **stargazer()**.

```{r, 776}
# calcular errores estándar robustos
rob_se <- list(sqrt(diag(sandwich(SR_AR1))),
               sqrt(diag(sandwich(SR_AR2))),
               sqrt(diag(sandwich(SR_AR4))))
```

```{r, 777, message=F, warning=F, results='asis', eval=F}
# generar una tabla usando 'stargazer()'
stargazer(SR_AR1, SR_AR2, SR_AR4,
  title = "Modelos autorregresivos de exceso de rentabilidad de existencias mensuales",
  header = FALSE, 
  model.numbers = F,
  omit.table.layout = "n",
  digits = 3, 
  column.labels = c("AR(1)", "AR(2)", "AR(4)"),
  dep.var.caption  = "Variable dependiente: Rendimientos excesivos en el índice ponderado por valor de CSRP",
  dep.var.labels.include = FALSE,
  covariate.labels = c("$excess return_{t-1}$", "$excess return_{t-2}$", 
                       "$excess return_{t-3}$", "$excess return_{t-4}$", 
                       "Intercept"),
  se = rob_se,
  omit.stat = "rsq") 
```

<!--html_preserve-->

```{r, 778, message=F, warning=F, results='asis', echo=F, purl=F, eval=my_output=="html"}
stargazer(SR_AR1, SR_AR2, SR_AR4,
  header = FALSE, 
  type = "html",
  model.numbers = F,
  omit.table.layout = "n",
  digits = 3, 
  column.labels = c("AR(1)", "AR(2)", "AR(4)"),
  dep.var.caption  = "Variable dependiente: Rendimientos excesivos en el índice ponderado por valor de CSRP ",
  dep.var.labels.include = FALSE,
  covariate.labels = c("$excess return_{t-1}$", "$excess return_{t-2}$", "$excess return_{t-3}$", "$excess return_{t-4}$", "Intercept"),
  se = rob_se,
  omit.stat = c("rsq")
  )

stargazer_html_title("Modelos autorregresivos de exceso de rentabilidad de existencias mensuales", "amomesr")
```

<!--/html_preserve-->

```{r, 779, message=F, warning=F, results='asis', echo=F, purl=F, eval=my_output=="latex"}
stargazer(SR_AR1, SR_AR2, SR_AR4,
  title = "\\label{tab:amomesr} Modelos autorregresivos de exceso de rentabilidad de existencias mensuales",
  header = FALSE, 
  type = "latex",
  model.numbers = F,
  omit.table.layout = "n",
  digits = 3, 
  column.labels = c("AR(1)", "AR(2)", "AR(4)"),
  dep.var.caption  = "Variable dependiente: Rendimientos excesivos en el índice ponderado por valor de CSRP",
  dep.var.labels.include = FALSE,
  covariate.labels = c("$excess return_{t-1}$", "$excess return_{t-2}$", "$excess return_{t-3}$", "$excess return_{t-4}$", "Intercept"),
  se = rob_se,
  omit.stat = c("rsq")
  ) 
```

Los resultados son consistentes con la hipótesis de mercados financieros eficientes: No existen coeficientes estadísticamente significativos en ninguno de los modelos estimados y la hipótesis de que todos los coeficientes son cero no puede rechazarse. $\bar{R}^2$ es casi cero en todos los modelos e incluso negativo para el modelo AR($4$). Esto sugiere que ninguno de los modelos es útil para pronosticar la rentabilidad de las acciones.

## Predictores adicionales y el modelo ADL {#PAMADL}

En lugar de utilizar únicamente los rezagos de la variable dependiente como predictores, un modelo de retraso distribuido autorregresivo (ADL) también utiliza rezagos de otras variables para el pronóstico. El modelo general de ADL se resume en el Concepto clave 14.4:

```{r, 780, eval = my_output == "html", results='asis', echo=F, purl=F}
cat('
<div class = "keyconcept" id="KC14.4">
<h3 class = "right"> Concepto clave 14.4 </h3>
<h3 class = "left"> El modelo de retardo distribuido autorregresivo </h3>
<p>

Un modelo ADL($p$, $q$) asume que una serie de tiempo $Y_t$ puede ser representada por una función lineal de $p$ de sus valores rezagados y $q$ rezagos de otra serie de tiempo $X_t$:

\\begin{align*}
  Y_t =& \\, \\beta_0 + \\beta_1 Y_{t-1} + \\beta_2 Y_{t-2} + \\dots + \\beta_p Y_{t-p} \\\\ 
      &+ \\, \\delta_1 X_{t-1} + \\delta_2 X_{t-2} + \\dots + \\delta_q X_{t-q} X_{t-q} + u_t.
\\end{align*}

es un *modelo de retraso distribuido autorregresivo* con $p$ retrasos de $Y_t$ y $q$ retrasos de $X_t$ donde $$E(u_t\\vert Y_{t-1}, Y_{t-2}, \\dots, X_{t-1}, X_{t-2}, \\dots)=0.$$

</p>
</div>
')
```

```{r, 781, eval = my_output == "latex", results='asis', echo=F, purl=F}
cat('\\begin{keyconcepts}[El modelo de retardo distribuido autorregresivo]{14.4}

Un modelo ADL($p$, $q$) asume que una serie de tiempo $Y_t$ puede ser representada por una función lineal de $p$ de sus valores rezagados y $q$ rezagos de otra serie de tiempo $X_t$:

\\begin{align*}
  Y_t =& \\, \\beta_0 + \\beta_1 Y_{t-1} + \\beta_2 Y_{t-2} + \\dots + \\beta_p Y_{t-p} \\\\ 
      &+ \\, \\delta_1 X_{t-1} + \\delta_2 X_{t-2} + \\dots + \\delta_q X_{t-q} X_{t-q} + u_t.
\\end{align*}

es un \\textit{modelo de retraso distribuido autorregresivo} con $p$ retrasos de $Y_t$ y $q$ retrasos de $X_t$ donde $$E(u_t\\vert Y_{t-1}, Y_{t-2}, \\dots, X_{t-1}, X_{t-2}, \\dots)=0.$$

\\end{keyconcepts}
')
```

#### Pronóstico del crecimiento del PIB mediante el diferencial de plazos {-}

Los tipos de interés de los bonos del tesoro a corto y largo plazo están estrechamente vinculados a las condiciones macroeconómicas. Si bien las tasas de interés de ambos tipos de bonos tienen las mismas tendencias a largo plazo, se comportan de manera bastante diferente a corto plazo.

La diferencia en las tasas de interés de dos bonos con vencimiento distinto se denomina *margen de plazo*.

Los siguientes fragmentos de código muestran las tasas de interés de los bonos del Tesoro de EE. UU. a 10 años y las letras del Tesoro de EE. UU. a 3 meses de 1960 a 2012.

```{r, 782}
# tasa de interés de las letras del Tesoro a 3 meses
TB3MS <- xts(USMacroSWQ$TB3MS, USMacroSWQ$Date)["1960::2012"]

# tasa de interés de los bonos del Tesoro a 10 años
TB10YS <- xts(USMacroSWQ$GS10, USMacroSWQ$Date)["1960::2012"]

# diferencial de plazo
TSpread <- TB10YS - TB3MS
```

```{r, 783, fig.align='center'}
# gráfico (a)
plot(merge(as.zoo(TB3MS), as.zoo(TB10YS)), 
     plot.type = "single", 
     col = c("darkred", "steelblue"),
     lwd = 2,
     xlab = "Fecha",
     ylab = "Porcentaje por año",
     main = "Tasas de interés")

# definir la función que transforma años en la clase 'yearqtr'
YToYQTR <- function(years) {
  return(
      sort(as.yearqtr(sapply(years, paste, c("Q1", "Q2", "Q3", "Q4"))))
  )
}

# recesiones
recessions <- YToYQTR(c(1961:1962, 1970, 1974:1975, 1980:1982, 1990:1991, 2001, 2007:2008))
          
# agregar sombreado de color para recesiones
xblocks(time(as.zoo(TB3MS)), 
        c(time(TB3MS) %in% recessions), 
        col = alpha("steelblue", alpha = 0.3))

# agrega una leyenda
legend("topright", 
       legend = c("TB3MS", "TB10YS"),
       col = c("darkred", "steelblue"),
       lwd = c(2, 2))

# gráfico (b)
plot(as.zoo(TSpread), 
     col = "steelblue",
     lwd = 2,
     xlab = "Fecha",
     ylab = "Porcentaje por año",
     main = "Diferencial de plazo")

# agregar sombreado de color para recesiones
xblocks(time(as.zoo(TB3MS)), 
        c(time(TB3MS) %in% recessions), 
        col = alpha("steelblue", alpha = 0.3))
```

Antes de las recesiones, la brecha entre las tasas de interés de los bonos a largo plazo y las letras a corto plazo se reduce y, en consecuencia, el diferencial de plazo se reduce drásticamente hacia cero o incluso se vuelve negativo en tiempos de tensión económica. Esta información podría utilizarse para mejorar las previsiones de crecimiento del PIB en el futuro.

Se verifica esto estimando un modelo ADL($2$, $1$) y un modelo ADL($2$, $2$) de la tasa de crecimiento del PIB utilizando rezagos del crecimiento del PIB y rezagos del diferencial de plazo como regresores. Luego se usan ambos modelos para pronosticar el crecimiento del PIB en 2013:Q1.

```{r, 784}
# convertir series de crecimiento y plazo en objetos ts
GDPGrowth_ts <- ts(GDPGrowth, 
                  start = c(1960, 1), 
                  end = c(2013, 4), 
                  frequency = 4)

TSpread_ts <- ts(TSpread, 
                start = c(1960, 1), 
                end = c(2012, 4), 
                frequency = 4)

# unir ambos objetos ts
ADLdata <- ts.union(GDPGrowth_ts, TSpread_ts)
```

```{r, 785}
# estimar el modelo ADL(2,1) de crecimiento del PIB
GDPGR_ADL21 <- dynlm(GDPGrowth_ts ~ L(GDPGrowth_ts) + L(GDPGrowth_ts, 2) + L(TSpread_ts), 
      start = c(1962, 1), end = c(2012, 4))

coeftest(GDPGR_ADL21, vcov. = sandwich)
```

La ecuación estimada del modelo ADL($2$, $1$) es:

\begin{align}
  \widehat{GDPGR}_t = \underset{(0.49)}{0.96} + \underset{(0.08)}{0.26} GDPGR_{t-1} + \underset{(0.08)}{0.19} GDPGR_{t-2} + \underset{(0.18)}{0.44} TSpread_{t-1} (\#eq:gdpgradl21)
\end{align}

Todos los coeficientes son significativos al nivel de $5\%$.

```{r, 786}
# 2012:Q3 / 2012:Q4 datos sobre el crecimiento del PIB y el diferencial de plazo
subset <- window(ADLdata, c(2012, 3), c(2012, 4))

# ADL(2,1) previsión de crecimiento del PIB para 2013:Q1
ADL21_forecast <- coef(GDPGR_ADL21) %*% c(1, subset[2, 1], subset[1, 1], subset[2, 2])
ADL21_forecast

# calcular el error de pronóstico
window(GDPGrowth_ts, c(2013, 1), c(2013, 1)) - ADL21_forecast
```

El modelo \@ref(eq:gdpgradl21) predice el crecimiento del PIB en 2013:Q1 será de $2.24\%$, lo que genera un error de pronóstico de $-1.10\%$.

Se estima la especificación de ADL($2$, $2$) para ver si agregar información adicional sobre el diferencial de plazos anteriores mejora el pronóstico.

```{r, 787}
# estimar el modelo ADL(2,2) de crecimiento del PIB
GDPGR_ADL22 <- dynlm(GDPGrowth_ts ~ L(GDPGrowth_ts) + L(GDPGrowth_ts, 2) 
                     + L(TSpread_ts) + L(TSpread_ts, 2), 
                     start = c(1962, 1), end = c(2012, 4))

coeftest(GDPGR_ADL22, vcov. = sandwich)
```

Se obtiene 

\begin{align}
  \begin{split}
    \widehat{GDPGR}_t =& \underset{(0.47)}{0.98} + \underset{(0.08)}{0.24} GDPGR_{t-1} \\
    & + \underset{(0.08)}{0.18} GDPGR_{t-2} -\underset{(0.42)}{0.14} TSpread_{t-1} + \underset{(0.43)}{0.66} TSpread_{t-2}.
  \end{split} (\#eq:gdpgradl22)
\end{align}

Los coeficientes en ambos rezagos del diferencial de plazo no son significativos al nivel de $10\%$.

```{r, 788}
# pronóstico de crecimiento del PIB ADL(2,2) para 2013:Q1
ADL22_forecast <- coef(GDPGR_ADL22) %*% c(1, subset[2, 1], subset[1, 1], subset[2, 2], subset[1, 2])
ADL22_forecast

# calcular el error de pronóstico
window(GDPGrowth_ts, c(2013, 1), c(2013, 1)) - ADL22_forecast
```

El pronóstico de ADL($2$, $2$) de crecimiento del PIB en 2013:Q1 es $2.27\%$, lo que implica un error de pronóstico de $1.14\%$.

¿Los modelos ADL \@ref(eq:gdpgradl21) y \@ref(eq:gdpgradl22) mejoran el modelo AR($2$) simple \@ref(eq:GDPGRAR2)? La respuesta es sí. No obstante, $SER$ y $\bar{R}^2$ solo mejoran ligeramente, una prueba $F$ sobre los coeficientes de diferenciación de términos en \@ref(eq:gdpgradl22) proporciona evidencia de que el modelo funciona mejor al explicar el crecimiento del PIB que el modelo AR($2$), ya que la hipótesis de que ambos coeficientes son cero no puede rechazarse al nivel de $5\%$.

```{r, 789}
# comparar R2 ajustada
c("Adj.R2 AR(2)" = summary(GDPGR_AR2)$r.squared,
  "Adj.R2 ADL(2,1)" = summary(GDPGR_ADL21)$r.squared,
  "Adj.R2 ADL(2,2)" = summary(GDPGR_ADL22)$r.squared)

# comparar SER
c("SER AR(2)" = summary(GDPGR_AR2)$sigma,
  "SER ADL(2,1)" = summary(GDPGR_ADL21)$sigma,
  "SER ADL(2,2)" = summary(GDPGR_ADL22)$sigma)

# prueba F sobre coeficientes de diferencias de plazos
linearHypothesis(GDPGR_ADL22, 
                 c("L(TSpread_ts)=0", "L(TSpread_ts, 2)=0"),
                 vcov. = sandwich)
```

#### Estacionariedad {-}

En general, los pronósticos se pueden mejorar mediante el uso de múltiples predictores, al igual que en la regresión transversal. Al construir modelos de series de tiempo, se debe tener en cuenta si las variables son *estacionarias* o *no estacionarias*. El Concepto clave 14.5 explica qué es la estacionariedad.

```{r, 790, eval = my_output == "html", results='asis', echo=F, purl=F}
cat('
<div class = "keyconcept" id="KC14.5">
<h3 class = "right"> Concepto clave 14.5 </h3>
<h3 class = "left"> Estacionariedad </h3>

Una serie de tiempo $Y_t$ es estacionaria si su distribución de probabilidad es independiente del tiempo; es decir, la distribución conjunta de $Y_{s+1}, Y_{s+2},\\dots,Y_{s+T}$ no cambia a medida que varía $s$, independientemente de $T$.

De manera similar, dos series de tiempo $X_t$ y $Y_t$ son *conjuntamente estacionarias* si la distribución conjunta de $(X_{s+1},Y_{s+1}, X_{s+2},Y_{s+2} \\dots, X_{s+T},Y_{s+T})$ no depende de $s$, independientemente de $T$.

En un sentido probabilístico, estacionariedad significa que la información sobre cómo evoluciona una serie de tiempo en el futuro es inherente a su pasado. Si este no es el caso, no se puede usar el pasado de una serie como una guía confiable para su futuro.

La estacionariedad facilita el aprendizaje de las características de los datos pasados.

</div>
')
```

```{r, 791, eval = my_output == "latex", results='asis', echo=F, purl=F}
cat('\\begin{keyconcepts}[Estacionariedad]{14.5}

Una serie de tiempo $Y_t$ es estacionaria si su distribución de probabilidad es independiente del tiempo; es decir, la distribución conjunta de $Y_{s+1}, Y_{s+2},\\dots,Y_{s+T}$ no cambia a medida que varía $s$, independientemente de $T$.\\newline

De manera similar, dos series de tiempo $X_t$ y $Y_t$ son \\textit{conjuntamente estacionarias} si la distribución conjunta de $(X_{s+1},Y_{s+1}, X_{s+2},Y_{s+2} \\dots, X_{s+T},Y_{s+T})$ no depende de $s$, independientemente de $T$.\\newline

En un sentido probabilístico, estacionariedad significa que la información sobre cómo evoluciona una serie de tiempo en el futuro es inherente a su pasado. Si este no es el caso, no se puede usar el pasado de una serie como una guía confiable para predecir su futuro.\\vspace{0.5cm}

La estacionariedad facilita el aprendizaje de las características de los datos pasados.

\\end{keyconcepts}
')
```

#### Regresión de series temporales con varios predictores {-}

El concepto de estacionariedad es un supuesto clave en el modelo de regresión de series de tiempo general con múltiples predictores. El Concepto clave 14.6 establece este modelo y sus supuestos.

```{r, 792, eval = my_output == "html", results='asis', echo=F, purl=F}
cat('
<div class = "keyconcept" id="KC14.6">
<h3 class = "right"> Concepto clave 14.6 </h3>
<h3 class = "left"> Regresión de series de tiempo con múltiples predictores </h3>

El modelo general de regresión de series de tiempo amplía el modelo ADL de manera que se incluyen múltiples regresores y sus rezagos. Utiliza $p$ retrasos de la variable dependiente y $q_l$ retrasos de $l$ predictores adicionales donde $l=1,\\dots,k$:

\\begin{equation}
  \\begin{aligned}
  Y_t =&  \\beta_0 + \\beta_1 Y_{t-1} + \\beta_2 Y_{t-2} + \\dots + \\beta_{p} Y_{t-p} \\\\
      &+  \\delta_{11} X_{1,t-1} + \\delta_{12} X_{1,t-2} + \\dots + \\delta_{1q} X_{1,t-q} \\\\
      &+  \\dots \\\\
      &+  \\delta_{k1} X_{k,t-1} + \\delta_{k2} X_{k,t-2} + \\dots + \\delta_{kq} X_{k,t-q} \\\\
      &+  u_t 
  \\end{aligned}
\\end{equation}

Para la estimación se hacen las siguientes suposiciones:

1. El término de error $u_t$ tiene una media condicional cero dados todos los regresores y sus retrasos:  $$E(u_t\\vert Y_{t-1}, Y_{t-2}, \\dots, X_{1,t-1}, X_{1,t-2} \\dots, X_{k,t-1}, X_{k,t-2}, \\dots)$$ Esta suposición es una extensión de la suposición de cero media condicional utilizada para los modelos AR y ADL. En este sentido, garantiza que el modelo de regresión de series de tiempo general indicado anteriormente da el mejor pronóstico de $Y_t$ dados sus rezagos, los regresores adicionales $X_{1,t},\\dots,X_{k,t}$ y sus rezagos.

2. La suposición de i.i.d. para los datos transversales no es (completamente) significativa para los datos de series de tiempo. Se reemplaza por el siguiente supuesto que consta de dos partes:

    (a) Los $(Y_{t}, X_{1,t}, \\dots, X_{k,t})$ tienen una distribución estacionaria (la parte "idénticamente distribuida" del supuesto i.i.d. para datos transversales). Si esto no se cumple, los pronósticos *pueden* estar sesgados y la inferencia *puede* ser muy engañosa.

    (b) $(Y_{t}, X_{1,t}, \\dots, X_{k,t})$ y $(Y_{t-j}, X_{1,t-j}, \\dots, X_{k,t-j})$ se vuelven independientes a medida que $j$ crece (la parte distribuida de forma "independiente" del supuesto i.i.d para datos transversales). Esta suposición también se llama *dependencia débil*. Garantiza que WLLN y CLT se mantengan en muestras grandes.

3. Es poco probable que existan valores atípicos grandes: $E(X_{1,t}^4), E(X_{2,t}^4), \\dots, E(X_{k,t}^4)$ y $E(Y_t^4)$ tienen cuartos momentos finitos distintos de cero.

4. Sin multicolinealidad perfecta.

</div>
')
```

```{r, 793, eval = my_output == "latex", results='asis', echo=F, purl=F}
cat('\\begin{keyconcepts}[Regresión de series de tiempo con múltiples predictores]{14.6}

El modelo general de regresión de series de tiempo amplía el modelo ADL de manera que se incluyen múltiples regresores y sus rezagos. Utiliza $p$ retrasos de la variable dependiente y $q_l$ retrasos de $l$ predictores adicionales donde $l=1,\\dots,k$:

\\begin{equation}
  \\begin{aligned}
  Y_t =& \\, \\beta_0 + \\beta_1 Y_{t-1} + \\beta_2 Y_{t-2} + \\dots + \\beta_{p} Y_{t-p} \\\\
      +& \\, \\delta_{11} X_{1,t-1} + \\delta_{12} X_{1,t-2} + \\dots + \\delta_{1q} X_{1,t-q} \\\\
      +& \\, \\dots \\\\
      +& \\, \\delta_{k1} X_{k,t-1} + \\delta_{k2} X_{k,t-2} + \\dots + \\delta_{kq} X_{k,t-q} \\\\
      +& \\, u_t 
  \\end{aligned}
\\end{equation}

Para la estimación se hacen las siguientes suposiciones:\\newline

\\begin{enumerate}
\\item El término de error $u_t$ tiene una media condicional cero dados todos los regresores y sus retrasos:  $$E(u_t\\vert Y_{t-1}, Y_{t-2}, \\dots, X_{1,t-1}, X_{1,t-2} \\dots, X_{k,t-1}, X_{k,t-2}, \\dots)$$ Esta suposición es una extensión de la suposición de cero media condicional utilizada para los modelos AR y ADL. En este sentido, garantiza que el modelo de regresión de series de tiempo general indicado anteriormente da el mejor pronóstico de $Y_t$ dados sus rezagos, los regresores adicionales $X_{1,t},\\dots,X_{k,t}$ y sus rezagos.
\\item La suposición de i.i.d. para los datos transversales no es (completamente) significativa para los datos de series de tiempo. Se reemplaza por el siguiente supuesto que consta de dos partes:\\newline

\\begin{itemize}
    \\item[(a)] Los $(Y_{t}, X_{1,t}, \\dots, X_{k,t})$ tienen una distribución estacionaria (la parte "idénticamente distribuida" del supuesto i.i.d. para datos transversales). Si esto no se cumple, los pronósticos \\textit{pueden} estar sesgados y la inferencia \\textit{puede} ser muy engañosa.   
  
    \\item[(b)] $(Y_{t}, X_{1,t}, \\dots, X_{k,t})$ y $(Y_{t-j}, X_{1,t-j}, \\dots, X_{k,t-j})$ se vuelven independientes a medida que $j$ crece (la parte distribuida de forma "independiente" del supuesto i.i.d para datos transversales). Esta suposición también se llama \\textit{dependencia débil}. Garantiza que WLLN y CLT se mantengan en muestras grandes.
\\end{itemize}
    
\\item Es poco probable que existan valores atípicos grandes: $E(X_{1,t}^4), E(X_{2,t}^4), \\dots, E(X_{k,t}^4)$ y $E(Y_t^4)$ tienen cuartos momentos finitos distintos de cero. 
\\item Sin multicolinealidad perfecta.
\\end{enumerate}
\\end{keyconcepts}
')
```

Dado que muchas series de tiempo económicas parecen no ser estacionarias, el supuesto dos del Concepto clave 14.6 es crucial en macroeconomía y finanzas aplicadas, razón por la cual se han desarrollado pruebas estadísticas de estacionariedad o no estacionariedad. Los capítulos \@ref(SLRUCI) y \@ref(NEIT) están dedicados a este tema.

#### Inferencia estadística y prueba de causalidad de Granger {-}

Si un $X$ es un predictor útil para $Y$, en una regresión de $Y_t$ con rezagos propios y rezagos de $X_t$, no todos los coeficientes de los rezagos en $X_t$ son cero. Este concepto se llama *causalidad de Granger* y es una hipótesis interesante para probar. El Concepto clave 14.7 resume la idea.

```{r, 794, eval = my_output == "html", results='asis', echo=F, purl=F}
cat('
<div class = "keyconcept" id="KC14.7">
<h3 class = "right"> Concepto clave 14.7 </h3>
<h3 class = "left"> Pruebas de causalidad de Granger </h3>
La prueba de causalidad de Granger @granger1969 es una prueba $F$ de la hipótesis nula de que *todos* los rezagos de una variable $X$ incluida en un modelo de regresión de series de tiempo no tienen poder predictivo para $Y_t$. La prueba de causalidad de Granger no prueba si $X$ realmente *causa* $Y$, sino si los retrasos incluidos son informativos en términos de predicción de $Y$.
</div>
')
```

```{r, 795, eval = my_output == "latex", results='asis', echo=F, purl=F}
cat('\\begin{keyconcepts}[Pruebas de causalidad de Granger]{14.7}
La prueba de causalidad de Granger\\citep{granger1969} es una prueba $F$ de la hipótesis nula de que \\textit{todos} los rezagos de una variable $X$ incluida en un modelo de regresión de series de tiempo no tienen poder predictivo para $Y_t$. La prueba de causalidad de Granger no prueba si $X$ realmente \\textit{causa} $Y$, sino si los retrasos incluidos son informativos en términos de predicción de $Y$.
\\end{keyconcepts}
')
```

Ya se ha realizado una prueba de causalidad de Granger sobre los coeficientes de diferencial de plazo en \@ref(eq:gdpgradl22), el modelo ADL($2$, $2$) de crecimiento del PIB y se concluye que al menos uno de los dos primeros rezagos del diferencial de plazo tiene poder predictivo para el crecimiento del PIB.

### Incertidumbre de pronóstico e intervalos de pronóstico {-}

En general, es una buena práctica informar una medida de la incertidumbre al presentar los resultados que se ven afectados por este último. La incertidumbre es particularmente interesante cuando se pronostica una serie de tiempo. Por ejemplo, considere un modelo ADL$(1,1)$ simple

\begin{align*}
  Y_t = \beta_0 + \beta_1 Y_{t-1} + \delta_1 X_{t-1} + u_t
\end{align*}

donde $u_t$ es un término de error homocedástico. El error de previsión es

\begin{align*}
  Y_{T+1} - \widehat{Y}_{T+1\vert T} = u_{T+1} - \left[(\widehat{\beta}_0 - \beta_0) + (\widehat{\beta}_1 - \beta_1) Y_T + (\widehat{\delta_1} - \delta_1) X_T \right].
\end{align*}

El error de pronóstico cuadrático medio (EPCM) y el RMFSE son

\begin{align*}
  MFSE =& \, E\left[(Y_{T+1} - \widehat{Y}_{T+1\vert T})^2 \right] \\
       =& \, \sigma_u^2 + Var\left[ (\widehat{\beta}_0 - \beta_0) + (\widehat{\beta}_1 - \beta_1) Y_T + (\widehat{\delta_1} - \delta_1) X_T \right], \\
  RMFSE =& \, \sqrt{\sigma_u^2 + Var\left[ (\widehat{\beta}_0 - \beta_0) + (\widehat{\beta}_1 - \beta_1) Y_T + (\widehat{\delta_1} - \delta_1) X_T \right]}.
\end{align*}

Un intervalo de pronóstico de $95\%$ es un intervalo que cubre el valor real de $Y_{T+1}$ en $95\%$ de aplicaciones repetidas. Existe una gran diferencia en el cálculo de un intervalo de confianza y un intervalo de pronóstico: Al calcular un intervalo de confianza de una estimación puntual, se utilizan grandes aproximaciones de muestra que están justificadas por el TLC y, por lo tanto, son válidas para una amplia gama de distribuciones de términos de error. Sin embargo, para calcular un intervalo de pronóstico de $Y_{T+1}$, se debe hacer una suposición adicional sobre la distribución de $u_{T+1}$, el término de error en el período $T+1$. Suponiendo que $u_{T+1}$ se distribuye normalmente, se puede construir un *intervalo de pronóstico* de $95\%$ para $Y_{T+1}$ usando $SE(Y_{T+1} - \widehat{Y}_{T+1\vert T})$, una estimación del EPRCM:

\begin{align*}
  \widehat{Y}_{T+1\vert T} \pm 1.96 \cdot SE(Y_{T+1} - \widehat{Y}_{T+1\vert T})
\end{align*}

Por supuesto, el cálculo se vuelve más complicado cuando el término de error es heterocedástico o si se está interesado en calcular un intervalo de pronóstico para $T+s, s>1$.

En algunas aplicaciones, es útil informar múltiples intervalos de pronóstico para períodos posteriores. Estos se pueden visualizar en un llamado diagrama de abanico. No se replicará el diagrama de abanico porque el modelo subyacente es mucho más complejo que los modelos AR y ADL simples que se tratan aquí. En su lugar, en el siguiente ejemplo se usan datos de series de tiempo simuladas y se estima un modelo AR ($2$) que luego se utiliza para pronosticar los subsiguientes $25$ resultados futuros de la serie.

```{r, 796, fig.align='center'}
# sembrar semilla
set.seed(1234)

# simular la serie temporal
Y <- arima.sim(list(order = c(2, 0, 0), ar = c(0.2, 0.2)),  n = 200)

# estimar un modelo AR(2) usando 'arima()', ver `?arima`
model <- arima(Y, order = c(2, 0, 0))

# calcular pronósticos de puntos e intervalos de predicción para los próximos 25 períodos
fc <- forecast(model, h = 25, level = seq(5, 99, 10))

# graficar un diagrama de abanico
plot(fc, 
     main = "Fan Chart de pronóstico para el modelo AR(2) de datos simulados", 
     showgap = F, 
     fcol = "red",
     flty = 2)
```

**arima.sim()** simula modelos de media móvil integrada autorregresiva (ARIMA). Los modelos AR pertenecen a esta clase de modelos. Se usa **list(order = c(2, 0, 0), ar = c(0.2, 0.2))**, por lo que el DGP (Data Generating Process) es $$Y_t = 0.2 Y_{t-1} + 0.2 Y_{t-2} + u_t.$$

Se elige **level = seq(5, 99, 10)** en la llamada de **forecast()**, de modo que se calculen los intervalos de pronóstico con niveles  $5\%, 15\%, \dots, 95\%$ para cada pronóstico puntual de la serie.

La línea roja discontinua muestra los pronósticos puntuales de la serie para los próximos 25 períodos basados en un modelo $ADL(1,1)$ y las áreas sombreadas representan los intervalos de predicción. El grado de sombreado indica el nivel del intervalo de predicción. La más oscura de las bandas azules muestra los intervalos de pronóstico de $5\%$ y el color se desvanece hacia el gris a medida que aumenta el nivel de los intervalos.

## Selección de la longitud de retraso utilizando criterios de información {#SLRUCI}

La selección de las longitudes de los rezagos en los modelos AR y ADL a veces puede estar guiada por la teoría económica. Sin embargo, existen métodos estadísticos que son útiles para determinar cuántos rezagos deben incluirse como regresores. En general, demasiados rezagos inflan los errores estándar de las estimaciones de coeficientes y, por lo tanto, implican un aumento en el error de pronóstico, mientras que omitir los rezagos que deberían incluirse en el modelo pueden dar lugar a un sesgo de estimación.

El orden de un modelo AR se puede determinar mediante dos enfoques:

1. **El enfoque de la prueba F**

    Estimar un modelo AR($p$) y pruebar la significancia del rezago o los rezagos más grandes. Si la prueba es rechazada, elimine el o los respectivos rezagos del modelo. Este enfoque tiene la tendencia a producir modelos donde el orden es demasiado grande: En una prueba de significancia siempre se enfrenta al riesgo de rechazar una verdadera hipótesis nula.

2. **Basándose en un criterio de información**

    Para evitar el problema de producir modelos demasiado grandes, se puede elegir el orden de retraso que minimiza uno de los siguientes dos criterios de información:
    
      + El *criterio de información de Bayes* (CIB):
      
        $$CIB(p) = \log\left(\frac{SSR(p)}{T}\right) + (p + 1) \frac{\log(T)}{T}$$
        
      + El *criterio de información de Akaike* (CIA):

        $$CIA(p) = \log\left(\frac{SSR(p)}{T}\right) + (p + 1) \frac{2}{T}$$

    Ambos criterios son estimadores de la longitud de rezago óptimo $p$. El orden de rezago $\widehat{p}$ que minimiza el criterio respectivo se llama *estimación CIB* o *estimación CIA* del orden de modelo óptimo. La idea básica de ambos criterios es que el $SSR$ disminuye a medida que se agregan retrasos adicionales al modelo, de modo que el primer término disminuye mientras que el segundo aumenta a medida que crece el orden de retraso. Se puede demostrar que el $CIB$ es un estimador consistente del verdadero orden de retraso, mientras que el $CIA$ no lo es, lo que se debe a los diferentes factores en el segundo sumando. Sin embargo, ambos estimadores se utilizan en la práctica donde el $CIA$ a veces se usa como una alternativa cuando el $CIB$ produce un modelo con "muy pocos" rezagos.

La función **dynlm()** no calcula criterios de información por defecto. Por lo tanto, se escribe una función corta que informa el $CIB$ (junto con el orden de retraso elegido $p$ y la $R^2$) para objetos de clase **dynlm**.

```{r, 797}
# calcular BIC (Bayesian information criterion) para objetos de modelo AR de clase 'dynlm'
BIC <- function(model) {
  
  ssr <- sum(model$residuals^2)
  t <- length(model$residuals)
  npar <- length(model$coef)
  
  return(
    round(c("p" = npar - 1,
          "BIC" = log(ssr/t) + npar * log(t)/t,
          "R2" = summary(model)$r.squared), 4)
  )
}
```

Se calcula el $CIB$ para los modelos AR ($p$) de crecimiento del PIB con orden $p=1,\dots,6$. El resultado final se puede reproducir fácilmente usando **sapply()** y la función **BIC()** definida anteriormente.

```{r, 798}
# aplicar el BIC() a un modelo de crecimiento del PIB de un solo intercepto
BIC(dynlm(ts(GDPGR_level) ~ 1))

# bucle BIC sobre modelos de diferentes órdenes
order <- 1:6

BICs <- sapply(order, function(x) 
        "AR" = BIC(dynlm(ts(GDPGR_level) ~ L(ts(GDPGR_level), 1:x))))

BICs
```

Se debe tomar en cuenta que aumentar el orden de retraso aumenta $R^2$ porque el $SSR$ disminuye a medida que se agregan retrasos adicionales al modelo, pero de acuerdo con el $CIB$, se debería conformar con el modelo AR($2$) en lugar del modelo AR($6$). Ayuda a decidir si la disminución en $SSR$ es suficiente para justificar la adición de un regresor adicional.

Si se tuvieran que comparar un conjunto más grande de modelos, una forma conveniente de seleccionar el modelo con el $CIB$ más bajo es usar la función **which.min()**.

```{r, 799}
# seleccione el modelo AR con el BIC más pequeño
BICs[, which.min(BICs[2, ])]
```

El $CIB$ también se puede usar para seleccionar longitudes de retardo en modelos de regresión de series de tiempo con múltiples predictores. En un modelo con coeficientes de $K$, incluida la intersección, se tiene:

\begin{align*}
    CIB(K) = \log\left(\frac{SSR(K)}{T}\right) + K \frac{\log(T)}{T}.
\end{align*}

Se debe tener en cuenta que elegir el modelo óptimo de acuerdo con el $CIB$ puede ser computacionalmente exigente porque puede haber muchas combinaciones diferentes de longitudes de retardo cuando existen múltiples predictores.

Para dar un ejemplo, se estima el modelos ADL($p$, $q$) de crecimiento del PIB donde, como se indicó anteriormente, la variable adicional es el diferencial de plazo entre bonos a corto y largo plazo. Se impone la restricción de que $p=q_1=\dots=q_k$ de modo que solo se deben estimar $p_{max}$ modelos ($p=1,\dots,p_{max}$). En el siguiente ejemplo, se elige $p_{max} = 12$.

```{r, 800}
# bucle 'BIC()' sobre múltiples modelos ADL 
order <- 1:12

BICs <- sapply(order, function(x) 
         BIC(dynlm(GDPGrowth_ts ~ L(GDPGrowth_ts, 1:x) + L(TSpread_ts, 1:x), 
                   start = c(1962, 1), end = c(2012, 4))))

BICs
```

De la definición de **BIC()**, para los modelos ADL con $p = q$ se deduce que **p** informa el número de coeficientes estimados *excluyendo* la intersección. Por lo tanto, el orden de retraso se obtiene dividiendo **p** entre 2.

```{r, 801}
# seleccionar el modelo ADL con el BIC más pequeño
BICs[, which.min(BICs[2, ])]
```

El $CIB$ está a favor del modelo ADL($2$, $2$) \@ref(eq:gdpgradl22) que se ha estimado antes.

## No estacionariedad I: Tendencias {#NEIT}

Si una serie no es estacionaria, las pruebas de hipótesis convencionales, los intervalos de confianza y los pronósticos pueden ser muy engañosos. El supuesto de estacionariedad se viola si una serie presenta tendencias o rupturas y las complicaciones resultantes en un análisis econométrico dependen del tipo específico de no estacionariedad. Esta sección se centra en las series de tiempo que muestran tendencias.

Se dice que una serie muestra una tendencia si tiene un movimiento persistente a largo plazo. Se distingue entre tendencias *deterministas* y *estocásticas*.

+ Una tendencia es *determinista* si es una función del tiempo no aleatoria.

+ Se dice que una tendencia es *estocástica* si es una función aleatoria del tiempo.

Las cifras que se han elaborado en el capítulo \@ref(DSTCS) revelan que muchas series de tiempo económicas muestran un comportamiento de tendencia que probablemente se modele mejor mediante tendencias estocásticas. Por eso el curso se centra en el tratamiento de las tendencias estocásticas.

#### El modelo de caminata aleatoria de una tendencia {-}

La forma más sencilla de modelar una serie de tiempo $Y_t$ que tiene una tendencia estocástica es el *paseo aleatorio* (*random walk*):

\begin{align}
  Y_t = Y_{t-1} + u_t, (\#eq:randomwalk)
\end{align}

donde los $u_t$ son errores i.i.d. con $E(u_t\vert Y_{t-1}, Y_{t-2}, \dots) = 0$. Se debe tener en cuenta que:

\begin{align*}
  E(Y_t\vert Y_{t-1}, Y_{t-2}\dots) =& \, E(Y_{t-1}\vert Y_{t-1}, Y_{t-2}\dots) + E(u_t\vert Y_{t-1}, Y_{t-2}\dots) \\
  =& \, Y_{t-1}
\end{align*}

por lo que el mejor pronóstico para $Y_t$ es la observación de ayer  $Y_{t-1}$. Por tanto, la diferencia entre $Y_t$ y $Y_{t-1}$ es impredecible. La ruta seguida por $Y_t$ consta de pasos aleatorios $u_t$, por lo que se denomina caminata aleatoria.

Suponga que $Y_0$, el valor inicial de la caminata aleatoria, es $0$. Otra forma de escribir \@ref(eq:randomwalk) es:

\begin{align*}
  Y_0 =& \, 0 \\
  Y_1 =& \, 0 + u_1 \\
  Y_2 =& \, 0 + u_1 + u_2 \\
  \vdots & \, \\
  Y_t =& \, \sum_{i=1}^t u_i.
\end{align*}

Por lo tanto se tiene que:

\begin{align*}
  Var(Y_t) =& \, Var(u_1 + u_2 + \dots + u_t) \\
           =& \, t \sigma_u^2.
\end{align*}

Por lo tanto, la varianza de una caminata aleatoria depende de $t$, lo que viola el supuesto presentado en el Concepto clave 14.5: Una caminata aleatoria no es estacionaria.

Obviamente, \@ref(eq:randomwalk) es un caso especial de un modelo AR($1$) donde $\beta_1 = 1$. Se puede demostrar que una serie de tiempo que sigue un modelo AR($1$) es estacionaria si $\lvert\beta_1\rvert < 1$. En un modelo AR($p$) general, la estacionariedad está vinculada a las raíces del polinomio $$1-\beta_1 z - \beta_2 z^2 - \beta_3 z^3 - \dots - \beta_p z^p.$$ Si todas las raíces son mayores que $1$ en valor absoluto, la serie AR($p$) es estacionaria. Si al menos una raíz es igual a $1$, se dice que el AR($p$) tiene una *raíz unitaria* y, por lo tanto, tiene una tendencia estocástica.

Es sencillo simular paseos aleatorios en **R** usando **arima.sim()**. La función **matplot()** es conveniente para gráficos simples de las columnas de una matriz.

```{r, 802, fig.align='center'}
# simular y graficar paseos aleatorios a partir de 0
set.seed(1)

RWs <- ts(replicate(n = 4, 
            arima.sim(model = list(order = c(0, 1 ,0)), n = 100)))

matplot(RWs, 
        type = "l", 
        col = c("steelblue", "darkgreen", "darkred", "orange"), 
        lty = 1, 
        lwd = 2,
        main = "Cuatro paseos al azar",
        xlab = "Tiempo",
        ylab = "Valor")
```

Agregar una constante a \@ref(eq:randomwalk) produce

\begin{align}
  Y_t = \beta_0 + Y_{t-1} + u_t (\#eq:randomwalkdrift),
\end{align}

un *modelo de paseo aleatorio con deriva* (*random walk model with drift*) permite modelar si la tendencia de una serie se mueve hacia arriba o hacia abajo. Si $\beta_0$ es positivo, la serie se desplaza hacia arriba y sigue una tendencia a la baja si $\beta_0$ es negativo.

```{r, 803, fig.align='center'}
# simular y graficar paseos aleatorios con deriva
set.seed(1)

RWsd <- ts(replicate(n = 4, 
           arima.sim(model = list(order = c(0, 1, 0)), 
                     n = 100,
                     mean = -0.2)))

matplot(RWsd, 
        type = "l", 
        col = c("steelblue", "darkgreen", "darkred", "orange"), 
        lty = 1, 
        lwd = 2,
        main = "Cuatro paseos aleatorios con deriva",
        xlab = "Tiempo",
        ylab = "Valor")
```

#### Problemas causados por tendencias estocásticas {-}

La estimación por MCO de los coeficientes en regresores que tienen una tendencia estocástica es problemática porque la distribución del estimador y su estadístico $t$ no es normal, incluso asintóticamente. Esto tiene varias consecuencias:

+ **Sesgo a la baja de los coeficientes autorregresivos**:
  
    Si $Y_t$ es una caminata aleatoria, $\beta_1$ puede estimarse consistentemente por MCO, pero el estimador está sesgado hacia cero. Este sesgo es aproximadamente $E(\widehat{\beta}_1) \approx 1 - 5.3/T$ que es sustancial para los tamaños de muestra que se encuentran típicamente en macroeconomía. Este sesgo de estimación hace que los pronósticos de $Y_t$ funcionen peor que un modelo de caminata aleatoria pura.
  
+ **Estadísticos $t$ no distribuidos normalmente**:

    La distribución no normal del coeficiente estimado de un regresor estocástico se traduce en una distribución no normal de su estadístico $t$, de modo que los valores críticos normales no son válidos y, por lo tanto, los intervalos de confianza habituales y las pruebas de hipótesis también son inválidos, y la verdadera distribución del estadístico $t$ no se puede determinar fácilmente.
    
+ **Regresión espuria**:

    Cuando dos series de tiempo con tendencia estocástica retroceden entre sí, la relación estimada puede parecer muy significativa utilizando valores críticos normales convencionales, aunque las series no están relacionadas. Esto es lo que los econometristas llaman una relación *espuria*.
    
Como ejemplo de regresión espuria, considere nuevamente los paseos aleatorios verde y rojo que se han simulado anteriormente. Se sabe que no existe relación entre ambas series: Se generaron independientemente una de la otra.

```{r, 804, fig.align='center'}
# graficar relación espuria
matplot(RWs[, c(2, 3)], 
        lty = 1,
        lwd = 2,
        type = "l",
        col = c("darkgreen", "darkred"),
        xlab = "Time",
        ylab = "",
        main = "Una relación espuria")    
```

Imagina que no tiene esta información y en su lugar conjetura que la serie verde es útil para predecir la serie roja y así termina estimando el modelo ADL($0$, $1$)

\begin{align*}
  Red_t = \beta_0 + \beta_1 Green_{t-1} + u_t.
\end{align*}
    
```{r, 805}
# estimar modelo AR espurio
summary(dynlm(RWs[, 2] ~ L(RWs[, 3])))$coefficients
```

El resultado es obviamente falso: El coeficiente en $Green_{t-1}$ se estima en aproximadamente $1$ y el valor $p$ de $1.14 \cdot 10^{-10}$ de la prueba $t$ correspondiente indica que el coeficiente es muy significativo mientras que su valor real es de hecho cero.

Como ejemplo empírico, considere la tasa de desempleo de Estados Unidos y la producción industrial japonesa. Ambas series muestran un comportamiento de tendencia ascendente desde mediados de la década de 1960 hasta principios de la de 1980.

```{r, 806}
# graficar la tasa de desempleo de EE. UU. y la producción industrial japonesa
plot(merge(as.zoo(USUnemp), as.zoo(JPIndProd)), 
     plot.type = "single", 
     col = c("darkred", "steelblue"),
     lwd = 2,
     xlab = "Fecha",
     ylab = "",
     main = "Regresión espuria: Series de tiempo macroeconómicas")

# agrega una leyenda
legend("topleft", 
       legend = c("USA-Desempleo", "JPN-ProducciónIndustrial"),
       col = c("darkred", "steelblue"),
       lwd = c(2, 2))
```

```{r, 807}
# estimar la regresión utilizando datos de 1962 a 1985
SR_Unemp1 <- dynlm(ts(USUnemp["1962::1985"]) ~ ts(JPIndProd["1962::1985"]))
coeftest(SR_Unemp1, vcov = sandwich)
```

Una regresión simple de la tasa de desempleo de EE. UU. sobre la producción industrial japonesa utilizando datos de los rendimientos entre 1962 a 1985

\begin{align}
  \widehat{U.S. UR}_t = -\underset{(1.12)}{2.37} + \underset{(0.29)}{2.22} \log(JapaneseIP_t). (\#eq:urjpip1)
\end{align}

Esta parece ser una relación significativa: La estadística $t$ del coeficiente en $\log(JapaneseIP_t)$ es mayor que 7.

```{r, 808}
# estimar la regresión usando datos de 1986 a 2012
SR_Unemp2 <- dynlm(ts(USUnemp["1986::2012"]) ~ ts(JPIndProd["1986::2012"]))
coeftest(SR_Unemp2, vcov = sandwich)
```

Al estimar el mismo modelo, esta vez con datos de 1986 a 2012, se obtiene 

\begin{align}
  \widehat{U.S. UR}_t = \underset{(5.41)}{41.78} -\underset{(1.17)}{7.78} \log(JapaneseIP)_t (\#eq:urjpip2)
\end{align}

que sorprendentemente es bastante diferente. \@ref(eq:urjpip1) indica una relación positiva moderada, en contraste con el gran coeficiente negativo en \@ref(eq:urjpip2). Este fenómeno se puede atribuir a las tendencias estocásticas de la serie: Dado que no existe un razonamiento económico que relacione ambas tendencias, ambas regresiones pueden ser espurias.

#### Prueba para una raíz unitaria AR {-}

@dickey1979 han propuesto una prueba formal para una tendencia estocástica que, por lo tanto, se denomina *prueba de Dickey-Fuller*. Como se mencionó anteriormente, una serie de tiempo que sigue un modelo AR($1$) con $\beta_1 = 1$ tiene una tendencia estocástica. Por tanto, el problema de las pruebas es:

\begin{align*}
  H_0: \beta_1 = 1 \ \ \ \text{vs.} \ \ \ H_1: \lvert\beta_1\rvert < 1.
\end{align*}

La hipótesis nula es que el modelo AR($1$) tiene una raíz unitaria y la hipótesis alternativa es que es estacionario. A menudo, uno reescribe el modelo AR($1$) restando $Y_{t-1}$ en ambos lados:

\begin{align}
  Y_t = \beta_0 + \beta_1 Y_{t-1} + u_t \ \ \Leftrightarrow \ \ \Delta Y_t = \beta_0 + \delta Y_{t-1} + u_t (\#eq:dfmod)
\end{align}

donde $\delta = \beta_1 - 1$. El problema de la prueba se convierte en

\begin{align*}
  H_0: \delta = 0 \ \ \ \text{vs.} \ \ \ H_1: \delta < 0
\end{align*}

lo cual es conveniente, ya que el estadístico de prueba correspondiente es reportado por muchas funciones relevantes en **R**.^[El estadístico $t$ de la prueba de Dickey-Fuller se calcula usando solamente errores estándar de homocedasticidad, ya que bajo la hipótesis nula, el estadístico $t$ es robusto a la heterocedasticidad condicional.]

La prueba de Dickey-Fuller también se puede aplicar en un modelo AR($p$). La *prueba de Dickey-Fuller aumentada (ADF)* se resume en el Concepto clave 14.8.

```{r, 809, eval = my_output == "html", results='asis', echo=F, purl=F}
cat('
<div class = "keyconcept" id="KC14.8">
<h3 class = "right"> Concepto clave 14.8 </h3>
<h3 class = "left"> La prueba ADF para una raíz unitaria </h3>
<p>

Considere la regresión

\\begin{align}
  \\Delta Y_t = \\beta_0 + \\delta Y_{t-1} + \\gamma_1 \\Delta_1 Y_{t-1} + \\gamma_2 \\Delta Y_{t-2} + \\dots + \\gamma_p \\Delta Y_{t-p} + u_t. (\\#eq:ADFreg1)
\\end{align}

La prueba ADF para una raíz unitaria autorregresiva prueba la hipótesis $H_0: \\delta = 0$ (tendencia estocástica) contra la alternativa unilateral $H_1: \\delta < 0$ (estacionariedad) utilizando el valor habitual de MCO, el estadístico $t$.

Si se supone que $Y_t$ es estacionario alrededor de una tendencia de tiempo lineal determinista, el regresor $t$ aumenta el modelo:

\\begin{align}
  \\Delta Y_t = \\beta_0 + at + \\delta Y_{t-1} + \\gamma_1 \\Delta_1 Y_{t-1} + \\gamma_2 \\Delta Y_{t-2} + \\dots + \\gamma_p \\Delta Y_{t-p} + u_t,  (\\#eq:ADFreg2)
\\end{align}

donde nuevamente $H_0: \\delta = 0$ se prueba contra $H_1: \\delta < 0$.

La longitud óptima del retraso $p$ se puede estimar utilizando criterios de información. En (\\@ref(eq:ADFreg1)), $p=0$ (no se usan rezagos de $\\Delta Y_t$ como regresores) corresponde a un AR($1$) simple.

Bajo el valor de la hipótesis nula, el estadístico $t$ correspondiente a $H_0: \\delta = 0$ no tiene una distribución normal. Los valores críticos solo se pueden obtener de la simulación y difieren para las regresiones \\@ref(eq:ADFreg1) y \\@ref(eq:ADFreg2), ya que la distribución del estadístico de prueba ADF es sensible a los componentes deterministas incluidos en la regresión.

</p>
</div>
')
```

```{r, 810, eval = my_output == "latex", results='asis', echo=F, purl=F}
cat('\\begin{keyconcepts}[La prueba ADF para una raíz unitaria]{14.8}

Considere la regresión

\\begin{align}
  \\Delta Y_t = \\beta_0 + \\delta Y_{t-1} + \\gamma_1 \\Delta_1 Y_{t-1} + \\gamma_2 \\Delta Y_{t-2} + \\dots + \\gamma_p \\Delta Y_{t-p} + u_t. \\label{eq:ADFreg1}
\\end{align}

La prueba ADF para una raíz unitaria autorregresiva prueba la hipótesis $H_0: \\delta = 0$ (tendencia estocástica) contra la alternativa unilateral $H_1: \\delta < 0$ (estacionariedad) utilizando el valor habitual de MCO, el estadístico $t$.\\newline

Si se supone que $Y_t$ es estacionario alrededor de una tendencia de tiempo lineal determinista, el regresor $t$ aumenta el modelo:

\\begin{align}
  \\Delta Y_t = \\beta_0 + at + \\delta Y_{t-1} + \\gamma_1 \\Delta_1 Y_{t-1} + \\gamma_2 \\Delta Y_{t-2} + \\dots + \\gamma_p \\Delta Y_{t-p} + u_t,  \\label{eq:ADFreg2}
\\end{align}

donde nuevamente $H_0: \\delta = 0$ se prueba contra $H_1: \\delta < 0$.\\newline


La longitud óptima del retraso $p$ se puede estimar utilizando criterios de información. En (\\@ref(eq:ADFreg1)), $p=0$ (no se usan rezagos de $\\Delta Y_t$ como regresores) corresponde a un AR($1$) simple.\\newline


Bajo el valor de la hipótesis nula, el estadístico $t$ correspondiente a $H_0: \\delta = 0$ no tiene una distribución normal. Los valores críticos solo se pueden obtener de la simulación y difieren para las regresiones \\@ref(eq:ADFreg1) y \\@ref(eq:ADFreg2), ya que la distribución del estadístico de prueba ADF es sensible a los componentes deterministas incluidos en la regresión.

\\end{keyconcepts}
')
```

#### Valores críticos para el estadístico ADF {-}

El Concepto clave 14.8 establece que los valores críticos para la prueba ADF en las regresiones \@ref(eq:ADFreg1) y \@ref(eq:ADFreg2) solo se pueden determinar mediante simulación. La idea del estudio de simulación es simular una gran cantidad de estadísticos de prueba ADF y usarlos para estimar cuantiles de su distribución *asintótica*. Esta sección muestra cómo se puede hacer esto usando **R**.

Primero, se debe considerar el siguiente modelo AR($1$) con intercepción:

\begin{align*}
  Y_t =& \, \alpha + z_t, \ \ z_t = \rho z_{t-1} + u_t.
\end{align*}

Esto se puede escribir como:

\begin{align*}
  Y_t =& \, (1-\rho) \alpha + \rho y_{t-1} + u_t,
\end{align*}

es decir, $Y_t$ es un paseo aleatorio sin deriva bajo la hipótesis nula $\rho = 1$. Se puede demostrar que $Y_t$ es un proceso estacionario con una media de $\alpha$ para $\lvert\rho\rvert<1$.

El procedimiento para simular valores críticos de una prueba de raíz unitaria usando la relación $t$ de $\delta$ en \@ref(eq:dfmod) es el siguiente:

+ Simular $N$ paseos aleatorios con $n$ observaciones utilizando el proceso de generación de datos:

\begin{align*}
  Y_t =& \, a + z_t, \ \ z_t = \rho z_{t-1} + u_t,
\end{align*}

$t=1,\dots,n$ donde $N$ y $n$ son números grandes, $a$ es una constante y $u$ es un término de error medio cero.

+ Para cada caminata aleatoria, se estima la regresión:

\begin{align*}
  \Delta Y_t =& \, \beta_0 + \delta Y_{t-1} + u_t
\end{align*}

y calcular el estadístico de prueba ADF. Guardar todas los estadísticos de prueba de $N$.

+ Estimar cuantiles de la distribución del estadístico de prueba ADF utilizando los estadísticos de prueba $N$ obtenidos de la simulación.

Para el caso de *deriva* y *tendencia* de tiempo lineal, se reemplaza el proceso de generación de datos por:

\begin{align}
  Y_t =& \, a + b \cdot t + z_t, \ \ z_t = \rho z_{t-1} + u_t (\#eq:rwdt)
\end{align}

donde $b \cdot t$ es una tendencia de tiempo lineal. $Y_t$ en \@ref(eq:rwdt) es una caminata aleatoria con (sin) deriva si $b\neq0$ ($b=0$) bajo el valor nulo de $\rho=1$ (¿puede demostrar esto?). Se estima la regresión:

\begin{align*}
  \Delta Y_t =& \, \beta_0 + \alpha \cdot t + \delta Y_{t-1} + u_t.
\end{align*}

En términos generales, la precisión de los cuantiles estimados depende de dos factores: $n$, la longitud de la serie subyacente y $N$, el número de estadísticos de prueba utilizados. Dado que se está interesado en estimar cuantiles de la distribución *asintótica* (la distribución de Dickey-Fuller) del estadístico de prueba ADF, tanto el uso de muchas observaciones como un gran número de estadísticos de prueba simulados aumentará la precisión de los cuantiles estimados. Se elige $n = N = 1000$, ya que la carga computacional crece rápidamente con $n$ y $N$.

```{r, 811}
# repeticiones
N <- 1000

# observaciones
n <- 1000

# definir constante, tendencia y rho
drift <- 0.5
trend <- 1:n
rho <- 1

# función que simula un proceso AR(1)
AR1 <- function(rho) {
  out <- numeric(n)
  for(i in 2:n) {
    out[i] <- rho * out[i-1] + rnorm(1)
  }
  return(out)
}

# simular desde PIB con constante
RWD <- ts(replicate(n = N, drift + AR1(rho)))

# calcular estadísticos de prueba de ADF y almacenarlos en 'ADFD'
ADFD <- numeric(N)

for(i in 1:ncol(RWD)) {
  ADFD[i] <- summary(
    dynlm(diff(RWD[, i], 1) ~ L(RWD[, i], 1)))$coef[2, 3]
}

# simular desde PIB con constante y tendencia
RWDT <- ts(replicate(n = N, drift + trend + AR1(rho)))

# calcular estadísticos de prueba de ADF y almacenarlos en 'ADFDT'
ADFDT <- numeric(N)

for(i in 1:ncol(RWDT)) {
  ADFDT[i] <- summary(
    dynlm(diff(RWDT[, i], 1) ~ L(RWDT[, i], 1) + trend(RWDT[, i]))
  )$coef[2, 3]
}
```

```{r, 812}
# estimar cuantiles para la regresión ADF con una deriva
round(quantile(ADFD, c(0.1, 0.05, 0.01)), 2)

# estimar cuantiles para regresión ADF con deriva y tendencia
round(quantile(ADFDT, c(0.1, 0.05, 0.01)), 2)
```

Los cuantiles estimados están cerca de los valores críticos de muestras grandes del estadístico de prueba ADF.

| Regresores deterministas         | 10%    | 5%     | 1%    |
|:---------------------------------|:-------|:-------|:------|
| Intercepto solo                  | -2.57  | -2.86  | -3.43 |
| Intercepto y tendencia temporal  | -3.12  | -3.41  | -3.96 |

Table: (\#tab:DFcrits) Muestras grandes de valores críticos de la prueba ADF

Los resultados muestran que el uso de valores críticos normales estándar es erróneo: El valor crítico del $5\%$ de la distribución normal estándar es $-1.64$. Para las distribuciones de Dickey-Fuller, los valores críticos estimados son $-2.87$ (deriva) y $-3.43$ (deriva y tendencia lineal en el tiempo). Esto implica que una verdadera hipótesis nula (la serie tiene una tendencia estocástica) se rechazaría con demasiada frecuencia si se usaran valores críticos normales inapropiados.

Se pueden utilizar las estadísticas de prueba simuladas para una comparación gráfica de la densidad normal estándar y (estimaciones de) ambas densidades Dickey-Fuller.

```{r, 813, fig.align='center'}
# graficar densidad normal estándar
curve(dnorm(x), 
      from = -6, to = 3, 
      ylim = c(0, 0.6), 
      lty = 2,
      ylab = "Densidad",
      xlab = "Estadístico t",
      main = "Distribuciones de estadísticos de prueba de ADF",
      col = "darkred", 
      lwd = 2)

# gráficos de estimaciones de densidad de ambas distribuciones de Dickey-Fuller
lines(density(ADFD), lwd = 2, col = "darkgreen")
lines(density(ADFDT), lwd = 2, col = "blue")

# add a legend
legend("topleft", 
       c("N(0,1)", "Deriva", "Deriva + Tendencia"),
       col = c("darkred", "darkgreen", "blue"),
       lty = c(2, 1, 1),
       lwd = 2)
```

Las desviaciones de la distribución normal estándar son significativas: Ambas distribuciones Dickey-Fuller están sesgadas hacia la izquierda y tienen una cola izquierda más pesada que la distribución normal estándar.

#### ¿Tiene el PIB de EE. UU. una raíz unitaria? {-}

Como ejemplo empírico, se utiliza la prueba ADF para evaluar si existe una tendencia estocástica en el PIB de EE. UU. utilizando la regresión:

\begin{align*}
  \Delta\log(GDP_t) = \beta_0 + \alpha t + \beta_1 \log(GDP_{t-1}) + \beta_2 \Delta \log(GDP_{t-1}) + \beta_3 \Delta \log(GDP_{t-2}) + u_t.
\end{align*}

```{r, 814}
# generar series de logaritmos del PIB
LogGDP <- ts(log(GDP["1962::2012"]))

# estimar el modelo
coeftest(
  dynlm(diff(LogGDP) ~ trend(LogGDP, scale = F) + L(LogGDP) 
                     + diff(L(LogGDP)) + diff(L(LogGDP), 2)))
```

La estimación produce

\begin{align*}
  \Delta\log(GDP_t) =& \underset{(0.118)}{0.28} + \underset{(0.0001)}{0.0002} t -\underset{(0.014)}{0.033} \log(GDP_{t-1}) \\
   & + \underset{(0.113)}{0.083} \Delta \log(GDP_{t-1}) + \underset{(0.071)}{0.188} \Delta \log(GDP_{t-2}) + u_t,
\end{align*}

por lo que el estadístico de prueba ADF es $t=-0.033/0.014 = - 2.35$. El valor crítico correspondiente de $5\%$ de la Tabla \@ref(tab:DFcrits) es $-3.41$, por lo que no se puede rechazar la hipótesis nula de que $\log(GDP)$ tiene una tendencia estocástica a favor de la alternativa que es estacionario alrededor de una tendencia de tiempo lineal determinista.

La prueba ADF se puede realizar cómodamente usando **ur.df()** del paquete **urca**.

```{r, 815}
# prueba de raíz unitaria en el PIB usando 'ur.df()' del paquete 'urca'
summary(ur.df(LogGDP, 
              type = "trend", 
              lags = 2, 
              selectlags = "Fixed"))
```

La primera estadística de prueba en la parte inferior de la salida es la que interesa. La cantidad de estadísticas de prueba informadas depende de la regresión de la prueba. Para **type = "trend"**, la segunda estadística corresponde a la prueba de que no existe raíz unitaria ni tendencia temporal, mientras que la tercera corresponde a una prueba de la hipótesis de que existe una raíz unitaria, sin tendencia temporal ni término de deriva.

## No estacionariedad II: Pausas {#NEIIP}

Cuando existen cambios discretos (en una fecha distinta) o graduales (en el tiempo) en los coeficientes de regresión de la población, la serie es no estacionaria. Estos cambios se denominan *rupturas*. Existe una variedad de razones por las cuales pueden ocurrir rupturas en las series de tiempo macroeconómicas, pero la mayoría de las veces están relacionadas con cambios en la política económica o cambios importantes en la estructura de la economía.

Si las rupturas no se tienen en cuenta en el modelo de regresión, las estimaciones de MCO reflejarán la relación promedio. Dado que estas estimaciones pueden ser muy engañosas y dar como resultado una previsión de baja calidad, se está interesado en probar las pausas. Se debe distinguir entre probar una pausa cuando se conoce la fecha y probar una pausa con una fecha es desconocida.

Dejar que $\tau$ denote una fecha de ruptura conocida y sea $D_t(\tau)$ una variable binaria que indique períodos de tiempo antes y después de la ruptura. La incorporación de la ruptura en un modelo de regresión ADL($1$, $1$) produce:

\begin{align*}
  Y_t =& \beta_0 + \beta_1 Y_{t-1} + \delta_1 X_{t-1} + \gamma_0 D_t(\tau) + \gamma_1\left[D_t(\tau) \cdot Y_{t-1}\right] \\ 
  &+ \, \gamma_2\left[ D_t(\tau) \cdot X_{t-1} \right] + u_t,
\end{align*}

donde se permiten cambios discretos en $\beta_0$, $\beta_1$ y $\beta_2$ en la fecha de ruptura $\tau$. La hipótesis nula de no ruptura, $$H_0: \gamma_0=\gamma_1=\gamma_2=0,$$ se puede contrastar con la alternativa de que al menos uno de los $\gamma$ no es cero usando una prueba $F$. Esta idea se llama prueba de Chow por Gregory @chow1960.

Cuando se desconoce la fecha de ruptura, se puede utilizar la *prueba* [@quandt1960] mejor conocida como *razón de verosimilitud de Quandt* (QLR). Es una versión modificada de la prueba de Chow que utiliza el mayor de todos los estadísticos $F$ obtenidos al aplicar la prueba de Chow para todas las posibles fechas de ruptura en un rango predeterminado $\left[\tau_0,\tau_1\right]$. La prueba QLR se resume en el Concepto clave 14.9.

```{r, 816, eval = my_output == "html", results='asis', echo=F, purl=F}
cat('
<div class = "keyconcept" id="KC14.9">
<h3 class = "right"> Concepto clave 14.9 </h3>
<h3 class = "left"> La prueba QLR para la estabilidad del coeficiente </h3>
<p>

La prueba QLR se puede utilizar para probar una ruptura en la función de regresión de la población si se desconoce la fecha de la ruptura. El estadístico de prueba QLR es el estadístico más grande (Chow) $F(\\tau)$ calculado sobre un rango de fechas de ruptura elegibles $\\tau_0 \\leq \\tau \\leq \\tau_1$:

\\begin{align}
  QLR = \\max\\left[F(\\tau_0),F(\\tau_0 +1),\\dots,F(\\tau_1)\\right]. (\\#eq:QLRstatistic)
\\end{align}
</p>

Las propiedades más importantes son:

+ La prueba QLR se puede aplicar para probar si un subconjunto de los coeficientes en la función de regresión de población se rompe, pero la prueba también rechaza si existe una evolución lenta de la función de regresión.

+ Cuando existe una sola ruptura discreta en la función de regresión poblacional que se encuentra en una fecha dentro del rango probado, el estadístico de prueba $QLR$ es $F(\\widehat{\\tau})$ y $\\widehat{\\tau}/T$ es un estimador consistente de la fracción de la muestra en la que se encuentra la ruptura.

+ La distribución de muestra grande de $QLR$ depende de $q$, el número de restricciones que se están probando y ambas razones de los puntos finales del tamaño de la muestra, $\\tau_0/T, \\tau_1/T$.

+ Similar a la prueba ADF, la distribución de muestra grande de $QLR$ no es estándar.

</div>
')
```

```{r, 817, eval = my_output == "latex", results='asis', echo=F, purl=F}
cat('\\begin{keyconcepts}[La prueba QLR para la estabilidad del coeficiente]{14.9}

La prueba QLR se puede utilizar para probar una ruptura en la función de regresión de la población si se desconoce la fecha de la ruptura. El estadístico de prueba QLR es el estadístico más grande (Chow) $F(\\tau)$ calculado sobre un rango de fechas de ruptura elegibles $\\tau_0 \\leq \\tau \\leq \\tau_1$:

\\begin{align}
  QLR = \\max\\left[F(\\tau_0),F(\\tau_0 +1),\\dots,F(\\tau_1)\\right]. \\label{eq:QLRstatistic}
\\end{align}\\vspace{0.5cm}

Las propiedades más importantes son:

\\begin{itemize}
\\item La prueba QLR se puede aplicar para probar si un subconjunto de los coeficientes en la función de regresión de población se rompe, pero la prueba también rechaza si existe una evolución lenta de la función de regresión.
\\item Cuando existe una sola ruptura discreta en la función de regresión poblacional que se encuentra en una fecha dentro del rango probado, el estadístico de prueba $QLR$ es $F(\\widehat{\\tau})$ y $\\widehat{\\tau}/T$ es un estimador consistente de la fracción de la muestra en la que se encuentra la ruptura.
\\item La distribución de muestra grande de $QLR$ depende de $q$, el número de restricciones que se están probando y ambas razones de los puntos finales del tamaño de la muestra, $\\tau_0/T, \\tau_1/T$. 
\\item Similar a la prueba ADF, la distribución de muestra grande de $QLR$ no es estándar.
\\end{itemize}
\\end{keyconcepts}
')
```

#### ¿Se ha mantenido estable el poder predictivo del diferencial de plazo? {-}

Usando el estadístico QLR se puede probar si existe una ruptura en los coeficientes de los rezagos del diferencial de plazo en \@ref(eq:gdpgradl22), el modelo de regresión ADL($2$, $2$) del crecimiento del PIB. Siguiendo el Concepto clave 14.9, se modifica la especificación de \@ref(eq:gdpgradl22) agregando una variable ficticia de ruptura $D(\tau)$ y sus interacciones con ambos rezagos del margen de plazo y se elige el rango de puntos de ruptura que se probarán como 1970:Q1 - 2005:Q2 (estos períodos son el centro del $70\%$ de los datos de la muestra que va de 1962:Q2 - 2012:Q4). Por tanto, el modelo se convierte en

\begin{align*}
    GDPGR_t =&\, \beta_0 + \beta_1 GDPGR_{t-1} + \beta_2 GDPGR_{t-2} \\
            &+\,  \beta_3  TSpread_{t-1} + \beta_4 TSpread_{t-2} \\
            &+\, \gamma_1 D(\tau) + \gamma_2 (D(\tau) \cdot TSpread_{t-1}) \\
            &+\, \gamma_3 (D(\tau) \cdot TSpread_{t-2}) \\
            &+\, u_t.
\end{align*}

A continuación, se estima el modelo para cada punto de ruptura y se calcula el estadístico $F$ correspondiente a la hipótesis nula $H_0: \gamma_1=\gamma_2=\gamma_3=0$. El estadístico $QLR$ es el más grande de los estadísticos $F$ obtenidos de esta manera.

```{r, 818}
# configurar un rango de posibles fechas de ruptura
tau <- seq(1970, 2005, 0.25)

# inicializar vector de estadísticos F
Fstats <- numeric(length(tau))

# ciclo de estimación sobre fechas de ruptura
for(i in 1:length(tau)) {

  # configurar variable ficticia
  D <- time(GDPGrowth_ts) > tau[i]

  # estimar el modelo ADL(2,2) con interacciones
  test <- dynlm(GDPGrowth_ts ~ L(GDPGrowth_ts) + L(GDPGrowth_ts, 2) + 
                D*L(TSpread_ts) + D*L(TSpread_ts, 2),
                start = c(1962, 1), 
                end = c(2012, 4))
  
  # calcular y guardar el estadístico F
  Fstats[i] <- linearHypothesis(test, 
                                c("DTRUE=0", "DTRUE:L(TSpread_ts)", 
                                  "DTRUE:L(TSpread_ts, 2)"),
                                vcov. = sandwich)$F[2]

}
```

Se determina el estadístico $QLR$ usando **max()**.

```{r, 819}
# identificar el estadístico QLR
QLR <- max(Fstats)
QLR
```

Se comprueba que el estadístico $QLR$ es el estadístico $F$ obtenido para la regresión donde se elige 1980:Q4 como fecha de ruptura.

```{r, 820}
# identificar el período de tiempo en el que se observa el estadístico QLR
as.yearqtr(tau[which.max(Fstats)])
```

Dado que se prueba las hipótesis $q = 3$ y se considera que el $70\%$ de los datos centrales de la muestra contienen rupturas, el valor crítico correspondiente de $1\%$ de la prueba $QLR$ es $6.02$. Se rechaza la hipótesis nula de que todos los coeficientes (los coeficientes en ambos rezagos del margen de plazo y la intersección) son estables, ya que el estadístico de $QLR$ calculado excede este umbral. Por lo tanto, la evidencia de la prueba $QLR$ sugiere que existe una ruptura en el modelo ADL($2$, $2$) de crecimiento del PIB a principios de los años ochenta.

Se convierte el vector de estadísticos secuenciales de punto de ruptura $F$ en un objeto de serie de tiempo y luego se genera una gráfica simple con algunas anotaciones.

```{r, 821, fig.align='center'}
# serie de estadísticos F
Fstatsseries <- ts(Fstats, 
                   start = tau[1], 
                   end = tau[length(tau)], 
                   frequency = 4)

# graficar los estadísticos F
plot(Fstatsseries, 
     xlim = c(1960, 2015),
     ylim = c(1, 7.5),
     lwd = 2,
     col = "steelblue",
     ylab = "Estadístico F",
     xlab = "Fecha de ruptura",
     main = "Prueba de una ruptura en la regresión del PIB ADL(2,2) en diferentes fechas")

# líneas horizontales discontinuas para valores críticos y estadístico QLR
abline(h = 4.71, lty = 2)
abline(h = 6.02, lty = 2)
segments(0, QLR, 1980.75, QLR, col = "darkred")
text(2010, 6.2, "1% Valor crítico")
text(2010, 4.9, "5% Valor crítico")
text(1980.75, QLR+0.2, "Estadístico QLR")
```

#### Pronóstico pseudo fuera de la muestra {-}

Los pronósticos pseudo fuera de la muestra se utilizan para simular el rendimiento fuera de la muestra (el rendimiento del pronóstico en tiempo real) de un modelo de regresión de series de tiempo. En particular, los pronósticos pseudo fuera de muestra permiten la estimación de los $EPRCM$ del modelo y permiten a los investigadores comparar diferentes especificaciones del modelo respecto de su poder predictivo. El Concepto clave 14.10 resume esta idea.

```{r, 822, eval = my_output == "html", results='asis', echo=F, purl=F}
cat('
<div class = "keyconcept" id="KC14.10">
<h3 class = "right"> Concepto clave 14.10 </h3>
<h3 class = "left"> Pronóstico pseudo fuera de la muestra </h3>
1. Dividir los datos de la muestra en $s = T-P$ y $P$ observaciones subsiguientes. Las observaciones $P$ se utilizan como observaciones pseudo fuera de la muestra.

2. Estimar el modelo usando las primeras $s$ observaciones.

3. Calcular el pseudo-pronóstico $\\overset{\\sim}{Y}_{s+1\\vert s}$.

4. Calcular el error de pseudo-pronóstico $\\overset{\\sim}{u}_{s+1} = Y_{s+1} - \\overset{\\sim}{Y}_{s+1\\vert s}$.

5. Repetir los pasos del 2 al 4 para todas las fechas restantes pseudo-fuera de muestra; es decir, volver a estimar el modelo en cada fecha.
</div>
')
```

```{r, 823, eval = my_output == "latex", results='asis', echo=F, purl=F}
cat('\\begin{keyconcepts}[Pronóstico pseudo fuera de la muestra]{14.10}
\\begin{enumerate}
\\item Dividir los datos de la muestra en $s = T-P$ y $P$ observaciones subsiguientes. Las observaciones $P$ se utilizan como observaciones pseudo fuera de la muestra.
\\item Estimar el modelo usando las primeras $s$ observaciones.
\\item Calcular el pseudo-pronóstico $\\overset{\\sim}{Y}_{s+1\\vert s}$.
\\item Calcular el error de pseudo-pronóstico $\\overset{\\sim}{u}_{s+1} = Y_{s+1} - \\overset{\\sim}{Y}_{s+1\\vert s}$.
\\item Repetir los pasos del 2 al 4 para todas las fechas restantes pseudo-fuera de la muestra; es decir, volver a estimar el modelo en cada fecha.
\\end{enumerate}
\\end{keyconcepts}
')
```

#### ¿Cambió el poder predictivo de la extensión del término durante la década de 2000? {-}

El conocimiento adquirido en la sección anterior da razones para suponer que el desempeño pseudo-fuera de la muestra de los modelos de ADL($2$, $2$) estimados usando datos *después* de la ruptura a principios de la década de 1980 no deberían deteriorarse en relación con el uso de la muestra completa: Siempre que los coeficientes de la función de regresión poblacional sean estables después de la ruptura potencial en 1980:Q4, estos modelos deben tener un buen poder predictivo. Se verifica esto calculando pronósticos pseudo-fuera de la muestra para el período 2003:Q1 - 2012:Q4, un rango que cubre 40 períodos, donde el pronóstico para 2003:Q1 se realiza utilizando datos de 1981:Q1 - 2002:Q4, el pronóstico para 2003:Q2 se basa en datos de 1981:Q1 - 2003:Q1 y así sucesivamente.

De manera similar, para la prueba $QLR$ se usa un bucle **for()** para la estimación de los 40 modelos y se recopila sus $SER$s y los pronósticos obtenidos en un vector que luego se usa para calcular pseudo errores de salida de pronóstico de la muestra.

```{r, 824}
# fechas de finalización de la muestra
EndOfSample <- seq(2002.75, 2012.5, 0.25)

# inicializar los pronósticos matriciales
forecasts <- matrix(nrow = 1, 
                    ncol = length(EndOfSample))

# inicializar el vector SER
SER  <- numeric(length(EndOfSample))

# bucle de estimación al final de las fechas de muestra
for(i in 1:length(EndOfSample)) {

  # estimar modelo ADL(2,2)
  m <- dynlm(GDPGrowth_ts ~ L(GDPGrowth_ts) + L(GDPGrowth_ts, 2) 
                          + L(TSpread_ts) + L(TSpread_ts, 2), 
                start = c(1981, 1), 
                end = EndOfSample[i])
  
  SER[i] <- summary(m)$sigma
  
  # datos de muestra para el pronóstico de un período por delante
  s <- window(ADLdata, EndOfSample[i] - 0.25, EndOfSample[i])
  
  # calcular pronóstico
  forecasts[i] <- coef(m) %*% c(1, s[1, 1], s[2, 1], s[1, 2], s[2, 2]) 
}
```

```{r, 825}
# calcular errores de pronóstico psuedo fuera de muestra
POOSFCE <- c(window(GDPGrowth_ts, c(2003, 1), c(2012, 4))) - forecasts
```

A continuación, se traducen los pronósticos pseudo fuera de la muestra en un objeto de clase **ts** y se grafica la tasa de crecimiento del PIB real contra la serie pronosticada.

```{r, 826, fig.align='center'}
# series de pronósticos pseudo fuera de la muestra
PSOSSFc <- ts(c(forecasts), 
              start = 2003, 
              end = 2012.75, 
              frequency = 4)

# graficar la serie de tiempo de crecimiento del PIB
plot(window(GDPGrowth_ts, c(2003, 1), c(2012, 4)),
     col = "steelblue",
     lwd = 2,
     ylab = "Porcentaje",
     main = "Pronósticos pseudo fuera de la muestra de crecimiento del PIB")

# agregar la serie de pronósticos pseudo fuera de la muestra
lines(PSOSSFc, 
      lwd = 2, 
      lty = 2)

# área sombreada entre curvas (el error de pseudo pronóstico)
polygon(c(time(PSOSSFc), rev(time(PSOSSFc))), 
        c(window(GDPGrowth_ts, c(2003, 1), c(2012, 4)), rev(PSOSSFc)),
        col = alpha("blue", alpha = 0.3),
        border = NA)

# agregar una leyenda
legend("bottomleft", 
       lty = c(1, 2, 1),
       lwd = c(2, 2, 10),
       col = c("steelblue", "black", alpha("blue", alpha = 0.3)), 
       legend = c("Tasa de crecimiento real del PIB",
         "Tasa de crecimiento del PIB pronosticada",
         "Error de pseudo pronóstico"))
```

Aparentemente, los pseudo pronósticos siguen bastante bien la tasa de crecimiento real del PIB, excepto por el problema en 2009 que se puede atribuir a la reciente crisis financiera.

El $SER$ del primer modelo (estimado con datos de 1981:Q1 a 2002:Q4) es $2.39$, por lo que, según el ajuste dentro de la muestra, se esperaría que los errores de pronóstico fuera de la muestra tengan una media cero y una raíz del error de pronóstico cuadrático medio de alrededor de $2.39$.

```{r, 827}
# SER del modo ADL(2,2) usando datos de 1981:Q1 - 2002:Q4
SER[1]
```

La raíz del error de pronóstico cuadrático medio de los pronósticos pseudo fuera de muestra es algo mayor.

```{r, 828}
# calcular la raíz del error de pronóstico cuadrático medio
sd(POOSFCE)
```

Una hipótesis interesante es si el error medio de pronóstico es cero; es decir, los pronósticos de ADL($2$, $2$) son correctos, en promedio. Esta hipótesis se prueba fácilmente usando la función **t.test()**.

```{r, 829}
# probar si el error medio de pronóstico es cero
t.test(POOSFCE)
```

La hipótesis no puede rechazarse al nivel de significancia de $10\%$. En conjunto, el análisis sugiere que los coeficientes del modelo ADL($2$, $2$) se han mantenido estables desde la supuesta ruptura a principios de los años ochenta.

## ¿Puedes ganarle al mercado? (Parte II)

El rendimiento por dividendo (la relación entre los dividendos actuales y el precio de la acción) se puede considerar como un indicador de dividendos futuros: Si una acción tiene un rendimiento por dividendo actual alto, se puede considerar infravalorada y se puede presumir que el precio de la acción aumenta en el futuro, lo que significa que los rendimientos excedentes futuros aumentan.

Esta presunción se puede examinar utilizando modelos ADL de exceso de rendimiento, donde los rezagos del logaritmo del rendimiento por dividendo de la acción sirven como regresores adicionales.

Desafortunadamente, una inspección gráfica de la serie temporal del logaritmo del rendimiento por dividendo arroja dudas sobre el supuesto de que la serie es estacionaria, lo cual, como se discutió en el Capítulo \@ref(NEIT), es necesario para realizar inferencias estándar en un análisis de regresión.

```{r, 830, fig.align='center'}
# graficar el logaritmo de la serie de rendimiento de dividendos
plot(StockReturns[, 2], 
     col = "steelblue", 
     lwd = 2, 
     ylab = "Logaritmo", 
     main = "Rendimiento de dividendos para el índice CRSP")
```

El estadístico de prueba de Dickey-Fuller para una raíz unitaria autorregresiva en un modelo AR($1$) con deriva proporciona más evidencia de que la serie podría ser no estacionaria.

```{r, 831}
# prueba de raíz unitaria en el PIB usando 'ur.df()' del paquete 'urca'
summary(ur.df(window(StockReturns[, 2], 
                     c(1960,1), 
                     c(2002, 12)), 
              type = "drift", 
              lags = 0))
```

Se usa **window()** para obtener observaciones desde enero de 1960 hasta diciembre de 2012 únicamente.

Dado que el valor $t$ para el coeficiente en el logaritmo rezagado del rendimiento por dividendo es $-1.27$, la hipótesis de que el coeficiente verdadero es cero no puede rechazarse, incluso en el nivel de significancia de $10\%$.

Sin embargo, es posible examinar si el rendimiento por dividendo tiene poder predictivo para rendimientos en exceso utilizando sus diferencias en un modelo de ADL($1$, $1$) y ADL($2$, $2$) (recuerde que diferenciar una serie con un raíz unitaria produce una serie estacionaria), aunque estas especificaciones del modelo no corresponden al razonamiento económico mencionado anteriormente. Por lo tanto, también se estima una regresión ADL($1$, $1$) utilizando el nivel del logaritmo del rendimiento por dividendo.

Es decir, se estiman tres especificaciones diferentes:

\begin{align*}
  excess \, returns_t =& \, \beta_0 + \beta_1 excess \, returns_{t-1} + \beta_3 \Delta \log(dividend yield_{t-1}) + u_t \\
  \\
  excess \, returns_t =& \, \beta_0 + \beta_1 excess \, returns_{t-1} + \beta_2 excess \, returns_{t-2} \\ &+ \, \beta_3 \Delta \log(dividend yield_{t-1}) + \beta_4 \Delta \log(dividend yield_{t-2}) + u_t \\
  \\
  excess \, returns_t =& \, \beta_0 + \beta_1 excess \, returns_{t-1} + \beta_5 \log(dividend yield_{t-1}) + u_t \\
\end{align*}

```{r, 832}
# ADL(1,1) (1ª diferencia de rentabilidad por dividendo logarítmico)
CRSP_ADL_1 <- dynlm(ExReturn ~ L(ExReturn) + d(L(ln_DivYield)), 
                    data = StockReturns,
                    start = c(1960, 1), end = c(2002, 12))

# ADL(2,2) (1ª y 2ª diferencia de rentabilidad por dividendo logarítmico)
CRSP_ADL_2 <- dynlm(ExReturn ~ L(ExReturn) + L(ExReturn, 2) 
                    + d(L(ln_DivYield)) + d(L(ln_DivYield, 2)), 
                    data = StockReturns,
                    start = c(1960, 1), end = c(2002, 12))

# ADL(1,1) (nivel de rendimiento de dividendos logarítmicos)
CRSP_ADL_3 <- dynlm(ExReturn ~ L(ExReturn) + L(ln_DivYield),
                    data = StockReturns,
                    start = c(1960, 1), end = c(1992, 12))
```

```{r, 833}
# recopilar errores estándar robustos
rob_se_CRSP_ADL <- list(sqrt(diag(sandwich(CRSP_ADL_1))),
                        sqrt(diag(sandwich(CRSP_ADL_2))),
                        sqrt(diag(sandwich(CRSP_ADL_3))))
```

Luego se puede generar una representación tabular de los resultados usando **stargazer()**.

```{r, 834, message=F, warning=F, results='asis', eval=F}
stargazer(CRSP_ADL_1, CRSP_ADL_2, CRSP_ADL_3,
  title = "Modelos ADL de la existencia de rendimientos excesivos mensuales",
  header = FALSE, 
  type = "latex",
  column.sep.width = "-5pt",
  no.space = T,
  digits = 3, 
  column.labels = c("ADL(1,1)", "ADL(2,2)", "ADL(1,1)"),
  dep.var.caption  = "Variable dependiente: Rendimientos excesivos en el índice ponderado por valor CSRP",
  dep.var.labels.include = FALSE,
  covariate.labels = c("$excess return_{t-1}$", 
                       "$excess return_{t-2}$", 
                       "$1^{st} diff log(dividend yield_{t-1})$", 
                       "$1^{st} diff log(dividend yield_{t-2})$", 
                       "$log(dividend yield_{t-1})$", 
                       "Constant"),
  se = rob_se_CRSP_ADL) 
```

<!--html_preserve-->

```{r, 835, message=F, warning=F, results='asis', echo=F, purl=F, eval=my_output=="html"}
stargazer(CRSP_ADL_1, CRSP_ADL_2, CRSP_ADL_3,
  header = FALSE, 
  type = "html",
  digits = 3, 
  column.labels = c("ADL(1,1)", "ADL(2,2)", "ADL(1,1)"),
  dep.var.caption  = "Variable dependiente: Rendimientos excesivos en el índice ponderado por valor de CSRP",
  dep.var.labels.include = FALSE,
  covariate.labels = c("$excess return_{t-1}$", "$excess return_{t-2}$", "$1^{st} diff log(dividend yield_{t-1})$", "$1^{st} diff log(dividend yield_{t-2})$", "$log(dividend yield_{t-1})$", "Constant"),
  se = rob_se_CRSP_ADL,
  no.space = T
  )

stargazer_html_title("Modelos ADL de la existencia de rendimientos excesivos mensuales", "adlmomesr")
```

<!--/html_preserve-->

```{r, 836, message=F, warning=F, results='asis', echo=F, purl=F, eval=my_output=="latex"}
stargazer(CRSP_ADL_1, CRSP_ADL_2, CRSP_ADL_3,
  title = "\\label{tab:adlmomesr} ADL Models of Monthly Excess Stock Returns",
  header = FALSE, 
  type = "latex",
  column.sep.width = "-5pt",
  no.space = T,
  digits = 3, 
  column.labels = c("ADL(1,1)", "ADL(2,2)", "ADL(1,1)"),
  dep.var.caption  = "Variable dependiente: Rendimientos excesivos en el índice ponderado por valor de CSRP",
  dep.var.labels.include = FALSE,
  covariate.labels = c("$excess return_{t-1}$", "$excess return_{t-2}$", "$1^{st} diff log(dividend yield_{t-1})$", "$1^{st} diff log(dividend yield_{t-2})$", "$log(dividend yield_{t-1})$", "Constant"),
  se = rob_se_CRSP_ADL
  ) 
```

Para los modelos (1) y (2), ninguna de las estadísticas individuales $t$ sugieren que los coeficientes sean diferentes de cero. Además, no se puede rechazar la hipótesis de que ninguno de los rezagos tiene poder predictivo de rendimientos excesivos en ningún nivel común de significancia (una prueba $F$ que los rezagos tienen poder predictivo no rechaza para ambos modelos).

Las cosas son diferentes para el modelo (3). El coeficiente en el nivel del logaritmo del rendimiento por dividendo es diferente de cero en el nivel de $5\%$ y también se rechaza la prueba de $F$. Pero se debe sospechar: El alto grado de persistencia en la serie de rendimiento de dividendos probablemente hace que esta inferencia sea dudosa porque los estadísticos de $t$ y $F$ pueden seguir distribuciones que se desvían considerablemente de sus distribuciones teóricas de muestra grande, de modo que en el valor crítico habitual los valores no se pueden aplicar.

Si el modelo (3) fuera útil para predecir rendimientos en exceso, los pronósticos pseudo fuera de la muestra basados en (3) deberían al menos superar los pronósticos de un modelo de solo intercepción en términos del EPRCM de muestra. Se puede realizar este tipo de comparación utilizando el código **R** a manera de las aplicaciones del Capítulo \@ref(NEIIP).

```{r, 837, cache=T}
# fechas de finalización de la muestra
EndOfSample <- as.numeric(window(time(StockReturns), c(1992, 12), c(2002, 11)))

# inicializar pronósticos matriciales
forecasts <- matrix(nrow = 2, 
                    ncol = length(EndOfSample))

# bucle de estimación al final de las fechas de muestra
for(i in 1:length(EndOfSample)) {

  # modelo de estimación (3)
  mod3 <- dynlm(ExReturn ~ L(ExReturn) + L(ln_DivYield), data = StockReturns, 
                start = c(1960, 1), 
                end = EndOfSample[i])
  
  # estimar modelo de solo intercepción
  modconst <- dynlm(ExReturn ~ 1, data = StockReturns, 
                start = c(1960, 1), 
                end = EndOfSample[i])
  
  # datos de muestra para el pronóstico de un período por delante
  t <- window(StockReturns, EndOfSample[i], EndOfSample[i])
  
  # calcular pronóstico
  forecasts[, i] <- c(coef(mod3) %*% c(1, t[1], t[2]), coef(modconst))
                     
}
```

```{r, 838}
# reunir datos
d <- cbind("Excess Returns" = c(window(StockReturns[,1], c(1993, 1), c(2002, 12))),
           "Model (3)" = forecasts[1,], 
           "Intercept Only" = forecasts[2,], 
           "Always Zero" =  0)

# calcular EPRCM
c("ADL model (3)" = sd(d[, 1] - d[, 2]),
  "Intercept-only model" = sd(d[, 1] - d[, 3]),
  "Always zero" = sd(d[,1] - d[, 4]))
```

La comparación indica que el modelo (3) no es útil, ya que el modelo de solo intercepto lo supera en términos de EPRCM de muestra. Un modelo que pronostica el exceso de rendimiento siempre en cero tiene un EPRCM de muestra aún más bajo. Este hallazgo es consistente con la hipótesis de eficiencia de forma débil que establece que toda la información disponible públicamente se contabiliza en los precios de las acciones de manera que no hay forma de predecir los precios futuros de las acciones o los rendimientos en exceso utilizando observaciones pasadas, lo que implica que la relación significativa percibida e indicada por el modelo (3) es incorrecta.

#### Resumen {-}

Este capítulo trató temas introductorios en el análisis de regresión de series de tiempo, donde las variables generalmente se correlacionan de una observación a la siguiente, un concepto denominado correlación serial. Se presentaron varias formas de almacenar y graficar datos de series de tiempo usando **R** y se usaron para el análisis informal de datos económicos.

Se han introducido los modelos AR y ADL. Asimismo, se han aplicado en el contexto de la previsión de series de tiempo macroeconómicas y financieras utilizando **R**. La discusión también incluyó el tema de la selección de la duración de los rezagos. Se mostró cómo configurar una función simple que calcula el CIB para un objeto suministrado, como un modelo.

También se ha visto cómo escribir un código **R** simple para realizar y evaluar pronósticos. De igual forma, se demostraron algunos enfoques más sofisticados para realizar pronósticos pseudo fuera de la muestra para evaluar el poder predictivo de un modelo para los resultados futuros no observados de una serie, para comprobar la estabilidad del modelo y comparar diferentes modelos.

Además, se abordaron algunos aspectos más técnicos como el concepto de estacionariedad. Esto incluyó aplicaciones para probar una raíz unitaria autorregresiva con la prueba de Dickey-Fuller y la detección de una ruptura en la función de regresión poblacional usando el estadístico $QLR$. Para ambos métodos, la distribución del estadístico de prueba relevante no es normal, incluso en muestras grandes. Respecto a la prueba de Dickey-Fuller, se han utilizado las instalaciones de generación de números aleatorios de **R** para producir evidencia de esto por medio de una simulación de Monte-Carlo y el uso motivado de los cuantiles tabulados.

De manera adicional, estudios empíricos sobre la validez de las hipótesis de eficiencia de forma débil y fuerte que se presentan en las aplicaciones *¿Puedes ganarle al mercado? Parte I y II* se han reproducido utilizando **R**.

En todas las aplicaciones del presente capítulo, la atención se centró en pronosticar resultados futuros en lugar de estimar las relaciones causales entre las variables de series de tiempo. Sin embargo, los métodos necesarios para este último son bastante similares. El capítulo \@ref(EECD) está dedicado a la estimación de los llamados *efectos causales dinámicos*.

<!--chapter:end:Capitulo_15.Rmd-->

# Estimación de efectos causales dinámicos {#EECD}

```{r, echo = F}
options(knitr.duplicate.label = "allow")
```

```{r, 839, child="_setup.Rmd"}
```

```{r, 840, eval=knitr::opts_knit$get("rmarkdown.pandoc.to") == "html", results='asis', echo=FALSE}
cat('<hr style="background-color:#03193b;height:2px">')
```

A veces es interesante conocer el tamaño de la reacción actual y futura de $Y$ a un cambio en $X$. Esto se denomina *efecto causal dinámico* en $Y$ de un cambio en $X$. En este capítulo se analiza cómo estimar los efectos causales dinámicos en aplicaciones **R**, donde se investiga el efecto dinámico del clima frío en Florida sobre el precio del concentrado de jugo de naranja.

La discusión cubre:

+ Estimación de modelos de rezagos distribuidos.
+ Errores estándar consistentes con heterocedasticidad y autocorrelación (HAC).
+ Estimación de mínimos cuadrados generalizados (MCG) de modelos ADL.

Para reproducir ejemplos de código, se necesitan instalar los paquetes **R** que se enumeran a continuación:

+ **AER** [@R-AER]
+ **dynlm** [@R-dynlm]
+ **nlme** [@R-nlme]
+ **orcutt** [@R-orcutt]
+ **quantmod** [@R-quantmod]
+ **stargazer** [@R-stargazer]

Debe asegurarse de que el fragmento de código subsiguiente se ejecute sin errores:

```{r, 841, warning=FALSE, message=FALSE}
library(AER)
library(quantmod)
library(dynlm)
library(orcutt)
library(nlme)
library(stargazer)
```

## Los datos del jugo de naranja

La región de cultivo de naranjas más grande de los EE. UU. se encuentra en Florida, que generalmente tiene un clima ideal para el crecimiento de la fruta. Por lo tanto, es la fuente de casi todo el concentrado de jugo congelado que se produce en el país. Sin embargo, de vez en cuando y dependiendo de su severidad, las olas de frío provocan una pérdida de cosechas de tal manera que la oferta de naranjas disminuye y en consecuencia sube el precio del concentrado de jugo. El momento de los aumentos de precios es complicado: Un recorte en el suministro actual de concentrado influye no solo en el precio actual, sino también en los precios futuros porque el suministro en períodos futuros también disminuirá. Claramente, la magnitud de los aumentos de precios actuales y futuros debido a la congelación es una pregunta empírica que puede investigarse utilizando un modelo de retardo distribuido, un modelo de series de tiempo que relaciona los cambios de precios con las condiciones climáticas.

Para comenzar con el análisis, se genera una gráfica que muestra el índice de precios del jugo de naranja concentrado congelado, cambios porcentuales en el precio y grados-día de congelación mensuales en Orlando, el centro de la región productora de naranjas de Florida.

```{r, 842}
# cargar el conjunto de datos de jugo de naranja concentrado congelado
data("FrozenJuice")

# calcular el índice de precios del jugo de naranja concentrado congelado
FOJCPI <- FrozenJuice[, "price"]/FrozenJuice[, "ppi"]
FOJC_pctc <- 100 * diff(log(FOJCPI))
FDD <- FrozenJuice[, "fdd"]
```

```{r, 843, fig.align='center'}
# convertir series a objetos xts
FOJCPI_xts <- as.xts(FOJCPI)
FDD_xts <- as.xts(FrozenJuice[, 3])

# graficar el índice de precios del jugo de naranja
plot(as.zoo(FOJCPI),
     col = "steelblue", 
     lwd = 2,
     xlab = "Fecha",
     ylab = "Índice de precio", 
     main = "Jugo de naranja concentrado congelado")
```

```{r, 844, fig.align='center', fig.height=7}
# dividir el área de graficado
par(mfrow = c(2, 1))

# graficar los cambios porcentuales en los precios
plot(as.zoo(FOJC_pctc),
     col = "steelblue", 
     lwd = 2,
     xlab = "Fecha",
     ylab = "Porcentaje",
     main = "Cambios mensuales en el precio del jugo de naranja concentrado congelado")

# graficar grados días de congelación
plot(as.zoo(FDD),
     col = "steelblue", 
     lwd = 2,
     xlab = "Fecha",
     ylab = "Grados días de congelación",
     main = "Días de grado de congelación mensuales en Orlando, FL")
```

Los períodos con una gran cantidad de grados día de congelación son seguidos por grandes cambios de precios de mes a mes. Estos movimientos coincidentes motivan una regresión simple de los cambios de precio ($\%ChgOJC_t$) en grados día de congelación ($FDD_t$) para estimar el efecto de un día de grado de congelación adicional en el precio en el mes actual. Para ello, como para todas las demás regresiones de este capítulo, se usa $T = 611$ observaciones (de enero de 1950 a diciembre de 2000).

```{r, 845}
# regresión simple de cambios porcentuales en grados día de congelación
orange_SR <- dynlm(FOJC_pctc ~ FDD)
coeftest(orange_SR, vcov. = vcovHAC)
```

Observe que los errores estándar se calculan utilizando un estimador "HAC" de la matriz de varianza-covarianza; consulte el Capítulo \@ref(PAMADL) para una discusión de este estimador.

\begin{align*}
  \widehat{\%ChgOJC_t} = -\underset{(0.19)}{0.42} + \underset{(0.13)}{0.47} FDD_t
\end{align*}

El coeficiente estimado en $FDD_t$ tiene la siguiente interpretación: Un día de grado de congelación adicional en el mes $t$ conduce a un aumento de precio de $0.47$ puntos porcentuales en el mismo mes.

Para considerar los efectos de las olas de frío en el precio del jugo de naranja durante los períodos subsiguientes, se incluyen valores rezagados de $FDD_t$ en el modelo, lo que conduce a un *modelo de regresión de rezagos distribuidos*. Se estima una especificación utilizando un valor contemporáneo y seis valores rezagados de $FDD_t$ como regresores.

```{r, 846}
# modelo de retraso distribuido con 6 retrasos de grados día de congelación
orange_DLM <- dynlm(FOJC_pctc ~ FDD + L(FDD, 1:6))
coeftest(orange_DLM, vcov. = vcovHAC)
```

Como resultado se obtiene:

\begin{align}
  \begin{split}
  \widehat{\%ChgOJC_t} =& -\underset{(0.21)}{0.69} + \underset{(0.14)}{0.47} FDD_t + \underset{(0.08)}{0.15} FDD_{t-1} + \underset{(0.06)}{0.06} FDD_{t-2} + \underset{(0.05)}{0.07} FDD_{t-3} \\ &+ \underset{(0.03)}{0.04} FDD_{t-4} + \underset{(0.03)}{0.05} FDD_{t-5} + \underset{(0.05)}{0.05} FDD_{t-6},
  \end{split}
  (\#eq:orangemod1)
\end{align}

donde el coeficiente de $FDD_{t-1}$ estima el aumento de precio en el período $t$ causado por un día de grado de congelación adicional en el mes anterior, el coeficiente de $FDD_{t-2}$ estima el efecto de un día de grado de congelación hace dos meses y así sucesivamente. En consecuencia, los coeficientes en \@ref(eq:orangemod1) pueden interpretarse como cambios de precio en períodos actuales y futuros debido a un aumento unitario en los grados día de congelación del mes actual.

## Efectos causales dinámicos

Esta sección del libro describe la idea general de un efecto causal dinámico y cómo el concepto de un experimento controlado aleatorio se puede traducir a aplicaciones de series de tiempo, usando varios ejemplos.

En general, para los intentos empíricos de medir un efecto causal dinámico, los supuestos de estacionariedad (ver Concepto clave 14.5) y exogeneidad deben ser válidos. En aplicaciones de series de tiempo hasta aquí, se ha asumido que el término de error del modelo tiene una media condicional cero dados los valores actuales y pasados de los regresores. Para la estimación de un efecto causal dinámico utilizando un modelo de rezago distribuido, puede ser útil asumir una forma más fuerte denominada *exogeneidad estricta*. La exogeneidad estricta establece que el término de error tiene una media cero condicionada a los valores pasados, presentes *y futuros* de las variables independientes.

Los dos conceptos de exogeneidad y el modelo de rezago distribuido se resumen en el Concepto clave 15.1.

```{r, 847, eval = my_output == "html", results='asis', echo=F, purl=F}
cat('
<div class = "keyconcept" id="KC15.1">
<h3 class = "right"> Concepto clave 15.1 </h3>
<h3 class = "left"> El modelo de rezago distribuido y la exogeneidad </h3>
<p>

El modelo de rezago distribuido general es:

\\begin{align}
  Y_t = \\beta_0 + \\beta_1 X_t + \\beta_2 X_{t-1} + \\beta_3 X_{t-2} + \\dots + \\beta_{r+1} X_{t-r} + u_t, (\\#eq:dlm)
\\end{align}

donde se supone que:

1. $X$ es una variable exógena, $$E(u_t\\vert X_t, X_{t-1}, X_{t-2},\\dots) = 0.$$

2.
     + a $X_t,Y_t$ tienen una distribución estacionaria.
     + b $(Y_t,X_t)$ y $(Y_{t-j},X_{t-j})$ se distribuyen de forma independiente a medida que $j$ crece.

3. Es poco probable que existan valores atípicos grandes. En particular, se necesita que todas las variables tengan más de ocho momentos distintos de cero y finitos, una suposición más sólida que antes (cuatro momentos finitos distintos de cero) que se requiere para el cálculo del estimador de matriz de covarianza HAC.

4. No existe una multicolinealidad perfecta.

El modelo de retardo distribuido puede ampliarse para incluir valores pasados y contemporáneos de regresores adicionales.

**Sobre el supuesto de exogeneidad**

+ Existe otra forma de exogeneidad denominada *exogeneidad estricta* que asume $$E(u_t\\vert \\dots, X_{t+2},X_{t+1},X_t,X_{t-1},X_{t-2},\\dots)=0,$$ ese es el término de error que tiene una media cero condicionada a valores pasados, presentes y futuros de $X$. La exogeneidad estricta implica exogeneidad (como se define en 1. arriba), pero no al revés. Por tanto, a partir de este punto se distingue entre exogeneidad y exogeneidad estricta. 

+ La exogeneidad como en 1. es suficiente para que los estimadores MCO del coeficiente en los modelos de rezagos distribuidos sean consistentes. Sin embargo, si el supuesto de exogeneidad estricta es válido, se pueden aplicar estimadores más eficientes.

</p>
</div>
')
```

```{r, 848, eval = my_output == "latex", results='asis', echo=F, purl=F}
cat('\\begin{keyconcepts}[El modelo de rezago distribuido y la exogeneidad]{15.1}

El modelo de rezago distribuido general es:

\\begin{align}
  Y_t = \\beta_0 + \\beta_1 X_t + \\beta_2 X_{t-1} + \\beta_3 X_{t-2} + \\dots + \\beta_{r+1} X_{t-r} + u_t, \\label{eq:dlm}
\\end{align}

donde se supone que:\\newline

\\begin{enumerate}
\\item $X$ es una variable exógena, $$E(u_t\\vert X_t, X_{t-1}, X_{t-2},\\dots) = 0.$$
\\item \\vspace{0.5cm}
\\begin{itemize}
\\item[(a)] $X_t,Y_t$ tienen una distribución estacionaria.
\\item[(b)] $(Y_t,X_t)$ y $(Y_{t-j},X_{t-j})$ se distribuyen de forma independiente a medida que $j$ crece.
\\end{itemize}\\vspace{0.5cm}
\\item Es poco probable que existan valores atípicos grandes. En particular, se necesita que todas las variables tengan más de ocho momentos distintos de cero y finitos, una suposición más sólida que antes (cuatro momentos finitos distintos de cero) que se requiere para el cálculo del estimador de matriz de covarianza HAC.
\\item No existe una multicolinealidad perfecta.\\vspace{0.5cm}
\\end{enumerate}\\vspace{0.5cm}

El modelo de retardo distribuido puede ampliarse para incluir valores pasados y contemporáneos de regresores adicionales.\\vspace{0.5cm}

\\textbf{Sobre el supuesto de exogeneidad}\\vspace{0.5cm}

\\begin{itemize}
\\item Existe otra forma de exogeneidad denominada \\textit{exogeneidad estricta} que asume $$E(u_t\\vert \\dots, X_{t+2},X_{t+1},X_t,X_{t-1},X_{t-2},\\dots)=0,$$ ese es el término de error que tiene una media cero condicionada a valores pasados, presentes y futuros de $X$. La exogeneidad estricta implica exogeneidad (como se define en 1. arriba), pero no al revés. Por tanto, a partir de este punto se distingue entre exogeneidad y exogeneidad estricta.\\vspace{0.5cm}  
\\item La exogeneidad como en 1. es suficiente para que los estimadores MCO del coeficiente en los modelos de rezagos distribuidos sean consistentes. Sin embargo, si el supuesto de exogeneidad estricta es válido, se pueden aplicar estimadores más eficientes.
\\end{itemize}
\\end{keyconcepts}
')
```

## Multiplicadores dinámicos y multiplicadores dinámicos acumulativos

La siguiente terminología respecto a los coeficientes en el modelo de retardo distribuido \@ref(eq:dlm) es útil para las próximas aplicaciones:

  + El efecto causal dinámico también se denomina *multiplicador dinámico*. $\beta_{h+1}$ en \@ref(eq:dlm) es el multiplicador dinámico del periodo $h$.

  + El efecto contemporáneo de $X$ sobre $Y$, $\beta_1$, se denomina *efecto de impacto*.

  + El multiplicador dinámico acumulativo *del período $h$* de un cambio de unidad en $X$ y $Y$ se define como la suma acumulativa de los multiplicadores dinámicos. En particular, $\beta_1$ es el multiplicador dinámico acumulativo de período cero, $\beta_1 + \beta_2$ es el multiplicador dinámico acumulativo de un período y así sucesivamente.

     Los multiplicadores dinámicos acumulativos del modelo de retardo distribuido \@ref(eq:dlm) son los coeficientes $\delta_1,\delta_2,\dots,\delta_r,\delta_{r+1}$ en la regresión modificada
\begin{align}
  Y_t =& \, \delta_0 + \delta_1 \Delta X_t + \delta_2 \Delta X_{t-1} + \dots + \delta_r \Delta X_{t-r+1} + \delta_{r+1} X_{t-r} + u_t (\#eq:DCMreg)
\end{align} y, por lo tanto, se pueden estimar directamente utilizando MCO, lo que hace que sea conveniente calcular sus errores estándar de HAC. $\delta_{r+1}$ se denomina *multiplicador dinámico acumulativo de largo plazo*.

Es sencillo calcular los multiplicadores dinámicos acumulativos para \@ref(eq:orangemod1), la regresión de retardo distribuida estimada de los cambios en los precios del concentrado de jugo de naranja en grados-día de congelación, utilizando el objeto modelo correspondiente **orange_DLM** y la función **cumsum()**.

```{r, 849}
# calcular multiplicadores acumulativos
cum_mult <-cumsum(orange_DLM$coefficients[-1])

# cambiar el nombre de las entradas
names(cum_mult) <- paste(0:6, sep = "-", "period CDM")

cum_mult
```

Al traducir el modelo de rezago distribuido con seis rezagos de $FDD$ a \@ref(eq:DCMreg), se puede ver que las estimaciones del coeficiente de MCO en este modelo coinciden con los multiplicadores almacenados en **cum_mult**.

```{r, 850}
# estimar multiplicadores dinámicos acumulativos usando la regresión modificada
cum_mult_reg <-dynlm(FOJC_pctc ~ d(FDD) + d(L(FDD,1:5)) + L(FDD,6))
coef(cum_mult_reg)[-1]
```

Como se señaló anteriormente, el uso de una especificación de modelo como en \@ref(eq:DCMreg) permite obtener fácilmente errores estándar para los multiplicadores acumulativos dinámicos estimados.

```{r, 851}
# obtener un resumen de coeficientes que informe los errores estándar de HAC
coeftest(cum_mult_reg, vcov. = vcovHAC)
```

## Errores estándar de HAC

El término de error $u_t$ en el modelo de retardo distribuido \@ref(eq:dlm) puede estar correlacionado en serie debido a determinantes correlacionados en serie de $Y_t$ que no se incluyen como regresores. Cuando estos factores no están correlacionados con los regresores incluidos en el modelo, los errores correlacionados en serie no violan el supuesto de exogeneidad, de modo que el estimador de MCO permanece insesgado y consistente.

Sin embargo, los errores estándar autocorrelacionados invalidan los errores estándar habituales de solo *homocedasticidad* y *heterocedasticidad* robustos y pueden causar inferencias engañosas. Los errores de HAC son una solución.

```{r, 852, eval = my_output == "html", results='asis', echo=F, purl=F}
cat('
<div class = "keyconcept" id="KC15.2">
<h3 class = "right"> Concepto clave 15.2 </h3>
<h3 class = "left"> Errores estándar de HAC </h3>
<p>

**Problema**:

Si el término de error $u_t$ en el modelo de retardo distribuido (\\@ref(eq:dlm)) está correlacionado en serie, la inferencia estadística que se basa en errores estándar habituales (robustos a la heterocedasticidad) puede ser muy engañosa.

**Solución**:

Los estimadores consistentes con heterocedasticidad y autocorrelación (HAC) de la matriz de varianza-covarianza evitan este problema. Existen funciones <tt>R</tt> como <tt>vcovHAC()</tt> del paquete <tt>sandwich</tt> que son convenientes para el cálculo de tales estimadores.

El paquete <tt>sandwich</tt> también contiene la función <tt>NeweyWest()</tt>, una implementación del estimador de varianza-covarianza HAC propuesto por @newey1987.

</p>
</div>
')
```

```{r, 853, eval=my_output=="latex", results='asis', echo=F, purl=F}
cat('\\begin{keyconcepts}[Errores estándar de HAC]{15.2}
\\textbf{Problema}:\\newline

Si el término de error $u_t$ en el modelo de retardo distribuido (\\@ref(eq:dlm)) está correlacionado en serie, la inferencia estadística que se basa en errores estándar habituales (robustos a la heterocedasticidad) puede ser muy engañosa.\\newline

\\textbf{Solución}:\\newline

Los estimadores consistentes con heterocedasticidad y autocorrelación (HAC) de la matriz de varianza-covarianza evitan este problema. Existen funciones \\texttt{R} como \\texttt{vcovHAC()} del paquete \\texttt{sandwich} que son convenientes para el cálculo de tales estimadores.\\newline

El paquete \\texttt{sandwich} también contiene la función \\texttt{NeweyWest()}, una implementación del estimador de varianza-covarianza HAC propuesto por \\cite{newey1987}.

\\end{keyconcepts}
')
```

Considere el modelo de regresión de rezagos distribuidos sin rezagos y un solo regresor $X_t$:

\begin{align*}
  Y_t = \beta_0 + \beta_1 X_t + u_t.
\end{align*}

con errores autocorrelacionados. Una breve derivación de

\begin{align}
  \overset{\sim}{\sigma}^2_{\widehat{\beta}_1} = \widehat{\sigma}^2_{\widehat{\beta}_1} \widehat{f}_t (\#eq:nwhac)
\end{align}

el llamado *estimador de varianza de Newey-West* para la varianza del estimador de MCO de $\beta_1$. $\widehat{\sigma}^2_{\widehat{\beta}_1}$ en \@ref(eq:nwhac) es la estimación de la varianza robusta a la heterocedasticidad de $\widehat{\beta}_1$ y

\begin{align}
  \widehat{f}_t = 1 + 2 \sum_{j=1}^{m-1} \left(\frac{m-j}{m}\right) \overset{\sim}{\rho}_j (\#eq:nwhacf)
\end{align}

es un factor de corrección que se ajusta a los errores correlacionados en serie e involucra estimaciones de $m-1$ coeficientes de autocorrelación $\overset{\sim}{\rho}_j$. Como resultado, usar la autocorrelación de muestra implementada en **acf()** para estimar los coeficientes de autocorrelación hace que \@ref(eq:nwhac) sea inconsistente. Por tanto, se utiliza un estimador algo diferente. Para una serie de tiempo $X$ se tiene $$ \ \overset{\sim}{\rho}_j = \frac{\sum_{t=j+1}^T \hat v_t \hat v_{t-j}}{\sum_{t=1}^T \hat v_t^2}, \ \text{with} \ \hat v= (X_t-\overline{X}) \hat u_t. $$ Implementando este estimador en la función **acf_c()** a continuación:

$m$ in \@ref(eq:nwhacf) is a truncation parameter to be chosen. A rule of thumb for choosing $m$ is
\begin{align}
  m = \left \lceil{0.75 \cdot T^{1/3}}\right\rceil. (\#eq:hactruncrot)
\end{align}

Se simula una serie de tiempo que, como se indicó anteriormente, sigue un modelo de retardo distribuido con errores autocorrelacionados y luego se muestra cómo calcular la estimación de Newey-West HAC de $SE(\widehat{\beta}_1)$ usando **R**. Esto se hace a través de dos enfoques separados pero, como se verá, idénticos: Al principio se sigue la derivación presentada paso a paso y se calcula la estimación "manualmente". Luego se muestra que el resultado es exactamente la estimación obtenida al usar la función **NeweyWest()**.

```{r, 854}
# función que calcula rho tilde
acf_c <- function(x, j) {
  return(
    t(x[-c(1:j)]) %*% na.omit(Lag(x, j)) / t(x) %*% x
  )
}

# simular series de tiempo con errores correlacionados en serie
set.seed(1)

N <- 100

eps <- arima.sim(n = N, model = list(ma = 0.5))
X <- runif(N, 1, 10)
Y <- 0.5 * X + eps

# calcular los residuos de MCO
res <- lm(Y ~ X)$res

# compute v
v <- (X - mean(X)) * res

# calcular una estimación sólida de la varianza beta_1
var_beta_hat <- 1/N * (1/(N-2) * sum((X - mean(X))^2 * res^2) ) / 
                        (1/N * sum((X - mean(X))^2))^2

# parámetro de truncamiento de la regla general
m <- floor(0.75 * N^(1/3))

# calcular el factor de corrección
f_hat_T <- 1 + 2 * sum(
  (m - 1:(m-1))/m * sapply(1:(m - 1), function(i) acf_c(x = v, j = i))
  ) 

# calcular la estimación de Newey-West HAC del error estándar
sqrt(var_beta_hat * f_hat_T)
```

Para que el código sea reutilizable en otras aplicaciones, se usa **sapply()** para estimar las $m-1$ autocorrelaciones $\overset{\sim}{\rho}_j$.

```{r, 855}
# usando NeweyWest():
NW_VCOV <- NeweyWest(lm(Y ~ X), 
              lag = m - 1, prewhite = F, 
              adjust = T)

# calcular el error estándar
sqrt(diag(NW_VCOV))[2]
```

Al elegir **lag = m-1** se asegura que el orden máximo de autocorrelaciones utilizadas sea $m-1$ --- tal como en la ecuación \@ref(eq:nwhacf). Observe que se establecen los argumentos **prewhite = F** y **adjust = T** para asegurar que se usa la fórmula \@ref(eq:nwhac) y se realizan ajustes de muestra finitos.

Se encontró que los errores estándar calculados coinciden. Por supuesto, una estimación de la matriz de varianza-covarianza calculada por **NeweyWest()** se puede proporcionar como el argumento **vcov** en **coeftest()**, tal que el estadístico $t$ de HAC y los valores $p$ son proporcionados por este último.

```{r, 856}
example_mod <- lm(Y ~ X)
coeftest(example_mod, vcov = NW_VCOV)
```

## Estimación de efectos causales dinámicos con regresores estrictamente exógenos

En general, los errores en un modelo de retardo distribuido están correlacionados, lo que requiere el uso de errores estándar de HAC para una inferencia válida. Sin embargo, si la suposición de exogeneidad (la primera suposición establecida en el Concepto clave 15.1) se reemplaza por exogeneidad estricta, es $$E(u_t\vert \dots, X_{t+1}, X_{t}, X_{t-1}, \dots) = 0,$$ se encuentran disponibles enfoques más eficientes que la estimación MCO de los coeficientes. Para un modelo de rezago distribuido general con $r$ rezagos y errores AR($p$), estos enfoques se resumen en el Concepto clave 15.4.

```{r, 857, eval = my_output == "html", results='asis', echo=F, purl=F}
cat('
<div class = "keyconcept" id="KC15.4">
<h3 class = "right"> Concepto clave 15.4 </h3>
<h3 class = "left"> Estimación de multiplicadores dinámicos bajo estricta exogeneidad </h3>
<p>

Considere el modelo de retraso distribuido general con $r$ retrasos y errores después de un proceso AR($p$),

\\begin{align}
  Y_t =& \\, \\beta_0 + \\beta_1 X_t + \\beta_2 X_{t-1} + \\dots + \\beta_{r+1} X_{t-r} + u_t (\\#eq:dlmar) \\\\
  u_t =& \\, \\phi_1 u_{t-1} + \\phi u_{t-2} + \\dots + \\phi_p u_{t-p} + \\overset{\\sim}{u}_t. (\\#eq:dlmarerrors)
\\end{align}

Bajo exogeneidad estricta de $X_t$, se puede reescribir el modelo anterior en la especificación ADL

\\begin{align*}
  Y_t =& \\, \\alpha_0 + \\phi_1 Y_{t-1} + \\phi_2 Y_{t-2} + \\dots + \\phi_p Y_{t-p} \\\\
      &+ \\, \\delta_0 X_t + \\delta_1 X_{t-1} + \\dots + \\delta_q X_{t-q} + \\overset{\\sim}{u}_t
\\end{align*}

donde $q=r+p$ y calcular estimaciones de los multiplicadores dinámicos $\\beta_1, \\beta_2, \\dots, \\beta_{r+1}$ utilizando estimaciones de MCO de $\\phi_1, \\phi_2, \\dots, \\phi_p, \\delta_0, \\delta_1, \\dots, \\delta_q$. 

Una alternativa es estimar los multiplicadores dinámicos utilizando MCG factible; es decir, aplicar el estimador MCO a una especificación de cuasi-diferencia de \\@ref(eq:dlmar). Bajo exogeneidad estricta, el enfoque MCG factible es el estimador AZUL para los multiplicadores dinámicos en muestras grandes.

Por un lado, la estimación por MCO de la representación de AVD puede ser beneficiosa para la estimación de los multiplicadores dinámicos en modelos de rezagos distribuidos grandes porque permite un modelo más parsimonioso que puede ser una buena aproximación al modelo grande. Por otro lado, el enfoque MCG es más eficiente que el estimador de AVD si el tamaño de la muestra es grande.

</p>
</div>
')
```

```{r, 858, eval = my_output == "latex", results='asis', echo=F, purl=F}
cat('\\begin{keyconcepts}[Estimación de multiplicadores dinámicos bajo estricta exogeneidad]{15.4}

Considere el modelo de retraso distribuido general con $r$ retrasos y errores después de un proceso AR($p$),

\\begin{align}
  Y_t =& \\, \\beta_0 + \\beta_1 X_t + \\beta_2 X_{t-1} + \\dots + \\beta_{r+1} X_{t-r} + u_t \\\\
  u_t =& \\, \\phi_1 u_{t-1} + \\phi u_{t-2} + \\dots + \\phi_p u_{t-p} + \\overset{\\sim}{u}_t. (\\#eq:dlmarerrors)
\\end{align}

Bajo exogeneidad estricta de $X_t$, se puede reescribir el modelo anterior en la especificación ADL

\\begin{align*}
  Y_t =& \\, \\alpha_0 + \\phi_1 Y_{t-1} + \\phi_2 Y_{t-2} + \\dots + \\phi_p Y_{t-p} \\\\
      &+ \\, \\delta_0 X_t + \\delta_1 X_{t-1} + \\dots + \\delta_q X_{t-q} + \\overset{\\sim}{u}_t
\\end{align*}

donde $q=r+p$ y calcular estimaciones de los multiplicadores dinámicos $\\beta_1, \\beta_2, \\dots, \\beta_{r+1}$ utilizando estimaciones de MCO de $\\phi_1, \\phi_2, \\dots, \\phi_p, \\delta_0, \\delta_1, \\dots, \\delta_q$.\\newline 

Una alternativa es estimar los multiplicadores dinámicos utilizando MCG factible; es decir, aplicar el estimador MCO a una especificación de cuasi-diferencia de \\@ref(eq:dlmar). Bajo exogeneidad estricta, el enfoque MCG factible es el estimador AZUL para los multiplicadores dinámicos en muestras grandes.\\newline

Por un lado, la estimación por MCO de la representación de AVD puede ser beneficiosa para la estimación de los multiplicadores dinámicos en modelos de rezagos distribuidos grandes porque permite un modelo más parsimonioso que puede ser una buena aproximación al modelo grande. Por otro lado, el enfoque MCG es más eficiente que el estimador de AVD si el tamaño de la muestra es grande.

\\end{keyconcepts}
')
```

En breve se revisa cómo se pueden obtener las diferentes representaciones de un modelo de retardo distribuido pequeño y se muestra cómo se puede estimar esta especificación mediante MCO y MCG usando **R**.

El modelo es

\begin{align}
  Y_t = \beta_0 + \beta_1 X_t + \beta_2 X_{t-1} + u_t, (\#eq:dldynamic)
\end{align}

por lo que se modela un cambio en $X$ para que afecte a $Y$ simultáneamente ($\beta_1$) y en el siguiente período ($\beta_2$). Se supone que el término de error $u_t$ sigue un proceso AR($1$), $$u_t = \phi_1 u_{t-1} + \overset{\sim}{u_t},$$ donde $\overset{\sim}{u_t}$ no está correlacionado en serie.

Se puede demostrar que la representación de ADL de este modelo es

\begin{align}
  Y_t = \alpha_0 + \phi_1 Y_{t-1} + \delta_0 X_t + \delta_1 X_{t-1} + \delta_2 X_{t-2} + \overset{\sim}{u}_t, (\#eq:adl21dynamic)
\end{align}

con las restricciones

\begin{align*}
  \beta_1 =& \, \delta_0, \\
  \beta_2 =& \, \delta_1 + \phi_1 \delta_0,
\end{align*}

#### Cuasi-diferencias {-}

Otra forma de escribir la representación de ADL($1$,$2$) \@ref(eq:adl21dynamic) es el *modelo de cuasi-diferencia*

\begin{align}
  \overset{\sim}{Y}_t = \alpha_0 + \beta_1 \overset{\sim}{X}_t + \beta_2   \overset{\sim}{X}_{t-1} + \overset{\sim}{u}_t, (\#eq:qdm)
\end{align}

donde $\overset{\sim}{Y}_t = Y_t - \phi_1 Y_{t-1}$ y $\overset{\sim}{X}_t = X_t - \phi_1 X_{t-1}$ Observe que el término de error $\overset{\sim}{u}_t$ no está correlacionado en ambos modelos y $$E(u_t\vert X_{t+1}, X_t, X_{t-1}, \dots) = 0,$$ que está implícito en el supuesto de exogeneidad estricta.

Se continua simulando una serie de tiempo de observaciones de $500$ usando el modelo \@ref(eq:dldynamic) con $\beta_1 = 0.1$, $\beta_2 = 0.25$, $\phi = 0.5$ y $\overset{\sim}{u}_t \sim \mathcal{N}(0,1)$, así como estimar las diferentes representaciones, comenzando con el modelo de retardo distribuido \@ref(eq:dldynamic).

```{r, 859}
# sembrar la semilla para la reproducibilidad
set.seed(1)

# simular una serie de tiempo con errores correlacionados en serie
obs <- 501
eps <- arima.sim(n = obs-1 , model = list(ar = 0.5))
X <- arima.sim(n = obs, model = list(ar = 0.25))
Y <- 0.1 * X[-1] + 0.25 * X[-obs] + eps
X <- ts(X[-1])

# estimar el modelo de rezago distribuido
dlm <- dynlm(Y ~ X + L(X))
```

Comprobar que los residuos de este modelo exhiben autocorrelación usando **acf()**.

```{r, 860, fig.align='center'}
# comprobar que los residuales estén correlacionados en serie
acf(residuals(dlm))
```

En particular, el patrón revela que los residuos siguen un proceso autorregresivo, ya que la función de autocorrelación de la muestra decae rápidamente durante los primeros rezagos y probablemente sea cero para los órdenes de rezago más altos. En cualquier caso, deben utilizarse errores estándar de HAC.

```{r, 861}
# resumen de coeficientes utilizando las estimaciones de errores estándar de Newey-West
coeftest(dlm, vcov = NeweyWest, prewhite = F, adjust = T)
```

#### Estimación MCO del modelo ADL {-}

A continuación, se estima el modelo ADL($1$,$2$) \@ref(eq:adl21dynamic) usando MCO. Los errores no están correlacionados en esta representación del modelo. Esta afirmación está respaldada por un gráfico de la función de autocorrelación muestral de la serie residual.

```{r, 862, fig.align='center'}
# estimar la representación de ADL(2,1) del modelo de rezago distribuido
adl21_dynamic <- dynlm(Y ~ L(Y) + X + L(X, 1:2))

# graficar las autocorrelaciones de muestra de residuos
acf(adl21_dynamic$residuals)
```

Los coeficientes estimados de `adl21_dynamic$coefficients` *no son los multiplicadores dinámicos que interesan*, sino que pueden calcularse de acuerdo con las restricciones en \@ref(eq:adl21dynamic), donde los coeficientes verdaderos son reemplazados por las estimaciones de MCO.

```{r, 863}
# calcular efectos dinámicos estimados usando restricciones de coeficientes
# en la representación de ADL(2,1)
t <- adl21_dynamic$coefficients

c("hat_beta_1" = t[3],
  "hat_beta_2" = t[4] + t[3] * t[2])
```

#### Estimación de mínimos cuadrados generalizados (MCG) {-}

La exogeneidad estricta permite la estimación por MCO del modelo de cuasi-diferencia \@ref(eq:qdm). La idea de aplicar el estimador MCO a un modelo donde las variables se transforman linealmente, de modo que los errores del modelo no están correlacionados y son homocedásticos, se denomina *mínimos cuadrados generalizados* (MCG).

El estimador MCO en \@ref(eq:qdm) se llama estimador *MCG* inviable porque $\overset{\sim}{Y}$ y $\overset{\sim}{X}$ no se pueden calcular sin saber $\phi_1$, el coeficiente autorregresivo en el modelo de error AR($1$), que generalmente se desconoce en la práctica.

Suponga que se sabe que $\phi = 0.5$. Entonces se pueden obtener las estimaciones MCG inviables de los multiplicadores dinámicos en \@ref(eq:dldynamic) aplicando MCO a los datos transformados.

```{r, 864}
# MCG: estimación de la especificación cuasi-diferenciada por MCO
iGLS_dynamic <- dynlm(I(Y- 0.5 * L(Y)) ~ I(X - 0.5 * L(X)) + I(L(X) - 0.5 * L(X, 2)))

summary(iGLS_dynamic)
```

El estimador *MCG factible* utiliza una estimación preliminar de los coeficientes en el modelo de término de presunto error, calcula los datos cuasidiferenciados y luego estima el modelo utilizando MCO. Esta idea fue introducida por @cochrane1949 y puede ampliarse continuando este proceso de forma iterativa. Dicho procedimiento se implementa en la función **cochrane.orcutt()** del paquete **orcutt**.

```{r, 865}
X_t <- c(X[-1])
# crear primer retraso
X_l1 <- c(X[-500])
Y_t <- c(Y[-1])

# procedimiento cochrane-orcutt iterado
summary(cochrane.orcutt(lm(Y_t ~ X_t + X_l1)))
```

Algunos métodos más sofisticados para la estimación de MCG se proporcionan con el paquete **nlme**. La función **gls()** se puede utilizar para ajustar modelos lineales mediante algoritmos de estimación de máxima verosimilitud y permite especificar una estructura de correlación para el término de error.

```{r, 866}
# procedimiento factible de estimación de máxima verosimilitud de MCG
summary(gls(Y_t ~ X_t + X_l1, correlation = corAR1()))
```

Observe que en este ejemplo, las estimaciones de coeficientes producidas por MCG están algo más cerca de sus valores verdaderos y que los errores estándar son los más pequeños para el estimador de MCG.

## Precios del jugo de naranja y clima frío

Esta sección investiga las dos preguntas siguientes utilizando los métodos de regresión de series de tiempo que se analizan aquí:

+ ¿Qué tan persistente es el efecto de una sola congelación en los precios del concentrado de jugo de naranja?

+ ¿El efecto se ha mantenido estable durante todo el período de tiempo?

Se comienza estimando los efectos causales dinámicos con un modelo de rezagos distribuidos donde $\%ChgOJC_t$ se regresa en $FDD_t$ y 18 rezagos. Una segunda especificación del modelo considera una transformación del modelo de rezago distribuido que permite estimar los 19 multiplicadores dinámicos acumulativos utilizando MCO. El tercer modelo, agrega 11 variables binarias (una para cada uno de los meses de febrero a diciembre) para ajustar un posible sesgo de variable omitida que surge de la correlación de $FDD_t$ y temporadas agregando **season(FDD)** al lado derecho de la mano de la fórmula del segundo modelo.

```{r, 867}
# estimar modelos de retardo distribuido de cambios en el precio del jugo de naranja congelado
FOJC_mod_DM <- dynlm(FOJC_pctc ~ L(FDD, 0:18))
FOJC_mod_CM1 <- dynlm(FOJC_pctc ~ L(d(FDD), 0:17) + L(FDD, 18))
FOJC_mod_CM2 <- dynlm(FOJC_pctc ~ L(d(FDD), 0:17) + L(FDD, 18) + season(FDD))
```

Los modelos anteriores incluyen una gran cantidad de retrasos con etiquetas predeterminadas que se corresponden con el grado de diferenciación y los órdenes de retraso, lo que dificulta la lectura del resultado. Las etiquetas de regresor de un objeto modelo se pueden alterar anulando el atributo **names** de la sección de coeficientes usando la función **attr()**. Por lo tanto, para una mejor legibilidad, se utilizan las órdenes de retraso como etiquetas regresivas.

```{r, 868, echo=T}
# establecer órdenes de retraso como etiquetas regresivas
attr(FOJC_mod_DM$coefficients, "names")[1:20] <- c("(Intercept)", as.character(0:18))
attr(FOJC_mod_CM1$coefficients, "names")[1:20] <- c("(Intercept)", as.character(0:18))
attr(FOJC_mod_CM2$coefficients, "names")[1:20] <- c("(Intercept)", as.character(0:18))
```

A continuación, se calculan los errores estándar de HAC para cada modelo usando **NeweyWest()** y se recopilan los resultados en una lista que luego se suministra como el argumento **se** a la función **stargazer()**, observe abajo. La muestra consta de 612 observaciones:

```{r, 869}
length(FDD)
```

De acuerdo con \@ref(eq:hactruncrot), la regla general para elegir el parámetro de truncamiento $m$ del error estándar HAC, se elige

$$m = \left\lceil0.75 \cdot 612^{1/3} \right\rceil = \lceil6.37\rceil = 7.$$

Para comprobar la sensibilidad de los errores estándar a las diferentes opciones del parámetro de truncamiento en el modelo que se utiliza para estimar los multiplicadores acumulativos, también se calcula el estimador de Newey-West para $m = 14$.

```{r, 870}
# recopilar errores estándar de HAC en una lista
SEs <- list(sqrt(diag(NeweyWest(FOJC_mod_DM, lag = 7, prewhite = F))), 
            sqrt(diag(NeweyWest(FOJC_mod_CM1, lag = 7, prewhite = F))), 
            sqrt(diag(NeweyWest(FOJC_mod_CM1, lag = 14, prewhite = F))),
            sqrt(diag(NeweyWest(FOJC_mod_CM2, lag = 7, prewhite = F))))
```

Luego, los resultados se utilizan para producir la siguiente tabla:

```{r, 871, eval=F}
stargazer(FOJC_mod_DM , FOJC_mod_CM1, FOJC_mod_CM1, FOJC_mod_CM2,
  title = "Efectos dinámicos de un día de grados de congelación en el precio del jugo de naranja",
  header = FALSE, 
  digits = 3, 
  column.labels = c("Multiplicadores dinámicos", rep("Multiplicadores acumulativos dinámicos", 3)),
  dep.var.caption  = "Variable dependiente: Cambio porcentual mensual en el precio del jugo de naranja",
  dep.var.labels.include = FALSE,
  covariate.labels = as.character(0:18),
  omit = "season",
  se = SEs,
  no.space = T,
  add.lines = list(c("¿Indicadores mensuales?","no", "no", "no", "yes"),
                   c("Truncamiento de HAC","7", "7", "14", "7")),
  omit.stat = c("rsq", "f","ser")) 
```

<!--html_preserve-->

```{r, 872, eval = my_output == "html", results = 'asis', echo = F, purl = F, message = F, warning = F}
stargazer(FOJC_mod_DM , FOJC_mod_CM1, FOJC_mod_CM1, FOJC_mod_CM2,
  header = FALSE, 
  type = "html",
  digits = 3, 
  column.labels = c("Multiplicadores dinámicos", rep("Multiplicadores acumulativos dinámicos", 3)),
  dep.var.caption  = "Variable dependiente: Cambio porcentual mensual en el precio del jugo de naranja",
  dep.var.labels.include = FALSE,
  covariate.labels = as.character(0:18),
  omit = "season",
  se = SEs,
  no.space = T,
  add.lines = list(
                   c("¿Indicadores mensuales?","no", "no", "no", "yes"),
                   c("Truncamiento de HAC","7", "7", "14", "7")
                   ),
  omit.stat = c("rsq", "f","ser")
  ) 

stargazer_html_title("Efectos dinámicos de un día de grados de congelación en el precio del jugo de naranja", "deoafddotpooj")
```

<!--/html_preserve-->

```{r, 873, eval = my_output == "latex", results = 'asis', echo = F, purl = F, message = F, warning = F}
stargazer(FOJC_mod_DM , FOJC_mod_CM1, FOJC_mod_CM1, FOJC_mod_CM2,
  title = "\\label{tab:deoafddotpooj} Efectos dinámicos de un día de grados de congelación en el precio del jugo de naranja",
  header = FALSE, 
  type = "latex",
  column.sep.width = "20pt",
  float.env = "sidewaystable",
  single.row = T,
  digits = 3, 
  column.labels = c("Multiplicadores dinámicos", rep("Multiplicadores acumulativos dinámicos", 3)),
  dep.var.caption  = "Variable dependiente: Cambio porcentual mensual en el precio del jugo de naranja",
  dep.var.labels.include = FALSE,
  covariate.labels = paste("lag",as.character(0:18)),
  omit = "season",
  se = SEs,
  no.space = T,
  add.lines = list(
                   c("¿Indicadores mensuales?","no", "no", "no", "yes"),
                   c("Truncamiento de HAC","7", "7", "14", "7")
                   ),
  omit.stat = c("rsq", "f","ser")
  ) 
```

Según la columna (1) de la Tabla \@ref(tab:deoafddotpooj), el efecto contemporáneo de un día de grado de congelación es un aumento de $0.5\%$ en los precios del jugo de naranja. El efecto estimado es de solo $0.17\%$ para el próximo mes y cercano a cero para los meses siguientes. De hecho, para todos los rezagos mayores que 1, no se puede rechazar la hipótesis nula de que los coeficientes respectivos son cero usando pruebas de $t$ individuales. El modelo **FOJC_mod_DM** solo explica poco de la variación en la variable dependiente ($\bar{R}^2 = 0.11$).

Las columnas (2) y (3) presentan estimaciones de los multiplicadores acumulativos dinámicos del modelo **FOJC_mod_CM1**. Aparentemente, no importa si se elige $m = 7$ o $m = 14$ al calcular los errores estándar de HAC, por lo que se queda con $m = 7$ y los errores estándar informados en la columna (2).

Si la demanda de jugo de naranja es mayor en invierno, $FDD_t$ estaría correlacionado con el término de error, ya que las heladas ocurren más bien en invierno, por lo que se enfrentaría un sesgo de variable omitido. La estimación del tercer modelo, **FOJC_mod_CM2**, tiene en cuenta este posible problema mediante el uso de un conjunto adicional de 11 variables ficticias mensuales. Por brevedad, las estimaciones de los coeficientes ficticios se excluyen de la salida producida por Stargazer (esto se logra estableciendo **omit = 'season'**). Se puede comprobar que se omitió la variable ficticia de enero para evitar una multicolinealidad perfecta.

```{r, 874}
# estimaciones sobre variables ficticias mensuales
FOJC_mod_CM2$coefficients[-c(1:20)]
```

Una comparación de las estimaciones presentadas en las columnas (3) y (4) indica que la adición de variables ficticias mensuales tiene un efecto insignificante. Otra evidencia de esto proviene de una prueba conjunta de la hipótesis de que los 11 coeficientes ficticios son cero. En lugar de usar **linearHypothesis()**, se usa la función **waldtest()** y se proporcionan dos objetos de modelo en su lugar: **unres_model**, el objeto de modelo sin restricciones que es lo mismo que **FOJC_mod_CM2** (excepto por los nombres de los coeficientes, ya que se han modificado arriba) y **res_model**, el modelo donde se impone la restricción de que todos los coeficientes ficticios son cero. **res_model** se obtiene convenientemente usando la función **update()**. Extrae el argumento **formula** de un objeto modelo, lo actualiza según lo especificado y luego vuelve a ajustar el modelo. Estableciendo **formula = . ~ . - season(FDD)** se impone que los maniquíes mensuales no ingresen al modelo.

```{r, 875}
# probar si los coeficientes de las variables ficticias mensuales son cero
unres_model <- dynlm(FOJC_pctc ~ L(d(FDD), 0:17) + L(FDD, 18) + season(FDD))

res_model <- update(unres_model, formula = . ~ . - season(FDD))

waldtest(unres_model, 
         res_model, 
         vcov = NeweyWest(unres_model, lag = 7, prewhite = F))
```

El valor de $p$ es $0.47$, por lo que no se puede rechazar la hipótesis de que los coeficientes de las variables ficticias mensuales son cero, incluso en el nivel de $10\%$. Se concluye que las fluctuaciones estacionales en la demanda de jugo de naranja no representan una amenaza seria para la validez interna del modelo.

Es conveniente utilizar gráficos de multiplicadores dinámicos y multiplicadores dinámicos acumulativos. Los siguientes dos fragmentos de código reproducen gráficas que muestran estimaciones puntuales de multiplicadores dinámicos y acumulativos junto con los límites superior e inferior de sus intervalos de confianza de $95\%$ calculados utilizando los errores estándar de HAC anteriores.

```{r, 876, fig.align='center', fig.cap="Multiplicadores Dinámicos", label = MD}
# límites de IC del 95%
point_estimates <- FOJC_mod_DM$coefficients

CI_bounds <- cbind("lower" = point_estimates - 1.96 * SEs[[1]],
                   "upper" = point_estimates + 1.96 * SEs[[1]])[-1, ]

# graficar los multiplicadores dinámicos estimados
plot(0:18, point_estimates[-1], 
     type = "l", 
     lwd = 2, 
     col = "steelblue", 
     ylim = c(-0.4, 1),
     xlab = "Retraso",
     ylab = "Multiplicador dinámico",
     main = "Efecto dinámico de FDD en el precio del jugo de naranja")

# agregar una línea discontinua en 0
abline(h = 0, lty = 2)

# agregar límites de CI
lines(0:18, CI_bounds[,1], col = "darkred")
lines(0:18, CI_bounds[,2], col = "darkred")
```

Los intervalos de confianza de $95\%$ graficados en la Figura \@ref(fig:DM) de hecho incluyen cero para retrasos mayores que 1, de modo que el nulo de un multiplicador cero no puede rechazarse para estos retrasos.

```{r, 877, fig.align='center', fig.cap="Multiplicadores Acumulativos Dinámicos", label = MAD}
# límites de IC del 95%
point_estimates <- FOJC_mod_CM1$coefficients

CI_bounds <- cbind("lower" = point_estimates - 1.96 * SEs[[2]],
                   "upper" = point_estimates + 1.96 * SEs[[2]])[-1,]


# graficar multiplicadores dinámicos estimados
plot(0:18, point_estimates[-1], 
     type = "l", 
     lwd = 2, 
     col = "steelblue", 
     ylim = c(-0.4, 1.6),
     xlab = "Retraso",
     ylab = "Multiplicador dinámico acumulativo",
     main = "Efecto dinámico acumulativo de FDD en el precio del jugo de naranja")

# agregar línea discontinua en 0
abline(h = 0, lty = 2)

# agregar límites de CI
lines(0:18, CI_bounds[, 1], col = "darkred")
lines(0:18, CI_bounds[, 2], col = "darkred")
```

Como se puede ver en la Figura \@ref(fig:DCM), los multiplicadores acumulativos dinámicos estimados crecen hasta el séptimo mes cuando ocurre un aumento del precio de aproximadamente $0.91\%$ y luego disminuyen ligeramente hasta el multiplicador acumulativo estimado a largo plazo de $0.37\%$ que, sin embargo, no es significativamente diferente de cero en el nivel de $5\%$.

¿Los multiplicadores dinámicos se han mantenido estables en el tiempo? Una forma de ver esto es estimar estos multiplicadores para diferentes subperíodos del intervalo muestral. Por ejemplo, considere los períodos 1950-1966, 1967-1983 y 1984-2000. Si los multiplicadores son los mismos para los tres períodos, las estimaciones deben ser cercanas y, por lo tanto, los multiplicadores acumulativos estimados también deben ser similares. Se investiga esto volviendo a estimar **FOJC_mod_CM1** para los tres períodos de tiempo diferentes y luego se grafican los multiplicadores dinámicos acumulativos estimados para la comparación.

```{r, 878}
# estimar multiplicadores acumulativos usando diferentes períodos de muestra
FOJC_mod_CM1950 <- update(FOJC_mod_CM1, start = c(1950, 1), end = c(1966, 12))

FOJC_mod_CM1967 <- update(FOJC_mod_CM1, start = c(1967, 1), end = c(1983, 12))

FOJC_mod_CM1984 <- update(FOJC_mod_CM1, start = c(1984, 1), end = c(2000, 12))
```

```{r, 879, fig.align='center'}
# graficar multiplicadores acumulativos dinámicos estimados (1950-1966)
plot(0:18, FOJC_mod_CM1950$coefficients[-1], 
     type = "l", 
     lwd = 2, 
     col = "steelblue",
     xlim = c(0, 20),
     ylim = c(-0.5, 2),
     xlab = "Retraso",
     ylab = "Multiplicador dinámico acumulativo",
     main = "Efecto dinámico acumulativo para diferentes períodos de muestra")

# graficar multiplicadores dinámicos estimados (1967-1983)
lines(0:18, FOJC_mod_CM1967$coefficients[-1], lwd = 2)

# graficar multiplicadores dinámicos estimados (1984-2000)
lines(0:18, FOJC_mod_CM1984$coefficients[-1], lwd = 2, col = "darkgreen")

# agregar línea discontinua en 0
abline(h = 0, lty = 2)

# agregar anotaciones
text(18, -0.24, "1984 - 2000")
text(18, 0.6, "1967 - 1983")
text(18, 1.2, "1950 - 1966")
```

Claramente, los multiplicadores dinámicos acumulativos han cambiado considerablemente con el tiempo. El efecto de una congelación fue más fuerte y persistente en las décadas de 1950 y 1960. Para la década de 1970, la magnitud del efecto fue menor pero aún muy persistente. Se observa una magnitud aún menor para el tercio final del intervalo de la muestra (1984 - 2000), donde el efecto a largo plazo es mucho menos persistente y esencialmente cero después de un año.

Una prueba QLR para una ruptura en la regresión de retardo distribuido de la columna (1) en la Tabla \@ref(tab:deoafddotpooj) con un recorte de $15\%$ utilizando una estimación de matriz de varianza-covarianza de HAC respalda la conjetura de que los coeficientes de regresión de la población han cambiado tiempo extraordinario.

```{r, 880, cache=TRUE}
# configurar un rango de posibles fechas de descanso
tau <- c(window(time(FDD), 
                time(FDD)[round(612/100*15)], 
                time(FDD)[round(612/100*85)]))

# inicializar el vector de estadísticos F
Fstats <- numeric(length(tau))

# el modelo restringido
res_model <- dynlm(FOJC_pctc ~ L(FDD, 0:18))

# estimación, repetir las fechas de descanso
for(i in 1:length(tau)) {
  
  # configurar variable ficticia
  D <- time(FOJC_pctc) > tau[i]
  
  # estimar el modelo DL con interacciones
  unres_model <- dynlm(FOJC_pctc ~ D * L(FDD, 0:18))
                 
  # calcular y guardar el estadístico F
  Fstats[i] <- waldtest(res_model, 
                        unres_model, 
                        vcov = NeweyWest(unres_model, lag = 7, prewhite = F))$F[2]
    
}
```

Se debe tener en cuenta que este código tarda un par de segundos en ejecutarse, ya que se estima un total de regresiones de **length(tau)** con coeficientes de modelo de $40$ cada una.

```{r, 881}
# estadístico de prueba de QLR
max(Fstats)
```

El estadístico QLR es **round(max(Fstats),2)**. Se puede observar que el valor crítico $1\%$ para la prueba QLR con recorte del $15\%$ y $q = 20$ restricciones es $2.43$. Dado que esta es una prueba del lado derecho, el estadístico QLR se encuentra claramente en la región de rechazo, por lo que se puede descartar la hipótesis nula de no ruptura en la función de regresión de la población.

Se recomienda buscar ejemplos empíricos donde se cuestione si el supuesto de exogeneidad (pasada y presente) de los regresores es plausible.

#### Resumen {-}

+ Se ha visto cómo **R** puede usarse para estimar la trayectoria temporal del efecto en $Y$ de un cambio en $X$ (el efecto causal dinámico en $Y$ de un cambio en $X$) usando datos de series de tiempo en ambos. El modelo correspondiente se llama modelo de rezago distribuido. Los modelos de retraso distribuidos se estiman convenientemente usando la función **dynlm()** del paquete **dynlm**.

+ El error de regresión en los modelos de rezagos distribuidos a menudo se correlaciona en serie de modo que los errores estándar que son robustos a la *heterocedasticidad* y *autocorrelación* deben usarse para obtener una inferencia válida. El paquete **sandwich** proporciona funciones para el cálculo de los denominados estimadores de matriz de covarianza HAC, por ejemplo **vcovHAC()** y **NeweyWest()**.

+ Cuando $X$ es *estrictamente exógeno*, se pueden obtener estimaciones más eficientes utilizando un modelo ADL o mediante la estimación MCG. Se pueden encontrar algoritmos MCG viables en los paquetes **orcutt** y **nlme** de **R**. Se debe enfatizar que el supuesto de exogeneidad estricta es a menudo inverosímil en aplicaciones empíricas.

<!--chapter:end:Capitulo_16.Rmd-->

# Temas adicionales en la regresión de series temporales {#TARST}

```{r, echo = F}
options(knitr.duplicate.label = "allow")
```

```{r, 882, child="_setup.Rmd"}
```

```{r, 883, eval=knitr::opts_knit$get("rmarkdown.pandoc.to") == "html", results='asis', echo=FALSE}
cat('<hr style="background-color:#03193b;height:2px">')
```

Este capítulo analiza los siguientes temas avanzados en la regresión de series de tiempo y demuestra cómo se pueden aplicar las técnicas básicas usando **R**:

+ *Autorregresiones vectoriales* (VAR). Se enfoca en usar VAR para pronosticar. Otra rama de la literatura se ocupa de los llamados *VAR estructurales*, que están más allá del alcance de este capítulo.
+ Previsiones multiperiodo. Esto incluye una discusión de pronósticos iterados y directos (multivariados).
+ La prueba *DF-GLS*, una modificación de la prueba *ADF* que tiene más potencia que esta última cuando la serie tiene componentes deterministas y está cerca de ser no estacionaria.
+ Análisis de cointegración con aplicación a tipos de interés a corto y largo plazo. Se demuestra cómo estimar un modelo de corrección de errores vectoriales.
+ Modelos de *Heterocedasticidad condicional autorregresiva* (ARCH). Se muestra cómo un modelo *ARCH* generalizado simple (*GARCH*) puede ser útil para cuantificar el riesgo asociado con la inversión en el mercado de valores en términos de estimación y pronóstico de la volatilidad de los rendimientos de los activos.

Para reproducir los ejemplos de código, se necesita instalar los paquetes **R** que se enumeran a continuación. Asegúrese que el siguiente fragmento de código se ejecute sin errores.

+ **AER** [@R-AER]
+ **dynlm** [@R-dynlm]
+ **fGarch** [@R-fGarch]
+ **quantmod** [@R-quantmod]
+ **readxl** [@R-readxl]
+ **scales** [@R-scales]
+ **vars** [@R-vars]

```{r, 884, warning=FALSE, message=FALSE}
library(AER)
library(readxl)
library(dynlm)
library(vars)
library(quantmod)
library(scales)
library(fGarch)
```

## Autorregresiones vectoriales

Un modelo de *Vector autorregresivo* (VAR) es útil cuando se está interesado en predecir múltiples variables de series de tiempo usando un solo modelo. En esencia, el modelo VAR es una extensión del modelo autorregresivo univariante que se ha tratado en los capítulos \@ref(IRSTP) y \@ref(EECD). El concepto clave 16.1 resume los fundamentos del VAR.

```{r, 885, eval = my_output == "html", results='asis', echo=F, purl=F}
cat('
<div class = "keyconcept" id="KC16.1">
<h3 class = "right"> Concepto clave <br> 16.1</h3>          
<h3 class = "left">Autoregresiones vectoriales</h3>
<p>

El modelo de autorregresión vectorial (VAR) extiende la idea de autorregresión univariante a regresiones de series de tiempo $k$, donde los valores rezagados de *todas* $k$ series aparecen como regresores. Dicho de otra manera, en un modelo VAR se hace una regresión de un *vector* de variables de series de tiempo en vectores rezagados de estas variables. En cuanto a los modelos AR ($p$), el orden de retraso se denota por $p$ por lo que el modelo VAR ($p$) de dos variables $X_t$ y $Y_t$ ($k = 2$) viene dado por las ecuaciones:

\\begin{align*}
  Y_t =& \\, \\beta_{10} + \\beta_{11} Y_{t-1} + \\dots + \\beta_{1p} Y_{t-p} + \\gamma_{11} X_{t-1} + \\dots + \\gamma_{1p} X_{t-p} + u_{1t}, \\\\
  X_t =& \\, \\beta_{20} + \\beta_{21} Y_{t-1} + \\dots + \\beta_{2p} Y_{t-p} + \\gamma_{21} X_{t-1} + \\dots + \\gamma_{2p} X_{t-p} + u_{2t}.
\\end{align*}

Los $\\beta$s y $\\gamma$s se pueden estimar usando MCO en cada ecuación. Los supuestos para los VAR son los supuestos de series de tiempo presentados en el Concepto clave 14.6 aplicados a cada una de las ecuaciones.

Es sencillo estimar modelos VAR en <tt>R</tt>. Un enfoque factible es simplemente usar <tt>lm()</tt> para la estimación de las ecuaciones individuales. Además, el paquete <tt>R</tt> <tt>vars</tt> proporciona herramientas estándar para estimación, pruebas de diagnóstico y predicción utilizando este tipo de modelos.

</p>
</div>
')
```

```{r, 886, eval = my_output == "latex", results='asis', echo=F, purl=F}
cat('\\begin{keyconcepts}[Autoregresiones vectoriales]{16.1}

El modelo de autorregresión vectorial (VAR) extiende la idea de autorregresión univariante a regresiones de series de tiempo $k$, donde los valores rezagados de *todas* $k$ series aparecen como regresores. Dicho de otra manera, en un modelo VAR se hace una regresión de un \\textit{vector} de variables de series de tiempo en vectores rezagados de estas variables. En cuanto a los modelos AR ($p$), el orden de retraso se denota por $p$ por lo que el modelo VAR ($p$) de dos variables $X_t$ y $Y_t$ ($k = 2$) viene dado por las ecuaciones:

\\begin{align*}
  Y_t =& \\, \\beta_{10} + \\beta_{11} Y_{t-1} + \\dots + \\beta_{1p} Y_{t-p} + \\gamma_{11} X_{t-1} + \\dots + \\gamma_{1p} X_{t-p} + u_{1t}, \\\\
  X_t =& \\, \\beta_{20} + \\beta_{21} Y_{t-1} + \\dots + \\beta_{2p} Y_{t-p} + \\gamma_{21} X_{t-1} + \\dots + \\gamma_{2p} X_{t-p} + u_{2t}.
\\end{align*}

Los $\\beta$s y $\\gamma$s se pueden estimar usando MCO en cada ecuación. Los supuestos para los VAR son los supuestos de series de tiempo presentados en el Concepto clave 14.6 aplicados a cada una de las ecuaciones.\\newline

Es sencillo estimar modelos VAR en \\texttt{R}. Un enfoque factible es simplemente usar \\texttt{lm()} para la estimación de las ecuaciones individuales. Además, el paquete \\texttt{R} \\texttt{vars} proporciona herramientas estándar para estimación, pruebas de diagnóstico y predicción utilizando este tipo de modelos.

\\end{keyconcepts}
')
```

Cuando se cumplen los supuestos del Concepto clave 16.1, los estimadores MCO de los coeficientes VAR son consistentes y conjuntamente normales en muestras grandes, de modo que se pueden utilizar los métodos inferenciales habituales, como los intervalos de confianza y los estadísticos $t$.

La estructura de los VAR también permite probar conjuntamente las restricciones en múltiples ecuaciones. Por ejemplo, puede ser de interés probar si los coeficientes de todos los regresores del rezago $p$ son cero. Esto corresponde a probar el nulo de que el orden de retraso $p-1$ es correcto. La normalidad conjunta de muestras grandes de las estimaciones de coeficientes es conveniente porque implica que simplemente se puede usar una prueba $F$ para este problema de prueba. La fórmula explícita para un estadístico de prueba de este tipo es bastante complicada, pero afortunadamente estos cálculos se realizan fácilmente utilizando las funciones **R** con las que se trabaja en este capítulo. Otra forma de determinar las longitudes de retraso óptimas son los criterios de información como $BIC$ que se ha introducido para las regresiones de series de tiempo univariadas en el Capítulo \@ref(SLRUCI). Al igual que en el caso de una sola ecuación, para un modelo de ecuaciones múltiples se elige la especificación que tiene el menor $BIC(p)$, donde

\begin{align*}
  BIC(p) =& \, \log\left[\text{det}(\widehat{\Sigma}_u)\right] + k(kp+1) \frac{\log(T)}{T}.
\end{align*}

donde $\widehat{\Sigma}_u$ denota la estimación de la matriz de covarianza $k \times k$ de los errores VAR y $\text{det}(\cdot)$ denota el determinante.

En cuanto a los modelos de rezagos distribuidos univariados, se debe pensar detenidamente en las variables que se incluirán en un VAR, ya que agregar variables no relacionadas reduce la precisión del pronóstico al aumentar el error de estimación. Esto es particularmente importante porque el número de parámetros a estimar crece cuadráticamente al número de variables modeladas por el VAR. En la aplicación siguiente se verá que la teoría económica y la evidencia empírica son útiles para esta decisión.

#### Un modelo VAR de la tasa de crecimiento del PIB y el margen temporal {-}

Ahora se muestra cómo estimar un modelo VAR de la tasa de crecimiento del PIB, $GDPGR$, y el diferencial de plazo, $TSpread$. Como sigue la discusión sobre la no estacionariedad del crecimiento del PIB en el Capítulo \@ref(NEIT) (recuerde la posible ruptura a principios de la década de 1980 detectada por la estadística de prueba $QLR$), se usan datos de 1981:Q1 a 2012:Q4. Las dos ecuaciones del modelo son

\begin{align*}
 GDPGR_t =& \, \beta_{10} + \beta_{11} GDPGR_{t-1} + \beta_{12} GDPGR_{t-2} + \gamma_{11} TSpread_{t-1} + \gamma_{12} TSpread_{t-2} + u_{1t}, \\
 TSpread_t =& \, \beta_{20} + \beta_{21} GDPGR_{t-1} + \beta_{22} GDPGR_{t-2} + \gamma_{21} TSpread_{t-1} + \gamma_{22} TSpread_{t-2} + u_{2t}.
\end{align*}

El conjunto de datos **us_macro_quarterly.xlsx** se proporciona en el sitio web complementario a @stock2015 y se puede descargar [aquí](http://wps.aw.com/aw_stock_ie_3/178/45691/11696965.cw/index.html). Contiene datos trimestrales sobre el PIB real de EE. UU. (es decir, ajustado a la inflación) de 1947 a 2004. Se comienza importando el conjunto de datos y aplicando un formato (ya se trabajó con este conjunto de datos en el Capítulo \@ref(IRSTP), por lo que se pueden omitir estos pasos si ya se han cargado los datos en el entorno de trabajo).

```{r, 887, eval=FALSE}
# cargar el conjunto de datos macroeconómicos de EE. UU.
USMacroSWQ <- read_xlsx("data/us_macro_quarterly.xlsx",
                         sheet = 1,
                         col_types = c("text", rep("numeric", 9)))

# establecer los nombres de las columnas
colnames(USMacroSWQ) <- c("Date", "GDPC96", "JAPAN_IP", "PCECTPI", "GS10", 
                          "GS1", "TB3MS", "UNRATE", "EXUSUK", "CPIAUCSL")

# formatear la columna de fecha
USMacroSWQ$Date <- as.yearqtr(USMacroSWQ$Date, format = "%Y:0%q")

# definir el PIB como objeto ts
GDP <- ts(USMacroSWQ$GDPC96,
          start = c(1957, 1), 
          end = c(2013, 4), 
          frequency = 4)

# definir el crecimiento del PIB como un objeto ts
GDPGrowth <- ts(400*log(GDP[-1]/GDP[-length(GDP)]),
                start = c(1957, 2), 
                end = c(2013, 4), 
                frequency = 4)

# tasa de interés de las letras del Tesoro a 3 meses como objeto 'ts'
TB3MS <- ts(USMacroSWQ$TB3MS,
            start = c(1957, 1), 
            end = c(2013, 4), 
            frequency = 4)

# tasa de interés de los bonos del Tesoro a 10 años como objeto 'ts'
TB10YS <- ts(USMacroSWQ$GS10, 
              start = c(1957, 1), 
              end = c(2013, 4), 
              frequency = 4)

# generar la serie diferencial por plazo
TSpread <- TB10YS - TB3MS
```

```{r, 888, echo=F, purl=F}
library(xts)
# cargar datos macroeconómicos de EE. UU.
USMacroSWQ <- read_xlsx("data/us_macro_quarterly.xlsx",
                         sheet = 1,
                         col_types = c("text", rep("numeric", 9)))

# establecer nombres de columna
colnames(USMacroSWQ) <- c("Date", "GDPC96", "JAPAN_IP", "PCECTPI", "GS10", "GS1", "TB3MS", "UNRATE", "EXUSUK", "CPIAUCSL")

# formatear columna de fecha
USMacroSWQ$Date <- as.yearqtr(USMacroSWQ$Date, format = "%Y:0%q")

# eludir error
GDP <- xts(USMacroSWQ$GDPC96, USMacroSWQ$Date)["1960::2013"]
GDPGrowth <- xts(400 * log(GDP/lag(GDP)))
GDP <- ts(GDP,
          start = c(1960, 1), 
          end = c(2013, 4), 
          frequency = 4)

GDPGrowth <- ts(GDPGrowth,
                start = c(1960, 1), 
                end = c(2013, 4), 
                frequency = 4)

# tasa de interés de las letras del Tesoro a 3 meses como objeto 'ts'
TB3MS <- ts(USMacroSWQ$TB3MS,
            start = c(1957, 1), 
            end = c(2013, 4), 
            frequency = 4)

# tasa de interés de las letras del Tesoro a 10 meses como objeto 'ts'
TB10YS <- ts(USMacroSWQ$GS10, 
              start = c(1957, 1), 
              end = c(2013, 4), 
              frequency = 4)

# generar la serie de diferencial por plazo
TSpread <- TB10YS - TB3MS
```

Se estiman ambas ecuaciones por separado por MCO y se usa **coeftest()** para obtener errores estándar robustos.

```{r, 889}
# estimar ambas ecuaciones usando 'dynlm()'
VAR_EQ1 <- dynlm(GDPGrowth ~ L(GDPGrowth, 1:2) + L(TSpread, 1:2), 
                 start = c(1981, 1), 
                 end = c(2012, 4))

VAR_EQ2 <- dynlm(TSpread ~ L(GDPGrowth, 1:2) + L(TSpread, 1:2),
                 start = c(1981, 1),
                 end = c(2012, 4))

# cambiar el nombre de los regresores para una mejor legibilidad
names(VAR_EQ1$coefficients) <- c("Intercept", 
                                 "Growth_t-1", 
                                 "Growth_t-2", 
                                 "TSpread_t-1", 
                                 "TSpread_t-2")
names(VAR_EQ2$coefficients) <- names(VAR_EQ1$coefficients)

# resúmenes robustos de coeficientes
coeftest(VAR_EQ1, vcov. = sandwich)
coeftest(VAR_EQ2, vcov. = sandwich)
```

Se termina con los siguientes resultados:

\begin{align*}
 GDPGR_t =& \, \underset{(0.46)}{0.52} + \underset{(0.11)}{0.29} GDPGR_{t-1} + \underset{(0.09)}{0.22} GDPGR_{t-2} -\underset{(0.36)}{0.90} TSpread_{t-1} + \underset{(0.39)}{1.33} TSpread_{t-2} \\
 TSpread_t =& \, \underset{(0.12)}{0.46} + \underset{(0.02)}{0.01} GDPGR_{t-1} -\underset{(0.03)}{0.06} GDPGR_{t-2} + \underset{(0.10)}{1.06} TSpread_{t-1} -\underset{(0.11)}{0.22} TSpread_{t-2} 
\end{align*}

La función **VAR()** se puede utilizar para obtener las mismas estimaciones de coeficientes que se presentaron anteriormente, ya que también se aplica MCO por ecuación.

```{r, 890}
# configurar datos para la estimación usando `VAR()`
VAR_data <- window(ts.union(GDPGrowth, TSpread), start = c(1980, 3), end = c(2012, 4))

# estimar los coeficientes del modelo usando `VAR()`
VAR_est <- VAR(y = VAR_data, p = 2)
VAR_est
```

**VAR()** devuelve una **lista** de objetos **lm** que se pueden pasar a las funciones habituales, por ejemplo **summary()** y, por lo tanto, es sencillo obtener estadísticas del modelo para el ecuaciones individuales.

```{r, 891}
# obtener el R^2 ajustado de la salida de 'VAR()'
summary(VAR_est$varresult$GDPGrowth)$adj.r.squared
summary(VAR_est$varresult$TSpread)$adj.r.squared
```

Se pueden utilizar los objetos del modelo individual para realizar pruebas de causalidad de Granger.

```{r, 892}
# Pruebas de causalidad de Granger:

# probar si el diferencial de plazo no tiene poder para explicar el crecimiento del PIB
linearHypothesis(VAR_EQ1, 
                 hypothesis.matrix = c("TSpread_t-1", "TSpread_t-2"),
                 vcov. = sandwich)

# probar si el crecimiento del PIB no tiene poder para explicar el diferencial de plazo
linearHypothesis(VAR_EQ2, 
                 hypothesis.matrix = c("Growth_t-1", "Growth_t-2"),
                 vcov. = sandwich)
```

Ambas pruebas de causalidad de Granger se rechazan al nivel de $5\%$. Esto es evidencia a favor de la conjetura de que el diferencial por plazo tiene poder para explicar el crecimiento del PIB y viceversa.

#### Pronósticos multivariados iterados usando un VAR iterado {-}

La idea de un pronóstico iterado para el período $T + 2$ basado en observaciones hasta el período $T$ es utilizar el pronóstico de un período adelantado como paso intermedio; es decir, el pronóstico para el período $T + 1$ se usa como una observación al predecir el nivel de una serie para el período $T + 2$. Esto se puede generalizar a un pronóstico de $h$ para el período futuro en el que todos los períodos intermedios entre $T$ y $T + h$ deben pronosticarse, ya que se utilizan como observaciones en el proceso. Los pronósticos iterados de múltiples períodos se resumen en el Concepto clave 16.2.

```{r, 893, eval = my_output == "html", results='asis', echo=F, purl=F}
cat('
<div class = "keyconcept" id="KC16.2">
<h3 class = "right"> Concepto clave <br> 16.2 </h3>          
<h3 class = "left"> Pronósticos de varios períodos iterados </h3>
<p>

Los pasos para un *pronóstico de RA iterado de varios períodos* son:

1. Estimar el modelo AR ($p$) usando MCO y calcular el pronóstico de un período adelantado.

2. Utilizar el pronóstico de un período adelantado para obtener el pronóstico de dos períodos adelante.

3. Continuar iterando para obtener pronósticos más lejanos en el futuro.

Un *pronóstico VAR iterado de múltiples períodos* se realiza de la siguiente manera:

1. Estimar el modelo VAR ($p$) usando MCO por ecuación y calcular el pronóstico de un período adelantado para *todas* las variables en el VAR.

2. Utilizar los pronósticos de un período por delante para obtener los pronósticos de dos períodos adelante.

3. Continuar iterando para obtener pronósticos de todas las variables en el VAR en el futuro.

</p>
</div>
')
```

```{r, 894, eval = my_output == "latex", results='asis', echo=F, purl=F}
cat('\\begin{keyconcepts}[Pronósticos de varios períodos iterados]{16.2}

Los pasos para un \\textit{pronóstico de RA iterado de varios períodos} son:\\newline

\\begin{enumerate}
\\item Estimar el modelo AR ($p$) usando MCO y calcular el pronóstico de un período adelantado.
\\item Utilizar el pronóstico de un período adelantado para obtener el pronóstico de dos períodos adelante.
\\item Continuar iterando para obtener pronósticos más lejanos en el futuro.
\\end{enumerate}\\vspace{0.5cm}

Un \\textit{pronóstico VAR iterado de múltiples períodos} se realiza de la siguiente manera:\\newline

\\begin{enumerate}
\\item Estimar el modelo VAR ($p$) usando MCO por ecuación y calcular el pronóstico de un período adelantado para \\textit{todas} las variables en el VAR.
\\item Utilizar los pronósticos de un período por delante para obtener los pronósticos de dos períodos adelante.
\\item Continuar iterando para obtener pronósticos de todas las variables en el VAR en el futuro.
\\end{enumerate}
\\end{keyconcepts}
')
```

Dado que un VAR modela todas las variables usando rezagos de las otras variables respectivas, se necesitan calcular pronósticos para *todas* las variables. Puede resultar engorroso hacerlo cuando el VAR es grande, pero afortunadamente existen funciones **R** que facilitan esto. Por ejemplo, la función **predict()** se puede utilizar para obtener pronósticos multivariados iterados para modelos VAR estimados por la función **VAR()**.

El siguiente fragmento de código muestra cómo calcular pronósticos iterados para el crecimiento del PIB y el diferencial por plazo hasta el período 2015:Q1, que es $h = 10$, utilizando el objeto modelo **VAR_est**.

```{r, 895}
# calcular pronósticos iterados para el crecimiento del PIB y el diferencial por plazo para los próximos 10 trimestres
forecasts <- predict(VAR_est)
forecasts
```

Esto revela que el pronóstico de dos trimestres de crecimiento del PIB en 2013:Q2 utilizando datos hasta 2012:Q4 es $1.69$. Para el mismo período, el pronóstico de VAR iterado para el diferencial de plazo es $1.88$.

Las matrices devueltas por `predict(VAR_est)` también incluyen intervalos de predicción de $95\%$ (sin embargo, la función no se ajusta para la autocorrelación o heterocedasticidad de los errores).

También se pueden trazar los pronósticos iterados para ambas variables llamando a **plot()** en la salida de `predict(VAR_est)`.

```{r, 896, fig.align='center', fig.height=7}
# visualizar los pronósticos iterados
plot(forecasts)
```

#### Pronósticos directos para múltiples períodos {-}

Un pronóstico directo de múltiples períodos utiliza un modelo en el que los predictores se retrasan de manera adecuada, de modo que las observaciones disponibles se puedan usar *directamente* para realizar el pronóstico. La idea de la predicción directa de varios períodos se resume en el Concepto clave 16.3.

```{r, 897, eval = my_output == "html", results='asis', echo=F, purl=F}
cat('
<div class = "keyconcept" id="KC16.3">
<h3 class = "right"> Concepto clave <br> 16.3 </h3>          
<h3 class = "left"> Pronósticos directos para múltiples períodos </h3>
<p>

Un *pronóstico directo para períodos múltiples* que pronostica períodos de $h$ en el futuro utilizando un modelo de $Y_t$ y un predictor adicional $X_t$ con retrasos de $p$ se realiza estimando primero:

\\begin{align*}
  Y_t =& \\, \\delta_0 + \\delta_1 Y_{t-h} + \\dots + \\delta_{p} Y_{t-p-h+1} + \\delta_{p+1} X_{t-h} \\\\
  +& \\dots + \\delta_{2p} Y_{t-p-h+1} + u_t,  
\\end{align*}

que luego se usa para calcular el pronóstico de $Y_{T + h}$ basado en observaciones durante el período $T$.

</p>
</div>
')
```

```{r, 898, eval = my_output == "latex", results='asis', echo=F, purl=F}
cat('\\begin{keyconcepts}[Direct Multiperiod Forecasts]{16.3}

Un \\textit{pronóstico directo para períodos múltiples} que pronostica períodos de $h$ en el futuro utilizando un modelo de $Y_t$ y un predictor adicional $X_t$ con retrasos de $p$ se realiza estimando primero:

\\begin{align*}
  Y_t =& \\, \\delta_0 + \\delta_1 Y_{t-h} + \\dots + \\delta_{p} Y_{t-p-h+1} + \\delta_{p+1} X_{t-h} \\\\
  +& \\dots + \\delta_{2p} Y_{t-p-h+1} + u_t,  
\\end{align*}

que luego se usa para calcular el pronóstico de $Y_{T + h}$ basado en observaciones durante el período $T$.

\\end{keyconcepts}
')
```

Por ejemplo, para obtener previsiones de dos trimestres de crecimiento del PIB y el margen de plazo, primero se estiman las ecuaciones:

\begin{align*}
 GDPGR_t =& \, \beta_{10} + \beta_{11} GDPGR_{t-2} + \beta_{12} GDPGR_{t-3} + \gamma_{11} TSpread_{t-2} + \gamma_{12} TSpread_{t-3} + u_{1t}, \\
 TSpread_t =& \, \beta_{20} + \beta_{21} GDPGR_{t-2} + \beta_{22} GDPGR_{t-3} + \gamma_{21} TSpread_{t-2} + \gamma_{22} TSpread_{t-3} + u_{2t}
\end{align*}

y luego se sustituyen los valores de $GDPGR_{2012:Q4}$, $GDPGR_{2012:Q3}$, $TSpread_{2012:Q4}$ y $TSpread_{2012:Q3}$ en ambas ecuaciones. Esto se hace fácilmente de forma manual.

```{r, 899}
# modelos de estimación para pronósticos directos a dos trimestres
VAR_EQ1_direct <- dynlm(GDPGrowth ~ L(GDPGrowth, 2:3) + L(TSpread, 2:3), 
                        start = c(1981, 1), end = c(2012, 4))

VAR_EQ2_direct <- dynlm(TSpread ~ L(GDPGrowth, 2:3) + L(TSpread, 2:3), 
                        start = c(1981, 1), end = c(2012, 4))

# calcular pronósticos directos a dos trimestres
coef(VAR_EQ1_direct) %*% c(1, # intercepto
                           window(GDPGrowth, start = c(2012, 3), end = c(2012, 4)), 
                           window(TSpread, start = c(2012, 3), end = c(2012, 4)))

coef(VAR_EQ2_direct) %*% c(1, # intercepto
                           window(GDPGrowth, start = c(2012, 3), end = c(2012, 4)), 
                           window(TSpread, start = c(2012, 3), end = c(2012, 4)))
```

Los economistas aplicados a menudo usan el método iterado, ya que estos pronósticos son más confiables en términos de $MSFE$, siempre que el modelo de un período adelantado se especifique correctamente. Si este no es el caso, por ejemplo, porque se cree que una ecuación en un VAR está mal especificada, puede ser beneficioso usar pronósticos directos, ya que el método iterado estará sesgado y, por lo tanto, tendrá un $MSFE$ más alto que el método directo. Consulte el Capítulo \@ref(OIPRUDFGLS) para obtener una discusión más detallada sobre las ventajas y desventajas de ambos métodos.

## Órdenes de integración y prueba de raíz unitaria DF-GLS {#OIPRUDFGLS}

Algunas series de tiempo económicas tienen tendencias más suaves que las variables que pueden describirse mediante modelos de recorridos aleatorios. Una forma de modelar estas series de tiempo es $$\Delta Y_t = \beta_0 + \Delta Y_{t-1} + u_t,$$ donde $u_t$ es un término de error no correlacionado en serie. Este modelo establece que la primera diferencia de una serie es un paseo aleatorio. En consecuencia, la serie de segundas diferencias de $Y_t$ es estacionaria. El Concepto clave 16.4 resume la notación.

```{r, 900, eval = my_output == "html", results='asis', echo=F, purl=F}
cat('
<div class = "keyconcept" id="KC16.4">
<h3 class = "right"> Concepto clave <br> 16.4 </h3>          
<h3 class = "left"> Órdenes de integración, diferenciación y estacionariedad </h3>
<p>

+ Cuando una serie de tiempo $Y_t$ tiene una raíz autorregresiva unitaria, $Y_t$ se integra de orden uno. Esto a menudo se denota por $Y_t \\sim I(1)$. Simplemente se dice que $Y_t$ es $I(1)$. Si $Y_t$ es $I(1)$, su primera diferencia $\\Delta Y_t$ es estacionaria.

+ $Y_t$ es $I(2)$ cuando $Y_t$ necesita diferenciarse dos veces para obtener una serie estacionaria. Usando la notación presentada aquí, si $Y_t$ es $I(2)$, su primera diferencia $\\Delta Y_t$ es $I(1)$ y su segunda diferencia $\\Delta^2 Y_t$ es estacionaria. $Y_t$ es $I(d)$ cuando $Y_t$ debe diferenciarse $d$ veces para obtener una serie estacionaria.

+ Cuando $Y_t$ es estacionario, se integra del orden $0$ por lo que $Y_t$ es $I(0)$.

Es bastante fácil obtener diferencias de series de tiempo en <tt>R</tt>. Por ejemplo, la función <tt>diff()</tt> devuelve diferencias adecuadamente rezagadas e iteradas de vectores numéricos, matrices y objetos de series de tiempo de la clase <tt>ts</tt>.

</p>
</div>
')
```

```{r, 901, eval = my_output == "latex", results='asis', echo=F, purl=F}
cat('\\begin{keyconcepts}[Órdenes de integración, diferenciación y estacionariedad]{16.4}
\\begin{itemize}
\\item Cuando una serie de tiempo $Y_t$ tiene una raíz autorregresiva unitaria, $Y_t$ se integra de orden uno. Esto a menudo se denota por $Y_t \\sim I(1)$. Simplemente se dice que $Y_t$ es $I(1)$. Si $Y_t$ es $I(1)$, su primera diferencia $\\Delta Y_t$ es estacionaria.
\\item $Y_t$ es $I(2)$ cuando $Y_t$ necesita diferenciarse dos veces para obtener una serie estacionaria. Usando la notación presentada aquí, si $Y_t$ es $I(2)$, su primera diferencia $\\Delta Y_t$ es $I(1)$ y su segunda diferencia $\\Delta^2 Y_t$ es estacionaria. $Y_t$ es $I(d)$ cuando $Y_t$ debe diferenciarse $d$ veces para obtener una serie estacionaria.
\\item Cuando $Y_t$ es estacionario, se integra del orden $0$ por lo que $Y_t$ es $I(0)$.
\\end{itemize}\\vspace{0.5cm}

Es bastante fácil obtener diferencias de series de tiempo en \\texttt{R}. Por ejemplo, la función \\texttt{diff()} devuelve diferencias adecuadamente rezagadas e iteradas de vectores numéricos, matrices y objetos de series de tiempo de la clase \\texttt{ts}.

\\end{keyconcepts}
')
```

Se toma el nivel de precios de los EE. UU. Medido por el *Índice de precios de gastos de consumo personal* como ejemplo.

```{r, 902, fig.align='center'}
# definir el objeto ts del índice de precios PCE de EE. UU.
PCECTPI <- ts(log(USMacroSWQ$PCECTPI), 
              start = c(1957, 1), 
              end = c(2012, 4), 
              freq = 4)

# graficar logaritmo del índice de precios de PCE
plot(log(PCECTPI),
     main = "Logaritmo del índice de precios PCE de Estados Unidos",
     ylab = "Logaritmo",
     col = "steelblue", 
     lwd = 2)
```

El logaritmo del nivel de precios tiene una tendencia que varía suavemente. Esto es típico de una serie $I(2)$. Si el nivel de precios es realmente $I(2)$, las primeras diferencias de esta serie deberían ser $I(1)$. Dado que se está considerando el logaritmo del nivel de precios, se obtienen tasas de crecimiento tomando las primeras diferencias. Por lo tanto, la serie de niveles de precios diferenciados es la serie de tasas de inflación trimestrales. Esto se hace rápidamente en **R** usando la función **Delt()** del paquete **quantmod**. Como se explica en el Capítulo \@ref(DSTCS), al multiplicar las tasas de inflación trimestrales por $400$ se obtiene la tasa de inflación trimestral, medida en puntos porcentuales a una tasa anual.

```{r, 903, fig.align='center'}
# graficar la inflación de precios del PCE de EE. UU.
plot(400 * Delt(PCECTPI),
     main = "United States PCE Price Index",
     ylab = "Porcentaje por año",
     col = "steelblue", 
     lwd = 2)

# agregar una línea discontinua en y = 0
abline(0, 0, lty = 2)
```

La tasa de inflación se comporta de manera mucho más errática que el gráfico uniforme del logaritmo del índice de precios del PCE.

#### La prueba DF-GLS para una raíz unitaria {-}

La prueba DF-GLS para una raíz unitaria ha sido desarrollada por @elliott1996 y tiene mayor potencia que la prueba ADF cuando la raíz autorregresiva es grande, pero menor que uno; es decir, el DF-GLS tiene una mayor probabilidad de rechazar el falso nulo de una tendencia estocástica cuando los datos de la muestra provienen de una serie de tiempo que está cerca de integrarse.

La idea de la prueba DF-GLS es probar una raíz unitaria autorregresiva en la serie sin tendencia, mediante la cual las estimaciones de GLS de los componentes deterministas se utilizan para obtener la versión sin tendencia de la serie original.

Una función que realiza la prueba DF-GLS se implementa en el paquete **urca** (este paquete es una dependencia del paquete **vars**, por lo que ya debería estar cargado si se adjunta **vars**). La función que calcula la estadística de prueba es **ur.ers**.

```{r, 904}
# prueba DF-GLS para raíz unitaria en el PIB
summary(ur.ers(log(window(GDP, start = c(1962, 1), end = c(2012, 4))),
        model = "trend", 
        lag.max = 2))
```

El resumen de la prueba muestra que el estadístico de la prueba es de aproximadamente $-1.2$. El valor crítico de $10\% $ para la prueba DF-GLS es $-2.57$. Sin embargo, este **no** es el valor crítico apropiado para la prueba ADF cuando se incluyen una intersección y una tendencia temporal en la regresión de Dickey-Fuller: ¡Las distribuciones asintóticas de ambas estadísticas de prueba difieren y también sus valores críticos!

La prueba es del lado izquierdo, por lo que no se puede rechazar la hipótesis nula de que la inflación de EE. UU. no es estacionaria, utilizando la prueba DF-GLS.

## Cointegración

```{r, 905, eval = my_output == "html", results='asis', echo=F, purl=F}
cat('
<div class = "keyconcept" id="KC16.5">
<h3 class = "right"> Concepto clave <br> 16.5 </h3>          
<h3 class = "left"> Cointegration </h3>
<p>

Cuando $X_t$ y $Y_t$ son $I(1)$ y si existe un $\\theta$ tal que $Y_t - \\theta X_t$ es $I(0)$, $X_t$ y $Y_t$ están cointegrados. Dicho de otra manera, la cointegración de $X_t$ y $Y_t$ significa que $X_t$ y $Y_t$ tienen la misma tendencia estocástica o una común y que esta tendencia puede eliminarse tomando una diferencia específica de la serie de modo que la serie resultante sea estacionario.

Las funciones <tt>R</tt> para el análisis de cointegración se implementan en el paquete <tt>urca</tt>.

</p>
</div>
')
```

```{r, 906, eval = my_output == "latex", results='asis', echo=F, purl=F}
cat('\\begin{keyconcepts}[Cointegration]{16.5}

Cuando $X_t$ y $Y_t$ son $I(1)$ y si existe un $\\theta$ tal que $Y_t - \\theta X_t$ es $I(0)$, $X_t$ y $Y_t$ están cointegrados. Dicho de otra manera, la cointegración de $X_t$ y $Y_t$ significa que $X_t$ y $Y_t$ tienen la misma tendencia estocástica o una común y que esta tendencia puede eliminarse tomando una diferencia específica de la serie de modo que la serie resultante sea estacionario.\\newline

Las funciones \\texttt{R}  para el análisis de cointegración se implementan en el paquete \\texttt{urca}.

\\end{keyconcepts}
')
```

Como ejemplo, se reconsidera la relación entre las tasas de interés a corto y largo plazo con el ejemplo de las letras del Tesoro de EE. UU. a 3 meses, los bonos del Tesoro de EE. UU. a 10 años y el diferencial en sus tasas de interés que se han introducido en el Capítulo \@ref(PGMPI). El siguiente fragmento de código muestra cómo crear el gráfico.

```{r, 907, fig.align='center'}
# graficar ambas series de interés
plot(merge(as.zoo(TB3MS), as.zoo(TB10YS)), 
     plot.type = "single", 
     lty = c(2, 1),
     lwd = 2,
     xlab = "Fecha",
     ylab = "Porcentaje por año",
     ylim = c(-5, 17),
     main = "Tasas de interés")

# agregar la serie de diferencial por plazo
lines(as.zoo(TSpread), 
     col = "steelblue",
     lwd = 2,
     xlab = "Fecha",
     ylab = "Porcentaje por año",
     main = "Diferencial por plazo")

# sombrear el diferencial por plazo
polygon(c(time(TB3MS), rev(time(TB3MS))), 
        c(TB10YS, rev(TB3MS)),
        col = alpha("steelblue", alpha = 0.3),
        border = NA)

# agregar línea horizontal agregar 0
abline(0, 0)

# agregar una leyenda
legend("topright", 
       legend = c("TB3MS", "TB10YS", "Diferencial por plazo"),
       col = c("black", "black", "steelblue"),
       lwd = c(2, 2, 2),
       lty = c(2, 1, 1))
```

El gráfico sugiere que las tasas de interés a largo y a corto plazo están cointegradas: Ambas series de intereses parecen tener el mismo comportamiento a largo plazo. Comparten una tendencia estocástica común. El diferencial de plazo, que se obtiene tomando la diferencia entre las tasas de interés de largo y corto plazo, parece estacionario. De hecho, la teoría de expectativas de la estructura de términos sugiere que el coeficiente de cointegración $\theta$ es 1. Esto es consistente con el resultado visual.

#### Pruebas de cointegración {-}

Siguiendo el Concepto clave 16.5, parece natural construir una prueba para la cointegración de dos series de la siguiente manera: Si dos series $X_t$ y $Y_t$ están cointegradas, la serie obtenida tomando la diferencia $Y_t - \theta X_t$ debe ser estacionario. Si las series no están cointegradas, $Y_t - \theta X_t$ no es estacionaria. Esta es una suposición que se puede probar mediante una prueba de raíz unitaria. Se tiene que distinguir entre dos casos:

1. **$\theta$ es conocido.**

    El conocimiento de $\theta$ permite calcular las diferencias $z_t = Y_t - \theta X_t$ para que las pruebas de raíz unitaria de Dickey-Fuller y DF-GLS se puedan aplicar a $z_t$. Para estas pruebas, los valores críticos son los valores críticos de la prueba ADF o DF-GLS.

2. **$\theta$ es desconocido.**

    Si se desconoce $\theta$, debe estimarse antes de que se pueda aplicar la prueba de raíz unitaria. Esto se hace estimando la regresión $$Y_t = \alpha + \theta X_t + z_t$$ usando MCO (esto se conoce como la regresión de la primera etapa). Luego, se usa una prueba de Dickey-Fuller para probar la hipótesis de que $z_t$ es una serie no estacionaria. Esto se conoce como prueba Engle-Granger Augmented Dickey-Fuller para cointegración (o *prueba EG-ADF*) después de @engle1987. Los valores críticos para esta prueba son especiales, ya que la distribución nula asociada no es normal y depende del número de variables $I(1)$ utilizadas como regresores en la regresión de la primera etapa. Cuando solo existen dos variables presuntamente cointegradas (y, por lo tanto, se usa una sola variable $I(1)$ en la regresión de MCO de la primera etapa), los valores críticos para los niveles $10\% $, $5\%$ y $1\%$ son $-3.12$, $-3.41$ y $-3.96$.
    
#### Aplicación a las tasas de interés {-}

Como se mencionó anteriormente, la teoría de la estructura temporal sugiere que las tasas de interés a largo y corto plazo están cointegradas con un coeficiente de cointegración de $\theta = 1$. En la sección anterior se ha visto que existe evidencia visual de esta conjetura, ya que el diferencial de las tasas de interés a 10 años y a 3 meses parece estacionario.

Se continua usando pruebas formales (la ADF y la prueba DF-GLS) para ver si las series de tasas de interés individuales están integradas y si su diferencia es estacionaria (por ahora, se asume que se conoce $\theta = 1$). Ambos se hacen convenientemente usando las funciones **ur.df()** para el cálculo de la prueba ADF y **ur.ers()** para realizar la prueba DF-GLS. Se usan datos desde 1962:Q1 hasta 2012:Q4 y se emplean modelos que incluyen un término de deriva. Se establece el orden de retraso máximo en $6$ y se usa $AIC$ para seleccionar la longitud de retraso óptima.

```{r, 908}
# prueba de no estacionariedad de letras del tesoro a 3 meses usando la prueba ADF
ur.df(window(TB3MS, c(1962, 1), c(2012, 4)), 
      lags = 6, 
      selectlags = "AIC", 
      type = "drift")

# prueba de no estacionariedad de bonos del tesoro a 10 años usando la prueba ADF
ur.df(window(TB10YS, c(1962, 1), c(2012, 4)), 
      lags = 6, 
      selectlags = "AIC", 
      type = "drift")

# prueba de no estacionariedad de letras del tesoro a 3 meses usando la prueba DF-GLS
ur.ers(window(TB3MS, c(1962, 1), c(2012, 4)),
       model = "constant", 
       lag.max = 6)

# prueba de no estacionariedad de bonos del tesoro a 10 años utilizando la prueba DF-GLS
ur.ers(window(TB10YS, c(1962, 1), c(2012, 4)),
       model = "constant", 
       lag.max = 6)
```

El valor crítico correspondiente de $10\%$ para ambas pruebas es $-2.57$, por lo que no se puede rechazar las hipótesis nulas de no estacionario para ninguna de las series, incluso en el nivel de significancia de $10\%$.^[Nota: **ur.df()** informa dos estadísticas de prueba cuando existe una desviación en la regresión ADF. El primero de los cuales (el que interesa aquí) es el estadístico $t$ para la prueba de que el coeficiente en el primer rezago de la serie es 0. El segundo es el estadístico $t$ para la prueba de hipótesis que el término de deriva es igual a $0$.] Se concluye que es plausible modelar ambas series de tasas de interés como $I(1)$.

A continuación, se aplica la prueba ADF y DF-GLS para probar la no estacionariedad de la serie de márgenes de plazo, lo que significa que se prueba la no cointegración de las tasas de interés a corto y largo plazo.

```{r, 909}
# probar si el diferencial de plazo es estacionario (cointegración de tasas de interés) usando ADF
ur.df(window(TB10YS, c(1962, 1), c(2012, 4)) - window(TB3MS, c(1962, 1), c(2012 ,4)), 
      lags = 6, 
      selectlags = "AIC", 
      type = "drift")

# probar si el diferencial de plazo es estacionario (cointegración de las tasas de interés) utilizando la prueba DF-GLS
ur.ers(window(TB10YS, c(1962, 1), c(2012, 4)) - window(TB3MS, c(1962, 1), c(2012, 4)),
       model = "constant", 
       lag.max = 6)
```

La Tabla \@ref(tab:spreadcoint) resume los resultados.

| Series         | Estadístic0 de prueba ADF | Estadístico de prueba DF-GLS |
|----------------|:-------------------------:|:----------------------------:|
| TB3MS          |          $-2.10$          |            $-1.80$           |
| TB10YS         |          $-1.01$          |            $-0.94$           |
| TB10YS - TB3MS |          $-3.93$          |            $-3.86$           |

Table: (\#tab:spreadcoint) Estadísticos de prueba ADF y DF-GLS para series de tipos de interés

Ambas pruebas rechazan la hipótesis de no estacionariedad de la serie de diferenciales por plazo en el nivel de significancia de $1\%$, lo que constituye una fuerte evidencia a favor de la hipótesis de que el diferencial de plazos es estacionario, lo que implica la cointegración de las tasas de interés de largo y corto plazo.

Dado que la teoría sugiere que $\theta=1$, no existe necesidad de estimar $\theta$, por lo que no es necesario utilizar la prueba EG-ADF que permite que $\theta$ sea desconocido. Sin embargo, dado que es instructivo hacerlo, se calcula este estadístico de prueba. La regresión MCO de la primera etapa es $$TB10YS_t = \beta_0 + \beta_1 TB3MS_t + z_t.$$

```{r, 910}
# estimar la regresión de la primera etapa de la prueba EG-ADF
FS_EGADF <- dynlm(window(TB10YS, c(1962, 1), c(2012, 4)) ~ window(TB3MS, c(1962, 1), c(2012, 4)))
FS_EGADF
```

Así se tiene que: 

\begin{align*}
  \widehat{TB10YS}_t = 2.46 + 0.81 \cdot TB3MS_t,
\end{align*}

donde $\widehat{\theta} = 0.81$. A continuación, se toma la serie residual $\{\widehat{z_t}\}$ y se calcula el estadístico de prueba ADF.

```{r, 911}
# calcular los residuos
z_hat <- resid(FS_EGADF)

# calcular el estadístico de prueba ADF
ur.df(z_hat, lags = 6, type = "none", selectlags = "AIC")
```

El estadístico de prueba es $-3.19$, que es menor que el valor crítico de $10\%$, pero mayor que el valor crítico de $5\%$. Por lo tanto, la hipótesis nula de no cointegración puede rechazarse en el nivel de $10\%$, pero no en el nivel de $5\%$. Esto indica una potencia más baja de la prueba EG-ADF debido a la estimación de $\theta$: Cuando $\theta = 1$ es el valor correcto, se espera la potencia de la prueba ADF para una raíz unitaria en la serie de residuos \widehat{z} = TB10YS - TB3MS$ mayor que cuando se usa una estimación de $\widehat{\theta}$.

##### Un modelo de corrección de errores vectoriales para $TB10YS_t$ y $TB3MS$ {-}

Si dos $I(1)$ series de tiempo $X_t$ y $Y_t$ están cointegradas, sus diferencias son estacionarias y se pueden modelar en un VAR que se aumenta con el regresor $Y_{t-1} - \theta X_{t-1}$. Esto se denomina *modelo de corrección de errores vectoriales* (VECM) y $Y_{t} - \theta X_{t}$ se denomina *término de corrección de errores*. Los valores rezagados del término de corrección de errores son útiles para predecir $\Delta X_t$ y/o $\Delta Y_t$.

Se puede utilizar un VECM para modelar las dos tasas de interés consideradas en las secciones anteriores. Se especifica el VECM para incluir dos rezagos de ambas series como regresores y elegir $\theta = 1$, como sugiere la teoría (ver arriba).

```{r, 912}
TB10YS <- window(TB10YS, c(1962, 1), c(2012 ,4))
TB3MS <- window(TB3MS, c(1962, 1), c(2012, 4))

# configurar el término de corrección de errores
VECM_ECT <- TB10YS - TB3MS

# estimar ambas ecuaciones del VECM usando 'dynlm()'
VECM_EQ1 <- dynlm(d(TB10YS) ~ L(d(TB3MS), 1:2) + L(d(TB10YS), 1:2) + L(VECM_ECT))
VECM_EQ2 <- dynlm(d(TB3MS) ~ L(d(TB3MS), 1:2) + L(d(TB10YS), 1:2) + L(VECM_ECT))

# cambiar el nombre de los regresores para una mejor legibilidad
names(VECM_EQ1$coefficients) <- c("Intercept", "D_TB3MS_l1", "D_TB3MS_l2",
                                  "D_TB10YS_l1", "D_TB10YS_l2", "ect_l1")
names(VECM_EQ2$coefficients) <- names(VECM_EQ1$coefficients)

# resúmenes de coeficientes utilizando errores estándar de HAC
coeftest(VECM_EQ1, vcov. = NeweyWest(VECM_EQ1, prewhite = F, adjust = T))
coeftest(VECM_EQ2, vcov. = NeweyWest(VECM_EQ2, prewhite = F, adjust = T))
```

Por tanto, las dos ecuaciones estimadas del VECM son

\begin{align*}
 \widehat{\Delta TB3MS}_t =& \, -\underset{(0.11)}{0.06} + \underset{(0.11)}{0.24} \Delta TB3MS_{t-1} -\underset{(0.15)}{0.16} \Delta TB3MS_{t-2} \\ &+ \underset{(0.13)}{0.11} \Delta TB10YS_{t-1} -\underset{(0.11)}{0.15} \Delta TB10YS_{t-2} + \underset{(0.05)}{0.03} ECT_{t-1} \\
 \widehat{\Delta TB10YS}_t =& \, \underset{(0.06)}{0.12} -\underset{(0.07)}{0.00} \Delta TB3MS_{t-1} -\underset{(0.04)}{0.07} \Delta TB3MS_{t-2} \\ &+ \underset{(0.10)}{0.23} \Delta TB10YS_{t-1} -\underset{(0.07)}{0.07} \Delta TB10YS_{t-2} -\underset{(0.03)}{0.09} ECT_{t-1}.
\end{align*}

El resultado producido por **coeftest()** muestra que existe poca evidencia de que los valores rezagados de la serie de intereses diferenciados sean útiles para la predicción. Este hallazgo es más pronunciado para la ecuación de la serie diferenciada de la tasa de la letra del tesoro a 3 meses, donde el término de corrección de errores (el diferencial de plazo rezagado) no es significativamente diferente de cero en ningún nivel común de significancia. Sin embargo, para la tasa diferenciada de los bonos del tesoro a 10 años, el término de corrección de errores es estadísticamente significativo a $1\%$ con una estimación de $-0.09$. Esto se puede interpretar de la siguiente manera: Aunque ambas tasas de interés no son estacionarias, su relación de conintegración permite predecir el *cambio* en la tasa de los bonos del tesoro a 10 años utilizando el VECM. En particular, la estimación negativa del coeficiente del término de corrección de errores indica que habrá un cambio negativo en la tasa de los bonos del tesoro a 10 años del próximo período cuando la tasa de los bonos del tesoro a 10 años sea inusualmente alta en relación con la tasa del tesoro a 3 meses en el período actual.

## Agrupación de volatilidad y heterocedasticidad condicional autorregresiva

Las series de tiempo financieras suelen presentar un comportamiento que se conoce como *agrupamiento de volatilidad*: La volatilidad cambia con el tiempo y su grado muestra una tendencia a persistir; es decir, existen periodos de baja volatilidad y periodos donde la volatilidad es alta. Los econometristas llaman a esto *heterocedasticidad condicional autorregresiva*. La heterocedasticidad condicional es una propiedad interesante porque puede explotarse para pronosticar la varianza de períodos futuros.

Como ejemplo, se consideran los cambios diarios en el índice bursátil Whilshire 5000. Los datos están disponibles para su descarga en la [Base de datos económicos de la Reserva Federal](https://fred.stlouisfed.org/series/WILL5000INDFC). Para mantener la coherencia con el libro, se descargaron los datos desde el 29 de diciembre de 1989 hasta el 31 de diciembre de 2013 (es necesario elegir este intervalo de tiempo algo mayor, ya que más adelante se trabajará con los cambios diarios de la serie).

El siguiente fragmento de código muestra cómo formatear los datos y crear un gráfico a partir de ellos.

```{r, 913}
# importar datos del índice Wilshire 5000
W5000 <- read.csv2("data/Wilshire5000.csv", 
                   stringsAsFactors = F, 
                   header = T, 
                   sep = ",", 
                   na.strings = ".")

# transformar las columnas
W5000$DATE <- as.Date(W5000$DATE)
W5000$WILL5000INDFC <- as.numeric(W5000$WILL5000INDFC)

# eliminar NAs
W5000 <- na.omit(W5000)

# calcular cambios porcentuales diarios
W5000_PC <- data.frame("Date" = W5000$DATE, 
                       "Value" = as.numeric(Delt(W5000$WILL5000INDFC) * 100))
W5000_PC <- na.omit(W5000_PC)
```

```{r pcw5000, 914, fig.align='center', fig.cap="Rendimientos porcentuales diarios en el índice Wilshire 5000"}
# graficar cambios porcentuales
plot(W5000_PC, 
     ylab = "Porcentaje", 
     main = "Cambios porcentuales diarios",
     type="l", 
     col = "steelblue", 
     lwd = 0.5)

# agregar una línea horizontal en y = 0
abline(0, 0)
```

La serie de cambios porcentuales diarios en el índice de Wilshire parece fluctuar aleatoriamente alrededor de cero, lo que significa que existe poca autocorrelación. Esto se confirma mediante un gráfico de la función de autocorrelación de la muestra.

```{r acfw5000, 915, fig.align='center', fig.cap="Autocorrelación en los cambios diarios de precios del índice W5000"}
# graficar la autocorrelación de la muestra de los cambios porcentuales diarios
acf(W5000_PC$Value, main = "Serie 5000 de Wilshire")
```

En la Figura \@ref(fig:acfw5000) se ve que las autocorrelaciones son bastante débiles, por lo que es difícil predecir los resultados futuros utilizando, por ejemplo, un modelo AR.

Sin embargo, existe evidencia visual en \@ref(fig:pcw5000) de que la serie de retornos exhibe heterocedasticidad condicional, ya que se observan agrupaciones de volatilidad. Para algunas aplicaciones, es útil medir y pronosticar estos patrones. Esto se puede hacer utilizando modelos que asumen que la volatilidad puede describirse mediante un proceso autorregresivo.

### Modelos ARCH y GARCH {-}

Considere $$Y_t = \beta_0 + \beta_1 Y_{t-1} + \gamma_1 X_{t-1} + u_t,$$ un modelo de regresión ADL($1$,$1$). El econométrico Robert @engle1982 propuso modelar $\sigma^2_t = Var(u_t | u_{t-1},u_{t-2},\ldots)$, la varianza condicional del error $u_t$ dado su pasado, por un modelo de retraso distribuido de orden $p$,

\begin{align}
 \sigma^2_t = \alpha_0 + \alpha_1 u_{t-1}^2 + \alpha_2 u_{t-2}^2 + \dots + \alpha_p u_{t-p}^2, (\#eq:archmodel)
\end{align}

llamado modelo *de heterocedasticidad condicional autorregresiva* (ARCH) de orden $p$, o ARCH corto ($p$).^[Aunque se presenta el modelo ARCH como un componente en un modelo ADL($1$,$1$), se puede usar para modelar el término de error condicional de media cero de cualquier modelo de serie temporal.] Se supone $\alpha_0>0$ y $\alpha_1,\ldots,\alpha_p\geq0$ para asegurar una varianza positiva $\sigma_t^2>0$. La idea general se desprende de la estructura del modelo: Los coeficientes positivos $\alpha_0,\alpha_1,\dots,\alpha_p$ implican que los grandes errores cuadrados recientes conducen a una gran varianza y, por lo tanto, a grandes errores cuadrados en el período actual.

El modelo ARCH generalizado (GARCH), desarrollado por Tim @bollerslev1986, es una extensión del modelo ARCH, donde se permite que $\sigma^2_t$ dependa de sus propios retrasos y de los retrasos del término de error al cuadrado. El modelo GARCH ($p$,$q$) viene dado por

\begin{align}
 \sigma^2_t = \alpha_0 + \alpha_1 u_{t-1}^2 + \alpha_2 u_{t-2}^2 + \dots + \alpha_p u_{t-p}^2 + \phi_1 \sigma^2_{t-1} + \dots + \phi_p \sigma^2_{t-q}. (\#eq:garchmodel)
\end{align}

El modelo GARCH es un modelo ADL ($p$,$q$) y, por lo tanto, puede proporcionar parametrizaciones más parsimoniosas que el modelo ARCH.

### Aplicación a la volatilidad del precio de las acciones {-}

Las estimaciones de máxima verosimilitud de los modelos ARCH y GARCH son eficientes y tienen distribuciones normales en muestras grandes, de modo que se pueden aplicar los métodos habituales para realizar inferencias sobre los parámetros desconocidos. El paquete **fGarch** en **R** consiste en una colección de funciones para analizar y modelar el comportamiento heterocedástico en modelos de series de tiempo. La función **garchFit()** es algo sofisticada, dado que permite diferentes especificaciones del procedimiento de optimización, diferentes distribuciones de errores y mucho más (use `?GarchFit` para una descripción detallada de los argumentos). En particular, los errores estándar reportados por **garchFit()** son robustos.

El modelo GARCH ($1$,$1$) de cambios diarios en el índice Wilshire 5000 que se estimó viene dado por:

\begin{align}
  R_t =& \, \beta_0 + u_t \ , \ u_t \sim \mathcal{N}(0,\sigma^2_t), \\
  \sigma^2_t =& \, \alpha_0 + \alpha_1 u_{t-1}^2 + \phi_1 \sigma_{t-1}^2 (\#eq:w5000g11)
\end{align}

donde $R_t$ es el cambio porcentual en el período $t$. $\beta_0$, $\alpha_0$, $\alpha_1$ y $\phi_1$ son coeficientes desconocidos y $u_t$ es un término de error con media condicional cero. No se incluyen predictores rezagados en la ecuación de $R_t$ porque los cambios diarios en el índice Wilshire 5000 reflejan rendimientos bursátiles diarios que son esencialmente impredecibles. Se debe tener en cuenta que se supone que $u_t$ tiene una distribución normal y la varianza $\sigma^2_t$ depende de $t$, ya que sigue la recursividad GARCH ($1$,$1$) \@ref(eq:w5000g11).

Es sencillo estimar este modelo usando **garchFit()**.

```{r, 916}
# estimar el modelo GARCH (1,1) de cambios porcentuales diarios
GARCH_Wilshire <- garchFit(data = W5000_PC$Value, trace = F)
```

Se obtiene:

\begin{align}
  \widehat{R}_t =& \, \underset{(0.010)}{0.068}, (\#eq:w5000g11est1) \\
  \widehat{\sigma}^2_t =& \, \underset{(0.002)}{0.011} + \underset{(0.007)}{0.081} u_{t-1}^2 + \underset{(0.008)}{0.909} \sigma_{t-1}^2, (\#eq:w5000g11est2)
\end{align}

por lo que los coeficientes en $u_{t-1}^2$ y $\sigma^2_{t-1}$ son estadísticamente significativos en cualquier nivel común de significancia. Se puede demostrar que la persistencia de movimientos en $\sigma^2_t$ está determinada por la suma de ambos coeficientes, que aquí es $0.99$. Esto indica que los movimientos en la varianza condicional son muy persistentes, lo que implica períodos prolongados de alta volatilidad, lo que es consistente con la evidencia visual de la agrupación de volatilidad presentada anteriormente.

La varianza condicional estimada $\widehat{\sigma}^2_t$ se puede calcular conectando los residuos de \@ref(eq:w5000g11est1) en la ecuación \@ref(eq:w5000g11est2). Esto lo realiza automáticamente **garchFit()**, por lo que para obtener las desviaciones estándar condicionales estimadas $\widehat{\sigma}_t$ solo se tienen que leer los valores de **GARCH_Wilshire** agregando $\textit{@sigma.t}$.

Usando $\widehat{\sigma}_t$ se grafican bandas de $\pm$ una desviación estándar condicional junto con las desviaciones de la serie de cambios porcentuales diarios en el índice Wilshire 5000 de su media. El siguiente fragmento de código genera el gráfico.

```{r, 917, fig.align='center'}
# calcular las desviaciones de los cambios porcentuales de su media
dev_mean_W5000_PC <- W5000_PC$Value - GARCH_Wilshire@fit$coef[1]

# graficar la desviación de los cambios porcentuales de la media
plot(W5000_PC$Date, dev_mean_W5000_PC, 
     type = "l", 
     col = "steelblue",
     ylab = "Porcentaje", 
     xlab = "Fecha",
     main = "Bandas estimadas de + - una desviación estándar condicional",
     lwd = 0.2)

# agregar una línea horizontal en y = 0
abline(0, 0)

# agregar bandas de confianza GARCH(1,1) (una desviación estándar) al gráfico
lines(W5000_PC$Date, 
      GARCH_Wilshire@fit$coef[1] + GARCH_Wilshire@sigma.t, 
      col = "darkred", 
      lwd = 0.5)

lines(W5000_PC$Date, 
      GARCH_Wilshire@fit$coef[1] - GARCH_Wilshire@sigma.t, 
      col = "darkred", 
      lwd = 0.5)
```

Las bandas de las desviaciones estándar condicionales estimadas siguen bastante bien la heterocedasticidad observada en la serie de cambios diarios del índice Wilshire 5000. Esto es útil para cuantificar la volatilidad variable en el tiempo y el riesgo resultante para los inversores que poseen acciones resumidas por el índice. Además, este modelo GARCH también se puede utilizar para producir intervalos de pronóstico cuyos anchos dependen de la volatilidad de los períodos más recientes.

### Resumen {-}

+ Se ha discutido cómo las autorregresiones vectoriales se estiman convenientemente y se usan para pronosticar en **R** mediante funciones del paquete **vars**.

+ El paquete **urca** proporciona métodos avanzados para el análisis de la raíz unitaria y cointegración como las pruebas DF-GLS y EG-ADF. En una aplicación, se ha encontrado evidencia de que las tasas de interés a 3 meses y 10 años tienen una tendencia estocástica común (es decir, están cointegradas) y, por lo tanto, pueden modelarse utilizando un modelo de corrección de errores vectoriales.

+ Además, se ha introducido el concepto de agrupamiento de volatilidad y demostrado cómo se puede emplear la función **garchFit()** del paquete **fGarch** para estimar un modelo GARCH($1$,$1$) de la heterocedasticidad condicional inherente a los rendimientos del índice bursátil Wilshire 5000.

<!--chapter:end:Capitulo_17.Rmd-->

# Ciencia de datos

```{r, child="_setup.Rmd"}
```

```{r, 918, eval=knitr::opts_knit$get("rmarkdown.pandoc.to") == "html", results='asis', echo=FALSE}
cat('<hr style="background-color:#03193b;height:2px">')
```

<!--chapter:end:Capitulo_18.Rmd-->

# Ciencia de datos

```{r, child="_setup.Rmd"}
```

```{r, eval=knitr::opts_knit$get("rmarkdown.pandoc.to") == "html", results='asis', echo=FALSE}
cat('<hr style="background-color:#03193b;height:2px">')
```

<!--chapter:end:Capitulo_19.Rmd-->

# Ciencia de datos

```{r, child="_setup.Rmd"}
```

```{r, eval=knitr::opts_knit$get("rmarkdown.pandoc.to") == "html", results='asis', echo=FALSE}
cat('<hr style="background-color:#03193b;height:2px">')
```

<!--chapter:end:Capitulo_20.Rmd-->

`r if (knitr:::is_html_output()) '# Referencias bibliográficas {-}'`

```{r include=FALSE}
# generate a BibTeX database automatically for some R packages
  knitr::write_bib(c(.packages(), 'bookdown', 'knitr', 'rmarkdown'), 
                   'packages.bib', width = 500)

# some adjustments
library(dplyr)
l <- gsub(pattern = "note = ", replacement = "version = ", x = readLines("packages.bib")) %>% gsub(pattern = "R package version ", replacement = "")

l[grep("title = ", l)] <- gsub("\\{", "\\{\\{", l[grep("title = ", l)]) %>% gsub(pattern = "\\}", replacement = "\\}\\}")

writeLines(l, con = "packages.bib")
```

<!--chapter:end:Referencias_bibliograficas.Rmd-->

